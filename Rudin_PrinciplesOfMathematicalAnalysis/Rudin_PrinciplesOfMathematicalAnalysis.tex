\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{commath}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt



\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}



\title{\textbf{Solutions to Principles Of Mathematical Analysis}}
\date{March, 2021}
\author{Meng-Gen Tsai \\ plover@gmail.com}



\begin{document}
\maketitle



\tableofcontents



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 1: The Real and Complex Number Systems \\}
\addcontentsline{toc}{section}{Chapter 1: The Real and Complex Number Systems}



Unless the contrary is explicitly stated, all numbers that are mentioned in these exercise
are understood to be real. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.1.}
\addcontentsline{toc}{subsection}{Exercise 1.1.}
\emph{If $r$ is a rational ($r \neq 0$) and $x$ is irrational,
prove that $r + x$ and $rx$ are irrational.} \\

\emph{Proof.}
Assume $r + x \in \mathbb{Q}$.
$\mathbb{Q}$ is a field, then $-r \in \mathbb{Q}$ for any $r \in \mathbb{Q}$.
So $(-r) + (r + x) = (-r + r) + x = 0 + x = x \in \mathbb{Q}$, a contradiction. \\

Similarly, assume $rx \in \mathbb{Q}$. $r \in \mathbb{Q}$ with $r \neq 0$ implies that
there exists an element $1/r \in \mathbb{Q}$ such that $r \cdot (1/r) = 1$.
So $(1/r) \cdot (rx) = ((1/r) \cdot r) \cdot x = 1 \cdot x = x \in \mathbb{Q}$, a contradiction.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.2.}
\addcontentsline{toc}{subsection}{Exercise 1.2.}
\emph{Prove that there is no rational number whose square is $12$.} \\

Apply the argument in Example 1.1.
Again we can examine this situation a little more closely.
Let $A$ be the set of all positive rational $p$ such that $p^2 < 12$ and
let $B$ be the set of all positive rational $p$ such that $p^2 > 12$.
We might show that
\emph{$A$ contains no largest number and
$B$ contains no largest number} again. \\

In fact, we can associate with each rational $p > 0$ the number
$$q = p - \frac{p^2 - 12}{p + 12} = \frac{12p + 12}{p + 12}.$$
Then
$$q^2 - 12 = \frac{132(p^2 - 12)}{(p + 12)^2}.$$
If $p \in A$ then $p^2 - 12 < 0$, $q > p$ and $q^2 < 12$. Thus $q \in A$.
If $p \in B$ then $p^2 - 12 > 0$, $0 < q < p$ and $q^2 > 12$. Thus $q \in B$. \\

\emph{Proof (Example 1.1).}
We now show that the equation
$$p^2 = 12$$
is not satisfied by any rational $p$.
If there were such a $p \in \mathbb{Q}$,
we could write $p = \frac{m}{n}$ where $m, n \in \mathbb{Z}$ are \emph{relatively prime}.
Let us assume this is done. Then $p^2 = 12$ implies
$$m^2 = 12 n^2.$$
This shows that $3 \mid m^2$. Hence $3 \mid m$ (since $3$ is a prime in $\mathbb{Z}$),
and so $m^2$ is divisible by $9$.
It follows that $12n^2$ is divisible by $9$,
so that $4n^2$ is divisible by $3$,
so that $n^2$ is divisible by $3$,
which implies that $3 \mid n$.
That is, both $m$ and $n$ have a common factor $3 > 1$,
contrary to our choice of $m$ and $n$.
Hence $p^2 = 12$ is impossible for rational $p$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.3.}
\addcontentsline{toc}{subsection}{Exercise 1.3.}
\emph{Prove Proposition 1.15.} \\

\textbf{Proposition 1.15.}
\emph{The axioms for multiplication imply the following statements.}
\begin{enumerate}
\item[(a)]
\emph{If $x \neq 0$ and $xy=xz$ then $y = z$.}
\item[(b)]
\emph{If $x \neq 0$ and $xy=x$ then $y = 1$.}
\item[(c)]
\emph{If $x \neq 0$ and $xy=1$ then $y = 1/x$.}
\item[(d)]
\emph{If $x \neq 0$ then $1(1/x) = x$.} \\
\end{enumerate}

\emph{Proof of (a).}
By the axioms for multiplication,
\begin{align*}
xy = xz, x \neq 0
&\Longrightarrow
\exists 1/x \in F, (1/x) \cdot (xy) = (1/x) \cdot (xz)
  &\text{(M5)} \\
&\Longrightarrow
((1/x)x)y = ((1/x)x)z
  &\text{(M3)} \\
&\Longrightarrow
(x(1/x))y = (x(1/x))z
  &\text{(M2)} \\
&\Longrightarrow
1y = 1z \\
&\Longrightarrow
y = z.
  &\text{(M4)}
\end{align*}
$\Box$ \\

\emph{Proof of (b).}
Let $z = 1$ in (a) and note that $x1 = 1x = x$ ((M2)(M4)).
$\Box$ \\

\emph{Proof of (c).}
Let $z = 1/x$ in (a) and note that $x(1/x) = 1$ ((M5)).
$\Box$ \\

\emph{Proof of (d).}
Since $x(1/x) = (1/x)x = 1$ ((M2)), by (c), $x = 1/(1/x)$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.4.}
\addcontentsline{toc}{subsection}{Exercise 1.4.}
\emph{Let $E$ be a nonempty subset of an ordered set;
suppose $\alpha$ is a lower bound of $E$ and
$\beta$ is an upper bound of $E$.
Prove that $\alpha \leq \beta$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Since $E \neq \varnothing$, there is $y \in E$.
\item[(2)]
By the definition of the upper bound,
$x \leq \beta$ for every $x \in E$.
In particular, $y \leq \beta$.
\item[(3)]
Similarly, $y \geq \alpha$.
\item[(4)]
By (2)(3), $\alpha \leq y \leq \beta$ for some $y \in E$.
In particular, $\alpha \leq \beta$ (Definition 1.5(ii)).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.5.}
\addcontentsline{toc}{subsection}{Exercise 1.5.}
\emph{Let $A$ be a nonempty set of real numbers which is bounded below.
Let $-A$ be the set of all numbers $-x$, where $x \in A$.
Prove that $$\inf A = -\sup(-A).$$}

\emph{Proof.}
Let $\alpha = \inf A$ and $\beta = \sup(-A)$.
\begin{enumerate}
\item[(1)]
\begin{align*}
x \geq \alpha \:\: \forall x \in A
&\Longrightarrow
-x \leq -\alpha \:\: \forall -x \in -A \\
&\Longrightarrow
-\alpha \text{ is an upper bound of $-A$} \\
&\Longrightarrow
\beta \leq -\alpha \\
&\Longrightarrow
\alpha \leq -\beta
\end{align*}
\item[(2)]
\begin{align*}
-x \leq \beta \:\: \forall -x \in -A
&\Longrightarrow
x \geq -\beta \:\: \forall x \in A \\
&\Longrightarrow
-\beta \text{ is a lower bound of $A$} \\
&\Longrightarrow
\alpha \geq -\beta
\end{align*}
\end{enumerate}
By (1)(2), $\alpha = -\beta$, or $\inf A = -\sup(-A)$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.6.}
\addcontentsline{toc}{subsection}{Exercise 1.6.}
\emph{Fix $b > 1$.}
\begin{enumerate}
\item[(a)]
\emph{If $m,n,p,q$ are integers, $n>0$, $q>0$, and $r=m/n=p/q$, prove that
$$(b^m)^{1/n} = (b^p)^{1/q}.$$
Hence it makes sense to define $b^r = (b^m)^{1/n}$.}
\item[(b)]
\emph{Prove that $b^{r+s} = b^r b^s$ if $r$ and $s$ are rational.}
\item[(c)]
\emph{If $x$ is real, define $B(x)$ to be the set of all numbers $b^t$,
where $t$ is rational and $t \leq x$.
Prove that
$$b^r = \sup B(r)$$
where $r$ is rational.
Hence it makes sense to define
$$b^x = \sup B(x)$$
for every real $x$.}
\item[(d)]
\emph{Prove that $b^{x+y} = b^x b^y$ for all real $x$ and $y$.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
Define $k = mq = np \in \mathbb{Z}$ (since $r = m/n = p/q$).
Notice that $nq > 0$ (since $n>0$ and $q>0$).
So there is one and only one $y \in \mathbb{R}$ such that $$y^{nq} = b^k$$
where $b^k$ is defined in $\mathbb{R}$ (Theorem 1.21).
\item[(2)]
\emph{Show that $y = (b^m)^{1/n}$ and $y = (b^p)^{1/q}$
are solutions of $y^{nq} = b^k$.}
In fact,
\begin{align*}
((b^m)^{1/n})^{nq} &= (b^m)^q = b^{mq} = b^k, \\
((b^p)^{1/q})^{nq} &= (b^p)^n = b^{pn} = b^k.
\end{align*}
\item[(3)]
By (1)(2), the uniqueness of $y$ shows that $(b^m)^{1/n} = (b^p)^{1/q}$,
or the map $r \mapsto b^r$ is well-defined for $r \in \mathbb{Q}$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (b).}
Write $r=m/n$ and $s=p/q$ where $m,n,p,q$ are integers with $n>0$, $q>0$.
\begin{align*}
b^{r+s}
&= b^{\frac{mq+np}{nq}} \\
&= (b^{mq} \cdot b^{np})^{\frac{1}{nq}}
  &\text{($mq+np \in \mathbb{Z}$)} \\
&= (b^{mq})^{\frac{1}{nq}} \cdot (b^{np})^{\frac{1}{nq}}
  &\text{(Corollary to Theorem 1.21)} \\
&= b^{\frac{mq}{nq}} \cdot b^{\frac{np}{nq}} \\
&= b^{\frac{m}{n}} \cdot b^{\frac{n}{n}}
  &\text{((a))} \\
&= b^r \cdot b^s.
\end{align*}
$\Box$ \\

\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
Given any $r \in \mathbb{Q}^+$, $b^r > 1$ since $b > 1$ is given.
\item[(2)]
Given any $r, s \in \mathbb{Q}$, $b^r > b^s$ whenever $r > s$.
In fact,
\begin{align*}
b^r
&= b^{r-s} b^s
  &\text{((b))} \\
&> 1 \cdot b^s
  &\text{((1))} \\
&= b^s.
\end{align*}
\item[(3)]
Given any $r \in \mathbb{Q}$, $b^t \leq b^r$ for any $t \in \mathbb{Q}$
whenever $t \leq r$.
So $\sup B(r) \leq b^r$.
Conversely, since $r \in B(r)$, $b^r \leq \sup B(r)$.
So $b^r = \sup B(r)$.
\item[(4)]
Given any $x \in \mathbb{R}$.
We can always find $r, s \in \mathbb{Q}$ such that $r < x < s$.
Therefore, $r \in B(x)$ and $B(s)$ is an upper bound of $B(x)$.
So there is a least upper bound $\sup B(x)$ for $B(x)$, i.e.,
$b^r = \sup B(r)$ is well-defined.
\end{enumerate}
$\Box$ \\

\textbf{Lemma.}
\emph{If $x$ is real, define $B'(x)$ to be the set of all numbers $b^t$,
where $t$ is rational and $t < x$.
Prove that $\sup B'(x) = \sup B(x)$ for all $x \in \mathbb{R}$.} \\

\emph{Proof of Lemma (Reductio ad absurdum).}
It suffices to show that $\sup B'(r) = \sup B(r) = b^r$ for all $r \in \mathbb{Q}$.
(The case $x \in \mathbb{R}-\mathbb{Q}$ is nothing to do.)
Clearly, $\sup B'(r) \leq b^r$.
If $\alpha = \sup B'(r) < b^r$, then for $\frac{b^r}{\alpha} > 1$ there is an integer
\[
  n > \frac{b-1}{\frac{b^r}{\alpha} - 1}
\]
such that
$$b^{\frac{1}{n}} < \frac{b^r}{\alpha}$$
(Exercise 1.7(c)).
So
$\alpha < b^{r - \frac{1}{n}}$.
Therefore, $b^{r - \frac{1}{n}} \in B'(r)$ since $r - \frac{1}{n} \in \mathbb{Q}$,
or we find an element in $B'(r)$ such that is greater than $\alpha$,
contrary to the maximality of $\alpha$.
$\Box$ \\

\emph{Proof of (d).}
Apply Lemma to use $B(x)$ or $B'(x)$ interchangeably.
\begin{enumerate}
\item[(1)]
\emph{Show that $$\sup B'(x+y) \leq \sup B'(x)\sup B'(y).$$ } \\
Given any $b^t \in B'(x+y)$ such that $t < x+y$.
There are rational numbers $r, s$ such that $r < x$, $s < y$ and $t=r+s$.
(Rewrite $t < x+y$ as $t-y < x$. So there is a rational number $r$ such that $t-y < r < x$.
Let $s = t-r < y$.)
(Here we use $B'(x+y)$ instead of $B(x+y)$ to ensure the existence of $r$ and $s$.
That is, if $0 = -\sqrt{2} + \sqrt{2}$, we cannot find rational numbers
$r \leq -\sqrt{2}$ and $s \leq \sqrt{2}$ such that $r + s = 0$.)
Therefore,
$$b^t = b^{r+s} = b^r b^s \leq \sup B'(x) \sup B'(y)$$
(by (b)). Take supremum, $\sup B'(x+y) \leq \sup B'(x) \sup B'(y)$.
\item[(2)]
\emph{Show that $$\sup B'(x+y) \geq \sup B'(x)\sup B'(y).$$ } \\
Given any $b^r \in B'(x)$, $b^s \in B'(y)$. $r < x$ and $s < y$.
So $b^r b^s = b^{r+s} \in B'(x+y)$ (by (b)).
So $b^r b^s \leq \sup B'(x+y)$.
So
$$b^r \leq \frac{\sup B'(x+y)}{b^s}$$
since $b^s > 0$ for any $s \in \mathbb{Q}$.
Here $\frac{\sup B'(x+y)}{b^s}$ is an upper bound for $B'(x)$.
So
$$\sup B'(x) \leq \frac{\sup B'(x+y)}{b^s},$$
or $b^s \leq \frac{\sup B'(x+y)}{B'(x)}$.
Use the same argument again,
$$\sup B'(y) \leq \frac{\sup B'(x+y)}{\sup B'(x)}$$
or $\sup B'(x) \sup B'(y) \leq \sup B'(x+y)$.
\end{enumerate}
By (1)(2), $\sup B'(x) \sup B'(y) = \sup B'(x+y)$ or $b^x b^y = b^{x+y}$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.7.}
\addcontentsline{toc}{subsection}{Exercise 1.7.}
\emph{Fix $b > 1$, $y > 0$, and prove that there is a unique real $x$
such that $b^x = y$, by completing the following outline.
(This $x$ is called the logarithm of $y$ to the base $b$).}
\begin{enumerate}
\item[(a)]
\emph{For any positive integer $n$, $b^n - 1 \geq n(b-1)$.}
\item[(b)]
\emph{Hence $b - 1 \geq n (b^{\frac{1}{n}} - 1)$.}
\item[(c)]
\emph{If $t > 1$ and $n > \frac{b-1}{t-1}$, then $b^{\frac{1}{n}} < t$.}
\item[(d)]
\emph{If $w$ is such that $b^w < y$, then $b^{w + \frac{1}{n}} < y$ for sufficiently large $n$;
to see this, apply part (c) with $t = y \cdot b^{-w}$.}
\item[(e)]
\emph{If $b^w > y$, then $b^{w - \frac{1}{n}} > y$ for sufficiently large $n$.}
\item[(f)]
\emph{Let $A$ be the set of all $w$ such that $b^w < y$, and show that $x = \sup A$
satisfies $b^x = y$.}
\item[(g)]
\emph{Prove that this $x$ is unique.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{align*}
b^n - 1
&= (b - 1)(b^{n-1} + b^{n-2} + \cdots + 1) \\
&\geq (b - 1)(1^{n-1} + 1^{n-2} + \cdots + 1) \\
&= (b - 1)n.
\end{align*}
The equality holds if and only if $n = 1$.
(Or proved by the induction.)
$\Box$ \\

\emph{Proof of (b).}
Put $b \mapsto b^{\frac{1}{n}}$ in (a).
$\Box$ \\

\emph{Proof of (c).}
Since $n > \frac{b-1}{t-1}$ and (b), $n(t-1) > b-1 \geq n (b^{\frac{1}{n}} - 1)$.
Cancel $n$ on the both sides, $t-1 > b^{\frac{1}{n}} - 1$ or $b^{\frac{1}{n}} < t$.
$\Box$ \\

\emph{Proof of (d).}
Let $t = y \cdot b^{-w} > 1$.
By (c), $b^{\frac{1}{n}} < y \cdot b^{-w}$ for $n > \frac{b-1}{y \cdot b^{-w}-1}$,
or $b^{w + \frac{1}{n}} < y$ for $n > \frac{b-1}{y \cdot b^{-w}-1}$.
$\Box$ \\

\emph{Proof of (e).}
Similar to (d).
Let $t = y^{-1} \cdot b^{w} > 1$.
By (c), $b^{\frac{1}{n}} < y^{-1} \cdot b^{w}$ for $n > \frac{b-1}{y^{-1}\cdot b^{w}-1}$,
or $b^{w + \frac{1}{n}} > y$ for $n > \frac{b-1}{y^{-1}\cdot b^{w}-1}$.
$\Box$ \\

\emph{Proof of (f).}
$x = \sup A < \infty$ by (a).
(As $n > \frac{y-1}{b-1}$, $b^n > y$.)
So there are only three possible cases.
\begin{enumerate}
\item[(1)]
$b^x < y$. By (d), $b^{x + \frac{1}{n}} < y$ for sufficiently large $n$,
contrary to the maximality of $x$.
\item[(2)]
$b^x > y$. By (e), $b^{x - \frac{1}{n}} > y$ for sufficiently large $n$,
contrary to the maximality of $x$.
\item[(3)]
By (1)(2), $b^x = y$ holds.
\end{enumerate}
$\Box$ \\

\emph{Proof of (g)(Reductio ad absurdum).}
If there were another real $x' \neq x$ such that $b^{x'} = y$,
then $x' > x$ or $x' < x$.
For the case $x' > x$, $y = b^{x'} = b^{x}b^{x'-x} > b^{x} = y$,
which is absurd.
For the case $x' < x$, $y = b^{x} = b^{x'}b^{x-x'} > b^{x'} = y$,
which is absurd too.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.8.}
\addcontentsline{toc}{subsection}{Exercise 1.8.}
\emph{Prove that no order can be defined in the complex field that turns it
into an ordered field.
(Hint: $-1$ is a square.)} \\

\emph{Proof (Reductio ad absurdum).}
If $\mathbb{C}$ were an ordered field, consider the complex number $i = \sqrt{-1}$.

\begin{enumerate}
\item[(1)]
$i \neq 0$.
If $i$ were $0$, then $i \cdot i = 0 \cdot i$ or $-1 = 0$,
or $1 = 0$, contrary to $1 > 0$ (Proposition 1.18).
\item[(2)]
Since $i \neq 0$, we have $i^2 > 0$ (Proposition 1.18).
So $-1 > 0$, or $1 < 0$, contrary to the fact $1 > 0$ (Proposition 1.18).
\end{enumerate}
$\Box$ \\

\textbf{Supplement ($x^2 > 0$ if $x \neq 0$).}
\emph{Show that the only automorphism of $\mathbb{R}$ is the identity.
(Hint: If $\sigma$ is an automorphism, show that $\sigma|_{\mathbb{Q}} = \text{id}$,
and if $a > 0$, then $\sigma(a) > 0$).} \\

It is an interesting fact that there are infinitely many automorphisms of $\mathbb{C}$,
even thought $[\mathbb{C}:\mathbb{R}] = 2$.
Why is this fact not a contradiction to this problem? \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.9.}
\addcontentsline{toc}{subsection}{Exercise 1.9.}
\emph{Suppose $z = a+bi$, $w = c+di$.
Define $z < w$ if $a < c$, and also if $a = c$ but $b < d$.
Prove that this turns the set of all complex numbers into an ordered set.
(This type of order relation is called a dictionary order,
or lexicographic order, for obvious reasons.)
Does this ordered set have the least-upper-bound property?} \\

\emph{Proof.}

\begin{enumerate}
\item[(1)]
\emph{Show that $\mathbb{C}$ is an ordered set.}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that if $x = a+bi, y = c+di \in \mathbb{C}$
  then one and only one of the statements
  $x < y, x = y, y < x$ is true.}
  Since $\mathbb{R}$ is an ordered set,
  then one and only one of the statements
  $a < c, a = c, c < a$ is true.
    \begin{enumerate}
    \item[(i)]
    $a < c$. Hence $x < y$ (in the sense of the dictionary order).
    \item[(ii)]
    $a = c$. Again since $\mathbb{R}$ is an ordered set,
    then one and only one of the statements
    $b < d, b = d, d < b$ is true.
    That is, one and only one of the statements
    $x < y, x = y, y < x$ is true (in the sense of the dictionary order).
    \item[(iii)]
    $c < a$. Hence $y < x$ (in the sense of the dictionary order).
    \end{enumerate}
    By (i)(ii)(iii), the result is established.
  \item[(b)]
  \emph{Show that if $x=a+bi, y=c+di, z=e+fi \in \mathbb{C}$,
  if $x < y$ and $y < z$, then $x < z$.}
  Observe that if $x < y$ (resp. $y < z$) then $a \leq c$ (resp. $c \leq e$).
  Therefore, $a \leq c \leq e$. Thus, there are only two possible cases.
    \begin{enumerate}
    \item[(i)]
    Not every equality holds. $a < e$ or $x < z$ (in the sense of the dictionary order).
    \item[(ii)]
    Every equality holds. $a = c = e$.
    Since $x < y$ (resp. $y < z$), $b < d$ (resp. $d < f$).
    So $b < d < f$, or $x < z$ (in the sense of the dictionary order).
    \end{enumerate}
    In any case, $x < z$ if $x < y$ and $y < z$.
  \end{enumerate}
  By (a)(b), $\mathbb{C}$ is an ordered set (Definition 1.5).
\item[(2)]
\emph{Show that has no least-upper-bound property.}
Assume $\mathbb{C}$ has the least-upper-bound property.
Consider $$E = \{ 0 \} \subseteq \mathbb{C}.$$
  \begin{enumerate}
  \item[(a)]
  $E$ is bounded by $0 \in \mathbb{C}$.
  Thus $E$ has the least upper bound $\alpha = a+bi \in \mathbb{C}$
  where $a, b \in \mathbb{R}$. Here $a \geq 0$. (In fact $a = 0$.)
  \item[(b)]
  Set $\gamma = a+(b-1)i < a+bi = \alpha$. Note that $a \geq 0$ and thus
  $\gamma$ is an upper bound of $E$, contrary to minimality of $\alpha$.
  \end{enumerate}
Thus $\mathbb{C}$ has no least-upper-bound property
although $E$ has the least upper bound ($=0$) in $\mathbb{R}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.10.}
\addcontentsline{toc}{subsection}{Exercise 1.10.}
\emph{Suppose $z=a+bi, w=u+vi$, and
$$a = \left( \frac{|w|+u}{2} \right)^{\frac{1}{2}},
b = \left( \frac{|w|-u}{2} \right)^{\frac{1}{2}}.$$
Prove that $z^2 = w$ if $v \geq 0$ and that $(\overline{z})^2=w$ if $v \leq 0$.
Conclude that every complex number (with one exception!)
has two complex square roots.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\begin{align*}
z^2
&= (a^2 - b^2) + 2abi \\
&= \left( \frac{|w|+u}{2} - \frac{|w|-u}{2} \right)
  + 2 \left( \frac{|w|+u}{2} \cdot \frac{|w|-u}{2} \right)^{\frac{1}{2}} i \\
&= u + ( |w|^2 - u^2 )^{\frac{1}{2}} i \\
&= u + ( v^2 )^{\frac{1}{2}} i \\
&= u + |v| i.
\end{align*}
Therefore, $z^2 = w$ if $v \geq 0$.
$z^2 = \overline{w}$ if $v \leq 0$, or $(\overline{z})^2 = w$ if $v \leq 0$.
\item[(2)]
Every complex number $w$
has two has two complex square roots $z$ and $-z$.
  \begin{enumerate}
  \item[(a)]
  When $w \neq 0$, two square roots are distinct.
  \item[(b)]
  When $w = 0$, two square roots are identical,
  or there is only one square root for $w = 0$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.11.}
\addcontentsline{toc}{subsection}{Exercise 1.11.}
\emph{If $z$ is a complex number, prove that there exists an $r \geq 0$
and a complex number $w$ with $|w| = 1$ such that $z = rw$.
Are $w$ and $r$ always uniquely determined by $z$?} \\

To decide $r$ and $w$ in the relation $z = rw$, it is natural to take
absolute values on the both sides. That is, $|z| = r|w| = r$. \\

\emph{Proof.}
Let $r = |z| \geq 0$.
\begin{enumerate}
\item[(1)]
$r \neq 0$.
Define $w = \frac{z}{r} \in \mathbb{C}$. $|w| = \frac{|z|}{r} = 1$.
In this case $w$ and $r$ are uniquely determined.
\item[(2)]
$r = 0$ (or $z = 0$).
Define $w = e^{ix} = \cos x + i \sin x$ for any $x \in \mathbb{R}$.
$|w| = 1$.
Here $r$ is uniquely determined
but $w$ is not uniquely determined.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.12.}
\addcontentsline{toc}{subsection}{Exercise 1.12.}
\emph{If $z_1, \ldots, z_n$ are complex, prove that}
\[
  |z_1 + z_2 + \cdots + z_n| \leq |z_1| + |z_2| + \cdots + |z_n|.
\]

\emph{Proof.}
Use mathematical induction on $n$. $n = 2$ is established by Theorem 1.33 (e).
Suppose the inequality holds on $n = k$, then $n = k + 1$ we again apply Theorem 1.33 (e)
to get the result, say
\begin{align*}
|z_1 + z_2 + \cdots + z_k + z_{k+1}|
&\leq |z_1 + z_2 + \cdots + z_k| + |z_{k+1}| \\
&\leq |z_1| + |z_2| + \cdots + |z_k| + |z_{k+1}|
\end{align*}
$\Box$ \\

\textbf{Supplement.}
\emph{If $\mathbf{x}_1, \ldots, \mathbf{x_n} \in \mathbb{R}^k$, then
$$|\mathbf{x_1} + \mathbf{x_2} + \cdots + \mathbf{x_n}|
\leq |\mathbf{x_1}| + |\mathbf{x_2}| + \cdots + |\mathbf{x_n}|.$$}

Here we might use Theorem 1.37 (e) to prove it.
Since the norm $|\cdot|$ on $\mathbb{C}$ is the same as the norm on $\mathbb{R}^2$,
we might prove this supplement first
and then set $k = 2$ on $\mathbb{R}^k = \mathbb{R}^2$
to give another proof of Exercise 1.12. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.13.}
\addcontentsline{toc}{subsection}{Exercise 1.13.}
\emph{If $x, y$ are complex, prove that
$$\abs{ \abs{x} - \abs{y} } \leq \abs{x-y}.$$}

We can show $f(x) = |x|$ is uniformly continuous in $\mathbb{R}$ by using this inequality. \\

\emph{Proof (Exercise 1.12).}
Since
\begin{align*}
|y| &\leq |x| + |y-x| = |x| + |x-y| \\
|x| &\leq |y| + |x-y|,
\end{align*}
we have
$$-|x-y| \leq |x| - |y| \leq |x-y|,$$
or
$$\abs{|x| - |y|} \leq |x-y|.$$
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.14.}
\addcontentsline{toc}{subsection}{Exercise 1.14.}
\emph{If $z$ is a complex number such that $|z|=1$, that is, such that $z\overline{z}=1$,
compute
$$|1+z|^2+|1-z|^2.$$}

\emph{Proof ($|z|^2 = z\overline{z}$).}
\begin{align*}
|1+z|^2 &= (1+z)\overline{(1+z)} = (1+z)(1+\overline{z}) = 1+z+\overline{z}+z\overline{z} \\
|1-z|^2 &= (1-z)\overline{(1-z)} = (1+z)(1-\overline{z}) = 1-z-\overline{z}+z\overline{z} \\
|1+z|^2+|1-z|^2 &= 2+2z\overline{z} = 2+2 = 4.
\end{align*}
$\Box$ \\

\emph{Proof (Exercise 1.17).}
Regard $\mathbb{C}$ as $\mathbb{R}^2$.
Then put $\mathbf{x} = 1, \mathbf{y} = z$ in the parallelogram law (Exercise 1.17)
to get $$|1+z|^2+|1-z|^2 = 2|1|^2 + 2|z|^2 = 4.$$
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.15.}
\addcontentsline{toc}{subsection}{Exercise 1.15.}
\emph{Under what conditions does equality hold in the Schwarz inequality?} \\

\textbf{Theorem 1.35 (Schwarz inequality).}
\emph{If $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ are complex numbers, then
$$\abs{\sum_{j=1}^n a_j \overline{b_j}}^2
\leq \sum_{j=1}^n |a_j|^2 \sum_{j=1}^n|b_j|^2.$$}

In fact, the Lagrange's identity for complex numbers shows
$$\abs{ \sum_{k=1}^{n} a_k \overline{b_k} }^2
= \sum_{k=1}^{n} \abs{a_k}^2 \sum_{k=1}^{n} \abs{b_k}^2
- \sum_{1 \leq k < j \leq n}
\abs{ a_k b_j - a_j b_k }^2.$$ \\

In general, the Binet-Cauchy identity shows
\begin{align*}
&\sum_{1 \leq k < j \leq n}
(a_k b_j - a_j b_k)(A_k B_j - A_j B_k) \\
= &\left( \sum_{k=1}^{n} a_k A_k \right)\left( \sum_{k=1}^{n} b_k B_k \right)
- \left( \sum_{k=1}^{n} a_k B_k \right)\left( \sum_{k=1}^{n} b_k A_k \right).
\end{align*} \\

\emph{Proof of Binet-Cauchy identity.}
\begin{align*}
&\sum_{1 \leq k < j \leq n}
(a_k b_j - a_j b_k)(A_k B_j - A_j B_k) \\
= &\sum_{1 \leq k < j \leq n}
(a_k b_j A_k B_j + a_j b_k A_j B_k)
- \sum_{1 \leq k < j \leq n}
(a_k b_j A_j B_k - a_j b_k A_k B_j) \\
= &\sum_{1 \leq k < j \leq n}
(a_k A_k b_j B_j + a_j A_j b_k B_k)
- \sum_{1 \leq k < j \leq n}
(a_k B_k b_j A_j + a_j B_j b_k A_k) \\
= &\sum_{1 \leq k \neq j \leq n} a_k A_k b_j B_j
 - \sum_{1 \leq k \neq j \leq n} a_k B_k b_j A_j \\
= &\sum_{1 \leq k, j \leq n} a_k A_k b_j B_j
 - \sum_{1 \leq k, j \leq n} a_k B_k b_j A_j \\
  & \text{(since $a_k A_k b_j B_j - a_k B_k b_j A_j = 0$ as $k = j$)} \\
= &\left( \sum_{k=1}^{n} a_k A_k \right)\left( \sum_{j=1}^{n} b_j B_j \right)
- \left( \sum_{k=1}^{n} a_k B_k \right)\left( \sum_{j=1}^{n} b_j A_j \right) \\
= &\left( \sum_{k=1}^{n} a_k A_k \right)\left( \sum_{k=1}^{n} b_k B_k \right)
- \left( \sum_{k=1}^{n} a_k B_k \right)\left( \sum_{k=1}^{n} b_k A_k \right).
\end{align*}
$\Box$ \\

\emph{Proof of Lagrange's identity.}
Put $(a_k, b_k, A_k, B_k) \mapsto (a_k, b_k, \overline{a_k}, \overline{b_k})$
in the Binet-Cauchy identity.
$\Box$ \\

\emph{Proof of Schwarz inequality (Lagrange's identity).}
Notice the term $$\sum_{1 \leq k < j \leq n} \abs{ a_k b_j - a_j b_k }^2 \geq 0.$$
$\Box$ \\

Write $\mathbf{a} = (a_1, \ldots, a_n)$ and $\mathbf{b} = (b_1, \ldots, b_n)$
as two vectors in the vector space $\mathbb{C}^n$ over $\mathbb{C}$.
Back to the exercise now. \\

\emph{Proof (Lagrange's identity).}
$\sum_{1 \leq k < j \leq n} \abs{ a_k b_j - a_j b_k }^2 = 0$
$\Longleftrightarrow$
$a_k b_j = a_j b_k$ for any $1 \leq k < j \leq n$.
The equality holds in the Schwarz inequality
$\Longleftrightarrow$
$\mathbf{a}$ and $\mathbf{b}$ are linearly dependent.
$\Box$ \\

\emph{Proof (Theorem 1.35).}
The equality holds in the Schwarz inequality.
$\Longleftrightarrow$
$B = 0$ or
the term $\sum |B a_j - C b_j|^2$ in the proof of Theorem 1.35 is $0$.
$\Longleftrightarrow$
$\mathbf{b} = \mathbf{0}$ or $\mathbf{a} = c\mathbf{b}$ for some $c \in \mathbb{C}$.
$\Longleftrightarrow$
$\mathbf{a}$ and $\mathbf{b}$ are linearly dependent.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.16.}
\addcontentsline{toc}{subsection}{Exercise 1.16.}
\emph{Suppose $k \geq 3$, $\mathbf{x}, \mathbf{y} \in \mathbb{R}^k$,
$\abs{\mathbf{x} - \mathbf{y}} = d > 0$, and $r > 0$. Prove:}
\begin{enumerate}
\item[(a)]
\emph{If $2r > d$, there are infinitely many $\mathbf{z} \in \mathbb{R}^k$
such that
$$\abs{\mathbf{z} - \mathbf{x}} = \abs{\mathbf{z} - \mathbf{y}} = r.$$}
\item[(b)]
\emph{If $2r = d$, there is exactly one such $\mathbf{z}$.}
\item[(c)]
\emph{If $2r < d$, there is no such $\mathbf{z}$.}
\end{enumerate}
\emph{How must these statements be modified if $k$ is $2$ or $1$?} \\

\emph{Proof (Brute-force).}
By Exercise 1.17, we have
\begin{align*}
\abs{\mathbf{z} - \mathbf{x}}^2 + \abs{\mathbf{z} - \mathbf{y}}^2
&= 2\abs{\mathbf{z} - \frac{\mathbf{x} + \mathbf{y}}{2}}^2
+ 2\abs{\frac{\mathbf{x} - \mathbf{y}}{2}}^2, \\
r^2 + r^2
&= 2\abs{\mathbf{z} + \frac{\mathbf{x} - \mathbf{y}}{2}}^2 + \frac{1}{2} d^2, \\
\abs{\mathbf{z} - \frac{\mathbf{x} + \mathbf{y}}{2}}^2
&= r^2 - \frac{d^2}{4}
\end{align*}
for every $k = 1, 2, 3, \ldots$
Let $\mathbf{w} = \mathbf{z} - \frac{\mathbf{x} + \mathbf{y}}{2}$.
So $\abs{\mathbf{w}}^2 = r^2 - \frac{d^2}{4}$.
\begin{enumerate}
\item[(a)]
Suppose $2r > d$.
\begin{enumerate}
\item[(i)]
\emph{Show that $\mathbf{w} \cdot (\mathbf{x}-\mathbf{y}) = 0.$}
\begin{align*}
\abs{\mathbf{z} - \mathbf{x}} = \abs{\mathbf{z} - \mathbf{y}}
&\Longleftrightarrow
\abs{\mathbf{z} - \mathbf{x}}^2 = \abs{\mathbf{z} - \mathbf{y}}^2 \\
&\Longleftrightarrow
\abs{\mathbf{z}}^2 - 2\mathbf{z} \cdot \mathbf{x} + \abs{\mathbf{x}}^2
= \abs{\mathbf{z}}^2 - 2\mathbf{z}\cdot\mathbf{y} + \abs{\mathbf{y}}^2 \\
&\Longleftrightarrow
2\mathbf{z}\cdot (\mathbf{x}-\mathbf{y})
= \abs{\mathbf{x}}^2 - \abs{\mathbf{y}}^2
= (\mathbf{x}+\mathbf{y}) \cdot (\mathbf{x}-\mathbf{y}) \\
&\Longleftrightarrow
\left( \mathbf{z} - \frac{\mathbf{x} + \mathbf{y}}{2} \right)
\cdot (\mathbf{x}-\mathbf{y}) = 0 \\
&\Longleftrightarrow
\mathbf{w} \cdot (\mathbf{x}-\mathbf{y}) = 0.
\end{align*}
\item[(ii)]
Since $\mathbf{x} \neq \mathbf{y}$, we may suppose that $x_1 \neq y_1$.
So the solution of $\mathbf{w} \cdot (\mathbf{x}-\mathbf{y}) = 0$ is
\begin{equation*}
  \begin{cases}
    w_1 = -\frac{1}{x_1-y_1}(t_2(x_2-y_2) + \cdots + t_{k}(x_{k}-y_{k})) \\
    w_2 = t_2 \\
    \cdots \\
    w_k = t_k
  \end{cases}
\end{equation*}
where $\mathbf{w} = (w_1, \ldots, w_k)$ and
$t_2, \ldots, t_k \in \mathbb{R}$.
\item[(iii)]
Also
\begin{align*}
&\abs{\mathbf{w}}^2 = r^2 - \frac{d^2}{4} \\
\Longleftrightarrow&
w_1^2 + \cdots + w_k^2 = r^2 - \frac{d^2}{4} \\
\Longleftrightarrow&
\frac{(t_2(x_2-y_2) + \cdots + t_{k}(x_{k}-y_{k}))^2}{(x_1-y_1)^2}
+ \cdots + t_k^2 = r^2 - \frac{d^2}{4}
\end{align*}
That is, $t_2$ is uniquely determined by $t_3, \ldots, t_k \in \mathbb{R}$.
Clearly, such $\mathbf{z} = \mathbf{w} + \frac{\mathbf{x} + \mathbf{y}}{2}$
satisfies $\abs{\mathbf{z} - \mathbf{x}} = \abs{\mathbf{z} - \mathbf{y}} = r$.
\item[(iv)]
As $k \geq 3$, there are infinitely many
$\mathbf{z} = \mathbf{w} + \frac{\mathbf{x} + \mathbf{y}}{2} \in \mathbb{R}^k$.
\item[(v)]
As $k = 2$,
$$\frac{t_2^2 (x_2-y_2)^2}{(x_1-y_1)^2} + t_2^2 = r^2 - \frac{d^2}{4}
\Longleftrightarrow
t_2^2 = \frac{r^2 - \frac{d^2}{4}}{1 + \frac{(x_2-y_2)^2}{(x_1-y_1)^2}} > 0,$$
that is, $t_2$ has exactly two solutions,
or $\mathbf{z}$ has two solutions in $\mathbb{R}^2$.
\item[(vi)]
As $k = 1$, there is no such $t_2$. So $\mathbf{w} = \mathbf{0}$,
contrary to the assumption $\abs{\mathbf{w}} > 0$.
In this case there are no solution $\mathbf{z}$ in $\mathbb{R}^2$.
\end{enumerate}
\item[(b)]
If $2r = d$, $\abs{\mathbf{w}}^2 = 0$.
$\mathbf{w} = 0$ or $\mathbf{z} = \frac{\mathbf{x}+\mathbf{y}}{2}$.
Such $\mathbf{z}$ satisfies
$\abs{\mathbf{z} - \mathbf{x}} = \abs{\mathbf{z} - \mathbf{y}} = \frac{d}{2} = r$
for every $k = 1, 2, 3, \ldots$
\item[(c)]
If $2r < d$, $\abs{\mathbf{w}}^2 < 0$,
which is impossible.
Therefore, there is no such $\mathbf{z}$
for every $k = 1, 2, 3, \ldots$
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.17.}
\addcontentsline{toc}{subsection}{Exercise 1.17.}
\emph{Prove that
$$|\mathbf{x}+\mathbf{y}|^2 + |\mathbf{x}-\mathbf{y}|^2
= 2|\mathbf{x}|^2 + 2|\mathbf{y}|^2$$
if $\mathbf{x} \in \mathbb{R}^k$ and $\mathbf{y} \in \mathbb{R}^k$.
Interpret this geometrically,
as a statement about parallelograms.} \\

\emph{Proof.}
\begin{align*}
&|\mathbf{x}+\mathbf{y}|^2 + |\mathbf{x}-\mathbf{y}|^2 \\
=& (\mathbf{x}+\mathbf{y})\cdot(\mathbf{x}+\mathbf{y})
  + (\mathbf{x}-\mathbf{y})\cdot(\mathbf{x}-\mathbf{y}) \\
=& (\mathbf{x}\cdot\mathbf{x} + 2\mathbf{x}\cdot\mathbf{y} + \mathbf{y}\cdot\mathbf{y})
  + (\mathbf{x}\cdot\mathbf{x} - 2\mathbf{x}\cdot\mathbf{y} + \mathbf{y}\cdot\mathbf{y}) \\
=& 2\mathbf{x}\cdot\mathbf{x} + 2\mathbf{y}\cdot\mathbf{y} \\
=& 2|\mathbf{x}|^2 + 2|\mathbf{y}|^2.
\end{align*}

Interpret this geometrically,
the sum of the squares of the lengths of the four sides of a parallelogram
equals the sum of the squares of the lengths of the two diagonals. \\

If the parallelogram is a rectangle, the two diagonals are of equal lengths,
so that the statement reduces to the Pythagorean theorem.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.18.}
\addcontentsline{toc}{subsection}{Exercise 1.18.}
\emph{If $k \geq 2$ and $\mathbf{x} \in \mathbb{R}^k$,
prove that there exists $\mathbf{y} \in \mathbb{R}^k$ such that
$\mathbf{y} \neq 0$ but $\mathbf{x} \cdot \mathbf{y} = 0$.
Is this also true if $k = 1$?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
There are only two possible cases.
  \begin{enumerate}
  \item[(a)]
  \emph{$\exists \: i$ such that $x_i = 0$.}
  Let $\mathbf{y} = (0, \ldots, 0, 1, 0, \ldots, 0) \neq 0$
  whose entries are all $0$ except for a $1$ in the $i$-th position.
  So $\mathbf{x} \cdot \mathbf{y} = 0 + \ldots + 0 = 0$.
  \item[(b)]
  \emph{$\forall \: i, x_i \neq 0$.}
  Since $k \geq 2$, we can define
  $\mathbf{y} = (x_2, -x_1, 0, \ldots, 0) \neq 0.$
  So $\mathbf{x} \cdot \mathbf{y} = x_1 x_2 + x_2 (-x_1) + 0 + \ldots + 0 = 0$.
  \end{enumerate}
\item[(2)]
It is not true for $k = 1$ since $\mathbb{R}^1 = \mathbb{R}$ is a field.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.19.}
\addcontentsline{toc}{subsection}{Exercise 1.19.}
\emph{Suppose $\mathbf{a} \in \mathbb{R}^k$, $\mathbf{b} \in \mathbb{R}^k$.
Find $\mathbf{c} \in \mathbb{R}^k$ and $r > 0$ such that
$$\abs{\mathbf{x} - \mathbf{a}}
= 2 \abs{\mathbf{x} - \mathbf{b}}$$
if and only if $\abs{\mathbf{x} - \mathbf{c}} = r$.
(Solution: $3\mathbf{c} = 4\mathbf{b} - \mathbf{a}$,
$3r = 2\abs{\mathbf{b} - \mathbf{a}}$.)} \\

Suppose $\mathbf{a} \neq \mathbf{b}$ to guarantee the existence of $r > 0$. \\

It is known as \textbf{circles of Apollonius}.
In general, for any $\mu > 1$,
$$\abs{\mathbf{x} - \mathbf{a}}
= \mu \abs{\mathbf{x} - \mathbf{b}}$$
if and only if $\abs{\mathbf{x} - \mathbf{c}} = r$
where
$\mathbf{c}
= \frac{\mu^2\mathbf{b}-\mathbf{a}}{\mu^2 - 1}$
and $r = \frac{\mu}{\mu^2-1}\abs{\mathbf{b} - \mathbf{a}}$. \\

\emph{Proof.}
\begin{align*}
&\abs{\mathbf{x} - \mathbf{a}}
= \mu \abs{\mathbf{x} - \mathbf{b}} \\
\Longleftrightarrow&
\abs{\mathbf{x} - \mathbf{a}}^2
= \mu^2 \abs{\mathbf{x} - \mathbf{b}}^2 \\
\Longleftrightarrow&
\abs{\mathbf{x}}^2 -2\mathbf{a}\cdot\mathbf{x} + \abs{\mathbf{a}}^2
= \mu^2\abs{\mathbf{x}}^2 -2\mu^2\mathbf{b}\cdot\mathbf{x} + \mu^2\abs{\mathbf{b}}^2 \\
\Longleftrightarrow&
(\mu^2-1)\abs{\mathbf{x}}^2
  -2(\mu^2\mathbf{b}-\mathbf{a})\cdot\mathbf{x}
  +(\mu^2\abs{\mathbf{b}}^2 - \abs{\mathbf{a}}^2)
= 0 \\
\Longleftrightarrow&
\abs{\mathbf{x}}^2
  - 2\frac{\mu^2\mathbf{b}-\mathbf{a}}{\mu^2-1}\cdot\mathbf{x}
  + \frac{\mu^2\abs{\mathbf{b}}^2 - \abs{\mathbf{a}}^2}{\mu^2-1}
= 0.
\end{align*}
Write $\mathbf{c}
= \frac{\mu^2\mathbf{b}-\mathbf{a}}{\mu^2 - 1}$
and $r = \frac{\mu}{\mu^2-1}\abs{\mathbf{b} - \mathbf{a}} > 0$.
Note that
$\abs{\mathbf{c}}^2 - r^2 = \frac{\mu^2\abs{\mathbf{b}}^2-\abs{\mathbf{a}}^2}{\mu^2-1}$.
Thus
\begin{align*}
&\abs{\mathbf{x} - \mathbf{a}}
= \mu \abs{\mathbf{x} - \mathbf{b}} \\
\Longleftrightarrow&
\abs{\mathbf{x}}^2
  - 2 \mathbf{c}\cdot\mathbf{x}
  + \abs{\mathbf{c}}^2 - r^2
= 0. \\
\Longleftrightarrow&
\abs{\mathbf{x} - \mathbf{c}}^2 = r^2 \\
\Longleftrightarrow&
\abs{\mathbf{x} - \mathbf{c}} = r.
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 1.20.}
\addcontentsline{toc}{subsection}{Exercise 1.20.}
\emph{With reference to the Appendix,
suppose that property (III) were omitted from the definition of a Dedekind cut.
Keep the same definitions of order and addition.
Show that the resulting ordered set has the least-upper-bound property,
that addition satisfies axioms (A1) to (A4) (with a slightly different zero-element!)
but that (A5) fails. } \\

\emph{Proof of the least-upper-bound property.}
\begin{enumerate}
\item[(1)]
Let $A$ be a nonempty subset of $\mathbb{R}$,
and assume that $\beta \in \mathbb{R}$ is an upper bound of $A$.
\item[(2)]
Define $\gamma$ to be the union of all $\alpha \in A$.
We shall prove that $\gamma \in \mathbb{R}$ and that $\gamma = \sup A$.
\item[(3)]
\emph{Show that $\gamma \in \mathbb{R}$.}
Property (I) is established by property (I) of $\alpha \in A$ and property (I) of $\beta$.
Property (II) is established by property (I) of $\gamma$ and property (II) of $\alpha \in A$.
\item[(4)]
\emph{Show that $\gamma = \sup A$.}
The result is established by property (II) of $\alpha \in A$.
\end{enumerate}
$\Box$ \\

\textbf{Supplement (Noetherian modules).}
\emph{Let $M$ be a module over a commutative ring $R$.
Then $M$ is Noetherian (that is, every submodule of $M$ is finitely generated)
implies that
every ascending chain of submodules of $M$ terminates
(``ascending chain condition'').} \\

\emph{Proof of Supplement.}
Given any ascending chain of submodules $N_1 \subseteq N_2 \subseteq \cdots$,
let $$N = \bigcup_{i=1}^{\infty} N_i.$$
\begin{enumerate}
\item[(a)]
\emph{$N$ is a submodule.}
By the ascending chain condition, each pair of elements in $N$ are in a common $N_m$.
\item[(b)]
$N$ is finitely generated by assumption.
By the ascending chain condition again, all generators of $N$ are in a common $N_m$.
So $N = N_m$ for some $m$.
\item[(c)]
Since $N_m = N \supseteq N_n$ whenever $n \geq m$,
$N_m = N_{m+1} = \cdots$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (A1).}
All the same as the textbook except:
\emph{show that $\alpha+\beta \in \mathbb{R}$.}
Both property (I)(II) are established by property (I)(II) of $\alpha$ and of $\beta$.
$\Box$ \\

\emph{Proof of (A2)(A3).}
Established by the definition of Dedekind cuts.
$\Box$ \\

\emph{Proof of (A4).}
\begin{enumerate}
\item[(1)]
In the textbook (page 18),
we cannot get the opposite inclusion $\alpha + 0^* \supseteq \alpha$
since no property (III) to guarantee the existence of $r \in \alpha$.
\item[(2)]
Therefore, we define $0^{\#} = \{ p \in \mathbb{Q} : p \leq 0 \}$.
\item[(3)]
\emph{Show that $\alpha + 0^{\#} = \alpha$.}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $\alpha + 0^{\#} \subseteq \alpha$.}
  Given any $r \in \alpha$, $s \in 0^{\#}$.
    \begin{enumerate}
    \item[(i)]
    If $s = 0$, $r+s = r \in \alpha$.
    \item[(ii)]
    If $s < 0$, $r+s < r$. So $r+s \in \alpha$ by property (II).
    \end{enumerate}
  Hence, $r+s$ is always in $\alpha$.
  \item[(b)]
  \emph{Show that $\alpha + 0^{\#} \supseteq \alpha$.}
  Given any $r \in \alpha$, $r = r + 0 \in \alpha + 0^{\#}$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\

\emph{Proof of failure of (A5)(Reductio ad absurdum).}
\begin{enumerate}
\item[(1)]
Consider $0^{*} = \{ p \in \mathbb{Q} : p < 0 \} \in \mathbb{R}$.
\item[(2)]
If (A5) were true,
then there were an element $\alpha \in \mathbb{R}$ such that $0^{*} + \alpha = 0^{\#}$.
\item[(3)]
Note that $0^{\#}$ has the maximal element (namely $0$), and thus
$0^{*} + \alpha$ has the maximal element $s+r$ where $s \in 0^{*}$ and $r \in \alpha$.
\item[(4)]
$s \in 0^{*}$ implies $s < 0$.
Then there exists $s' \in \mathbb{Q}$ such that $s < s' < 0$.
So $s' \in 0^{*}$ and $s'+r \in 0^{*} + \alpha$.
$s'+r > s+r$, contrary to the maximality of $s+r$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 2: Basic Topology \\}
\addcontentsline{toc}{section}{Chapter 2: Basic Topology}



\subsection*{Notation.}
\addcontentsline{toc}{subsection}{Notation.}
\begin{enumerate}
\item[(1)]
$E^{\circ}$ or $\mathrm{int}(E)$ is the interior of $E$.
\item[(2)]
$\overline{E}$ is the closure of $E$.
\item[(3)]
$\widetilde{E}$ is the complement of $E$.
\item[(4)]
$B(p;r)$ or $B(p)$ is the set of all points $q$ in a metric space $(M,d)$
such that $d_M(p,q) < r$. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.1.}
\addcontentsline{toc}{subsection}{Exercise 2.1.}
\emph{Prove that the empty set is a subset of every set.} \\

\emph{Proof.}
By Definitions 1.3,
\begin{enumerate}
\item[(1)]
The set which contains no element will be called the \textbf{empty set},
\item[(2)]
If $A$ and $B$ are sets, and if every element of $A$ is an element of $B$,
we say that $A$ is a \textbf{subset} of $B$,
\end{enumerate}
every element of the empty set (there are none) belongs to every set.
That is, the empty set is a subset of every set.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.2.}
\addcontentsline{toc}{subsection}{Exercise 2.2.}
\emph{A complex number $z$ is said to be algebraic if there are integers
$a_0, \ldots, a_n$, not all zero, such that
$$a_0 z^n + a_1 z^{n-1} + \cdots + a_{n-1} z + a_n = 0.$$
Prove that the set of all algebraic numbers is countable.
(Hint: For every positive integer $N$ there are only finitely many equations with
$$n + |a_0| + |a_1| + \cdots + |a_n| = N.$$}

Might assume $a_0 \neq 0$. \\

For example, all rational numbers are algebraic
since $p = \frac{\alpha}{\beta}$ (where $\alpha, \beta \in \mathbb{Z}$)
is a root of $\beta z - \alpha = 0$. \\

Besides, $z = \sqrt{2} + \sqrt{3}$ is algebraic since $z^4 - 10z^2 + 1 = 0$.
In fact, $z = \pm\sqrt{2} \pm\sqrt{3}$ are also algebraic since
$z^4 - 10z^2 + 1 =
(z - \sqrt{2} - \sqrt{3})(z + \sqrt{2} - \sqrt{3})
(z - \sqrt{2} + \sqrt{3})(z + \sqrt{2} + \sqrt{3})$. \\

\textbf{Lemma.}
\emph{The set of all polynomials over $\mathbb{Z}$ is countable implies that
the set of algebraic numbers is countable.} \\

\emph{Proof of Lemma.}
By definition, we write the set of algebraic numbers as
$$S = \bigcup_{f(x) \in \mathbb{Z}[x]} \{ z \in \mathbb{C} : f(z) = 0 \}.$$
Since each polynomial of degree $n$ has at most $n$ roots,
$\{ z \in \mathbb{C} : f(z) = 0 \}$ is finite for each given $f(x) \in \mathbb{Z}[x]$.
So $S$ is a countable union (by assumption) of finite sets, and hence at most countable.
$S$ is infinite since every integer $\alpha$ is a root of $f(z) = z - \alpha$.
So $S$ is countable.
$\Box$ \\

Thus, it suffices to show that
\emph{the set of all polynomials over $\mathbb{Z}$ is countable.} \\

\emph{Proof (Hint).}
For every positive integer $N$ there are only finitely many equations with
$n + |a_0| + |a_1| + \cdots + |a_n| = N.$
Write
$$P_N = \{ f(x) \in \mathbb{Z}[x] : n + |a_0| + |a_1| + \cdots + |a_n| = N \}$$
where $f(x) = a_0 z^n + a_1 z^{n-1} + \cdots + a_{n-1} z + a_n$ with $a_0 \neq 0$,
and
$$P = \bigcup_{N = 1}^{\infty} P_N.$$
$P$ is the set of all polynomials over $\mathbb{Z}$. \\

Each $P_N$ is finite for given $N$
(since the equation $n + |a_0| + |a_1| + \cdots + |a_n| = N$
has finitely many solutions
$(n, a_0, a_1, \ldots, a_n) \in \mathbb{Z}^{n+2}$).
So $P$ is a countable union of finite sets, and hence at most countable.
$P$ is infinite since $\mathbb{Z}$ is a subring of $\mathbb{Z}[x]$.
So $P$ is countable.
$\Box$ \\

\emph{Proof (Theorem 2.13).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\mathbb{Z}^N$ is countable for any integer $N > 0$.}
It is Theorem 2.13.
\item[(2)]
\emph{Show that the set of all polynomials over $\mathbb{Z}$ is countable.}
Let
$$P_n = \{ f \in \mathbb{Z}[x] : \deg f = n \},$$
and
$$P = \bigcup_{n = 1}^{\infty} P_n = \mathbb{Z}[x].$$

\emph{Claim: $P_n$ is countable.}
Define a 1-1 map $\varphi_n: P_n \rightarrow \mathbb{Z}^{n+1}$ by
$$\varphi_n(a_0 z^n + a_1 z^{n-1} + \cdots + a_n)
= (a_0, a_1, \ldots, a_{n-1}, a_n).$$
By (1) and Theorem 2.8, $P_n$ is countable.
($P_n$ is infinite since $a_n \in \mathbb{Z}$.)
Now $P$ is a countable union of countable sets,
and hence countable by Theorem 2.12.
\end{enumerate}
$\Box$ \\

\emph{Proof (Unique factorization theorem).}
\begin{enumerate}
\item[(1)]
\emph{The set of prime numbers is countable.}
Write all primes in the ascending order as $p_1, p_2, \ldots, p_n, \ldots$
where $p_1 = 2, p_2 = 3, \ldots, p_{10001} = 104743, \ldots$
(See \href{https://projecteuler.net/problem=7}{ProjectEuler 7: 10001st prime}.
Use sieve of Eratosthenes to get $p_{10001}$.)
\item[(2)]
\emph{The set of all polynomials over $\mathbb{Z}$ is countable.}
Let
$$P_n = \{ f \in \mathbb{Z}[x] : \deg f = n \},$$
and
$$P = \bigcup_{n = 1}^{\infty} P_n = \mathbb{Z}[x].$$

\emph{Claim: $P_n$ is countable.}
Define a map $\varphi_n: P_n \rightarrow \mathbb{Z}^+$ by
$$\varphi_n(a_0 z^n + a_1 z^{n-1} + \cdots + a_n)
= p_1^{\psi(a_0)} p_2^{\psi(a_1)} \cdots p_{n+1}^{\psi(a_n)},$$
where $\psi$ is a 1-1 correspondence from $\mathbb{Z}$ to $\mathbb{Z}^+$ (Example 2.5).
By the unique factorization theorem, $\varphi_n$ is 1-1.
So $P_n$ is countable by Theorem 2.8.
($P_n$ is infinite since $a_n \in \mathbb{Z}$.)
Now $P$ is a countable union of countable sets,
and hence countable by Theorem 2.12.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.3.}
\addcontentsline{toc}{subsection}{Exercise 2.3.}
\emph{Prove that there exist real numbers which are not algebraic.} \\

\emph{Proof (Exercise 2.2).}
If all real numbers were algebraic, then $\mathbb{R}$ is countable by Exercise 2.2,
contrary to the fact that
$\mathbb{R}$ is uncountable (Corollary to Theorem 2.43).
$\Box$ \\

\emph{Proof (Liouville, 1844).}
\begin{enumerate}
\item[(1)]
\textbf{Lemma.}
\emph{If $\xi$ is a real algebraic number of degree $n > 1$,
then there is a constant $A > 0$ (depending on $\xi$) such that
$$\left| \xi - \frac{h}{k} \right| \geq \frac{A}{k^n}$$
for all rational numbers $\frac{h}{k}$.}
\begin{enumerate}
\item[(a)]
If $\left| \xi - \frac{h}{k} \right| \geq 1$, pick $A = 1 > 0$.
\item[(b)]
If $\left| \xi - \frac{h}{k} \right| < 1$,
let $f(x) = a_0 + a_1 x + \cdots + a_n x^n$
be an irreducible polynomial of degree $n > 1$ over $\mathbb{Z}$ such that
$f(\xi) = 0$.
By the mean value theorem,
$$f(\xi) - f\left( \frac{h}{k} \right)
= \left( \xi - \frac{h}{k} \right) f'(c)$$
for some
$c
\in \left( \xi - \frac{h}{k}, \xi + \frac{h}{k} \right)
\subseteq (\xi - 1, \xi + 1)$.
Notice that
\begin{enumerate}
\item[(i)]
$f(\xi) = 0$ by definition.
\item[(ii)]
$f\left( \frac{h}{k} \right) \neq 0$ since $\frac{h}{k}$ cannot be a root of $f(x)$.
Otherwise $f$ is of degree $1$, contrary to the assumption of $f$.
\item[(iii)]
$\left| f\left( \frac{h}{k} \right) \right| \geq \frac{1}{k^n}$
since
\begin{align*}
  f\left( \frac{h}{k} \right)
  &= a_0 + a_1 \left( \frac{h}{k} \right) + \cdots + a_n \left( \frac{h}{k} \right)^n
  \neq 0, \\
  k^n f\left( \frac{h}{k} \right)
  &= a_0 k^n + h k^{n-1} a_1 + \cdots + h^n a_n
  \neq 0, \\
  k^n \left| f\left( \frac{h}{k} \right) \right|
  &\geq 1.
\end{align*}
\item[(iv)]
$|f'(c)| \leq \sup_{x \in [\xi - 1, \xi + 1]}|f'(x)|$ since
$c \in [\xi - 1, \xi + 1]$
and $f'(x)$ is continuous or bounded on a compact set $[\xi - 1, \xi + 1]$.
\end{enumerate}
By (i)-(iv),
\begin{align*}
  \left| f(\xi) - f\left( \frac{h}{k} \right) \right|
  &= \left| \left( \xi - \frac{h}{k} \right) f'(c) \right|, \\
  \frac{1}{k^n} \leq \left| f\left( \frac{h}{k} \right) \right|
  &= \left| \xi - \frac{h}{k} \right| |f'(c)|
  \leq \left| \xi - \frac{h}{k} \right| \cdot \sup_{x \in [\xi - 1, \xi + 1]}|f'(x)|.
\end{align*}
Pick $A = (1 + \sup_{x \in [\xi - 1, \xi + 1]}|f'(x)|)^{-1} > 0$.
\end{enumerate}
By (a)(b), we arrange
$A = \min(1, (1 + \sup_{x \in [\xi - 1, \xi + 1]}|f'(x)|)^{-1}) > 0$
to fit the inequality.
\item[(2)]
\emph{$\xi = \sum_{n=0}^{\infty} 10^{-n!}$ is transcendental.}
\begin{enumerate}
\item[(a)]
Let $k_j = 10^{j!}$, $h_j = 10^{j!} \sum_{n=0}^{j} 10^{-n!}$.
Then
$$\left| \xi - \frac{h_j}{k_j} \right|
= \sum_{n=j+1}^{\infty} 10^{-n!}
< \sum_{n=(j+1)!}^{\infty} 10^{-n}
= \frac{A_j}{k_j^{j}}
$$
where $A_j = \frac{10}{9} \cdot 10^{-j!}$.
\item[(b)]
If $\xi$ were a real algebraic number of degree $d > 1$,
then by Lemma and (a),
$$\frac{A}{k_j^{d}}
< \left| \xi - \frac{h_j}{k_j} \right|
< \frac{A_j}{k_j^{j}}
< \frac{A_j}{k_j^{d}}$$
for some $A > 0$ and $j \geq d$,
or $0 < A < A_j$.
Since $j$ is arbitrary,
$A_j \rightarrow 0$ as $j \rightarrow \infty$,
contrary to $A > 0$.
\item[(c)]
If $\xi$ were a real algebraic number of degree $d = 1$,
$\xi = \frac{h}{k}$ is a rational number.
So
$$\left| \xi - \frac{h_j}{k_j} \right|
= \left| \frac{h}{k} - \frac{h_j}{k_j} \right|
= \left| \frac{h k_j - k h_j}{k k_j} \right|
\geq \left| \frac{1}{k k_j} \right|
= \frac{|k|^{-1}}{k_j}$$
for all $j$.
(It is impossible that $h k_j - k h_j = 0$ or $\frac{h}{k} = \frac{h_i}{k_j}$
since $| \frac{h}{k} - \frac{h_j}{k_j} | = \sum_{n=j+1}^{\infty} 10^{-n!} > 0$ for all $j$.)
Again by (a),
$$\frac{|k|^{-1}}{k_j}
\leq \left| \xi - \frac{h_j}{k_j} \right|
< \frac{A_j}{k_j^{j}}
< \frac{A_j}{k_j},$$
or $0 < |k|^{-1} < A_j$.
(Similar to (b).)
Since $j$ is arbitrary,
$A_j \rightarrow 0$ as $j \rightarrow \infty$,
contrary to $|k|^{-1} > 0$.
\end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.4.}
\addcontentsline{toc}{subsection}{Exercise 2.4.}
\emph{Is the set of all irrational real numbers countable?} \\

\emph{Proof (Reductio ad absurdum).}
If $\mathbb{R}-\mathbb{Q}$ were countable,
then $\mathbb{R} = \mathbb{Q} \bigcup (\mathbb{R}-\mathbb{Q})$ is countable
(Theorem 2.12),
contrary to the fact that
$\mathbb{R}$ is uncountable (Corollary to Theorem 2.43).
$\Box$ \\

\emph{Proof (Exercise 2.18).}
Exercise 2.18 provides
some examples of uncountable subset $E$ of irrational real numbers.
\begin{enumerate}
  \item[(1)]
  Let $A$ be the set of all $y \in [0,1]$
  whose decimal expansion contains only the digits $4$ and $7$.
  Let $\xi = \sum_{n=0}^{\infty} 10^{-n!}$ and $$E = \{ y + \xi : y \in A \}.$$
  \item[(2)]
  Let $E$ be a subset of Liouville numbers as
  $$E = \left\{ \sum_{n=0}^{\infty} \frac{a_n}{10^{n!}} : a_n \in \{4, 7\} \right\}.$$
  \item[(3)]
  Let
  $$E = \left\{ \sum_{n=1989}^{\infty} \frac{a_n}{n!} : a_n \in \{6, 4\} \right\}.$$
\end{enumerate}
We can apply the same argument of Theorem 2.14 to prove that each $E$ is uncountable.
Then use Theorem 2.8 to get all irrational real numbers cannot be countable.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.5.}
\addcontentsline{toc}{subsection}{Exercise 2.5.}
\emph{Construct a bounded set of real numbers with exactly three limit points.} \\

\emph{Proof (Exercise 2.12).}
Let
$$K_{p} =
\{ p \}
\bigcup
\left\{ p + \frac{1}{n} : n \in \mathbb{Z}^+ \right\} \subseteq \mathbb{R}^1$$
be a compact set of $\mathbb{R}^1$ with exactly one limit point $p \in \mathbb{R}^1$
(Exercise 2.12).
Then
$$K_{1989} \cup K_{6} \cup K_{4}$$
is a compact set of $\mathbb{R}^1$ with exactly three limit points
$1989, 6, 4 \in \mathbb{R}^1$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.6.}
\addcontentsline{toc}{subsection}{Exercise 2.6.}
\emph{Let $E'$ be the set of all limit points of a set $E$.
Prove that $E'$ is closed.
Prove that $E$ and $\overline{E}$ have the same limit points.
(Recall that $\overline{E} = E \cup E'$.)
Do $E$ and $E'$ always have the same limit points?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $E'$ is closed.}
  \begin{enumerate}
  \item[(a)]
  \emph{Use Definition 2.18 (d).}
    \begin{enumerate}
    \item[(i)]
    It suffices to show every limit point of $E'$ is a limit point of $E$.
    Given a limit point $p$ of $E'$, so that every open neighborhood $U$ of $p$
    contains a point $q_0 \neq p$ such that $q_0 \in E'$.
    \item[(ii)]
    Since $q_0$ is a limit point of $E$,
    there is an open neighborhood $V$ of $q_0$ contains a point $q \neq q_0$
    such that $q \in E$, where
    $$V = U \cap B\left( q_0; \frac{1}{2}d(p,q_0) \right) \subseteq U$$
    ($B(x;r)$ is the open ball with center at $x$ and radius $r$).
    \item[(iii)]
    By the construction of $V$,
    for such open neighborhood $U$ of $p$,
    there is $q \neq p$ and $q \in V \subseteq U$ and $q \in E$.
    That is, $p$ is a limit point of $E$.
    \end{enumerate}
  \item[(b)]
  \emph{Use Definition 2.18 (e).}
    \begin{enumerate}
    \item[(i)]
    To show $E'$ is closed or $X-E'$ is open,
    it suffices to show every point of $X-E'$ is an interior point of $X-E'$.
    \item[(ii)]
    Given a point $p \in X-E'$, or $p$ is not a limit point of $E$.
    There is an open neighborhood $U$ of $p$ contains no point $q \neq p$ such that $q \in E$.
    \item[(iii)]
    To show $U$ is an open neighborhood of $p$ such that $U \subseteq X-E'$,
    it suffices to no point $q \neq p$ such that $q \in E'$.
    If there were a limit point $q$ of $E$ such that $q \neq p$ and $q \in U$,
    then
    $$V = U \cap B\left( q; \frac{1}{2}d(p,q) \right) \subseteq U$$
    is an open neighborhood of $q$ contains no point of $E$,
    contrary to the assumption $q \in E'$.
    So $U \subseteq X-E'$ is an open neighborhood of $p \in X-E'$.
    \end{enumerate}
  \end{enumerate}
\item[(2)]
\emph{Show that $E' = \overline{E}'$.}
It suffices to show $E' \supseteq \overline{E}'$.
($E' \subseteq \overline{E}'$ holds trivially since $E \subseteq \overline{E}$).
Given a limit point $p$ of $\overline{E} = E \cup E'$.
  \begin{enumerate}
  \item[(a)]
  $p$ is a limit point of $E$. Nothing to do.
  \item[(b)]
  $p$ is a limit point of $E'$.
  Since $p$ is a limit point of $E'$ and $E'$ is a closed set,
  $p \in E'$, or $p$ is a limit point of $E$.
  \end{enumerate}
In any case, $E' \supseteq \overline{E}'$.
\item[(3)]
\emph{$E$ and $E'$ might not have the same limit points.}
Let
$$E = \left\{ \frac{1}{n} : n \in \mathbb{Z}^+ \right\} \subseteq \mathbb{R}^1.$$
Then $E' = \{0\}$ and thus $(E')' = \varnothing$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.7.}
\addcontentsline{toc}{subsection}{Exercise 2.7.}
\emph{Let $A_1, A_2, A_3, \ldots$ be subsets of a metric space.}
\begin{enumerate}
\item[(a)]
\emph{If $B_n = \bigcup^{n}_{i=1} A_i$, prove that
$\overline{B_n} = \bigcup^{n}_{i=1}{\overline{A_i}}$, for $n = 1, 2, 3, \ldots$}
\item[(b)]
\emph{If $B = \bigcup^{\infty}_{i=1} A_i$,
prove that $\overline{B} \supseteq \bigcup^{\infty}_{i=1} \overline{A_i}$.}
\end{enumerate}
\emph{Show, by an example, that this inclusion can be proper. } \\

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $\overline{B_n} \subseteq \bigcup^{n}_{i=1}{\overline{A_i}}$.}
  Since $A_i \subseteq \overline{A_i}$ for any $i$, we have
  $$B_n = \bigcup^{n}_{i=1} A_i \subseteq \bigcup^{n}_{i=1} \overline{A_i}.$$
  Since $\bigcup^{n}_{i=1} \overline{A_i}$ is a union of finitely many
  closed set $\overline{A_i}$, $\bigcup^{n}_{i=1} \overline{A_i}$ is closed
  (Theorem 2.24(d)).
  By Theorem 2.27(c), $\overline{B_n} \subseteq \bigcup^{n}_{i=1} \overline{A_i}$.
  \item[(2)]
  \emph{Show that $\overline{B_n} \supseteq \bigcup^{n}_{i=1}{\overline{A_i}}$.}
  Same argument in the proof of (b).
\end{enumerate}
$\Box$\\

\emph{Proof of (b).}
Since $\bigcup^{\infty}_{j=1} A_j \supseteq A_i$ for any $i$,
by the monotonicity of closure, we have
$\overline{\bigcup^{\infty}_{j=1} A_j} \supseteq \overline{A_i}$ for any $i$,
or $\overline{B} \supseteq \bigcup^{\infty}_{i=1} \overline{A_i}$.
$\Box$\\

\emph{Proof of proper inclusion in (b).}
Let
$$A_{n} = \left( \frac{1}{n}, \infty \right) \subseteq \mathbb{R}^1$$
for any $n \in \mathbb{Z}^+$.
Then
\begin{align*}
\bigcup^{\infty}_{n=1} A_n = (0, \infty)
&\Longrightarrow
\overline{\bigcup^{\infty}_{n=1} A_n} = \overline{(0, \infty)} = [0, \infty), \\
\overline{A_n} = \left[ \frac{1}{n}, \infty \right)
&\Longrightarrow
\bigcup^{\infty}_{n=1} \overline{A_n}
= \bigcup^{\infty}_{n=1} \left[ \frac{1}{n}, \infty \right)
= (0, \infty).
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.8.}
\addcontentsline{toc}{subsection}{Exercise 2.8.}
\emph{Is every point of every open set $E \subseteq \mathbb{R}^2$ a limit point of $E$?
Answer the same question for closed sets in $\mathbb{R}^2$. } \\

It is not true for all metric spaces $X$.
The (discrete) metric in Exercise 2.10 implies no limit point exists in $X$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that for every open set $E \subseteq \mathbb{R}^k$, $E \subseteq E'$.}
Given any point $\mathbf{p} \in E$, we shall show $\mathbf{p}$ is a limit point of $E$.
  \begin{enumerate}
  \item[(a)]
  Since $E$ is open, there is an open neighborhood $B(\mathbf{p};r_0) \subseteq E$
  for some $r_0 > 0$.
  \item[(b)]
  \emph{In particular, given any $s \in \mathbb{R}$ such that $0 < s < r_0$, we can find
  $$\mathbf{q} \in B(\mathbf{p};s) \subseteq B(\mathbf{p};r_0) \subseteq E$$
  such that $\mathbf{q} \neq \mathbf{p}$.}
  Explicitly, write $$\mathbf{p} = (p_1, \ldots, p_k)$$ and
  choose
  $$\mathbf{q} = \left(p_1 + \frac{s}{89}, p_2, \ldots, p_k\right) \neq \mathbf{p}$$
  (since $s > 0$).
  Clearly, $\mathbf{q}$ is well-defined in $\mathbb{R}^k$ and
  $|\mathbf{q} - \mathbf{p}| = \frac{s}{89} < s$ or $\mathbf{q} \in B(\mathbf{p};s)$.
  \item[(c)]
  Now given every open neighborhood $B(\mathbf{p}, r)$ of $\mathbf{p}$.
  We can choose $s \in \mathbb{R}$ such that $0 < s < \min\{r_0,r\} \leq r_0$.
  (might pick $s = \frac{1}{64}\min\{r_0,r\}$.)
  By (b), there exists $\mathbf{q} \neq \mathbf{p}$ such that
  $$\mathbf{q} \in B(\mathbf{p};s) \subseteq B(\mathbf{p};r) \subseteq E.$$
  \end{enumerate}
\item[(2)]
\emph{Give an example of a closed set $E \subseteq \mathbb{R}^k$ such that $E \not\subseteq E'$.}
Pick $E = \{ \mathbf{0} \}$.
So $E' = \varnothing$ and thus $E \not\subseteq E'$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.9.}
\addcontentsline{toc}{subsection}{Exercise 2.9.}
\emph{Let $E^\circ$ denote the set of all interior points of a set $E$.
[See Definition 2.18(e); $E^\circ$ is called the interior of $E$.]}
\begin{enumerate}
\item[(a)]
\emph{Prove that $E^\circ$ is always open.}
\item[(b)]
\emph{Prove that $E$ is open if and only if $E^\circ = E$.}
\item[(c)]
\emph{If $G$ is contained in $E$ and $G$ is open,
prove that $G$ is contained in $E^\circ$.}
\item[(d)]
\emph{Prove that the complement of $E^\circ$ is the closure of the
complement of $E$.}
\item[(e)]
\emph{Do $E$ and $\overline{E}$ always have the same interiors?}
\item[(f)]
\emph{Do $E$ and $E^\circ$ always have the same closures?} \\
\end{enumerate}

Similar to Theorem 2.27. \\

\emph{Proof of (a).}
\emph{It is equivalent to show that
$E^{\circ} \subseteq (E^{\circ})^{\circ}.$}
\begin{enumerate}
\item[(1)]
Given any point $x \in E^{\circ}$, there is $r > 0$ such that $B(x;r) \subseteq E$.
\item[(2)]
It suffices to show that $B\left(x;\frac{2}{r}\right) \subseteq E^{\circ}$.
Given any point $y \in B\left(x;\frac{2}{r}\right)$,
we will show that there is an open neighborhood $B\left(y;\frac{2}{r}\right)$ of $y$
such that $B\left(y;\frac{2}{r}\right) \subseteq E$.
\item[(3)]
Given any point $z \in B\left(y;\frac{2}{r}\right)$, we have
$$d(z,x) \leq d(z,y) + d(y,x) < \frac{2}{r} + \frac{2}{r} = r,$$
or $z \in B(x;r) \subseteq E$.
Therefore, $B\left(y;\frac{2}{r}\right) \subseteq E$,
or $y \in E^{\circ}$,
or $B\left(x;\frac{2}{r}\right) \subseteq E^{\circ}$,
or $x \in (E^{\circ})^{\circ}$,
or $E^{\circ} \subseteq (E^{\circ})^{\circ}$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
($\Longrightarrow$)(Definition 2.18)
Since $E$ is open, every point of $E$ is an interior point of $E$.
Hence $E \subseteq E^{\circ}$.
Note that $E^{\circ} \subseteq E$ is trivial, and thus $E^\circ = E$.
\item[(2)]
($\Longleftarrow$)((a))
By (a), $E = E^{\circ}$ is always open.
\item[(3)]
($\Longleftarrow$)(Definition 2.18)
Every point of $E$ is an interior point of $E$ since $E = E^\circ$.
Hence $E$ is open by Definition 2.18(f).
\end{enumerate}
$\Box$ \\

\emph{Proof of (c).}
$G \subseteq E$ implies $G^{\circ} \subseteq E^{\circ}$.
$G = G^{\circ}$ since $G$ is open ((b)).
Hence $G = G^{\circ} \subseteq E^{\circ}$, that is,
$E^{\circ}$ is the largest open set contained in $E$.
(Similarly, $\overline{E}$ is the smallest closed set containing $E$.)
$\Box$ \\

\emph{Proof of (d).}
\emph{Show that
$X - E^{\circ} = \overline{X - E}$ and
$(X-E)^{\circ} = X - \overline{E}.$}
\begin{enumerate}
\item[(1)]
(Theorem 2.27 and (c))
\begin{align*}
X - E^{\circ}
&= X - \bigcup_{\text{Open } V \subseteq E} V \\
&= \bigcap_{\text{Open } V \subseteq E} (X - V) \\
&= \bigcap_{\text{Closed } W \supseteq X-E} W \\
&= \overline{X - E}. \\
X - \overline{E}
&= X - \bigcap_{\text{Closed } W \supseteq E} W \\
&= \bigcup_{\text{Closed } W \supseteq E} (X - W) \\
&= \bigcup_{\text{Open } V \subseteq X-E} V \\
&= (X - E)^{\circ}.
\end{align*}
\item[(2)]
(Brute-force)
\begin{align*}
x \in E^{\circ}
\Longleftrightarrow&
\exists r > 0 \text{ such that } B(x;r) \subseteq E \\
\Longleftrightarrow&
\exists r > 0 \text{ such that } B(x;r) \cap (X-E) = \varnothing \\
\Longleftrightarrow&
x \not\in \overline{X-E} \\
\Longleftrightarrow&
x \in X - \overline{X-E}. \\
x \in (X-E)^{\circ}
\Longleftrightarrow&
\exists r > 0 \text{ such that } B(x;r) \subseteq (X-E) \\
\Longleftrightarrow&
\exists r > 0 \text{ such that } B(x;r) \cap E = \varnothing \\
\Longleftrightarrow&
x \not\in \overline{E} \\
\Longleftrightarrow&
x \in X - \overline{E}.
\end{align*}
\end{enumerate}
Note that $X - E^{\circ} = \overline{X - E}$ is equivalent to
$(X-E)^{\circ} = X - \overline{E}$ by mapping $E \mapsto X-E$.
$\Box$ \\

\emph{Proof of (e).}
No.
\begin{enumerate}
\item[(1)]
Let $X = \mathbb{R}$ equipped with the Euclidean metric, and $E = \mathbb{Q} \subseteq X$.
\item[(2)]
$E^\circ = \varnothing$ since $\widetilde{\mathbb{Q}}$ is dense in $\mathbb{R}$.
\item[(3)]
$(\overline{E})^{\circ} = (\mathbb{R})^{\circ} = \mathbb{R}$
since $\mathbb{Q}$ is dense in $\mathbb{R}$ and $\mathbb{R}$ is open.
\end{enumerate}
$\Box$ \\

\emph{Proof of (f).}
No.
\begin{enumerate}
\item[(1)]
Let $X = \mathbb{R}$ equipped with the Euclidean metric, and $E = \mathbb{Q} \subseteq X$.
\item[(2)]
$\overline{E} = \mathbb{R}$ since $\mathbb{Q}$ is dense in $\mathbb{R}$.
\item[(3)]
$\overline{E^\circ} = \overline{\varnothing} = \varnothing$
since $\widetilde{\mathbb{Q}}$ is dense in $\mathbb{R}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.10.}
\addcontentsline{toc}{subsection}{Exercise 2.10.}
\emph{Let $X$ be an infinite set. For $p \in X$ and $q \in X$, define
\begin{equation*}
  d(p, q) =
    \begin{cases}
      1 & (\text{if } p \neq q) \\
      0 & (\text{if } p = q).
    \end{cases}
\end{equation*}
Prove that this is a metric.
Which subsets of the resulting metric space are open?
Which are closed?
Which are compact?} \\

(The statement holds even if $X$ is finite.)
We called $d$ the discrete metric,
and the corresponding topology on $X$ induces the discrete topology.
Conversely, if $X$ has the discrete topology, $X$ is always metrizable by the discrete metric. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)] \emph{$d(p, q)$ is a metric.}
\begin{enumerate}
\item[(a)] \emph{$d(p, q) > 0$ if $p \neq q$; $d(p, p) = 0$.} Trivial.
\item[(b)] \emph{$d(p, q) = d(q, p)$.} Trivial.
\item[(c)] \emph{$d(p, q) \leq d(p, r) + d(r, q)$ for any $r \in X$.}
If $p = q$, nothing to do. If $p \neq q$, $r \neq p$ or $r \neq q$ for any $r \in X$.
(Assume not true, $r = p$ and $r = q$ implies that $p = q$ which is a contradiction.)
In any case $d(p, r) + d(r, q) \geq 1 = d(p, q)$.
\end{enumerate}
\item[(2)] \emph{Every subset is open.}
Let $E$ be any subset of $X$.
Then every point $p \in E$ is an interior point of $E$.
In fact, we can pick one open neighborhood $U = B\left(p;\frac{1}{2}\right)$ of $p$
containing only one point $p \in E$ or $U = \{ p \}$,
and such open neighborhood $U$ is a subset of $E$.
So every subset of $X$ is open.
\item[(3)] \emph{Every subset is closed.}
Since every subset is open, every subset is closed by Theorem 2.23. \\

\textbf{Supplement.}
Might use Definition 2.18 (d) to prove directly since there are no limit points in $X$
if we consider one open neighborhood $U = B\left(p;\frac{1}{2}\right)$ of $p$.
Therefore, every subset is closed.
Again we apply Theorem 2.23 to get that every subset is open
without using Definition 2.18 (f).
\item[(4)] \emph{A subset is compact iff it is finite.}
\begin{enumerate}
\item[(a)]
\emph{Any finite subset is compact.}
Say $E = \{p_1, p_2, \ldots, p_k\}$, and $\{ G_{\alpha} \}$ be an open covering of $E$.
From $\{ G_{\alpha} \}$
we pick
$G_{\alpha_1}$ containing $p_1$,
$G_{\alpha_2}$ containing $p_2$, $\ldots$, and
$G_{\alpha_k}$ containing $p_k$.
This process can be done in the finitely many steps.
Therefore,
$$\{ G_{\alpha_1}, G_{\alpha_2}, \ldots, G_{\alpha_k} \}$$
is a finite subcovering of $\{ G_{\alpha} \}$ covering $E$.
\item[(b)]
\emph{Any infinite subset is not compact.}
Take a collection
$$\mathscr{G} = \left\{ G_p = \{ p \} \right\}$$
of open subsets where $p$ runs all points in $E$.
Clearly, $\{ G_p \}$ is an open covering.
Assume
$$\mathscr{G}' = \left\{ G_{p_1}, G_{p_2}, \ldots, G_{p_k} \right\}$$
is any finite subcovering of $\mathscr{G}$.
Since $E$ is infinite, there exist a point $p \in E$ such that
$p \neq p_1$,
$p \neq p_2$, $\ldots$,
$p \neq p_k$.
Therefore, $\mathscr{G}'$ does not cover $p$,
or $\mathscr{G}$ does not contains any finite subcovering $\mathscr{G}'$.
\end{enumerate}
\end{enumerate}
$\Box$ \\

Notice that every subset is bounded.
Therefore, every subset is closed and bounded,
but only finite subset is compact, i.e.,
Heine-Borel theorem is not true in the infinite discrete topology. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.11.}
\addcontentsline{toc}{subsection}{Exercise 2.11.}
\emph{For $x \in \mathbb{R}^1$ and $y \in \mathbb{R}^1$, define
\begin{align*}
  d_1(x,y) &= (x-y)^2, \\
  d_2(x,y) &= \sqrt{|x-y|}, \\
  d_3(x,y) &= |x^2 - y^2|, \\
  d_4(x,y) &= |x - 2y|, \\
  d_5(x,y) &= \frac{|x-y|}{1+|x-y|}.
\end{align*}
Determine, for each of these, whether it is a metric or not.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{$d = d_1$ is not a metric.}
(Reductio ad absurdum) If $d$ were a metric,
then $$d(0,2) > d(0,1)+d(1,2),$$
contrary to Definition 2.15(c) that $d(p,q) \leq d(p,r)+d(r,q)$.
\item[(2)]
\emph{$d = d_2$ is a metric.}
\emph{It suffices to show that
$d'(x,y) = \sqrt{d(x,y)}$ is a metric if $d(x,y)$ is a metric.}
  For any $p, q, r \in \mathbb{R}^1$,
  \begin{enumerate}
  \item[(a)]
  $d'(p,q) = \sqrt{d(p,q)} > 0$ if $p \neq q$;
  $d'(p,p) = \sqrt{d(p,p)} = 0$.
  \item[(b)]
  $d'(p,q) = \sqrt{d(p,q)} = \sqrt{d(q,p)} = d'(q,p)$.
  \item[(c)]
    \begin{align*}
    &\sqrt{d(p,r)+d(r,q)}
    \leq \sqrt{d(p,r)}+ \sqrt{d(r,q)} \\
    \Longleftrightarrow&
    (\sqrt{d(p,r)+d(r,q)})^2
    \leq (\sqrt{d(p,r)}+ \sqrt{d(r,q)})^2 \\
    \Longleftrightarrow&
    d(p,r)+d(r,q)
    \leq d(p,r)+d(r,q) + 2\sqrt{d(p,r)}\sqrt{d(r,q)} \\
    \Longleftrightarrow&
    0
    \leq 2\sqrt{d(p,r)}\sqrt{d(r,q)}.
    \end{align*}
  \item[(d)]
    \begin{align*}
    d'(p,q) &= \sqrt{d(p,q)} \\
    &\leq \sqrt{d(p,r)+d(r,q)}
      &\text{(Triangle inequality)} \\
    &\leq \sqrt{d(p,r)}+ \sqrt{d(r,q)}
      &\text{((c))} \\
    &= d'(p,r) + d'(r,q).
    \end{align*}
  \end{enumerate}
  By Definition 2.15, $d'$ is a metric.
\item[(3)]
\emph{$d = d_3$ is not a metric.}
(Reductio ad absurdum) If $d$ were a metric,
then $$d(1,-1) = 0,$$
contrary to Definition 2.15(a): $d(p,q) > 0$ if $p \neq q$; $d(p,p) = 0$.
\item[(4)]
\emph{$d = d_4$ is not a metric.}
(Reductio ad absurdum) If $d$ were a metric,
then $$d(1,1) = 1,$$
contrary to Definition 2.15(a): $d(p,q) > 0$ if $p \neq q$; $d(p,p) = 0$.
\item[(5)]
\emph{$d = d_5$ is a metric.}
\emph{It suffices to show that
$d'(x,y) = \frac{d(x,y)}{1+d(x,y)}$ is a metric if $d(x,y)$ is a metric.}
  For any $p, q, r \in \mathbb{R}^1$,
  \begin{enumerate}
  \item[(a)]
  $d'(p,q) = \frac{d(p,q)}{1+d(p,q)} > 0$ if $p \neq q$;
  $d'(p,p) = \frac{d(p,p)}{1+d(p,p)} = 0$.
  \item[(b)]
  $d'(p,q) = \frac{d(p,q)}{1+d(p,q)}
  = \frac{d(q,p)}{1+d(q,p)} = d'(q,p)$.
  \item[(c)]
  Write $x = d(p,q)$, $y = d(p,r)$ and $z = d(r,q)$.
  So $x, y, z \geq 0$ and
    \begin{align*}
    &x \leq y + z \\
    \Longleftrightarrow&
    x + x(y+z) \leq y+z + x(y+z) \\
    \Longleftrightarrow&
    x(1+y+z) \leq (1+x)(y+z) \\
    \Longleftrightarrow&
    \frac{x}{1+x} \leq \frac{y+z}{1+y+z}.
    \end{align*}
  \item[(d)]
    \begin{align*}
    d'(p,q) &= \frac{d(p,q)}{1+d(p,q)} \\
    &\leq \frac{d(p,r)+d(r,q)}{1+d(p,r)+d(r,q)}
      &\text{((c))} \\
    &= \frac{d(p,r)}{1+d(p,r)+d(r,q)} + \frac{d(r,q)}{1+d(p,r)+d(r,q)} \\
    &= \frac{d(p,r)}{1+d(p,r)} + \frac{d(r,q)}{1+d(r,q)} \\
    &= d'(p,r) + d'(r,q).
    \end{align*}
  \item[(e)]
  Or we can show $d'(p,q) \leq d'(p,r) + d'(r,q)$ by
    \begin{align*}
    &\frac{x}{1+x} \leq \frac{y}{1+y} + \frac{z}{1+z} \\
    \Longleftrightarrow&
    x(1+y)(1+z) \leq y(1+z)(1+x) + z(1+x)(1+y) \\
    \Longleftrightarrow&
    x+xy+xz+xyz \\
    &\leq (y+xy+yz+xyz) + (z+xz+yz+xyz) \\
    \Longleftrightarrow&
    x \leq y+z+2yz+xyz \\
    \Longleftarrow&
    x \leq y+z
      &\text{($d$ is nonnegative)}
    \end{align*}
  \end{enumerate}
  By Definition 2.15, $d'$ is a metric.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.12.}
\addcontentsline{toc}{subsection}{Exercise 2.12.}
\emph{Let $K \subseteq \mathbb{R}^1$
consist of $0$ and the numbers $\frac{1}{n}$, for $n = 1, 2, 3, \ldots$.
Prove that $K$ is compact directly from the definition
(without using the Heine-Borel theorem).} \\

\emph{Proof.}
Let $\{ G_{\alpha} \}$ be an open covering of $K$.
There is an open set $G_0 \in \{ G_{\alpha} \}$ containing $0$.
So there exists an open neighborhood $U = B(0;r)$ of $0$ such that $U \subseteq G_0$.
So $U$ contains all points $q = \frac{1}{n}$ of $K$ whenever $n > \frac{1}{r}$.
To construct a finite subcovering of $\{ G_{\alpha} \}$,
we need to pick finitely many open sets from $\{ G_{\alpha} \}$
to cover the remaining points $q = \frac{1}{n}$ where $n = 1, 2, \ldots, \left[\frac{1}{r}\right]$,
say $G_1$ contains $q = \frac{1}{1}$, $G_2$ contains $q = \frac{1}{2}$, $\ldots$,
$G_{\left[\frac{1}{r}\right]}$ contains $q = \frac{1}{\left[\frac{1}{r}\right]}$.
(Might be duplicated.)
Hence,
$$\left\{ G_0, G_1, G_2, \ldots, G_{\left[\frac{1}{r}\right]} \right\}$$
is a finite subcovering of $\{ G_{\alpha} \}$ covering $K$.
$\Box$ \\

\emph{Proof (Heine-Borel theorem).}
\begin{enumerate}
\item[(1)]
$K$ is closed. In fact, the only limit point of $K$ is $0$, which is in $K$.
\begin{enumerate}
\item[(a)]
\emph{$p = 0$ is a limit point.}
Given $r > 0$.
There always exists $n \in \mathbb{Z}^{+}$ such that $r > \frac{1}{n}$.
So any open neighborhood $B(0;r)$ of $p = 0$
contains at least one point $q = \frac{1}{n} \neq 0$ in $K$.
\item[(b)]
\emph{$p < 0$ is not a limit point.}
Pick an open neighborhood $B(p;r)$ of $p$ where $r = |p| > 0$.
Then $B(p;r) \cap K = \varnothing$.
\item[(c)]
\emph{$p > 0$ is not a limit point.}
There always exists $m \in \mathbb{Z}^+$ such that $p > \frac{1}{n}$ whenever $n \geq m$.
Pick an open neighborhood $B(p;r)$ of $p$ where $r = p - \frac{1}{m} > 0$.
Then $B(p;r)$ does not have all points $q = \frac{1}{n} \in K$ whenever $n \geq m$.
By Theorem 2.20, $p$ cannot be a limit point of $K$.
\end{enumerate}
\item[(2)]
$K$ is bounded. There is a real number $M = 2$ and a point $q = 0 \in \mathbb{R}^1$
such that $|p - q| = |p| < 2$ for all $p \in K$.
\end{enumerate}
By Heine-Borel theorem, $K$ is compact in $\mathbb{R}^1$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.13.}
\addcontentsline{toc}{subsection}{Exercise 2.13.}
\emph{Construct a compact set of real numbers whose limit points form a countable set.} \\

\emph{Proof (Exercise 2.12).}
Let $K(p;r) \subseteq \mathbb{R}^1$ be
$$K(p;r) = \left\{ p + \frac{r}{n} : n = 2, 3, \ldots \right\} \bigcup \{p\}$$
and
$$K
= \left(\bigcup_{i = 0}^{\infty} K(2^{-i};2^{-i})\right) \bigcup \{0\}.$$
\begin{enumerate}
\item[(1)]
\emph{The set of limit points of $K$ is $K' = \{ 2^{-i} : i = 0, 1, 2, \ldots \} \bigcup \{0\}$,
which is (infinitely) countable.}
  \begin{enumerate}
  \item[(a)]
  The unique limit point of $K(2^{-i};2^{-i})$ is $2^{-i}$ for each $i = 0, 1, 2, \ldots$
  (Exercise 2.12).
  \item[(b)]
  $0$ is a limit point of $K$.
  \item[(c)]
  No other limit points of $K$.
  Similar to the argument of the proof of Exercise 2.12.
  \end{enumerate}
\item[(2)]
$K$ is closed. All limit points are in $K$.
\item[(3)]
$K$ is bounded. There is a real number $M = 2$ and a point $q = 0 \in \mathbb{R}^1$
such that $|p - q| = |p| < 2$ for all $p \in K$.
\end{enumerate}
By Heine-Borel theorem, $K$ is compact in $\mathbb{R}^1$,
and has infinitely countable limit points.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.14.}
\addcontentsline{toc}{subsection}{Exercise 2.14.}
\emph{Give an example of an open cover of the segment $(0, 1)$
which has no finite subcover.} \\

\emph{Proof.}
In $\mathbb{R}^1$, take a collection
$$\mathscr{G} = \left\{ G_n = \left(\frac{1}{n}, 1\right) \right\}$$
of open subsets where $n \in \mathbb{Z}^+$.
\begin{enumerate}
\item[(1)]
\emph{$\mathscr{G}$ is an open covering of $(0, 1) \subseteq \mathbb{R}^1$.}
Actually, given $x \in (0, 1)$, there exists an positive integer $n$ such that $x > \frac{1}{n}$.
That is, $x \in \left(\frac{1}{n}, 1\right) = G_n$.
\item[(2)]
\emph{There is no finite subcovering of $\mathscr{G}$.}
Assume $$\mathscr{G}' = \left\{ G_{n_1}, G_{n_2}, \ldots, G_{n_k} \right\}$$
is any finite subcovering of $\mathscr{G}$ where $n_1 < n_2 < \cdots < n_k$.
Take $x \in \left(0, \frac{1}{n_k}\right) \neq \varnothing$,
$x = \frac{1}{2 n_k}$ for example.
Then $x \not\in G_{n_1}$, $x \not\in G_{n_1}$, $\ldots$, $x \not\in G_{n_k}$,
which contradicts that $\mathscr{G}'$ is a finite subcovering of $\mathscr{G}$ covering $(0, 1)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.15.}
\addcontentsline{toc}{subsection}{Exercise 2.15.}
\emph{
Show that Theorem 2.36 and its Corollary become false
(in $\mathbb{R}^1$, for example) if the word ``compact''
is replaced by ``closed'' or by ``bounded.''} \\

\emph{Recall:}
\begin{enumerate}
\item[(1)]
Theorem 2.36:
\emph{If $\{K_\alpha\}$ is a collection of compact subsets of a metric space $X$
such that the intersection of every finite subcollection of $\{K_\alpha\}$ is nonempty,
then $\bigcap K_\alpha$ is nonempty.}
\item[(2)]
Corollary:
\emph{If $\{K_n\}$ is a sequence of nonempty compact sets such that $K_n$
contains $K_{n+1}$ $(n=1,2,3,\ldots)$, then $\bigcap K_n$ is not empty.} \\
\end{enumerate}

\emph{Proof.}
Let $X = \mathbb{R}^1$ with the usual Euclidean metric.
\begin{enumerate}
\item[(1)]
For the closeness, let $K_n = [n,\infty) \subseteq X$.
\item[(2)]
For the boundedness, let $K_n = \left(0, \frac{1}{n}\right) \subseteq X$.
\end{enumerate}
In any case, $K_1 \supseteq K_2 \supseteq \cdots$ and $\bigcap K_n = \varnothing$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.16.}
\addcontentsline{toc}{subsection}{Exercise 2.16.}
\emph{Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space,
with $d(p,q) = |p-q|$.
Let $E$ be the set of all $p \in \mathbb{Q}$ such that $2 < p^2 < 3$.
Show that $E$ is closed and bounded in $Q$,
but that $E$ is not compact.
Is $E$ open in $\mathbb{Q}$?} \\

\textbf{Lemma.}
\emph{Assume $S \subseteq T \subseteq M$.
Then $S$ is compact in $(M,d)$ if, and only if,
$S$ is compact in the metric subspace $(T,d)$. } \\

\emph{Proof of Lemma.}
\begin{enumerate}
\item[(1)]
$(\Longrightarrow)$
Let $\mathscr{F}$ be an open covering of $S$ in $(T,d)$, say
$S \subseteq \bigcup_{A \in \mathscr{F}} A$ where $A$ is open in $T$.
Then $A = B \cap T$ for some open set $B$ in $M$ (Theorem 3.33).
Let $\mathscr{G}$ be the collection of $B$.
Then
$$S \subseteq
\bigcup_{A \in \mathscr{F}} A
= \bigcup_{B \in \mathscr{G}} (B \cap T)
\subseteq \bigcup_{B \in \mathscr{G}} B,$$
or $\mathscr{G}$ be an open covering of $S$ in $(M,d)$.
Since $S$ is compact in $(M,d)$,
$\mathscr{G}$ contains a finite subcovering, say
$$S \subseteq B_1 \cap \cdots \cap B_p.$$
So $$S \cap T \subseteq (B_1\cap T) \cap \cdots \cap (B_p \cap T),$$
or $$S \subseteq A_1 \cap \cdots \cap A_p$$ (since $S \subseteq T$ or $S \cap T = S$).
So there is a finite subcovering of $\mathscr{F}$ covering $S$,
or $S$ is compact in $(T,d)$.
\item[(2)]
$(\Longleftarrow)$
Let $\mathscr{G}$ be an open covering of $S$ in $(M,d)$, say
$S \subseteq \bigcup_{B \in \mathscr{G}} B$ where $B$ is open in $M$.
Then $A = B \cap T$ is open in $T$.
Let $\mathscr{F}$ be the collection of $A$.
Then
$$S \cap T \subseteq
\bigcup_{B \in \mathscr{G}} (B \cap T)
= \bigcup_{A \in \mathscr{F}} A,$$
or $\mathscr{F}$ be an open covering of $S \cap T = S$ in $(T,d)$.
Since $S$ is compact in $(T,d)$,
$\mathscr{F}$ contains a finite subcovering, say
$$S \subseteq A_1 \cap \cdots \cap A_p.$$
Clearly, $S \subseteq B_1 \cap \cdots \cap B_p$ since $A = B \cap T \subseteq B$.
So there is a finite subcovering of $\mathscr{G}$ covering $S$,
or $S$ is compact in $(M,d)$.
\end{enumerate}
$\Box$ \\

\emph{Proof.}
Write $E_0 = (\sqrt{2},\sqrt{3}) \bigcup (-\sqrt{3},-\sqrt{2})$,
and $E = E_0 \bigcap \mathbb{Q}$.
\begin{enumerate}
\item[(1)]
$E$ is a subset of $\mathbb{Q}$.
\item[(2)]
\emph{Show that $E$ is bounded in $\mathbb{Q}$.}
Since $\mathbb{Q}$ is dense in $\mathbb{R}$,
there is $p \in \mathbb{Q}$ such that $\sqrt{2} < p < \sqrt{3}$, or $p \in E$.
Let $r = p + \sqrt{3} > 0$.
Therefore, $E \subseteq B(p;r)$ for some $r > 0$ and $p \in E$,
or $E$ is bounded.
\item[(3)]
\emph{Show that $E$ is closed in $\mathbb{Q}$.}
It suffices to show its complement is open in $\mathbb{Q}$.
Given any
$$p \in \widetilde{E}
= ((-\infty, -\sqrt{3}] \cup [-\sqrt{2},\sqrt{2}] \cup [\sqrt{3}, \infty)) \cap \mathbb{Q}.$$
$p \leq -\sqrt{3}$ or $-\sqrt{2} \leq p \leq \sqrt{2}$ or $p \geq \sqrt{3}$.
  \begin{enumerate}
  \item[(a)]
  $p \leq -\sqrt{3}$. $p \neq -\sqrt{3}$ since $p \in \mathbb{Q}$ and $-\sqrt{3}$ is irrational.
  So $p < -\sqrt{3}$ and thus there exists $q \in \mathbb{Q}$ such that $p < q < -\sqrt{3}$
  since $\mathbb{Q}$ is dense in $\mathbb{R}$.
  Let $r = \max\{-\sqrt{3} - q, q - p\} > 0$.
  The ball $B(q;r)$ is contained in $\widetilde{E}$.
  \item[(b)]
  $-\sqrt{2} \leq p \leq \sqrt{2}$. Similar to (a).
  \item[(c)]
  $p \geq \sqrt{3}$. Similar to (a).
  \end{enumerate}
  By (a)(b), $\widetilde{E}$ is open in $\mathbb{Q}$, or $E$ is closed in $\mathbb{Q}$.
\item[(4)]
\emph{Show that $E$ is not compact in $\mathbb{Q}$.}
(Reductio ad absurdum)
If $E_0$ were compact in the metric space $\mathbb{Q}$,
$E_0$ is compact in the metric space $\mathbb{R}$ (Lemma),
which is absurd.
\item[(5)]
\emph{Show that $E$ is open.}
Similar to (3).
\end{enumerate}
$\Box$ \\\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.17.}
\addcontentsline{toc}{subsection}{Exercise 2.17.}
\emph{Let $E$ be the set of all $x \in [0,1]$
whose decimal expansion contains only the digits $4$ and $7$.
Is $E$ countable? Is $E$ dense in $[0,1]$? Is $E$ compact? Is $E$ perfect?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $E$ is uncountable.}
Same as Theorem 2.14.
Or show that $E$ is perfect and then apply Theorem 2.43.
\item[(2)]
\emph{Show that $E$ is not dense in $[0,1]$.}
Note that $E \subseteq \left[\frac{4}{9}, \frac{7}{9}\right]$.
So $$B\left(0;\frac{1}{64}\right) \bigcap E
\subseteq B\left(0;\frac{1}{64}\right) \bigcap \left[\frac{4}{9}, \frac{7}{9}\right]
= \varnothing$$ or $0$ is not a limit point of $E$.
Hence $E$ is not dense in $[0,1]$.
\item[(3)]
\emph{Show that $E$ is compact.}
\emph{It is equivalent to show that $E$ is closed and bounded (Theorem 2.41).}
  Let a decimal expansion of $x \in (0,1)$ be $0.x_1 x_2 \cdots$.
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $\widetilde{E}$ is open.}
  Since $E \subseteq \left[\frac{4}{9}, \frac{7}{9}\right]$,
  it suffices to show that every point $x \in (0,1) \cap \widetilde{E}$ is
  an interior point of $\widetilde{E}$.
  Say a decimal expansion of $x$ containing at least one digit $x_n \neq 4, 7$.
  Note that
  $$|x-y| \geq 10^{-n} > 0$$
  for any $y = 0.y_1y_2 \cdots \in E$.
  Hence there is an open neighborhood $B(x;10^{-n})$ of $x$ such that
  $B(x;10^{-n}) \cap E = \varnothing$, or $B(x;10^{-n}) \subseteq \widetilde{E}$,
  or $x$ is an interior point of $\widetilde{E}$.
  \item[(b)]
  \emph{Show that $E$ is closed.}
  Given any limit point $x \in \mathbb{R}^1$ of $E$, we want to show that $x \in E$.
  (Reductio ad absurdum) Similar to (a).
  \item[(c)]
  \emph{Show that $E$ is bounded.}
  $E \subseteq B(0;1)$.
  \end{enumerate}
\item[(4)]
\emph{Show that $E$ is perfect.}
  \begin{enumerate}
  \item[(a)]
  $E$ is closed (by (3)).
  \item[(b)]
  \emph{Show that every point of $E$ is a limit point of $E$.}
  Given any $x \in E$.
  Given any open neighborhood $B(x;r)$ of $x$, there is a positive integer $n$
  such that $$\frac{3}{10^n} < r.$$
  For such $n$, pick $y = 0.x_1x_2 \cdots x_{n-1} y_n \cdots x_{n+1} \cdots \in E$
  where
  \begin{equation*}
  y_n =
    \begin{cases}
      4 & (x_n = 7), \\
      7 & (x_n = 4).
    \end{cases}
  \end{equation*}
  $y \neq x$, and $|y-x| = \frac{3}{10^{n}} < r$.
  So that there is $y \neq x$ such that $y \in B(x;r)$, or $x$ is a limit point of $E$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.18.}
\addcontentsline{toc}{subsection}{Exercise 2.18.}
\emph{Is there a nonempty perfect set in $\mathbb{R}^1$ which contains no rational number?} \\

Yes. \\

\textbf{Lemma.}
\emph{$x \in \mathbb{Q}$ if and only if has repeating decimal expansion.} \\

\emph{Proof of Lemma.}
\begin{enumerate}
\item[(1)]
$(\Longleftarrow)$
Given any repeating decimal
$$x = x_0.x_1 x_2 \cdots x_n \overline{x_{n+1} \cdots x_{n+m}}$$
where $x_0 \in \mathbb{Z}$ and $x_1, \ldots, x_{n+m} \in \{ 0,1,2,3,4,5,6,7,8,9 \}$.
Thus $x = p/q$
where
$$p = (10^m-1) \sum_{i=0}^{n}10^{n-i} x_i
+ \sum_{j=1}^{m} 10^{m-j} x_{n+j} \in \mathbb{Z}$$
and
$$q = 10^n(10^m-1) \in \mathbb{Z}.$$
\item[(2)]
$(\Longrightarrow)$
(Euler's totient function)
Given any $x = p/q$ where $p, q \in \mathbb{Z}$, $q > 0$.
  \begin{enumerate}
  \item[(a)]
  Write $q = 2^{a} 5^{b} q_1$ where $a, b$ are nonnegative integers and
  $(q_1, 10) = 1$ (Unique factorization theorem).
  \item[(b)]
  Let $n = \max\{a,b\}$. Then $2^{n-a} 5^{n-b} q = 10^n q_1$.
  \item[(c)]
  Since $(q_1, 10) = 1$,
  $10^m \equiv 1 \pmod{q_1}$ where $m = \varphi(q_1)$ is Euler's totient function of $q_1$.
  Hence
  $10^m - 1 = q_1 q_2$ for some $q_2 \in \mathbb{Z}$,
  or
  $$2^{n-a} 5^{n-b} q_2 q = 10^n(10^m-1).$$
  Here $2^{n-a} 5^{n-b} q_2$, $n, m$ are nonnegative integers.
  \item[(d)]
  Now write
  $$x = \frac{p}{q}
  = \frac{2^{n-a} 5^{n-b} q_2 p}{10^n(10^m-1)}
  = \frac{(10^m-1) q_3 + r}{10^n(10^m-1)}
  = \frac{q_3}{10^n} + \frac{r}{10^n(10^m-1)}$$
  where $q_3, r \in \mathbb{Z}$ with $0 \leq r < 10^m-1$.
  Might assume $q_3 \geq 0$.
  (If $q_3 < 0$, apply the same argument to $-q_3$
  and then add the minus symbol ``$-$'' in the front of a decimal expansion.)
  Hence
  $$x = x_0.x_1 x_2 \cdots x_n \overline{x_{n+1} \cdots x_{n+m}}$$
  where
  \begin{align*}
    x_0 &= \left\lfloor \frac{q_3}{10^n} \right\rfloor \\
    x_i &= \text{last digit of } \left\lfloor \frac{q_3}{10^{n-i}} \right\rfloor
      &\text{($1 \leq i \leq n$)} \\
    x_{n+j} &= \text{last digit of } \left\lfloor \frac{r}{10^{m-j}} \right\rfloor
      &\text{($1 \leq j \leq m$)}
  \end{align*}
  \end{enumerate}
\item[(3)]
$(\Longrightarrow)$
(Pigeonhole principle)
Given any $x = p/q$ where $p, q \in \mathbb{Z}$, $q > 0$.
  \begin{enumerate}
  \item[(a)]
  Might assume $p \geq 0$.
  (If $p < 0$, apply the same argument to $-p$
  and then add the minus symbol ``$-$'' in the front of the decimal expansion.)
  Write $$x = x_0.x_1 x_2 \cdots.$$
  \item[(b)]
  Apply Euclidean algorithm to get
  $$p = x_0 q + r_0 \qquad \text{with} \qquad 0 \leq r_0 < q.$$
  $x_0$ is the integer part of $x = p/q$.
  Continue Euclidean algorithm to get $x_1$ by
  $$10 r_0 = x_1 q + r_1 \qquad \text{with} \qquad 0 \leq r_1 < q.$$
  In general, for $n \geq 1$, $x_n$ is given by
  $$10 r_{i-1} = x_i q + r_i \qquad \text{with} \qquad 0 \leq r_i < q.$$
  \item[(c)]
  The pigeonhole principle shows that there must be two equal remainders,
  that is, $$r_n = r_{n+m} \qquad \text{with} \qquad m > 0.$$
  By induction, $r_{n+k} = r_{n+m+k}$ for any $k \geq 0$.
  Thus $x_{n+k} = x_{n+m+k}$ holds for any $k > 0$, that is,
  $x$ has a decimal expansion
  $$x = x_0.x_1 x_2 \cdots x_n \overline{x_{n+1} \cdots x_{n+m}}.$$
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof (Exercise 2.17).}
Let $A$ be the set of all $y \in [0,1]$
whose decimal expansion contains only the digits $4$ and $7$.
Though $A \cap \mathbb{Q} \neq \varnothing$ since $\frac{4}{9} \in A$,
we can shift $A$ by a number $\xi = \sum_{n=0}^{\infty} 10^{-n!}$ (Exercise 2.3),
that is, we construct
$$E = \{ y + \xi : y \in A \}$$
and show that $E$ is our desired nonempty perfect set in $\mathbb{R}-\mathbb{Q}$.
\begin{enumerate}
\item[(1)]
Any number $x \in E$ has decimal expansion
$x = 0.x_1 x_2 \cdots$ with
$x_n \in \{5,8\}$ if $n$ is a factorial number; otherwise $x_n \in \{4,7\}$.
\item[(2)]
$E$ is a perfect set (Exercise 2.17).
\item[(3)]
$E \subseteq \mathbb{R}-\mathbb{Q}$.
It suffices to show that each $x \in E$ has no repeating decimal expansions (Lemma).
It is clear by the construction of $\xi = \sum_{n=0}^{\infty} 10^{-n!}$.
\end{enumerate}
$\Box$ \\



\emph{Proof (Exercise 2.3).}
Let $E$ be a subset of Liouville numbers as
$$E = \left\{ \sum_{n=0}^{\infty} \frac{a_n}{10^{n!}} : a_n \in \{4, 7\} \right\}.$$
$E$ is perfect. (The same argument of Exercise 2.17.)
Besides,
all numbers of $E$ are transcendental.
(Set $k_j = 10^{j!}$ and $h_j = 10^{j!} \sum_{n=0}^{j} \frac{a_n}{10^{n!}}$
and apply the same argument of Exercise 2.3.)
$\Box$ \\

\emph{Note.}
Or using Lemma to prove all numbers of $E$ are irrational. \\

\emph{Proof (Theorem 3.32).}
Let
$$E = \left\{ \sum_{n=1989}^{\infty} \frac{a_n}{n!} : a_n \in \{6, 4\} \right\}.$$
$E$ is perfect. (The same argument of Exercise 2.17.)
Besides,
all numbers of $E$ are irrational (The same argument of Theorem 3.32.)
$\Box$ \\

\emph{Proof (Non constructive existence proof).}
By Cantor-Bendixson theorem (Exercise 2.28),
\emph{it suffices to find a uncountable closed set in $\mathbb{R} - \mathbb{Q}$.}
\begin{enumerate}
\item[(1)]
Write $\mathbb{Q} = \{ r_1, r_2, \ldots \}$ since $\mathbb{Q}$ is countable.
Let $$I_n = B\left(r_n;\frac{1}{2^{n+1}}\right) \supseteq \{r_n\}$$
and
$$A = \bigcup_{n=1}^{\infty} I_n \supseteq \mathbb{Q}.$$
Hence $A$ is an open subset in $\mathbb{R}$.
\item[(2)]
Let $E = \mathbb{R} - A$.
By construction, $E$ is closed and $E \cap \mathbb{Q} = \varnothing$.
\item[(3)]
\emph{Show that $E$ is uncountable.}
\emph{It suffices to show that $m^{*}(E) > 0$.}
In fact, the outer measure of $U$ is
$$m^{*}(A) \leq \sum_{n=1}^{\infty} m^{*}(I_n) = \sum_{n=1}^{\infty} \frac{1}{2^n} = 1.$$
Thus,
$$m^{*}(E) \geq m^{*}(\mathbb{R}) - m^{*}(A) = \infty - 1 = \infty.$$
\end{enumerate}
Hence, the set of all condensation points of $E$ is our desired
nonempty perfect set in $\mathbb{R}-\mathbb{Q}$.
$\Box$ \\

\emph{Note.}
In fact, we can replace $\mathbb{Q}$
by the set of all real algebraic numbers (Exercise 2.2). \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.19.}
\addcontentsline{toc}{subsection}{Exercise 2.19.}
\begin{enumerate}
\item[(a)]
\emph{If $A$ and $B$ are disjoint closed sets in some metric space $X$,
prove that they are separated.}
\item[(b)]
\emph{Prove the same for disjoint open sets.}
\item[(c)]
\emph{Fix $p \in X$, $\delta > 0$,
define $A$ to be the set of all $q \in X$ for which $d(p,q) < \delta$,
define $B$ similarly, with $>$ in place of $<$.
Prove that $A$ and $B$ are separated.}
\item[(d)]
\emph{Prove that every connected metric space with at least two points
is uncountable.  Hint: Use (c).} \\
\end{enumerate}

\emph{Proof of (a).}
Since
\begin{align*}
A \cap \overline{B}
&= A \cap B
  &\text{($B$ is closed)} \\
&= \varnothing,
  &\text{($A$ and $B$ are disjoint)} \\
\overline{A} \cap B
&= A \cap B
  &\text{($A$ is closed)} \\
&= \varnothing.
  &\text{($A$ and $B$ are disjoint)}
\end{align*}
$A$ and $B$ are separated.
$\Box$ \\

\emph{Proof of (b)(Theorem 2.27(c)).}
Note that $\widetilde{A}$ is a closed set containing $B$.
Since $\overline{B}$ is the smallest closed set containing $B$,
$\widetilde{A} \supseteq \overline{B}$ (Theorem 2.27(c)).
Hence $$A \cap \overline{B} \subseteq A \cap \widetilde{A} = \varnothing.$$
Similarly, $\overline{A} \cap B = \varnothing$.
Hence $A$ and $B$ are separated.
$\Box$ \\

\emph{Proof of (c).}
Since both
$$A = \{ q \in X : d(p,q) < \delta \} \text{ and } B = \{ q \in X : d(p,q) > \delta \}$$
are open in $X$, they are separated by (b).
$\Box$ \\

\emph{Proof of (d).}
Let $X$ be a connected metric space.
\begin{enumerate}
\item[(1)]
Let $p, q \in X$ with $p \neq q$.
Hence $d_X(p,q) = r > 0$ (Definition 2.15(a)).
\item[(2)]
Given any $\delta \in (0, r)$.
Define
$$A = \{ x \in X : d(p,x) < \delta \} \text{ and } B = \{ x \in X : d(p,x) > \delta \}.$$
$p \in A \neq \varnothing$ and $q \in B \neq \varnothing$.
\item[(3)]
If there were no $y_{\delta} \in X$ such that $d(p,y_{\delta}) = \delta$,
we can write $X = A \cup B$ as a union of two nonempty separated sets ((c)),
contrary to the connectedness of $X$.
\item[(4)]
Collect these $y$ as $E$.
Since $d$ is a function, there is a one-to-one map from $(0,r)$ to $E$
defined by $\delta \mapsto y_{\delta}$ in (3).
Since $(0,r)$ is uncountable, $X \supseteq E$ is uncountable.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.20.}
\addcontentsline{toc}{subsection}{Exercise 2.20.}
\emph{Are closures and interiors of connected sets always connected?
(Look at subsets of $\mathbb{R}^2$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Interiors of connected sets are not always connected.}
Let $X = \mathbb{R}^2$ with the usual Euclidean metric be a metric space.
Take
$$E = B(89;1) \bigcup B(64;1) \bigcup \{ (x,0) \in \mathbb{R}^2 : 64 \leq x \leq 89 \}.$$
$E$ is connected and
$$E^{\circ} = B(89;1) \bigcup B(64;1)$$
is disconnected.
\item[(2)]
\emph{Closures of connected sets are always connected.}
\emph{It suffices to show that
$E$ is disconnected if $\overline{E}$ is disconnected.}
  \begin{enumerate}
  \item[(a)]
  Write $\overline{E} = A \cup B$ as a union of two nonempty separated sets.
  Here $A \neq \varnothing$, $B \neq \varnothing$,
  $A \cap \overline{B} = \varnothing$ and $\overline{A} \cap B = \varnothing$.
  \item[(b)]
  Write
  $$E = (A \cap E) \bigcup (B \cap E)$$
  and we will show that $E$ is disconnected.
  \item[(c)]
  \emph{Show that $A \cap E$ and $B \cap E$ are separated.}
  In fact,
  $$(A \cap E) \cap \overline{B \cap E}
  \subseteq A \cap \overline{B} = \varnothing,$$
  $$\overline{A \cap E} \cap (B \cap E)
  \subseteq \overline{A} \cap B = \varnothing.$$
  \item[(d)]
  \emph{Show that $A \cap E$ and $B \cap E$ are nonempty.}
  (Reductio ad absurdum)
  If $A \cap E = \varnothing$, then
  $$E = (A \cap E) \bigcup (B \cap E) = B \cap E \Longrightarrow E \subseteq B.$$
  So
  \begin{align*}
  A &= (A \cup B) \bigcap A
    &(A \subseteq A \cup B) \\
  &= \overline{E} \bigcap A \\
  &\subseteq \overline{B} \bigcap A
    &(E \subseteq B) \\
  &= \varnothing
  \end{align*}
  which contradicts $A \neq \varnothing$ in (a).
  Therefore, $A \cap E \neq \varnothing$.
  Similarly, $B \cap E \neq \varnothing$.
  \end{enumerate}
Hence, $E$ is disconnected if $\overline{E}$ is disconnected,
or closures of connected sets are always connected.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.21.}
\addcontentsline{toc}{subsection}{Exercise 2.21.}
\emph{Let $A$ and $B$ be separated subsets of some $\mathbb{R}^k$,
suppose $\mathbf{a} \in A$, $\mathbf{b} \in B$, and define
$$\mathbf{p}(t) = (1-t)\mathbf{a} + t\mathbf{b}$$
for $t \in \mathbb{R}^1$.
Put $A_0 = \mathbf{p}^{-1}(A)$,
$B_0 = \mathbf{p}^{-1}(B)$.
[Thus $t \in A_0$ if and only if $\mathbf{p}(t) \in A$.]}
\begin{enumerate}
\item[(a)]
\emph{Prove that $A_0$ and $B_0$ are separated subsets of $\mathbb{R}^1$.}
\item[(b)]
\emph{Prove that there exists $t_0 \in (0,1)$ such that
$\mathbf{p}(t_0) \notin A \bigcup B$.}
\item[(c)]
\emph{Prove that every convex subset of $\mathbb{R}^k$ is connected.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
Note that
  \begin{enumerate}
  \item[(a)]
  $\mathbf{a} \neq \mathbf{b}$ or $|\mathbf{a} - \mathbf{b}| > 0$
  since $A \cap B = \varnothing$.
  \item[(b)]
  $|\mathbf{p}(t) - \mathbf{p}(s)| = |t-s||\mathbf{a} - \mathbf{b}|$
  by a direct calculation.
  \item[(c)]
  $\mathbf{p}(t) = \mathbf{p}(s)$ if and only if $t = s$ by (a)(b).
  \end{enumerate}
\item[(2)]
\emph{Show that $A_0 \cap \overline{B_0} = \varnothing$.}
(Reductio ad absurdum)
If there were $t \in A_0 \cap \overline{B_0}$,
then $t \in A_0$ and $t$ is a limit point of $B_0$.
  \begin{enumerate}
  \item[(a)]
  $t \in A_0$ implies that $\mathbf{p}(t) \in A$.
  \item[(b)]
  \emph{Show that $t$ is a limit point of $B_0$ $\Longrightarrow$
  $\mathbf{p}(t)$ is a limit point of $B$.}
  Given any $\varepsilon > 0$,
  there is $s \in B_0$ such that
  $$|t-s| < \frac{\varepsilon}{|\mathbf{a} - \mathbf{b}|}
  \qquad \text{with} \qquad s \neq t$$
  since $t$ is a limit point of $B_0$.
  So by (1),
  $$|\mathbf{p}(t) - \mathbf{p}(s)| = |t-s||\mathbf{a} - \mathbf{b}|
  < \varepsilon.$$
  Here $\mathbf{p}(s) \in B$ and $\mathbf{p}(s) \neq \mathbf{p}(t)$.
  So $\mathbf{p}(t)$ is a limit point of $B$.
  \end{enumerate}
  By (a)(b), $\mathbf{p}(t) \in A \cap \overline{B} = \varnothing$,
  contrary to the assumption that $A$ and $B$ are separated.
\item[(3)]
\emph{Show that $\overline{A_0} \cap B_0 = \varnothing$.}
Similar to (2).
\end{enumerate}
By (2)(3), $A_0$ and $B_0$ are separated.
$\Box$ \\

\emph{Proof of (b).}
(Reductio ad absurdum)
If $\mathbf{p}(t)$ were in $A \bigcup B$ for all $t \in (0,1)$,
we will \emph{show that $[0,1]$ is separated by $A_0 \cap [0,1]$ and $B_0 \cap [0,1]$
to get a contradiction.}
\begin{enumerate}
\item[(1)]
  $\mathbf{p}(t)$ were in $A \bigcup B$ for all $t \in [0,1]$
  since
  $\mathbf{p}(0) = \mathbf{a} \in A \bigcup B$
  and
  $\mathbf{p}(1) = \mathbf{b} \in A \bigcup B$.
  Therefore,
  $$[0,1]
  \subseteq \mathbf{p}^{-1}(A \cup B)
  = \mathbf{p}^{-1}(A) \cup \mathbf{p}^{-1}(B)
  = A_0 \cup B_0.$$
\item[(2)]
  Let $A_1 = A_0 \cap [0,1]$ and $B_1 = B_0 \cap [0,1]$.
  So $[0,1] = A_1 \bigcup B_1$.
\item[(3)]
  \emph{Show that $A_1 \neq \varnothing$ and $B_1 \neq \varnothing$.}
  \begin{align*}
  \mathbf{p}(0) \in A
  &\Longleftrightarrow 0 \in \mathbf{p}^{-1}(A) = A_0 \\
  &\Longleftrightarrow 0 \in A_0 \text{ and } 0 \in [0,1] \\
  &\Longleftrightarrow 0 \in A_0 \cap [0,1] = A_1.
  \end{align*}
  Similarly, $1 \in B_1$. \\

  \emph{Note.} That's why we consider $[0,1]$ instead of $(0,1)$.
\item[(4)]
  \emph{Show that $A_1 \cap \overline{B_1} = \varnothing$
  and $\overline{A_1} \cap B_1 = \varnothing$.}
  Since $A_1 \subseteq A_0$ and $B_1 \subseteq B_0$,
  $A_1 \cap \overline{B_1} \subseteq A_0 \cap \overline{B_0} = \varnothing$
  or $A_1 \cap \overline{B_1} = \varnothing$.
  Similarly, $\overline{A_1} \cap B_1 = \varnothing$.
\end{enumerate}
By (2)(3)(4), $[0,1]$ is separated, contrary to the connectedness of $[0,1]$
(Theorem 2.47).
$\Box$ \\

\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
Let $E$ be a convex subset of $\mathbb{R}^k$.
Recall
$$\mathbf{p}(t) = (1-t)\mathbf{a} + t\mathbf{b} \in E$$
whenever $\mathbf{a}, \mathbf{b} \in E$ and $t \in (0,1)$.
\item[(2)]
(Reductio ad absurdum)
If $E$ were separated by $A$ and $B$, pick $\mathbf{a} \in A \subseteq E$
and $\mathbf{b} \in B \subseteq E$.
\item[(3)]
By (b), there exists $t_0 \in (0,1)$ such that $\mathbf{p}(t_0) \not\in A \cup B = E$,
contrary to the convexity of $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.22.}
\addcontentsline{toc}{subsection}{Exercise 2.22.}
\emph{A metric space is called separable if it contains a countable dense subset.
Show that $\mathbb{R}^k$ is separable.
(Hint: Consider the set of points which have only rational coordinates.)} \\

\emph{Proof.}
Let $E$ be the set of points which have only rational coordinates.
\begin{enumerate}
\item[(1)]
\emph{Show that $E$ is countable.} $\mathbb{Q}$ is countable
and thus $E = \mathbb{Q}^k$ is countable (Theorem 2.13).
\item[(2)]
\emph{Show that $E$ is dense.}
Given any $\mathbf{p} = (p_1, \ldots, p_k) \in \mathbb{R}^k$.
We want to show that $\mathbf{p}$ is a limit point of $E$.
  \begin{enumerate}
  \item[(a)]
  Given any open neighborhood $B(\mathbf{p};r)$ of $\mathbf{p}$, $r > 0$.
  \item[(b)]
  Since $\mathbb{Q}$ is dense in $\mathbb{R}$ (Theorem 1.20),
  every coordinate of $\mathbf{p}$ is a limit point of $\mathbb{Q}$.
  In particular, for every $i = 1, 2, \ldots, k$,
  the open neighborhood $B\left(p_i, \frac{r}{\sqrt{k}}\right)$ of $p_i$
  contains a point $q_i \neq p_i$ and $q_i \in \mathbb{Q}$.
  \item[(c)]
  Collect all $q_i$ in (b)
  and define $\mathbf{q} = (q_1, \ldots, q_k) \in \mathbb{Q}^k = E$.
  By construction $\mathbf{q} \neq \mathbf{p}$ and
  \begin{align*}
    |\mathbf{p} - \mathbf{q}|
    &= \sqrt{(p_1 - q_1)^2 + \cdots + (p_k - q_k)^2} \\
    &< \sqrt{\left(\frac{r}{\sqrt{k}}\right)^2 + \cdots
      + \left(\frac{r}{\sqrt{k}}\right)^2} \\
    &= \sqrt{k \cdot \frac{r^2}{k}} \\
    &= r
  \end{align*}
  or $\mathbf{q} \in B(\mathbf{p};r)$.
  \end{enumerate}
  By (a)(b)(c), $E$ is dense in $\mathbb{R}^k$.
\end{enumerate}
By (1)(2), $\mathbb{R}^k$ is separable.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.23.}
\addcontentsline{toc}{subsection}{Exercise 2.23.}
\emph{A collection $\{V_\alpha\}$ of open subsets of $X$ is
said to be a base for $X$ if the following is true:
For every $x \in X$ and every open set $G \subseteq X$ such that $x \in G$,
we have $x \in V_\alpha \subseteq G$ for some $\alpha$.
In other words, every open set in $X$ is the union of a subcollection of $\{V_\alpha\}$.} \\

\emph{Prove that every separable metric space has a countable base.
(Hint: Take all neighborhoods with rational radius and center
in some countable dense subset of $X$.)} \\

\emph{Note.}
$\mathbb{R}^k$ has a countable base (Exercise 2.22). \\

\emph{Proof (Hint).}
Let $X$ be a separable metric space,
and $E$ be a countable dense subset of $X$.
Let $\mathscr{B}$ be a collection of
all neighborhoods with rational radius and center in $E$.
\begin{enumerate}
\item[(1)]
$\mathscr{B}$ is countable (Theorem 2.12).
\item[(2)]
\emph{$\mathscr{B}$ is a base for $X$.}
Similar to Exercise 2.9(a).
Given any $p \in X$ and every open set $G \subseteq X$ such that
$p \in G$.
Since $p$ is in an open set $G$,
there exists an open neighborhood $B(p;r)$ of $p$ such that $B(p;r) \subseteq G$.
\item[(3)]
Let $r_0$ be rational such that $0 < r_0 < \frac{r}{2}$ (Theorem 1.20(b)).
Since $E$ is dense in $X$, there is $q \in E$ such that $d_X(p,q) < r_0$.
For such $r_0 \in \mathbb{Q}$ we pick an open neighborhood $B(q;r_0)$ of $q$.
Clearly, $B(q;r_0) \in \mathscr{B}$.
\item[(4)]
$p \in B(q;r_0)$ since $d_X(p,q) < r_0$.
\item[(5)]
\emph{Show that $B(q;r_0) \subseteq B(p;r) \subseteq G$.}
For any $z \in B(q;r_0)$, $d_X(z,p) \leq d_X(z,q)+ d_X(q,p) < r_0 + r_0
< \frac{r}{2} + \frac{r}{2} = r$.
That is, $z \in B(p;r)$.
\end{enumerate}
By (3)(4)(5), (2) is established.
By (1)(2), $\mathscr{B}$ is a countable base for $X$.
$\Box$ \\



\textbf{Supplement.}
\begin{enumerate}
\item[(1)]
In topology, a second-countable space, also called a completely separable space,
is a topological space whose topology has a countable base.
\item[(2)]
Every second-countable space is separable.
\item[(3)]
The reverse implication of (2) does not hold in general.
However, for metric spaces
the properties of being second-countable and separable are equivalent.
\item[(4)]
\emph{Show that every second-countable metric space $X$ is separable.}
  \begin{enumerate}
  \item[(a)]
  Let $\mathscr{B} = \{ B_n : n \in \mathbb{Z}^+ \}$ be a countable base of $X$.
  \item[(b)]
  For every $B_n \in \mathscr{B}$, pick any point $p_n$ of $B_n$
  and collect them as $$E = \{ p_n : p_n \in B_n \text{ for } n \in \mathbb{Z}^+ \}.$$
  \item[(c)]
  $E$ is countable.
  \item[(d)]
  \emph{Show that $E$ is dense.}
  Given any $x \in X$.
  For any open neighborhood $B(x)$ of $x$,
  $B(x)$ is a union of subcollection of $\mathscr{B}$.
  That is,
  there is always a point in $E$ by the construction of $E$.
  \end{enumerate}
  $\Box$ \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.24.}
\addcontentsline{toc}{subsection}{Exercise 2.24.}
\emph{Let $X$ be a metric space in which every infinite subset has a limit point.
Prove that $X$ is separable.} \\

\emph{(Hint: Fix $\delta>0$, and pick $x_1 \in X$.
Having chosen $x_1, \ldots, x_j \in X$, choose $x_{j+1}$, if possible,
so that $d(x_i,x_{j+1}) \geq \delta$ for $i=1,\ldots,j$.
Show that this process must stop after finite number of steps,
and that $X$ can therefore be covered by finite many neighborhoods of radius $\delta$.
Take $\delta = \frac{1}{n}$ $(n=1,2,3,\ldots)$
and consider the centers of the corresponding neighborhoods.) } \\

\emph{Note.}
The reverse implication does not hold (Exercise 2.10). \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Fix $\delta>0$, and pick $x_1 \in X$.
\emph{Show that every limit point compact metric space $X$ is totally bounded.}
  \begin{enumerate}
  \item[(a)]
  Having chosen $x_1, \ldots, x_j \in X$, choose $x_{j+1}$, if possible,
  so that $d(x_i,x_{j+1}) \geq \delta$ for $i=1,\ldots,j$.
  Let $E_\delta$ be the set of these $x_i$.
  \item[(b)]
  \emph{Show that this process must stop after finite number of steps,
  and that $X$ can therefore be covered by finite many neighborhoods of radius $\delta$.}
  (Reductio ad absurdum)
    \begin{enumerate}
    \item[(i)]
    If not, $E_\delta$ is an infinite subset of $X$.
    By assumption there is a limit point of $E_\delta$, say $p \in X$.
    \item[(ii)]
    In particular, an open neighborhood $B\left(p;\frac{\delta}{64}\right)$ of $p$
    contains a point $x_n \in E_\delta$ with $p \neq x_n$.
    \item[(iii)]
    The neighborhood $B\left(p;\frac{\delta}{64}\right)$ contains no other point
    $x_m \in E_\delta$ with $m \neq n$.
    If so,
    $$d_X(x_n,x_m) \leq d_X(x_n,p)+d_X(p,x_m)
    < \frac{\delta}{64} + \frac{\delta}{64} < \delta,$$
  contrary to the construction of $E_\delta$.
    \item[(iv)]
    Note that $p \not\in E_\delta$ as a corollary to (iii).
    \item[(v)]
    So another open neighborhood $B\left(p;r\right)$ of $p$ with $r = d_X(p,x_n) > 0$
    contains no points $x_m \in E_\delta$ with $p \neq x_m$,
    contrary to the assumption that $p$ is a limit point of $E_\delta$.
    \end{enumerate}
  \end{enumerate}
\item[(2)]
\emph{Show that every totally bounded metric space $X$ is separable.}
Take $\delta = \frac{1}{n}$ $(n=1,2,3,\ldots)$ in (1),
and union all $E_{\frac{1}{n}}$ as
$$E = \bigcup_{n=1}^{\infty} E_{\frac{1}{n}} \subseteq X.$$
\emph{Show that $E$ is a countable dense subset of $X$.}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $E$ is countable.}
  Since $E$ is the countable union of finite set $E_{\frac{1}{n}}$,
  $E$ is countable (Theorem 2.12).
  \item[(b)]
  \emph{Show that $E$ is dense in $X$.}
  Given any $p \in X$.
  \emph{It suffices to show that given any open neighborhood $B(p;r)$ of $p \in X-E$,
  there exists $q \in E$ such that $q \in B(p;r)$.}
  Pick any $n \in \mathbb{Z}^+$ such that $\frac{1}{n} < r$ (Theorem 1.20(a)).
  By the construction of $E_{\frac{1}{n}}$,
  there is $q  \in E_{\frac{1}{n}}$ such that $p \in B\left(q;\frac{1}{n}\right)$,
  or $d_X(p,q) < \frac{1}{n} < r$, or $q \in  B(p;r)$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\textbf{Supplement.}
\begin{enumerate}
\item[(1)]
A topological space $X$ is said to be limit point compact or weakly countably compact if every infinite subset of $X$ has a limit point in $X$.
\item[(2)]
In a metric space, limit point compactness, compactness, and sequential compactness are
all equivalent.
For general topological spaces, however, these three notions of compactness are not equivalent.
\item[(3)]
A metric space $X$ is totally bounded if and only if for every real number $\delta >0$,
there exists a finite collection of open balls in $X$ of radius $\delta$
whose union contains $X$. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.25.}
\addcontentsline{toc}{subsection}{Exercise 2.25.}
\emph{Prove that every compact metric space $K$ has a countable base,
and that $K$ is therefore separable.
(Hint: For every positive integer $n$,
there are finitely many neighborhood of radius $\frac{1}{n}$ whose union
covers $K$.)} \\

\emph{Proof (Exercise 2.24(a)).}
\begin{enumerate}
\item[(1)]
\emph{Show that every compact metric space $K$ is limit point compact.}
Given any subset $E \subseteq K$.
It suffices to show that if $E$ has no limit point, then $E$ must be finite.
  \begin{enumerate}
  \item[(a)]
  Since $E$ has no limit point, $E$ is closed.
  \item[(b)]
  For any point $p \in E$. Since $p$ is not a limit point,
  there is an open neighborhood $B(p)$ such that $B(p)$ contains no point other than $p$.
  \item[(c)]
  Similar to the proof of Theorem 2.35,
  let
  $$\mathscr{F} = \{ B(p) : p \in E \text{ with } B(p) \cap E = \{p\} \}
  \bigcup \widetilde{E}.$$
  Hence $\mathscr{F}$ is an open covering of $K$.
  \item[(d)]
  Since $K$ is compact by assumption,
  there is an finitely subcovering $\mathscr{F}'$ of $K$.
  Since $\widetilde{E}$ does not intersect $E$,
  each $B(p) \in \mathscr{F}'$
  contains only one point of $E$ and so $E$ is finite.
  \end{enumerate}
\item[(2)]
Since $K$ is limit point compact, $K$ is separable (Exercise 2.24).
\end{enumerate}
$\Box$ \\

\emph{Proof (Exercise 2.24(b)).}
\begin{enumerate}
\item[(1)]
\emph{Show that every compact metric space $K$ is totally bounded.}
Given any real number $\delta > 0$,
define an open covering $\mathscr{F}$ of $K$ by
$$\mathscr{F} = \{ B(p;\delta) : p \in K \}.$$
Since $K$ is compact,
there exists a finite subcovering $\mathscr{F}'$ of $K$.
$\mathscr{F}'$ is our desired finite collection of open balls in $X$
of radius $\delta$ whose union contains $X$.
\item[(2)]
Since $K$ is totally bounded, $K$ is separable (Exercise 2.24).
\end{enumerate}
$\Box$ \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Given any positive integer $n > 0$,
define an open covering $\mathscr{F}_n$ of $K$ by
$$\mathscr{F}_n = \left\{ B\left(p;\frac{1}{n}\right) : p \in K \right\}.$$
Since $K$ is compact,
there exists a finite subcovering $\mathscr{G}_n$ of $K$.
\item[(2)]
\emph{Show that every compact metric space $K$ is second-countable.}
  \begin{enumerate}
  \item[(a)]
  Define $$\mathscr{B} = \bigcup_{n \geq 1} \mathscr{G}_n$$ be a collection.
  Since $\mathscr{B}$ is a countable union of finite set $\mathscr{G}_n$,
  $\mathscr{B}$ is countable.
  Hence it suffices to show that
  for every $p \in K$ and every open set $G \subseteq K$ such that $p \in G$,
  there is $B \in \mathscr{B}$ such that $x \in B \subseteq G$.
  \item[(b)]
  Since $G$ is open,
  there is an open neighborhood $B(p;r)$ of $p$ such that $B(p;r) \subseteq G$.
  \item[(c)]
  For such $r > 0$, there is $n \in \mathbb{Z}^+$
  with $0 < \frac{1}{n} < \frac{r}{2}$ (Theorem 1.20(a)).
  So $p$ is in some $B\left(q;\frac{1}{n}\right) \in \mathscr{G}_n \subseteq \mathscr{B}$
  since $\mathscr{G}_n$ is a subcovering of $K$.
  \item[(d)]
  \emph{Show that $B\left(q;\frac{1}{n}\right) \subseteq B(p;r) \subseteq G$.}
  For any $z \in B(q;\frac{1}{n})$,
  $$d_K(z,p) \leq d_K(z,q)+ d_K(q,p) < \frac{1}{n} + \frac{1}{n}
  < \frac{r}{2} + \frac{r}{2} = r.$$
  That is, $z \in B(p;r)$, or $B\left(q;\frac{1}{n}\right) \subseteq B(p;r) \subseteq G$.
  \end{enumerate}
  By (a)(b)(c)(d), $K$ is second-countable.
\item[(3)]
\emph{Show that every second-countable metric space is separable.}
Supplement (4) to Exercise 2.23.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.26.}
\addcontentsline{toc}{subsection}{Exercise 2.26.}
\emph{Let $X$ be a metric space in which every infinite subsets has a limit point.
Prove that $X$ is compact.} \\

\emph{By Exercises 2.23 and 2.24, $X$ has a countable base.
It follows that every open cover of $X$ has a countable subcovering $\{G_n\}$,
$n = 1, 2, 3, \ldots$.
If no finite subcollection of $\{G_n\}$ covers $X$,
then the complement $F_n$ of $G_1 \cup \cdots \cup G_n$
is nonempty for each $n$, but $\bigcap F_n$ is empty.
If $E$ is a set contains a point from each $F_n$,
consider a limit point of $E$, and obtain a contradiction.} \\

\emph{Note.} In every metric space, we have
\begin{align*}
\{ \text{compact} \}
&\Longleftrightarrow
\{ \text{limit point compact} \} \\
&\Longleftrightarrow
\{ \text{complete and totally bounded} \} \\
&\Longrightarrow
\{ \text{totally bounded} \} \\
&\Longrightarrow
\{ \text{separable} \} \\
&\Longleftrightarrow
\{ \text{second-countable} \} \\
&\Longleftrightarrow
\{ \text{Lindelof} \}. \\
\end{align*}

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Since $X$ is limit point compact, $X$ is separable (Exercise 2.24).
Since $X$ is separable, $X$ is second-countable (Exercise 2.23).
\item[(2)]
\emph{Show that $X$ is Lindelof if $X$ is second-countable.}
Let $X$ be a second-countable metric space.
Let $\mathscr{B} = \{ B_n \}$ be a countable base of $X$.
  Given any open covering $\mathscr{F}$ of $X$.
  \begin{enumerate}
  \item[(a)]
  Iterate each $B_n \in \mathscr{B}$, pick one $G_n \in \mathscr{F}$ containing $B_n$,
  and collect them as
  $$\mathscr{G} = \{ G_n : G_n \supseteq B_n \text{ for } n \in \mathbb{Z}^+ \}.$$
  ($G_n$ might be duplicated.)
  \item[(b)]
  $\mathscr{G}$ is a countable subset of $\mathscr{F}$.
  \item[(c)]
  $\mathscr{G}$ covers $X$
  since $\mathscr{B}$ is a countable base of $X$.
  \end{enumerate}
\item[(3)]
Hence,
given any open covering $\mathscr{F}$ of $X$,
there is a countable subcovering $\mathscr{G} = \{ G_n \}$ of $X$.
(Reductio ad absurdum)
If there were no finite subcovering of $\mathscr{G}$,
then the complement $F_n$ of $G_1 \cup \cdots \cup G_n$
is nonempty for each $n$, but $\cap F_n$ is empty.
\item[(4)]
Let $E$ bet a set contains a point from each $F_n$.
$E$ is infinite and thus $E$ has a limit point, say $p$.
$p \in G_n$ for some $n$ since $\mathscr{G} = \{ G_n \}$ is an open covering of $X$.
Since $G_n$ is open, there is an open neighborhood $B(p)$ of $p$ such that $B(p) \subseteq G_n$.
By the construction of $F_n$,
$$B(p) \cap F_m = \varnothing$$ whenever $m \geq n$,
contrary to the assumption that $p$ is a limit point of $E$.
\end{enumerate}
Hence, $X$ is compact if $X$ is limit point compact.
$\Box$ \\



\textbf{Supplement.}
\begin{enumerate}
\item[(1)]
Lindelof space is a topological space in which every open covering has a countable subcovering.
\item[(2)]
\emph{Show that $X$ is second-countable if $X$ is Lindelof.}
Same as the Proof (Hint) of Exercise 2.25 except
changing the word ``compact'' to ``Lindelof'' and ``finite'' to ``countable.''
$\Box$
\item[(3)]In every metric space, we have
\begin{align*}
\{ \text{compact} \}
&\Longleftrightarrow
\{ \text{limit point compact} \}
\Longleftrightarrow
\{ \text{sequentially compact} \}. \\
\end{align*}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.27.}
\addcontentsline{toc}{subsection}{Exercise 2.27.}
\emph{
Define a point $p$ in a metric space $X$ to be a {\it condensation
point} of a set $E \subseteq X$ if every neighborhood of $p$ contains
uncountably many points of $E$.} \\

\emph{Suppose $E \subseteq \mathbb{R}^k$, $E$ is uncountable,
and let $P$ be the set of all condensation points of $E$.
Prove that $P$ is perfect and that at most countably many points of $E$ are not in $P$.
In other words, show that $\widetilde{P} \cap E$ is at most countable.} \\

\emph{(Hint: Let $\{V_n\}$ be a countable base of $\mathbb{R}^k$,
let $W$ be the union of those $V_n$ for which $E \cap V_n$ is at most countable,
and show that $P = \widetilde{W}$.)} \\

\emph{Note.}
The statement is also true for separable metric space. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let $\{V_n\}$ be a countable base of $\mathbb{R}^k$ (Exercise 2.22 and 2.23).
Let $W$ be the union of those $V_n$ for which $E \cap V_n$ is at most countable.
\item[(2)]
\emph{Show that $P = \widetilde{W}$.}
  \begin{enumerate}
  \item[(a)]
  $(P \subseteq \widetilde{W})$
  Given any $x \in P$.
  \begin{align*}
  x \in P
  \Longrightarrow&
  \text{$x$ is a condensation points of $E$} \\
  \Longrightarrow&
  \text{$\forall \: V_n \ni x, \exists B(x) \subseteq V_n$ such that
    $E \cap B(x)$ is uncountable} \\
  \Longrightarrow&
  \text{$E \cap V_n$ is uncountable} \\
  \Longrightarrow&
  x \not\in W.
  \end{align*}
  \item[(b)]
  $(P \supseteq \widetilde{W})$
  Given any $x \in \widetilde{W}$.
  Let $P(V_n)$ be the proposition that $E \cap V_n$ is at most countable.
  \begin{align*}
  x \in \widetilde{W}
  \Longrightarrow&
  x \not\in W = \bigcup_{P(V_n)} V_n \\
  \Longrightarrow&
  \text{$x \not\in V_n$ for which $E \cap V_n$ is at most countable} \\
  \Longrightarrow&
  \text{$\forall$ $B(x)$ of $x$, $x \in V_m \subseteq B(x)$ for some $V_m$}
    &\text{($\{V_n\}$: base of $X$)} \\
  \Longrightarrow&
  \text{$E \cap V_m$ is uncountable} \\
  \Longrightarrow&
  \text{$E \cap B(x) \supseteq E \cap V_m$ is uncountable} \\
  \Longrightarrow&
  \text{$x$ is a condensation point of $E$} \\
  \Longrightarrow&
  x \in P.
  \end{align*}
  \end{enumerate}
\item[(3)]
\emph{Show that $P$ is closed.}
$P$ is the complement of an open subset $W$.
\item[(4)]
\emph{Show that $P \subseteq P'$.}
(Reductio ad absurdum)
  \begin{enumerate}
  \item[(a)]
  If there were an isolated point $x \in P$, then
  there exists an open neighborhood $B(x)$ of $x$ such that $B(x) \cap P = \{x\}$.
  \item[(b)]
  Since $x$ is a condensation point of $E$,
  there are uncountably many points of $E$ in $B(x)$,
  and such points $y$ are not a condensation points of $E$ except $y = x$.
  \item[(c)]
  Given any point $y \in E \cap B(x)$ with $y \neq x$.
  Since $y$ is not a condensation point,
  there exists a neighborhood $B(y)$ of $y$ such that $B(y) \cap E$ is at most countable.
  Since $\{V_n\}$ is a base,
  for each $B(y)$ there exists $V_{n(y)}$ such that $y \in V_{n(y)} \subseteq B(y)$.
  Hence $$V_{n(y)} \cap E \subseteq B(y) \cap E$$
  is at most countable.
  \item[(d)]
  Hence,
  \begin{align*}
  E \cap B(x) - \{x\}
  &\subseteq \bigcup_{y \in E \cap B(x) - \{x\}} V_{n(y)}  \\
  &= \bigcup_{n(y)} V_{n(y)}
  \end{align*}
  is a countable union of at most countable sets,
  which is countable.
  Hence $E \cap B(x) - \{x\}$ or $E \cap B(x)$ is countable,
  contrary to the assumption that $E \cap B(x)$ is uncountable.
  \end{enumerate}
\item[(5)]
\emph{Show that $E \cap \widetilde{P}$ is at most countable.}
$$E \cap \widetilde{P}
= E \bigcap \left(\bigcup_{P(V_n)} V_n\right)
= \bigcup_{P(V_n)}(E \cap V_n)$$
is at most countable.
\end{enumerate}

$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.28.}
\addcontentsline{toc}{subsection}{Exercise 2.28.}
\emph{Prove that every closed set in a separable metric space is
the union of a (possible empty) perfect set and a set which is at most
countable.
(Corollary: Every countable closed set in $\mathbb{R}^k$ has isolated points.)
(Hint: Use Exercise 2.27.) } \\

\emph{Proof (Exercise 2.27).}
Let $E$ be a closed set in a separable metric space.
\begin{enumerate}
\item[(1)]
$E$ contains all limit points of $E$,
especially contains all condensation points of $E$.
So we can write $$E = P \cup (E-P)$$
where $P$ is the set of all condensation points of $E$.
\item[(2)]
By Exercise 2.27, $P$ is perfect and
$E-P = E \cap \widetilde{P}$ is at most countable.
\end{enumerate}
$\Box$ \\

\textbf{Cantor-Bendixson theorem.}
\begin{enumerate}
\item[(1)]
Closed sets of a Polish space $X$ have the perfect set property in a particularly strong form:
any closed subset of $X$ may be written uniquely as
the disjoint union of a perfect set and a countable set.
\item[(2)]
A Polish space is a separable completely metrizable topological space;
that is, a space homeomorphic to a complete metric space
that has a countable dense subset. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.29.}
\addcontentsline{toc}{subsection}{Exercise 2.29.}
\emph{Prove that every open set in $\mathbb{R}^1$ is the union of
an at most countable collection of disjoint segments.
(Hint: Use Exercise 2.22.)} \\

\emph{Proof.}
Let $E$ be an open subset of $\mathbb{R}^1$.
\begin{enumerate}
\item[(1)]
For each $x \in E$, let $I_x$ denote the largest open interval
containing $x$ and contained in $E$.
More precisely, since $E$ is open,
$x$ is contained in some small (non-trivial) interval,
and therefore if
$$a_x = \inf\{ a < x : (a,x) \subseteq E \} \text{ and }
b_x = \sup\{ b > x : (x,b) \subseteq E \}$$
we must have $a_x < x < b_x$ (with possibly infinite values for $a_x$ and $b_x$).
\item[(2)]
Let $I_x = (a_x, b_x)$, then by construction we have $x \in I_x$
as well as $I_x \subseteq E$.
Hence
$$E = \bigcup_{I_x \in \mathscr{F}} I_x,$$
where $\mathscr{F} = \{I_x\}_{x \in E}$.
\item[(3)]
Suppose that two intervals $I_x$ and $I_y$ intersect.
Then their union (which is also an open interval)
is contained in $E$ and contains $x$ (and $y$).
Since $I_x$ is maximal, $I_x \cup I_y \subseteq I_x$,
and similarly $I_x \cup I_y \subseteq I_y$.
This can happen only if $I_x = I_y$.
\item[(4)]
Therefore, any two distinct intervals in $\mathscr{F}$ must be disjoint.
Hence $\mathscr{F}$ is countable since each open interval $I_x \in \mathscr{F}$
contains a rational number.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 2.30.}
\addcontentsline{toc}{subsection}{Exercise 2.30.}
\emph{Imitate the proof of Theorem 2.43 to obtain the following result:}
\begin{quote}
\emph{If $\mathbb{R}^k = \bigcup_{n=1}^{\infty} F_n$,
where each $F_n$ is a closed subset of $\mathbb{R}^k$,
then at least one $F_n$ has a nonempty interior.} \\

\emph{Equivalent statement: If $G_n$ is a dense open subset of $\mathbb{R}^k$,
for $n = 1,2,3,\ldots$, then $\bigcap_{n=1}^{\infty} G_n$ is not empty
(in fact, it is dense in $\mathbb{R}^k$).}
\end{quote}
\emph{(This is a special case of Baire's theorem; see Exercise 3.22 for the general case.)} \\

\textbf{Baire category theorem.}
\emph{
If $G_n$ is a dense open subset of $\mathbb{R}^k$,
for $n = 1,2,\ldots$, then $$\bigcap_{n=1}^{\infty} G_n$$
is dense in $\mathbb{R}^k$.} \\

\emph{Proof of Baire category theorem.}
Given any open set $G_0$ in $\mathbb{R}^k$,
will show that $$\bigcap_{n=0}^{\infty} G_n \neq \varnothing.$$
\begin{enumerate}
\item[(1)]
Since $G_1$ is dense, $G_0 \cap G_1$ is nonempty.
Take any one point $\mathbf{x}_1$ in the open set $G_0 \cap G_1$,
then there exists an open neighborhood
$$V_1
= \{ \mathbf{y} \in \mathbb{R}^k : |\mathbf{y} - \mathbf{x}_1| < r_1 \}$$
of $\mathbf{x}_1$
such that
$$\overline{V_1}
= \{ \mathbf{y} \in \mathbb{R}^k : |\mathbf{y} - \mathbf{x}_1| \leq r_1 \}
\subseteq G_0 \cap G_1.$$
\item[(2)]
Suppose $V_n$ has been constructed,
take any one point $\mathbf{x}_{n+1}$ in the open set $V_n \cap G_{n+1}$,
then there exists an open neighborhood
$$V_{n+1}
= \{ \mathbf{y} \in \mathbb{R}^k : |\mathbf{y} - \mathbf{x}_{n+1}| < r_{n+1} \}$$
of $\mathbf{x}_{n+1}$ with $r_{n+1}$
such that
$$\overline{V_{n+1}}
= \{ \mathbf{y} \in \mathbb{R}^k : |\mathbf{y} - \mathbf{x}_{n+1}| \leq r_{n+1} \}
\subseteq V_n \cap G_{n+1}.$$
\item[(3)]
Note that
  \begin{enumerate}
  \item[(a)]
  each $\overline{V_n}$ is nonempty (containing $\mathbf{x}_n$) and compact.
  \item[(b)]
  $\overline{V_1} \supseteq \overline{V_2} \supseteq \cdots$
  (since
  $\overline{V_{n+1}} \subseteq V_n \cap G_{n+1} \subseteq V_n \subseteq \overline{V_n}$).
  \end{enumerate}
By Corollary to Theorem 2.36,
$$\bigcap_{n=1}^{\infty} \overline{V_n} \neq \varnothing.$$
\item[(4)]
Pick $\mathbf{x} \in \bigcap_{n=1}^{\infty} \overline{V_n}$.
Hence
\begin{align*}
\mathbf{x} \in \bigcap_{n=1}^{\infty} \overline{V_n}
&\Longleftrightarrow
\mathbf{x} \in \overline{V_n} \text{ for all } n=1,2,3,\ldots \\
&\Longrightarrow
\mathbf{x} \in \overline{V_1} \subseteq G_0 \cap G_1 \text{ and }
\mathbf{x} \in \overline{V_{n+1}} \subseteq V_n \cap G_{n+1} \subseteq G_{n+1} \\
&\Longrightarrow
\mathbf{x} \in G_0 \cap G_1 \cap \cdots = \bigcap_{n=0}^{\infty} G_n \\
&\Longrightarrow
\bigcap_{n=0}^{\infty} G_n \neq \varnothing.
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 3: Numerical Sequences and Series \\}
\addcontentsline{toc}{section}{Chapter 3: Numerical Sequences and Series}



\subsection*{Exercise 3.1.}
\addcontentsline{toc}{subsection}{Exercise 3.1.}
\emph{Prove that the convergence of $\{s_n\}$ implies
convergence of $\{\abs{s_n}\}$.  Is the converse true?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Since $\{s_n\}$ is convergent, there is $s \in \mathbb{R}^1$
with the following property:
given any $\varepsilon > 0$, there is $N$ such that
$\abs{s_n - s} < \varepsilon$ whenever $n \geq N$.
So
$$\abs{\abs{s_n}-\abs{s}} \leq \abs{s_n - s}  < \varepsilon$$
(Exercise 1.13). That is,
$\{\abs{s_n}\}$ converges to $\abs{s}$.
\item[(2)]
The converse is not true by considering $s_n = (-1)^{n+1}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.2.}
\addcontentsline{toc}{subsection}{Exercise 3.2.}
\emph{Calculate $\lim_{n \to \infty}(\sqrt{n^2 + n} - n)$.} \\

\emph{Proof.}
$$\sqrt{n^2 + n} - n
= \frac{n}{\sqrt{n^2+n} + n}
= \frac{1}{\sqrt{1+\frac{1}{n}} + 1} \to \frac{1}{1+1} = \frac{1}{2}$$
as $n \to \infty$.
$\Box$ \\

\emph{Proof ($\varepsilon-N$ argument).}
Let $s_n = \sqrt{n^2 + n} - n$.
\emph{Show that the sequence $\{s_n\}$ converges to $s = \frac{1}{2}$.}
Given any $\varepsilon > 0$, there is $N > \frac{1}{\varepsilon}$ such that
\begin{align*}
\abs{s_n - s}
&= \abs{(\sqrt{n^2 + n} - n) - \frac{1}{2}}
= \abs{\frac{1}{\sqrt{1+\frac{1}{n}} + 1} - \frac{1}{2}} \\
&= \abs{
\frac{2-\left(\sqrt{1+\frac{1}{n}} + 1\right)}
{2\left(\sqrt{1+\frac{1}{n}} + 1\right)}
}
= \abs{
\frac{1-\sqrt{1+\frac{1}{n}}}
{2\left(\sqrt{1+\frac{1}{n}} + 1\right)}
} \\
&= \abs{
\frac{1 - \left(1- \frac{1}{n}\right)}
{2\left(\sqrt{1+\frac{1}{n}} + 1\right)^2}}
= \abs{
\frac{- \frac{1}{n}}
{2\left(\sqrt{1+\frac{1}{n}} + 1\right)^2}}
< \frac{1}{n}
\leq \frac{1}{N}
< \varepsilon
\end{align*}
wheneven $n \geq N$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.3.}
\addcontentsline{toc}{subsection}{Exercise 3.3.}
\emph{If $s_1 = \sqrt{2}$ and
$$s_{n+1} = \sqrt{2+\sqrt{s_n}} \:\: (n=1,2,3,...),$$
prove that $\{s_n\}$ converges, and that $s_n < 2$ for $n=1,2,3,...$.} \\

The convergence of $\{s_n\}$
implies there is $s \in \mathbb{R}$ such that $s_n \to s$
where $s = \sqrt{2+\sqrt{s}}$ and $\sqrt{2} < s \leq 2$.
WolframAlpha shows that
$$s = \frac{1}{3}
\left(
-1 + \sqrt[3]{\frac{1}{2}(79 - 3 \sqrt{249})}
   + \sqrt[3]{\frac{1}{2}(79 + 3 \sqrt{249})}
\right).$$ \\

\emph{Proof (Theorem 3.14).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\{s_n\}$ is increasing (by mathematical induction).}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $s_2 > s_1$.}
  In fact,
  $$s_2 = \sqrt{2+\sqrt{s_1}} = \sqrt{2+\sqrt{\sqrt{2}}} < \sqrt{2} = s_1.$$
  \item[(a)]
  \emph{Show that $s_{n+1} > s_{n}$ if $s_{n} > s_{n-1}$.}
  $$s_{n+1} = \sqrt{2+\sqrt{s_{n}}} > \sqrt{2+\sqrt{s_{n-1}}} = s_n.$$
  \end{enumerate}
By mathematical induction, $\{s_n\}$ is (strictly) increasing.
\item[(2)]
\emph{Show that $\{s_n\}$ is bounded (by mathematical induction).}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $s_1 \leq 2$.}
  $\sqrt{2} \leq 2$.
  \item[(a)]
  \emph{Show that $s_{n+1} \leq 2$ if $s_{n} \leq 2$.}
  $$s_{n+1} = \sqrt{2+\sqrt{s_{n}}} \leq \sqrt{2+\sqrt{2}} < 2.$$
  \end{enumerate}
By mathematical induction, $\{s_n\}$ is bounded by $2$.
\end{enumerate}
Hence, $\{s_n\}$ converges since $\{s_n\}$ is increasing and bounded (Theorem 3.14).
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.4.}
\addcontentsline{toc}{subsection}{Exercise 3.4.}
\emph{Find the upper and lower limits of the sequences $\{s_n\}$ defined by
$$s_1 = 0; s_{2m} = \frac{s_{2m-1}}{2}; s_{2m+1} = \frac{1}{2} + s_{2m}.$$ } \\

Write out the first few terms of $\{s_n\}$:
$$0, 0, \frac{1}{2}, \frac{1}{4}, \frac{3}{4},
\frac{3}{8}, \frac{7}{8}, \frac{7}{16}, \frac{15}{16}, ...$$

It suggests us
\begin{align*}
s_{2m+1} &= 1 - \frac{1}{2^m} \:\: (m = 0, 1, 2, ...), \\
s_{2m} &= \frac{1}{2} - \frac{1}{2^m} \:\: (m = 1, 2, 3, ...). \\
\end{align*}

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
\begin{align*}
s_{2m+1} &= 1 - \frac{1}{2^m} \:\: (m = 0, 1, 2, ...), \\
s_{2m} &= \frac{1}{2} - \frac{1}{2^m}. \:\: (m = 1, 2, 3, ...)
\end{align*}}
Apply mathematical induction.
\item[(2)]
The upper limit is $1$.
\item[(3)]
The lower limit is $\frac{1}{2}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.5.}
\addcontentsline{toc}{subsection}{Exercise 3.5.}
\emph{For any two real sequences $\{a_n\}$, $\{b_n\}$, prove that
$$\limsup_{n \to \infty} (a_n + b_n)
\leq \limsup_{n \to \infty} a_n + \limsup_{n \to \infty} b_n$$
provided the sum of the right is not of the form $\infty - \infty$.} \\

\emph{Proof.}
Write
$\alpha = \limsup_{n \to \infty} a_n$
and
$\beta = \limsup_{n \to \infty} b_n$.
\begin{enumerate}
\item[(1)]
\emph{$\alpha = \infty$ and $\beta = \infty$.}
Nothing to do.
\item[(2)]
\emph{$\alpha = -\infty$ and $\beta = -\infty$.}
Since $\alpha = -\infty < \infty$,
there exists $M'$ such that $a_n < M'$ for all $n$.
For any real $M$, $a_n > M - M'$ for at most a finite number of values of $n$
(Theorem 3.17(a)).
Hence $a_n + b_n > M$ for at most a finite number of values of $n$.
Hence $\limsup_{n \to \infty} (a_n + b_n) = -\infty$,
or
$$\limsup_{n \to \infty} (a_n + b_n)
= \limsup_{n \to \infty} a_n + \limsup_{n \to \infty} b_n$$
in this case.
\item[(3)]
\emph{$\alpha$ and $\beta$ are finite.}
(Similar to the argument in Theorem 3.37.)
Choose $\alpha' > \alpha$ and $\beta' > \beta$.
There is an integer $N$ such that
$$\alpha' \geq a_n \text{ and } \beta' \geq b_n$$
whenever $n \geq N$.
Hence $$a_n + b_n \leq \alpha' + \beta'$$
whenever $n \geq N$. Take $\limsup$ to get
Hence $$\limsup_{n \to \infty} (a_n + b_n) \leq \alpha' + \beta'.$$
Since the inequality is true for every $\alpha' > \alpha$ and $\beta' > \beta$,
we have
$$\limsup_{n \to \infty} (a_n + b_n)
\leq \limsup_{n \to \infty} a_n + \limsup_{n \to \infty} b_n.$$
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.6.}
\addcontentsline{toc}{subsection}{Exercise 3.6.}
\emph{Investigate the behavior (convergence or divergence) of $\sum a_n$ if}
\begin{enumerate}
\item[(a)]
\emph{$a_n = \sqrt{n+1} - \sqrt{n}$.}
\item[(b)]
\emph{$a_n = \frac{\sqrt{n+1} - \sqrt{n}}{n}$.}
\item[(c)]
\emph{$a_n = (\sqrt[n]{n} - 1)^n$.}
\item[(d)]
\emph{$a_n = \frac{1}{1+z^n}$ for complex values of $z$.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
Divergence.
\item[(2)]
$\sum_{n=1}^{k}a_n = \sqrt{k+1} - 1 \to \infty$ as $k \to \infty$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Convergence.
\item[(2)]
Since
$$|a_n|
= \frac{1}{n(\sqrt{n+1}+\sqrt{n})} < \frac{1}{2 n^{\frac{3}{2}}}$$
holds for all $n$ and $\sum \frac{1}{2 n^{\frac{3}{2}}}$ converges
(Theorem 3.28 and Theorem 3.3),
by the comparison test (Theorem 3.25), $\sum a_n$ converges.
\end{enumerate}
$\Box$ \\

\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
Convergence.
\item[(2)]
Note that
$$\alpha
= \limsup_{n \to \infty} \sqrt[n]{|a_n|}
= \limsup_{n \to \infty} \sqrt[n]{n} - 1 = 0$$
(Theorem 3.20(c)).
Since $\alpha < 1$, $\sum a_n$ converges by the root test (Theorem 3.33).
\end{enumerate}
$\Box$ \\

\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
Convergence if $|z| > 1$; divergence if $|z| \leq 1$.
\item[(2)]
Note that
$|z^n+1| + |-1| \geq |z^n|$
(Theorem 1.33(e)),
or
$$|z^n+1| \geq |z|^n - 1.$$
\item[(3)]
If $|z| > 1$, then there is an integer $N$ such that
$$|z|^n \geq 2 \text{ whenever } n \geq N.$$
Therefore, for $n \geq N$ we have
  \begin{align*}
    |a_n|
    &= \frac{1}{|z^n+1|} \\
    &\leq \frac{1}{|z|^n - 1}
      &((2)) \\
    &\leq \frac{1}{|z|^n - \frac{1}{2}|z|^n} \\
    &= \frac{2}{|z|^n}.
  \end{align*}
The geometric series $\sum \frac{2}{|z|^n}$ converges,
by the comparison test (Theorem 3.25), $\sum a_n$ converges.
\item[(4)]
If $|z| \leq 1$, then $|a_n| \geq \frac{1}{2}$,
or $\lim a_n \neq 0$.
By Theorem 3.23 ($\lim a_n = 0$ if $\sum a_n$ converges),
$\sum a_n$ diverges.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.7.}
\addcontentsline{toc}{subsection}{Exercise 3.7.}
\emph{Prove that the convergence of $\sum a_n$ implies the convergence of
$$\sum \frac{\sqrt{a_n}}{n},$$
if $a_n \geq 0$.} \\

\emph{Proof (Cauchy's inequatity).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\sum\frac{\sqrt{a_n}}{n}$ is bounded.}
For any $k \in \mathbb{Z}^{+}$,
\begin{align*}
\left( \sum_{n=1}^{k} \frac{\sqrt{a_n}}{n} \right)^2
\leq&
\left( \sum_{n=1}^{k}{a_n} \right)
\left( \sum_{n=1}^{k}{\frac{1}{n^2}} \right)
  &(\text{Cauchy's inequatity}) \\
\leq& \left( \sum^{\infty}_{n=1}{a_n} \right)
\left( \sum^{\infty}_{n=1}{\frac{1}{n^2}} \right).
  &\left(\text{$\sum{a_n}, \sum{\frac{1}{n^2}}$: convergent}\right)
\end{align*}
Thus,
$\left( \sum_{n=1}^{k}\frac{\sqrt{a_n}}{n} \right)^2$ is bounded,
or $\sum_{n=1}^{k}\frac{\sqrt{a_n}}{n}$ is bounded.
\item[(2)]
\emph{Show that $\sum_{n=1}^{k} \frac{\sqrt{a_n}}{n}$ is increasing.}
It is clear due to $\frac{\sqrt{a_n}}{n} \geq 0$.
\end{enumerate}
By Theorem 3.14, $\sum_{n=1}^{\infty} \frac{\sqrt{a_n}}{n}$ converges.
$\Box$ \\

\emph{Proof (AM-GM inequality).}
\emph{Show that $\sum\frac{\sqrt{a_n}}{n}$ is bounded.}
\begin{align*}
\frac{\sqrt{a_n}}{n}
\leq&
\frac{1}{2}
\left( a_n + \frac{1}{n^2} \right)
  &(\text{AM-GM inequality}) \\
\sum_{n=1}^{k} \frac{\sqrt{a_n}}{n}
\leq&
\frac{1}{2}
\left( \sum_{n=1}^{k} a_n + \sum_{n=1}^{k} \frac{1}{n^2} \right) \\
\leq&
\frac{1}{2}
\left( \sum_{n=1}^{\infty} a_n + \sum_{n=1}^{\infty} \frac{1}{n^2} \right).
  &\left(\text{$\sum{a_n}, \sum{\frac{1}{n^2}}$: convergent}\right)
\end{align*}
Thus, $\sum_{n=1}^{k}\frac{\sqrt{a_n}}{n}$ is bounded.
The rest proof is the same as previous.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.8.}
\addcontentsline{toc}{subsection}{Exercise 3.8.}
\emph{If $\sum a_n$ converges, and if $\{b_n\}$ is monotonic and bounded,
prove that $\sum a_n b_n$ converges.} \\

\emph{Proof (Theorem 3.42).}
There are only two possible cases (might be overlapped).
\begin{enumerate}
\item[(1)]
\emph{$\{b_n\}$ is decreasing to $b$.}
Define $\{ \beta_n \}$ by $\beta_n = b_n - b$.
  \begin{enumerate}
  \item[(a)]
  The partial sums of $\sum a_n$ form a bounded sequence since
  $\sum a_n$ converges.
  \item[(b)]
  $\{ \beta_n \}$ is monotonically decreasing.
  \item[(c)]
  $\lim \beta_n = 0$.
  \end{enumerate}
  By (1)(2)(3), $\sum a_n \beta_n$ converges.
  Hence $$\sum a_n b_n = \sum a_n \beta_n + \sum a_n b$$ converges
  (Theorem 3.3(a)(b)).
\item[(2)]
\emph{$\{b_n\}$ is increasing to $b$.}
Similar to (1).
Define $\{ \beta_n \}$ by $\beta_n = b - b_n$.
Thus $\sum a_n \beta_n$ converges.
Hence
$$\sum a_n b_n = - \sum a_n \beta_n + \sum a_n b$$ converges.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.9.}
\addcontentsline{toc}{subsection}{Exercise 3.9.}
\emph{Find the radius of convergence of each of the following power series:}
\begin{enumerate}
\item[(a)]
\emph{$\sum n^3 z^n$,}
\item[(b)]
\emph{$\sum \frac{2^n}{n!} z^n$,}
\item[(c)]
\emph{$\sum \frac{2^n}{n^2} z^n$,}
\item[(d)]
\emph{$\sum \frac{n^3}{3^n} z^n$.} \\
\end{enumerate}

\emph{Proof of (a).}
Since
$$\alpha
= \limsup_{n \to \infty} \sqrt[n]{n^3}
= \limsup_{n \to \infty} (\sqrt[n]{n})^3
= 1$$ (Theorem 3.20(c)),
$R = \frac{1}{\alpha} = 1$.
$\Box$ \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Note that
$\sqrt[n]{n!} \leq \sqrt[n]{n^n} = n$.
\emph{Show that $\sqrt[n]{n!} \geq \sqrt{n}$.}
Note that
$$(n!)^2 = \prod_{k=1}^{n} k(n+1-k).$$
For each term $k(n+1-k)$ (where $k=1,\ldots,n$),
we have $$k(n+1-k) - n = (k-1)(n-k) \geq 0 \text{ or } k(n+1-k) > n.$$
or $k(n+1-k) > n$.
Hence,
$$(n!)^2 = \prod_{k=1}^{n} k(n+1-k) \geq \prod_{k=1}^{n} n = n^n,$$
or $\sqrt[n]{n!} \geq \sqrt{n}$.
\item[(2)]
Since
$$0 \leq \alpha
= \limsup_{n \to \infty} \sqrt[n]{\frac{2^n}{n!}}
= \limsup_{n \to \infty} \frac{2}{\sqrt[n]{n!}}
\leq \limsup_{n \to \infty} \frac{2}{\sqrt{n}}
= 0,$$
$\alpha = 0$ and
$R = \frac{1}{\alpha} = \infty$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (c).}
Similar to (a).
Since
$$\alpha
= \limsup_{n \to \infty} \sqrt[n]{\frac{2^n}{n^2}}
= \limsup_{n \to \infty} \frac{2}{\sqrt[n]{n}^2}
= 2$$ (Theorem 3.20(c)),
$R = \frac{1}{\alpha} = \frac{1}{2}$.
$\Box$ \\

\emph{Proof of (d).}
Similar to (a)(c).
Since
$$\alpha
= \limsup_{n \to \infty} \sqrt[n]{\frac{n^3}{3^n}}
= \limsup_{n \to \infty} \frac{\sqrt[n]{n}^3}{3}
= \frac{1}{3}$$ (Theorem 3.20(c)),
$R = \frac{1}{\alpha} = 3$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.10.}
\addcontentsline{toc}{subsection}{Exercise 3.10.}
\emph{Suppose that the coefficients of the power series $\sum a_n z^n$ are integers,
infinitely many of which are distinct from zero.
Prove that the radius of convergence is at most $1$.} \\

\emph{Proof (Theorem 3.39).}
$\alpha = \limsup_{n \to \infty} \sqrt[n]{|a_n|} \geq 1$ by assumption
that $\{a_n\}$ has infinitely many nonzero integers.
Hence the radius of convergence $R = \frac{1}{\alpha} \leq 1$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.11.}
\addcontentsline{toc}{subsection}{Exercise 3.11.}
\emph{Suppose $a_n > 0$, $s_n = a_1 + \cdots + a_n$, and $\sum a_n$ diverges.}
\begin{enumerate}
\item[(a)]
\emph{Prove that $\sum \frac{a_n}{1+a_n}$ diverges.}
\item[(b)]
\emph{Prove that
$$\frac{a_{N+1}}{s_{N+1}} + \cdots + \frac{a_{N+k}}{s_{N+k}}
\geq 1 - \frac{s_N}{s_{N+k}}$$
and deduce that $\sum \frac{a_n}{s_n}$ diverges.}
\item[(c)]
\emph{Prove that
$$\frac{a_n}{s_n^2} \leq \frac{1}{s_{n-1}} - \frac{1}{s_n}$$
and deduce that $\sum \frac{a_n}{s_n^2}$ converges.}
\item[(d)]
\emph{What can be said about
$$\sum \frac{a_n}{1+na_n} \:\: and \:\: \sum \frac{a_n}{1+n^2 a_n}?$$} \\
\end{enumerate}



\emph{Proof of (a).}
(Reductio ad absurdum)
\begin{enumerate}
  \item[(1)]
  If $\sum \frac{a_n}{1+a_n}$ were convergent,
  $\lim \frac{a_n}{1+a_n} = 0$ (Theorem 3.23).
  Note that $\frac{a_n}{1+a_n} = \frac{1}{1+\frac{1}{a_n}}$ implies
  $\lim a_n = 0$.
  \item[(2)]
  Since $\lim a_n = 0$, there is an integer $N$ such that
  $$0 < a_n < 1 \text{ whenever } n \geq N.$$
  Hence
  $$|a_n| = a_n \leq \frac{2a_n}{1+a_n} \text{ whenever } n \geq N.$$
  By the comparison test (Theorem 3.25),
  $\sum a_n$ converges, contrary to the divergence of $\sum a_n$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  Note that each $s_n > 0$ and $\{s_n\}$ is monotonic increasing.
  For $k \geq 1$,
  \begin{align*}
    \frac{a_{N+1}}{s_{N+1}} + \cdots + \frac{a_{N+k}}{s_{N+k}}
    &\geq \frac{a_{N+1}}{s_{N+k}} + \cdots + \frac{a_{N+k}}{s_{N+k}} \\
    &= \frac{a_{N+1} + \cdots + a_{N+k}}{s_{N+k}} \\
    &= \frac{s_{N+k} - s_{N}}{s_{N+k}} \\
    &= 1 - \frac{s_N}{s_{N+k}}.
  \end{align*}
  \item[(2)]
  (Reductio ad absurdum)
  If $\sum \frac{a_n}{s_n}$ were convergent,
  by the Cauchy criterion (Theorem 3.22), for $\varepsilon = \frac{1}{64} > 0$,
  there is an integer $N$ such that
  $$\abs{ \sum_{n=N+1}^{N+k} \frac{a_n}{s_n} } < \frac{1}{64}
  \:\: \text{ whenever } \:\: k \geq 1.$$
  So,
  $$\frac{1}{64} > \sum_{n=N+1}^{N+k} \frac{a_n}{s_n} > 1 - \frac{s_N}{s_{N+k}}
  \:\: \text{ or } \:\:
  s_{N+k} < \frac{64}{63} s_N,$$
  contrary to divergence of $\sum a_n = \infty$ (as $k \to \infty$).
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
  \item[(1)]
  For $n \geq 2$,
  $$\frac{1}{s_{n-1}} - \frac{1}{s_{n}}
  = \frac{s_n-s_{n-1}}{s_{n-1}s_n}
  = \frac{a_n}{s_{n-1}s_n}
  \geq \frac{a_n}{s_n^2}.
  $$
  \item[(2)]
  $\sum \frac{a_n}{s_n^2}$ is a series of nonnegative terms
  and its partial sums
  \begin{align*}
    \sum_{n=1}^{k} \frac{a_n}{s_n^2}
    &\leq \frac{a_1}{s_1^2}
      + \sum_{n=2}^{k}\left( \frac{1}{s_{n-1}} - \frac{1}{s_{n}} \right) \\
    &= \frac{a_1}{s_1^2}
      + \frac{1}{s_{1}} - \frac{1}{s_{k}} \\
    &= \frac{2}{a_1} - \frac{1}{s_{k}} \\
    &< \frac{2}{a_1}
  \end{align*}
  is bounded (by $\frac{2}{a_1}$).
  Therefore, $\sum \frac{a_n}{s_n^2}$ converges (Theorem 3.24).
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that there is a divergent series $\sum a_n$ with $a_n > 0$
  such that $\sum \frac{a_n}{1+na_n}$ converges or diverges.}
    \begin{enumerate}
      \item[(a)]
      Take $$a_n = \frac{1}{n(\log n)^p}$$
      where $0 \leq p \leq 1$.
      \item[(b)]
      Clearly,
      $$\sum_{n=3}^{\infty} a_n = \sum_{n=3}^{\infty} \frac{1}{n(\log n)^p}$$
      diverges (Theorem 3.29).
      \item[(c)]
      Note that
      \begin{align*}
        \sum_{n=3}^{\infty} \frac{a_n}{1+na_n}
        &= \sum_{n=3}^{\infty} \frac{1}{n(\log n)^p} \cdot \frac{1}{1 + (\log n)^p} \\
        &= \sum_{n=3}^{\infty} \frac{1}{n(\log n)^p + n(\log n)^{2p}}.
      \end{align*}
      Hence,
      $$ \sum_{n=3}^{\infty} \frac{1}{2n(\log n)^{2p}}
      \leq \sum_{n=3}^{\infty} \frac{a_n}{1+na_n}
      < \sum_{n=3}^{\infty} \frac{1}{n(\log n)^{2p}}.$$
      (Here we use the fact that $n(\log n)^p > 0$ and $(\log n)^p \geq 1$ if $n > e$.)
      Therefore,
      \begin{equation*}
        \sum_{n=3}^{\infty} \frac{a_n}{1+na_n} =
        \begin{cases}
          \text{ converges } & \text{if $1 \geq p > \frac{1}{2}$} \\
          \text{ diverges }  & \text{if $\frac{1}{2} \geq p \geq 0$}
        \end{cases}
      \end{equation*}
      by Theorem 3.29 and the comparison test (Theorem 3.24).
    \end{enumerate}
    \emph{Note.} If a series $\sum a_n$ with $a_n > 0$ is convergent,
    then $\sum \frac{a_n}{1+na_n}$ is always convergent
    by the comparison test (Theorem 3.24).
  \item[(2)]
  \emph{Given any series $\sum a_n$ with $a_n > 0$.
  Show that $$\sum \frac{a_n}{1+n^2a_n} < \infty$$
  converges.}
  Note that
  $$\abs{ \frac{a_n}{1+n^2a_n} }
  = \frac{1}{\frac{1}{a_n}+n^2}
  < \frac{1}{n^2}$$
  for any $n$ and $\sum_{n=1}^{\infty} \frac{1}{n^2}$ converges (to $\frac{\pi^2}{6}$).
  By the comparison test (Theorem 3.25),
  $\sum \frac{a_n}{1+n^2a_n}$ converges.
\end{enumerate}
$\Box$ \\

\emph{Note.}
Similar to (d),
\emph{what can be said about
$$\sum \frac{a_n}{1+n(\log n) a_n} \:\: and \:\:
\sum \frac{a_n}{1+n(\log n)^2 a_n}?$$} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.12.}
\addcontentsline{toc}{subsection}{Exercise 3.12.}
\emph{Suppose $a_n > 0$ and $\sum a_n$ converges.
Put
$$r_n = \sum_{m=n}^{\infty} a_m.$$}
\begin{enumerate}
\item[(a)]
\emph{Prove that
$$\frac{a_m}{r_m} + \cdots + \frac{a_n}{r_n} > 1 - \frac{r_n}{r_m}$$
if $m < n$, and deduce that $\sum \frac{a_n}{r_n}$ diverges.}
\item[(b)]
\emph{Prove that
$$\frac{a_n}{\sqrt{r_n}} < 2(\sqrt{r_n} - \sqrt{r_{n+1}})$$
and deduce that $\sum \frac{a_n}{\sqrt{r_n}}$ converges.} \\
\end{enumerate}

\emph{Note.}
\begin{enumerate}
\item[(1)]
Each $r_n$ is positive and finite (since $a_n > 0$ and $\sum a_n$ converges).
\item[(2)]
$\{r_n\}$ is monotonic decreasing (since $a_n > 0$).
\item[(3)]
$\{r_n\}$ converges to $0$ (since $\sum a_n$ converges). \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
\begin{align*}
  \frac{a_m}{r_m} + \cdots + \frac{a_n}{r_n}
  &> \frac{a_m}{r_m} + \cdots + \frac{a_n}{r_m}
    &(\text{$r_m > r_k$ for $k=m+1,\ldots,n$}) \\
  &= \frac{a_m + \cdots + a_n}{r_m} \\
  &= \frac{r_m - r_{n+1}}{r_m}
    &(\text{Definition of $r_k$}) \\
  &> \frac{r_m - r_{n}}{r_m}
    &(r_n > r_{n+1}) \\
  &= 1 - \frac{r_n}{r_m}.
\end{align*}
\item[(2)]
(Reductio ad absurdum)
If $\sum \frac{a_n}{r_n}$ were convergent,
then given $\varepsilon = \frac{1}{64} > 0$
there is an integer $N$ such that
$$\abs{\frac{a_m}{r_m} + \cdots + \frac{a_n}{r_n}} < \frac{1}{64}
\text{ whenever } n \geq m \geq N$$
(Theorem 3.22). By (1), let $m = N$ to get
$$1 - \frac{r_n}{r_N} < \frac{1}{64}
\text{ whenever } n \geq N,$$
or $$r_n > \frac{63}{64} r_N,$$
contrary to the assumption that
$\{r_n\}$ converges to $0$ (since $\sum a_n$ converges).
\end{enumerate}
$\Box$ \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Note that each $r_n$ is positive and finite, and thus
\begin{align*}
  \frac{a_n}{\sqrt{r_n}} < 2(\sqrt{r_n} - \sqrt{r_{n+1}})
  &\Longleftrightarrow
  \frac{r_n - r_{n+1}}{\sqrt{r_n}} < 2(\sqrt{r_n} - \sqrt{r_{n+1}}) \\
  &\Longleftrightarrow
  \frac{\sqrt{r_n} + \sqrt{r_{n+1}}}{\sqrt{r_n}} < 2 \\
  &\Longleftrightarrow
  \sqrt{r_n} + \sqrt{r_{n+1}} < 2 \sqrt{r_n} \\
  &\Longleftrightarrow
  \sqrt{r_{n+1}} < \sqrt{r_n} \\
  &\Longleftrightarrow
  r_{n+1} < r_n.
\end{align*}
The last statement holds since $\{r_n\}$ is monotonic decreasing.
\item[(2)]
  \begin{enumerate}
  \item[(a)]
  Each term $\frac{a_n}{\sqrt{r_n}}$ of $\sum \frac{a_n}{\sqrt{r_n}}$ is nonnegative.
  \item[(b)]
  The partial sum
  $$\sum_{k=1}^{n} \frac{a_k}{\sqrt{r_k}}
  < \sum_{k=1}^{n} 2(\sqrt{r_k} - \sqrt{r_{k+1}})
  = 2(\sqrt{r_1} - \sqrt{r_{n+1}})
  < 2\sqrt{r_1}$$
  is bounded by $2\sqrt{r_1}$.
  \end{enumerate}
  By (a)(b), $\sum \frac{a_n}{\sqrt{r_n}}$ converges (Theorem 3.24).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.13.}
\addcontentsline{toc}{subsection}{Exercise 3.13.}
\emph{Prove that the Cauchy product of two absolutely convergent series
converges absolutely.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Given two absolutely convergent series $\sum a_n$ and $\sum b_n$.
The Cauchy product is $\sum c_n$
where
$$c_n = \sum_{k=0}^{n} a_k b_{n-k} \:\: (n=0,1,2,\ldots).$$
Let $\sum |a_n| = A < \infty$ and $\sum |b_n| = B < \infty$.
\item[(2)]
Each term $|c_k|$ of $\sum_{k=0}^{n}|c_k|$ is nonnegative.
\item[(3)]
Thus,
\begin{align*}
  \sum_{k=0}^{n}|c_k|
  &= \sum_{k=0}^{n} \abs{ \sum_{m=0}^{k} a_m b_{k-m} } \\
  &\leq \sum_{k=0}^{n} \sum_{m=0}^{k} |a_m| |b_{k-m}| \\
  &= \sum_{k=0}^{n} |a_k| \sum_{m=0}^{n-k} |b_m| \\
  &\leq \sum_{k=0}^{n} |a_k| B \\
  &\leq AB \\
  &< \infty.
\end{align*}
\item[(4)]
By (2)(3), $\sum_{k=0}^{n}|c_k|$ converges (Theorem 3.24),
or $\sum_{k=0}^{n} c_k$ converges absolutely.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.14 (Ces\`aro convergence).}
\addcontentsline{toc}{subsection}{Exercise 3.14 (Ces\`aro convergence).}
\emph{If $\{s_n\}$ is a complex sequence, define its arithmetic means $\sigma_n$ by
$$\sigma_n
= \frac{s_0 + s_1 + \cdots + s_n}{n+1} \:\: (n=0,1,2,\ldots).$$}
\begin{enumerate}
\item[(a)]
\emph{If $\lim s_n = s$, prove that $\lim \sigma_n = s$.}
\item[(b)]
\emph{Construct a sequence $\{s_n\}$ which does not converge, although $\lim \sigma_n = 0$.}
\item[(c)]
\emph{Can it happen that $s_n > 0$ for all $n$ and that $\limsup s_n = \infty$,
although $\lim \sigma_n = 0$?}
\item[(d)]
\emph{Put $a_n = s_n - s_{n-1}$, for $n \geq 1$.
Show that
$$s_n - \sigma_n = \frac{1}{n+1} \sum_{k=1}^{n} ka_k.$$
Assume that $\lim (na_n) = 0$ and that $\{\sigma_n\}$ converges.
Prove that $\{s_n\}$ converges.
[This gives a converse of (a), but under the additional assumption that $na_n \to 0$.]}
\item[(e)]
\emph{Derive the last conclusion from a weaker hypothesis:
Assume $M \leq \infty$, $|na_n| < M$ for all $n$, and $\lim \sigma_n = \sigma$.
Prove that $\lim s_n = \sigma$, by completing the following outline:}

\emph{If $m < n$, then
$$s_n - \sigma_n
= \frac{m+1}{n-m}(\sigma_n - \sigma_m)
  + \frac{1}{n-m}\sum_{i=m+1}^{n}(s_n - s_i).$$
For these $i$,
$$|s_n - s_i|
\leq \frac{(n-i)M}{i+1}
\leq \frac{(n-m-1)M}{m+2}.$$
Fix $\varepsilon > 0$ and associate with each $n$ the integer $m$ that satisfies
$$m \leq \frac{n-\varepsilon}{1+\varepsilon} < m+1.$$
Then $\frac{m+1}{n-m} \leq \frac{1}{\varepsilon}$ and $|s_n - s_i| < M\varepsilon$.
Hence
$$\limsup_{n \to \infty} |s_n - \sigma| \leq M\varepsilon.$$
Since $\varepsilon$ was arbitrary, $\lim s_n = \sigma$.} \\
\end{enumerate}



\emph{Proof of (a).}
It is similar to Exercise 8.11.
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
For such $\varepsilon > 0$, there is an integer $N' \geq 1$ such that
$$|s_n-s| < \frac{\varepsilon}{64} \text{ whenever } n \geq N'.$$
\item[(2)]
For such $N'$, $\sum_{n=0}^{N'} |s_n-s|$ is finite.
Let $N''$ be an integer such that $$\sum_{n=0}^{N'} |s_n-s| < \frac{N''\varepsilon}{89}$$
(by taking $N'' = \left\lfloor \frac{89}{\varepsilon}\sum_{n=0}^{N'} |s_n-s| \right\rfloor + 1$).
\item[(3)]
Note that
  \begin{align*}
  |\sigma_n - s|
  &= \abs{ \left(\frac{1}{n+1}\sum_{k=0}^n s_k\right) - s } \\
  &= \abs{ \frac{1}{n+1}\sum_{k=0}^n (s_k - s) } \\
  &\leq \frac{1}{n+1}\sum_{k=0}^n |s_k - s|
  \end{align*}
holds for each $n=0,1,2,\ldots$.
In particular, for $n \geq N = \max\{N', N''\} \geq 1$, we have
  \begin{align*}
  |\sigma_{n} - s|
  &\leq \frac{1}{n+1}\sum_{k=0}^{n} |s_k - s| \\
  &\leq \left( \frac{1}{n+1}\sum_{k=0}^{N'}|s_k - s| \right)
    + \left( \frac{1}{n+1}\sum_{k=N'+1}^{n}|s_k - s| \right) \\
  &< \frac{1}{n+1} \cdot \frac{N''\varepsilon}{89}
    + \frac{1}{n+1} \cdot \frac{(n-N')\varepsilon}{64} \\
  &< \frac{\varepsilon}{89} + \frac{\varepsilon}{64} \\
  &< \varepsilon.
  \end{align*}
Therefore, $\lim \sigma_n = s$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
Define $\{s_n\}$ by $s_n = (-1)^{n+1}$.
$\Box$ \\



\emph{Proof of (c).}
Yes.
Define
\begin{equation*}
  s_n =
    \begin{cases}
      \frac{1}{n!} + m^{63}
        & \text{if $n = m^{89}$ for some $m \in \mathbb{Z}$}, \\
      \frac{1}{n!}
        & \text{otherwise}.
    \end{cases}
\end{equation*}
\begin{enumerate}
\item[(1)]
Clearly, $\limsup s_n = \infty$.
\item[(2)]
Given any $n$, there is $m \in \mathbb{Z}$ satisfying $m^{89} \leq n < (m+1)^{89}$.
So
\begin{align*}
  0 < \sigma_n
  &= \frac{1}{n+1}\sum_{k=0}^{n} s_k \\
  &\leq \frac{1}{m^{89}+1}\sum_{k=0}^{n} s_k \\
  &= \frac{1}{m^{89}+1}
    \left( \sum_{k=0}^{n} \frac{1}{n!} + \sum_{k=0}^{m} k^{63} \right) \\
  &\leq \frac{1}{m^{89}+1}
    \left( \sum_{k=0}^{\infty} \frac{1}{n!} + \sum_{k=0}^{m} m^{63} \right) \\
  &= \frac{e + m \cdot m^{63}}{m^{89}+1} \\
  &= \frac{m^{64} + e}{m^{89}+1}.
\end{align*}
Let $n \to \infty$, then $m \to \infty$ and thus $\lim \sigma_n = 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
  \frac{1}{n+1}\sum_{k=1}^{n}ka_k
  &= \frac{1}{n+1}\sum_{k=1}^{n}k(s_k - s_{k-1}) \\
  &= \frac{1}{n+1}\left( \sum_{k=1}^{n}ks_k - \sum_{k=1}^{n}ks_{k-1} \right) \\
  &= \frac{1}{n+1}\left( \sum_{k=1}^{n}ks_k
    - \sum_{k=1}^{n}(k-1)s_{k-1} - \sum_{k=1}^{n}s_{k-1} \right) \\
  &= \frac{1}{n+1}\left( ns_n - \sum_{k=1}^{n}s_{k-1} \right) \\
  &= \frac{1}{n+1}\left( (n+1)s_n - \sum_{k=1}^{n+1}s_{k-1} \right) \\
  &= s_n - \sigma_n.
  \end{align*}
\item[(2)]
Write
$$s_n = \sigma_n + \frac{1}{n+1}\sum_{k=1}^{n}ka_k.$$
Since $\lim_{n \to \infty} (na_n) = 0$,
$\lim_{n \to \infty} \frac{1}{n+1}\sum_{k=1}^{n}ka_k = 0$ ((a)).
Since $\{\sigma_n\}$ converges,
$$\lim_{n \to \infty} s_n
= \lim_{n \to \infty} \sigma_n + \lim_{n \to \infty} \frac{1}{n+1}\sum_{k=1}^{n}ka_k
= \lim_{n \to \infty} \sigma_n$$
(Theorem 3.3(a)).
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
If $m < n$, then
  \begin{align*}
  \sigma_n - \sigma_m
  &= \frac{1}{n+1}\sum_{k=0}^{n}s_{k} - \frac{1}{m+1}\sum_{k=0}^{m}s_{k} \\
  &= \frac{1}{n+1}\sum_{k=0}^{n}s_{k} - \frac{1}{m+1}\sum_{k=0}^{n}s_{k}
    + \frac{1}{m+1}\sum_{i=m+1}^{n}s_i \\
  &= \frac{m-n}{(m+1)(n+1)}\sum_{k=0}^{n}s_{k} + \frac{1}{m+1}\sum_{i=m+1}^{n}s_i \\
  &= \frac{m-n}{m+1}\sigma_n + \frac{1}{m+1}\sum_{i=m+1}^{n}s_i, \\
  \frac{m+1}{n-m}(\sigma_n - \sigma_m)
  &= -\sigma_n + \frac{1}{n-m}\sum_{i=m+1}^{n}s_i \\
  &= -\sigma_n - \frac{1}{n-m}\sum_{i=m+1}^{n}(-s_i) \\
  &= -\sigma_n - \left( \frac{1}{n-m}\sum_{i=m+1}^{n}(s_n - s_i) \right) + s_n, \\
  s_n - \sigma_n
  &= \frac{m+1}{n-m}(\sigma_n - \sigma_m) + \frac{1}{n-m}\sum_{i=m+1}^{n}(s_n - s_i).
  \end{align*}
\item[(2)]
For these $i$,
  \begin{align*}
  |s_n - s_i|
  &= \abs{ \sum_{k=i+1}^{n} a_k }
    &(s_n - s_i = \sum_{k=i+1}^{n} a_k) \\
  &\leq \sum_{k=i+1}^{n} |a_k|
    &\text{(Triangle inequality)} \\
  &< \sum_{k=i+1}^{n} \frac{M}{k}
    &(|ka_k| < M) \\
  &\leq \sum_{k=i+1}^{n} \frac{M}{i+1}
    &(k \geq i+1) \\
  &= \frac{(n-i)M}{i+1} \\
  &= \left( \frac{n-1}{i+1} - 1 \right) M \\
  &\leq \left( \frac{n-1}{m+2} - 1 \right) M
    &(i \geq m+1) \\
  &= \frac{(n-m-1)M}{m+2}.
  \end{align*}
\item[(3)]
Fix $1 > \varepsilon > 0$ and associate with each $n$ the integer $m$ that satisfies
$$m \leq \frac{n-\varepsilon}{1+\varepsilon} < m+1.$$
Clearly, $m \leq \frac{n-\varepsilon}{1+\varepsilon} < \frac{n}{1+\varepsilon} < n$.
Then
$$\frac{m+1}{n-m} \leq \frac{1}{\varepsilon}
\text{ and }
\frac{n-m-1}{m+2} < \varepsilon.$$
Hence $|s_n - s_i| < M\varepsilon$ by (2).
\item[(4)]
By (1)(3),
  \begin{align*}
  s_n - \sigma
  &=
  (\sigma_n - \sigma)
    + \frac{m+1}{n-m}(\sigma_n - \sigma_m)
    + \frac{1}{n-m}\sum_{i=m+1}^{n}(s_n - s_i), \\
  |s_n - \sigma|
  &\leq
  |\sigma_n - \sigma|
    + \frac{m+1}{n-m}|\sigma_n - \sigma_m|
    + \frac{1}{n-m}\sum_{i=m+1}^{n}|s_n - s_i| \\
  &<
  |\sigma_n - \sigma|
    + \frac{1}{\varepsilon} |\sigma_n - \sigma_m|
    + \frac{1}{n-m}\sum_{i=m+1}^{n} M\varepsilon \\
  &=
  |\sigma_n - \sigma|
    + \frac{1}{\varepsilon} |\sigma_n - \sigma_m|
    + M\varepsilon \\
  \end{align*}
holds for any $n$ and $m$ satisfying
$m \leq \frac{n-\varepsilon}{1+\varepsilon} < m+1.$
Since $\{ \sigma_n \}$ converges,
there is an integer $N$ such that
$$|\sigma_n - \sigma_m| < \varepsilon^2 \text{ whenever } m,n \geq N,$$
$$|\sigma_n - \sigma| < \varepsilon \text{ whenever } n \geq N.$$
So,
$$|s_n - \sigma| < (M+2) \varepsilon$$
holds for any $n \geq 2N+3$ (and the corresponding $m$
satisfying $m \leq \frac{n-\varepsilon}{1+\varepsilon} < m+1$
(which implies $m > \frac{n-\varepsilon}{1+\varepsilon} - 1
\geq \frac{n-1}{2} - 1 \geq N$)).
Take limit to get
$$\limsup_{n \to \infty} |s_n - \sigma| \leq (M+2) \varepsilon.$$
Since $\varepsilon$ was arbitrary, $\lim s_n = \sigma$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.15.}
\addcontentsline{toc}{subsection}{Exercise 3.15.}
\emph{Definition 3.21 can be extended to the case in which the $a_n$
lie in some fixed $\mathbb{R}^k$.
Absolute convergence is defined as convergence of $\sum |\mathbf{a}_n|$.
Show that Theorems 3.22, 3.23, 3.25(a), 3.33, 3.34, 3.42, 3.45, 3.47, and 3.55 are true
in this more general settings.
(Only slight modifications are required in any of the proofs.)} \\

\textbf{Definition 3.21.}
\emph{Given a sequence $\{ \mathbf{a}_n \} \subseteq \mathbb{R}^k$,
we use the notation
$$\sum_{n=p}^{q} \mathbf{a}_n \:\: (p \leq q)$$
to denote the sum
$\mathbf{a}_p + \mathbf{a}_{p+1} + \cdots + \mathbf{a}_q$.
With $\{ \mathbf{a}_n \}$ we associate a sequence $\{ \mathbf{s}_n \}$, where
$$\mathbf{s}_n = \sum_{k=1}^{n} \mathbf{a}_k.$$
For $\{ \mathbf{s}_n \}$ we also use the symbolic expression
$$\mathbf{a}_1 + \mathbf{a}_2 + \mathbf{a}_3 + \cdots$$
or, more precisely,
\[
  \sum_{n=1}^{\infty} \mathbf{a}_n. \tag{4}
\]} \\

\emph{The symbol (4) we call an \textbf{infinite series}, or just a \textbf{series}.
The number $\{ \mathbf{s}_n \}$, are called the \textbf{partial sums} of the series.
If $\{ \mathbf{s}_n \}$ converges to $\mathbf{s}$,
we say that the series \textbf{converges}, and write
\[
  \sum_{n=1}^{\infty} \mathbf{a}_n = \mathbf{s}.
\]
The number $\mathbf{s}$ is called the sum of the series;
but it should be clearly understood that
\textbf{$\mathbf{s}$ is the limit of a sequence of sums},
and is not obtained simply by addition.} \\

\emph{If $\{ \mathbf{s}_n \}$ diverges, the series said to be diverge.} \\

\emph{Sometimes, for convenience of notation, we shall consider series of the form
\[
  \sum_{n=0}^{\infty} \mathbf{a}_n. \tag{5}
\]
And frequently, when there is no possible ambiguity,
or when the distinction is immaterial, we shall simply write $\sum \mathbf{a}_n$
in place of (4) or (5).} \\

\emph{It is clear that every theorem about sequences can be stated in terms of
series
(putting $\mathbf{a}_1 = \mathbf{s}_1$
and $\mathbf{a}_n = \mathbf{s}_n - \mathbf{s}_{n-1}$ for $n > 1$),
and vice versa.
But it is nevertheless useful to consider both concepts.} \\\\



\textbf{Theorem 3.22 over $\mathbb{R}^k$.}
\emph{$\sum \mathbf{a}_n$ converges if and only if for every $\varepsilon > 0$
there is an integer $N$ such that
$$\abs{ \sum_{k=n}^m \mathbf{a}_k } \leq \varepsilon$$
if $m \geq n \geq N$.} \\

\emph{Proof of Theorem 3.22 over $\mathbb{R}^k$.}
The Cauchy criterion (Theorem 3.11) can be restated in this form.
$\Box$ \\\\



\textbf{Theorem 3.23 over $\mathbb{R}^k$.}
\emph{If $\sum \mathbf{a}_n$ converges,
then $\lim_{n \to \infty} \mathbf{a}_n = \mathbf{0}$.} \\

\emph{Proof of Theorem 3.23 over $\mathbb{R}^k$.}
By taking $m=n$ in Theorem 3.22 over $\mathbb{R}^k$,
$$\abs{ \mathbf{a}_n } \leq \varepsilon \:\: \text{ whenever } n \geq N.$$
$\Box$ \\\\



\textbf{Theorem 3.25(a) over $\mathbb{R}^k$ (Comparison Test).}
\emph{If $|\mathbf{a}_n| \leq c_n$ for $n \geq N_0$, where $N_0$ is some fixed integer,
and if $\sum c_n$ converges, then $\sum \mathbf{a}_n$ converges.} \\

\emph{Proof of Theorem 3.25(a) over $\mathbb{R}^k$.}
Given $\varepsilon > 0$, there exists $N \geq N_0$ such that $m \geq n \geq N$ implies
\[
  \sum_{k=n}^{m} c_k \leq \varepsilon,
\]
by the Cauchy criterion. Hence
\[
  \abs{ \sum_{k=n}^{m} \mathbf{a}_k }
  \leq \sum_{k=n}^{m} |\mathbf{a}_k|
  \leq \sum_{k=n}^{m} c_k
  \leq \varepsilon,
\]
and (a) follows.
$\Box$ \\\\



\textbf{Theorem 3.33 over $\mathbb{R}^k$ (Root Test).}
\emph{Given $\sum \mathbf{a}_n$,
put $\alpha = \limsup_{n \to \infty} \sqrt[n]{|\mathbf{a}_n|}$.
Then}
\begin{enumerate}
\item[(a)]
\emph{if $\alpha < 1$, $\sum \mathbf{a}_n$ converges;}
\item[(b)]
\emph{if $\alpha > 1$, $\sum \mathbf{a}_n$ diverges;}
\item[(c)]
\emph{if $\alpha = 1$, the test gives no information.} \\
\end{enumerate}

\emph{Proof of Theorem 3.33(a) over $\mathbb{R}^k$.}
If $\alpha < 1$, we can choose $\beta$ so that $\alpha < \beta < 1$,
and an integer $N$ such that
\[
  \sqrt[n]{|\mathbf{a}_n|} < \beta
\]
for $n \geq N$ [by Theorem 3.17(b)].
That is, $n \geq N$ implies
\[
  |\mathbf{a}_n| < \beta^n.
\]
Since $0 < \beta < 1$, $\sum \beta^n$ converges.
Convergence of $\sum \mathbf{a}_n$ follows now from the comparison test.
$\Box$ \\

\emph{Proof of Theorem 3.33(b) over $\mathbb{R}^k$.}
If $\alpha > 1$, again by Theorem 3.17, there is a sequence $\{ n_k \}$ such that
\[
  \sqrt[n_k]{|\mathbf{a}_{n_k}|} \to \alpha.
\]
Hence $|\mathbf{a}_{n}| > 1$ for infinitely many values of $n$,
so that the condition $\mathbf{a}_{n} \to \mathbf{0}$,
necessary for convergence of $\sum \mathbf{a}_n$,
does not hold (Theorem 3.23 over $\mathbb{R}^k$).
$\Box$ \\

\emph{Proof of Theorem 3.33(c) over $\mathbb{R}^k$.}
Same as the original proof.
$\Box$ \\\\



\textbf{Theorem 3.34 over $\mathbb{R}^k$ (Ratio Test).}
\emph{The series $\sum \mathbf{a}_n$}
\begin{enumerate}
\item[(a)]
\emph{converges if
$\limsup_{n \to \infty} \frac{|\mathbf{a}_{n+1}|}{|\mathbf{a}_n|} < 1$,}
\item[(b)]
\emph{diverges if
$\frac{|\mathbf{a}_{n+1}|}{|\mathbf{a}_n|} \geq 1$ for $n \geq N_0$,
where $N_0$ is some fixed integer.} \\
\end{enumerate}

\emph{Proof of Theorem 3.34(a) over $\mathbb{R}^k$.}
If condition (a) holds, we can find $\beta < 1$, and an integer $N$,
such that
\[
  \frac{|\mathbf{a}_{n+1}|}{|\mathbf{a}_n|} < \beta
\]
for $n \geq N$. In particular,
\begin{align*}
  |\mathbf{a}_{N+1}| &< \beta|\mathbf{a}_{N}|, \\
  |\mathbf{a}_{N+2}| &< \beta|\mathbf{a}_{N+1}| < \beta^2 |\mathbf{a}_{N}|, \\
  &\cdots \\
  |\mathbf{a}_{N+p}| &< \beta^p |\mathbf{a}_{N}|.
\end{align*}
That is,
\[
  |\mathbf{a}_{n}| < |\mathbf{a}_{N}|\beta^{-N} \cdot \beta^n
\]
for $n \geq N$, and (a) follows from the comparison test,
since $\sum \beta^n$ converges.
$\Box$ \\

\emph{Proof of Theorem 3.34(b) over $\mathbb{R}^k$.}
If $|\mathbf{a}_{n+1}| \geq |\mathbf{a}_n|$ for $n \geq N_0$,
it is easily seen that the condition $\mathbf{a}_n \to \mathbf{0}$
does not hold, and (b) follows.
$\Box$ \\

\emph{Note.}
The knowledge that $\lim \frac{|\mathbf{a}_{n+1}|}{|\mathbf{a}_n|} = 1$
implies nothing about the convergence of $\sum \mathbf{a}_n$.
The series $\sum \frac{1}{n}$ and $\sum \frac{1}{n^2}$ demonstrate this. \\\\



\textbf{Theorem 3.42 over $\mathbb{R}^k$.}
\emph{Suppose}
\begin{enumerate}
\item[(a)]
\emph{the partial sums $\mathbf{A}_n$ of $\sum \mathbf{a}_n$ form a bounded sequence;}
\item[(b)]
\emph{$b_0 \geq b_1 \geq b_2 \geq \cdots$;}
\item[(c)]
\emph{$\lim_{n \to \infty} b_n = 0$.}
\end{enumerate}
\emph{Then $\sum \mathbf{a}_n b_n$ converges.} \\

\emph{Proof of Theorem 3.42 over $\mathbb{R}^k$.}
Choose $M > 0$ such that $|\mathbf{A}_n| \leq M$ for all $n$.
Given $\varepsilon > 0$, there is an integer $N$ such that $b_N \leq \frac{\varepsilon}{2M}$.
For $N \leq p \leq q$, we have
\begin{align*}
  \abs{ \sum_{n=p}^{q} \mathbf{a}_n b_n }
  &= \abs{ \sum_{n=p}^{q-1} \mathbf{A}_n(b_n - b_{n+1})
    + \mathbf{A}_q b_q - \mathbf{A}_{p-1} b_p} \\
  &\leq M \abs{ \sum_{n=p}^{q-1} (b_n - b_{n+1})
    + b_q + b_p}
    &(b_n - b_{n+1} \geq 0) \\
  &= 2Mb_p \\
  &\leq 2Mb_N \\
  &\leq \varepsilon.
\end{align*}
Convergence now follows from the Cauchy criterion.
$\Box$ \\\\



The series $\sum \mathbf{a}_n$ is said to \textbf{converge absolutely}
if the series $\sum |\mathbf{a}_n|$ converges. \\

\textbf{Theorem 3.45 over $\mathbb{R}^k$.}
\emph{If $\sum \mathbf{a}_n$ converges absolutely,
then $\sum \mathbf{a}_n$ converges.} \\

\emph{Proof of Theorem 3.45 over $\mathbb{R}^k$.}
The assertion follows from the inequality
\[
  \abs{ \sum_{k=n}^{m} \mathbf{a}_k } \leq \sum_{k=n}^{m} |\mathbf{a}_k|
\]
plus the Cauchy criterion.
$\Box$ \\\\



\textbf{Theorem 3.47 over $\mathbb{R}^k$.}
\emph{If $\sum \mathbf{a}_n = \mathbf{A}$, and $\sum \mathbf{b}_n = \mathbf{B}$,
then $\sum (\mathbf{a}_n+\mathbf{b}_n) = \mathbf{A}+\mathbf{B}$,
and $\sum c\mathbf{a}_n = c\mathbf{A}$ for any fixed $c \in \mathbb{R}$.} \\

\emph{Proof of Theorem 3.47 over $\mathbb{R}^k$.}
Let
\[
  \mathbf{A}_n = \sum_{k=0}^{n} \mathbf{a}_k, \qquad
  \mathbf{B}_n = \sum_{k=0}^{n} \mathbf{b}_k.
\]
Then
\[
  \mathbf{A}_n + \mathbf{B}_n = \sum_{k=0}^{n} (\mathbf{a}_k+\mathbf{b}_k).
\]
Since
$\lim_{n \to \infty} \mathbf{A}_n = \mathbf{A}$ and
$\lim_{n \to \infty} \mathbf{B}_n = \mathbf{B}$, we see that
\[
  \lim_{n \to \infty} (\mathbf{A}_n+\mathbf{B}_n) = \mathbf{A}+\mathbf{B}.
\]
The proof of the second assertion is even simpler.
\[
  c\mathbf{A}_n = \sum_{k=0}^{n} (c\mathbf{a}_k).
\]
Since
$\lim_{n \to \infty} \mathbf{A}_n = \mathbf{A}$, we see that
\[
  \lim_{n \to \infty} (c\mathbf{A}_n) = c\mathbf{A}.
\]
$\Box$ \\\\



\textbf{Theorem 3.55 over $\mathbb{R}^k$.}
\emph{If $\sum \mathbf{a}_n$ is a series in $\mathbb{R}^k$ which converges absolutely,
then every rearrangement of $\sum \mathbf{a}_n$ converges,
and they all converge to the same sum.} \\

\emph{Proof of Theorem 3.55 over $\mathbb{R}^k$.}
Let $\sum \mathbf{a}_n'$ be a rearrangement, with partial sums $\mathbf{s}_n'$.
Given $\varepsilon > 0$, there exists an integer $N$ such that $m \geq n \geq N$ implies
\[
  \sum_{i=n}^{m} |\mathbf{a}_i| \leq \varepsilon \tag{26}.
\]
Now choose $p$ such that the integers $1,2,\ldots,N$ are all contained in the set
$k_1, k_2, \ldots, k_p$ (we use the notation of Definition 3.52).
Then if $n > p$, the numbers $\mathbf{a}_1, \ldots, \mathbf{a}_N$
will cancel in the difference $\mathbf{s}_n - \mathbf{s}_n'$,
so that $|\mathbf{s}_n - \mathbf{s}_n'| \leq \varepsilon$, by (26).
Hence $\{ \mathbf{s}_n' \}$ converges to the same sum as $\{ \mathbf{s}_n \}$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.16.}
\addcontentsline{toc}{subsection}{Exercise 3.16.}
\emph{Fix a positive number $\alpha$.
Choose $x_1 > \sqrt{\alpha}$, and define $x_2, x_3, x_4, \ldots$,
by the recursion formula
$$x_{n+1} = \frac{1}{2}\left( x_n + \frac{\alpha}{x_n} \right).$$}
\begin{enumerate}
\item[(a)]
\emph{Prove that $\{x_n\}$ decreases monotonically and that
$\lim x_n = \sqrt{\alpha}$.}
\item[(b)]
\emph{Put $\varepsilon_n = x_n - \sqrt{\alpha}$, and show that
$$\varepsilon_{n+1}
= \frac{\varepsilon_n^2}{2 x_n}
< \frac{\varepsilon_n^2}{2 \sqrt{\alpha}}$$
so that, setting $\beta = 2 \sqrt{\alpha}$,
$$\varepsilon_{n+1}
< \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^n} \:\: (n =1,2,3,\ldots).$$}
\item[(c)]
\emph{This is a good algorithm for computing square roots,
since the recursion formula is simple and the convergence is extremely rapid.
For example, if $\alpha = 3$ and $x_1 = 2$, show that
$\frac{\varepsilon_1}{\beta} < \frac{1}{10}$ and therefore
$$\varepsilon_5 < 4 \cdot 10^{-16}, \:\:
\varepsilon_6 < 4 \cdot 10^{-32}.$$} \\
\end{enumerate}

\emph{Note.}
\begin{enumerate}
  \item[(1)]
  It is the Newton's method described in Exercise 5.25.
  (Here $f(x) = x^2 - \alpha$.)
  \item[(2)]
  It is a special case of Exercise 3.18 by letting $p = 2$. \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $x_n > 0$ for $n=1,2,\ldots$}
  It is trivial by induction on $n$.
  \item[(2)]
  \emph{Show that $x_n > \sqrt{\alpha}$ for $n=1,2,\ldots$}.
  Put $\varepsilon_n = x_n - \sqrt{\alpha}$ as in (b).
  \emph{It is equivalent to show that $\varepsilon_n > 0$ for $n=1,2,\ldots$.}
  Since $x_1 > \sqrt{\alpha}$,
  $\varepsilon_1 = x_1 - \sqrt{\alpha} > 0$. For $n \geq 1$,
  \begin{align*}
    \varepsilon_{n+1}
    &= x_{n+1} - \sqrt{\alpha} \\
    &= \frac{1}{2}\left( x_n + \frac{\alpha}{x_n} \right) - \sqrt{\alpha} \\
    &= \frac{x_n^2 + \alpha - 2\sqrt{\alpha}x_n}{2 x_n} \\
    &= \frac{(x_n - \sqrt{\alpha})^2}{2 x_n} \\
    &> 0
  \end{align*}
  by (1).
  Therefore, $\varepsilon_n > 0$ or $x_n > \sqrt{\alpha}$.
  \item[(3)]
  \emph{Show that $\{x_n\}$ decreases monotonically.}
  \begin{align*}
    x_{n+1} - x_n
    &= \frac{1}{2}\left( x_n + \frac{\alpha}{x_n} \right) - x_n \\
    &= \frac{\alpha - x_n^2}{2x_n} \\
    &< 0
  \end{align*}
  for $n = 1,2,\ldots$ ((1)(2)).
  Hence $\{x_n\}$ decreases monotonically.
  \item[(4)]
  Since $\{x_n\}$ is monotonic and bounded by (2)(3),
  $\{x_n\}$ converges to $x > 0$ (Theorem 3.14).
  $x$ satisfies $$x = \frac{1}{2}\left( x + \frac{\alpha}{x} \right)$$
  (since $\lim x_{n+1} = \lim x_n = x$),
  or $x = \pm \sqrt{\alpha}$.
  Therefore, $\lim x_n = x = \sqrt{\alpha}$ since $x \geq 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By (a)(2), we have
  $$\varepsilon_{n+1}
  = \frac{(x_n - \sqrt{\alpha})^2}{2 x_n}
  = \frac{\varepsilon_n^2}{2 x_n}
  < \frac{\varepsilon_n^2}{2 \sqrt{3}}.$$
\item[(2)]
\emph{Show that
$$\varepsilon_{n+1}
< \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^n}.$$}

Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  $n = 1$.
  $$\varepsilon_2
  < \frac{\varepsilon_1^2}{2 \sqrt{3}}
  = \frac{\varepsilon_1^2}{\beta}
  = \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^1}.$$
  \item[(b)]
  Assume $n=k$ the statement holds.
  Then as $n=k+1$, we have
  \begin{align*}
    \varepsilon_{k+2}
    &< \frac{\varepsilon_{k+1}^2}{\beta}
      &((1)) \\
    &< \frac{1}{\beta} \left( \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^k} \right)^2
      &\text{(Induction hypothesis)}\\
    &= \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^{k+1}}.
  \end{align*}
  \end{enumerate}
  By induction, the statement holds for all $n \in \mathbb{Z}^+$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Since $\varepsilon_1 = x_1 - \sqrt{\alpha} = 2 - \sqrt{3}$ and
  $\beta = 2 \sqrt{\alpha} = 2 \sqrt{3}$ and $\sqrt{3} < 1.8$,
  $$\frac{\varepsilon_1}{\beta}
  = \frac{2 - \sqrt{3}}{2 \sqrt{3}}
  = \frac{2\sqrt{3} - 3}{6}
  < \frac{2\cdot 1.8 - 3}{6}
  = \frac{1}{10}.$$
\item[(2)]
  Since $\beta = 2 \sqrt{\alpha} = 2 \sqrt{3} < 4$,
  by (b) we have
  $$\varepsilon_5
  < \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^4}
  < 4 \cdot (10^{-1})^{16}
  = 4 \cdot 10^{-16},$$
  $$\varepsilon_6
  < \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^5}
  < 4 \cdot (10^{-1})^{32}
  = 4 \cdot 10^{-32}.$$
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.17.}
\addcontentsline{toc}{subsection}{Exercise 3.17.}
\emph{Fix $\alpha > 1$. Take $x_1 > \sqrt{\alpha}$, and define
$$x_{n+1} = \frac{\alpha+x_n}{1+x_n} = x_n + \frac{\alpha-x_n^2}{1+x_n}.$$}
\begin{enumerate}
\item[(a)]
\emph{Prove that $x_1 > x_3 > x_5 > \cdots$.}
\item[(b)]
\emph{Prove that $x_2 < x_4 < x_6 < \cdots$.}
\item[(c)]
\emph{Prove that $\lim x_n = \sqrt{\alpha}$.}
\item[(d)]
\emph{Compare the rapidity of convergence of this process
with the one described in Exercise 3.16.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
\begin{align*}
  x_{n+1} - \sqrt{\alpha}
  &= \frac{\alpha+x_n}{1+x_n} - \sqrt{\alpha} \\
  &= -\frac{(\sqrt{\alpha}-1)(x_n - \sqrt{\alpha})}{x_n+1}
\end{align*}
holds for $n \geq 1$.
\item[(2)]
\begin{align*}
  x_{n+2} - x_n
  &= \frac{\alpha+x_{n+1}}{1+x_{n+1}} - x_n \\
  &= \frac{\alpha+\frac{\alpha+x_n}{1+x_n}}{1+\frac{\alpha+x_n}{1+x_n}} - x_n \\
  &= \frac{\alpha x_n + x_n + 2\alpha}{2 x_n + \alpha + 1} - x_n \\
  &= -\frac{2 (x_n^2 - \alpha)}{2 x_n + \alpha + 1}
\end{align*}
holds for $n \geq 1$.
\item[(3)]
Since $x_1,x_3,x_5,\ldots > \sqrt{\alpha}$ (by (1)),
$x_1 > x_3 > x_5 > \cdots$ by (2).
\end{enumerate}
$\Box$ \\

\emph{Proof of (b).}
Since $x_1 > \sqrt{\alpha}$, $x_2 < \sqrt{\alpha}$ by (a)(1).
Hence $x_2 < x_4 < x_6 < \cdots$ by (a)(2).
$\Box$ \\

\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Since $\{x_{2n+1}\}$ is monotonic and bounded by (a),
  $\{x_{2n+1}\}$ converges to $x_1 \geq \sqrt{\alpha}$ (Theorem 3.14).
\item[(2)]
  Since $\{x_{2n}\}$ is monotonic and bounded by (a),
  $\{x_{2n}\}$ converges to $x_2 \leq \sqrt{\alpha}$ (Theorem 3.14).
\item[(3)]
  In any case, $x = x_1$ or $x = x_2$ satisfy
  \[
    0 = -\frac{2(x^2 - \alpha)}{2x + \alpha + 1}
  \]
  by (a)(2)
  (since $\lim x_{n+2} = \lim x_n = x$),
  or $x = \pm \sqrt{\alpha}$.
  Therefore, $\lim x_{2n+1} = \lim x_{2n} = x = \sqrt{\alpha}$ since $x \geq 0$.
  Hence $\lim x_n = x = \sqrt{\alpha}$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (d).}
Put $\varepsilon_n = |x_n - \sqrt{\alpha}|$, and by (a)(1) we have
$$\varepsilon_{n+1}
\leq \frac{\sqrt{\alpha} - 1}{x_1+1} \varepsilon_n$$
for $n \geq 1$. (Here $0 < x_n \leq x_1$.)
Therefore, the convergence is geometric, not quadratically geometric in Exercise 3.16,
that is, the rate of convergence is slower than one in Exercise 3.16.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.18.}
\addcontentsline{toc}{subsection}{Exercise 3.18.}
\emph{Replace the recursion formula of Exercise 3.16 by
$$x_{n+1} = \frac{p-1}{p} x_n + \frac{\alpha}{p} x_n^{-p+1}$$
where $p$ is a fixed positive integer,
and describe the behavior of the resulting sequences $\{x_n\}$.} \\

Might assume that $p > 1$ since the case $p = 1$ is nothing to do. \\

\emph{Outline.}
Let $\xi = \alpha^{\frac{1}{p}}$.
\begin{enumerate}
\item[(a)]
\emph{Prove that $\{x_n\}$ decreases monotonically and that
$\lim x_n = \xi$.}
\item[(b)]
\emph{Put $\varepsilon_n = x_n - \xi$, and show that
\[
  \varepsilon_{n+1}
  < \frac{(p-1)^2\varepsilon_n^2}{p x_n} \\
  < \frac{(p-1)^2\varepsilon_n^2}{p \alpha^{\frac{1}{p}}}
\]
so that, setting $\beta = \frac{p \alpha^{\frac{1}{p}}}{(p-1)^2}$,
$$\varepsilon_{n+1}
< \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^n} \:\: (n =1,2,3,\ldots).$$ } \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $x_n > 0$ for $n=1,2,\ldots$}
  It is trivial by induction on $n$.
  \item[(2)]
  \emph{Show that $x_n > \xi$ for $n=1,2,\ldots$}.
  Put $\varepsilon_n = x_n - \xi$ as in (b).
  \emph{It is equivalent to show that $\varepsilon_n > 0$ for $n=1,2,\ldots$.}
  Since $x_1 > \xi$,
  $\varepsilon_1 = x_1 - \xi > 0$. For $n \geq 1$,
  \begin{align*}
    \varepsilon_{n+1}
    &= x_{n+1} - \xi \\
    &= \frac{p-1}{p} x_n + \frac{\alpha}{p} x_n^{-p+1} - \xi \\
    &= \frac{p-1}{p} (x_n-\xi)
      - \frac{1}{p}\left( \xi - \xi^p x_n^{-p+1} \right) \\
    &= \frac{p-1}{p} (x_n-\xi)
      - \frac{\xi}{p x_n^{p-1}}( x_n^{p-1} - \xi^{p-1} ) \\
    &= \frac{p-1}{p} (x_n-\xi)
      - \frac{\xi}{p x_n^{p-1}}(x_n - \xi)( x_n^{p-2} + \cdots + \xi^{p-2} ) \\
    &> \frac{p-1}{p} (x_n-\xi)
      - \frac{\xi}{p x_n^{p-1}}(x_n - \xi)(p-1)x_n^{p-2} \\
    &= \frac{p-1}{p} (x_n-\xi)\left( 1 - \frac{\xi}{x_n} \right) \\
    &= \frac{(p-1)(x_n-\xi)^2}{p x_n} \\
    &> 0
  \end{align*}
  by (1).
  Therefore, $\varepsilon_n > 0$ or $x_n > \sqrt{\alpha}$.
  \item[(3)]
  \emph{Show that $\{x_n\}$ decreases monotonically.}
  \begin{align*}
    x_{n+1} - x_n
    &= \frac{p-1}{p} x_n + \frac{\alpha}{p} x_n^{-p+1} - x_n \\
    &= \frac{\xi^p - x_n^p}{p x_n^{p-1}} \\
    &< 0
  \end{align*}
  for $n = 1,2,\ldots$ ((1)(2)).
  Hence $\{x_n\}$ decreases monotonically.
  \item[(4)]
  Since $\{x_n\}$ is monotonic and bounded by (2)(3),
  $\{x_n\}$ converges to $x > 0$ (Theorem 3.14).
  $x$ satisfies $$x = \frac{p-1}{p} x + \frac{\alpha}{p} x^{-p+1}$$
  (since $\lim x_{n+1} = \lim x_n = x$),
  or $x^p = \alpha$.
  Therefore, $\lim x_n = x = \alpha^{\frac{1}{p}}$ since $x \geq 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By (a)(2), we have
  \begin{align*}
    \varepsilon_{n+1}
    &= \frac{p-1}{p} (x_n-\xi)
      - \frac{\xi}{p x_n^{p-1}}(x_n - \xi)( x_n^{p-2} + \cdots + \xi^{p-2} ) \\
    &< \frac{p-1}{p} (x_n-\xi)
      - \frac{\xi}{p x_n^{p-1}}(x_n-\xi) (p-1)\xi^{p-2} \\
    &= \frac{(p-1)\varepsilon_n}{p x_n^{p-1}}( x_n^{p-1} - \xi^{p-1}) \\
    &= \frac{(p-1)\varepsilon_n}{p x_n^{p-1}}(x_n - \xi)( x_n^{p-2} + \cdots + \xi^{p-2} ) \\
    &< \frac{(p-1)\varepsilon_n}{p x_n^{p-1}}(x_n - \xi)(p-1)x_n^{p-2} \\
    &= \frac{(p-1)^2\varepsilon_n^2}{p x_n} \\
    &< \frac{(p-1)^2\varepsilon_n^2}{p \alpha^{\frac{1}{p}}}.
  \end{align*}
\item[(2)]
\emph{Show that
$$\varepsilon_{n+1}
< \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^n}.$$}

Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  $n = 1$.
  $$\varepsilon_2
  < \frac{(p-1)^2\varepsilon_1^2}{p \alpha^{\frac{1}{p}}}
  = \frac{\varepsilon_1^2}{\beta}
  = \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^1}.$$
  \item[(b)]
  Assume $n=k$ the statement holds.
  Then as $n=k+1$, we have
  \begin{align*}
    \varepsilon_{k+2}
    &< \frac{\varepsilon_{k+1}^2}{\beta}
      &((1)) \\
    &< \frac{1}{\beta} \left( \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^k} \right)^2
      &\text{(Induction hypothesis)}\\
    &= \beta \left( \frac{\varepsilon_1}{\beta} \right)^{2^{k+1}}.
  \end{align*}
  \end{enumerate}
  By induction, the statement holds for all $n \in \mathbb{Z}^+$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.19.}
\addcontentsline{toc}{subsection}{Exercise 3.19.}
\emph{Associate to each sequence $a = \{\alpha_n\}$,
in which $\alpha_n$ is $0$ or $2$, the real number
$$x(a) = \sum_{n=1}^{\infty} \frac{\alpha_n}{3^n}.$$
Prove that the set of all $x(a)$ is precisely the Cantor set described in Sec. 2.44.} \\

\emph{Cantor set.}
Let $E_0$ be the interval $[0,1]$.
Remote the segment $(\frac{1}{3},\frac{2}{3})$,
and let $E_1$ be the union of the intervals
\[
  \left[ 0,\frac{1}{3} \right] \text{ and } \left[ \frac{2}{3},1 \right].
\]
Remote the middle thirds of these intervals,
and let $E_2$ be the union of the intervals
\[
  \left[ 0,\frac{1}{9} \right],
  \left[ \frac{2}{9},\frac{3}{9} \right],
  \left[ \frac{6}{9},\frac{7}{9} \right] \text{ and }
  \left[ \frac{8}{9},1 \right].
\]
Continuing in this way, we obtain a sequence of compact set $E_n$, such that
\begin{enumerate}
\item[(a)]
$E_1 \supseteq E_2 \supseteq E_3 \supseteq \cdots$;
\item[(b)]
$E_n$ is the union of $2^n$ intervals, each of length $3^{-n}$.
\end{enumerate}
The set
\[
  P = \bigcap_{n=1}^{\infty} E_n
\]
is called the Cantor set.
$P$ is compact, non empty, perfect, uncountable and measure zero. \\



\emph{Proof.}
Let
\[
  C = \{ x(a) : \text{$a = \{ \alpha_n \}$, in which $\alpha_n$ is $0$ or $2$ } \}.
\]
\begin{enumerate}
\item[(1)]
$(P \subseteq C)$.
Given any
\[
  x \in P = \bigcap_{n=1}^{\infty} E_n.
\]
Hence $x \in E_n$ for all $n \geq 1$.
Write $x = \sum_{n=1}^{\infty} \frac{\alpha_n}{3^n}$
where $\alpha_n \in \{0,1,2\}$ for $n \geq 1$.
(It is possible since $0 \leq x \leq 1$
and every point in the $[0,1]$ has the ternary notation.)
  \begin{enumerate}
  \item[(a)]
  $x \in E_1$. So
  \begin{align*}
    &x \in \left[ 0,\frac{1}{3} \right] \bigcup \left[ \frac{2}{3},1 \right] \\
    \Longleftrightarrow&
    x \in \left[ 0,\frac{1}{3} \right], \left[ \frac{2}{3},1 \right] \\
    \Longleftrightarrow&
    \alpha_1 \in \{ 0,2 \}.
  \end{align*}
  Here we express $\frac{1}{3}$ as $(0.0\overline{2})_{3}$ instead of $(0.1)_{3}$.
  \item[(b)]
  $x \in E_2$. So
  \begin{align*}
    &x \in
      \left[ 0,\frac{1}{9} \right] \bigcup
      \left[ \frac{2}{9},\frac{3}{9} \right] \bigcup
      \left[ \frac{6}{9},\frac{7}{9} \right] \bigcup
      \left[ \frac{8}{9},1 \right] \\
    \Longleftrightarrow&
    x \in \left[ 0,\frac{1}{9} \right],
      \left[ \frac{2}{9},\frac{3}{9} \right],
      \left[ \frac{6}{9},\frac{7}{9} \right],
      \left[ \frac{8}{9},1 \right] \\
    \Longleftrightarrow&
    \alpha_1 \in \{ 0,2 \},
    \alpha_2 \in \{ 0,2 \}.
  \end{align*}
  \item[(c)]
  Continuing in this way, we obtain a sequence of $\alpha_n$
  such that $\alpha_n \in \{0,2\}$ for $n \geq 1$.
  Therefore, $x \in C$.
  \end{enumerate}
\item[(2)]
$(C \subseteq P)$.
Given any
\[
  x = \sum_{n=1}^{\infty} \frac{\alpha_n}{3^n} \in C.
\]
Applying the same argument in (1), we have
$x \in E_n$ for all $n \geq 1$.
Therefore, $x \in \bigcap E_n = P$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.20.}
\addcontentsline{toc}{subsection}{Exercise 3.20.}
\emph{Suppose $\{p_n\}$ is a Cauchy sequence in a metric space $X$,
and some subsequence $\{p_{n_i}\}$ converges to a point $p \in X$.
Prove that the full sequence $\{p_n\}$ converges to $p$. } \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Since $\{p_n\}$ is a Cauchy sequence, there exists a positive integer $N_1$ such that
$$d(p_n,p_m) < \frac{\varepsilon}{2} \text{ whenever } n, m \geq N_1.$$
\item[(2)]
Since the subsequence $\{p_{n_i}\}$ converges to a point $p \in X$,
there exists a positive integer $N_2$ such that
$$d(p_{n_i},p) < \frac{\varepsilon}{2} \text{ whenever } n_i \geq N_2.$$
\item[(3)]
Let $N = \max\{N_1, N_2\}$ be a positive integer.
So
\begin{align*}
d(p_n,p)
&\leq d(p_n,p_{n_i}) + d(p_{n_i}, p)
  &\text{(Definition 2.15(c))} \\
&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \text{ whenever } n, n_i \geq N
  &\text{((1)(2))} \\
&= \varepsilon \text{ whenever } n \geq N.
\end{align*}
Hence the full sequence $\{p_n\}$ converges to $p$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.21.}
\addcontentsline{toc}{subsection}{Exercise 3.21.}
\emph{Prove the following analogue of Theorem 3.10(b):
If $\{E_n\}$ is a sequence of closed and bounded sets in a complete metric space $X$,
if $E_n \supseteq E_{n+1}$, and if
$$\lim_{n \to \infty} \mathrm{diam}(E_n) = 0,$$
then $\bigcap_{n=1}^{\infty} E_n$ consists of exactly one point.} \\

Assume $E_n \neq \varnothing$. It is unnecessary to assume that $E_n$ is bounded
since we have the condition that $\lim_{n \to \infty} \mathrm{diam}(E_n) = 0$.\\

\emph{Note.}
Every compact metric space is complete, but complete spaces need not be compact.
In fact, a metric space is compact if and only if it is complete and totally bounded. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Pick $p_n \in E_n$ for $n = 1, 2, \ldots$.
\item[(2)]
\emph{Show that $\{p_n\}$ is a Cauchy sequence.}
Given any $\varepsilon > 0$.
There is a positive integer $N$ such that
$\mathrm{diam}(E_n) < \varepsilon$ whenever $n \geq N$.
Especially, $$\mathrm{diam}(E_N) < \varepsilon.$$
As $m, n \geq N$, $p_m \in E_m \subseteq E_N$ and $p_n \in E_n \subseteq E_N$.
By the definition of the diameter of $E_N$,
$$d(p_m,p_n) \leq \mathrm{diam}(E_N) < \varepsilon \text{ whenever } m,n \geq N.$$
\item[(3)]
Since $X$ is complete, $\{p_n\}$ converges to a point $p \in X$.
\item[(4)]
\emph{Show that $p \in \bigcap_{n=1}^{\infty} E_n$.}
(Reductio ad absurdum)
If there were some $n$ such that $p \not\in E_{n}$.
Consider the subsequence
$$p_{n}, p_{n+1}, p_{n+2}, \ldots.$$
Note that all $p_{n}, p_{n+1}, \ldots$ are in $E_n$.
By (3), it converges to $p$. Thus $p$ is a limit point of $E_n$.
Since $E_n$ is closed, $p \in E_n$, which is absurd.
\item[(5)]
\emph{Show that $\bigcap_{n=1}^{\infty} E_n = \{p\}$.}
(Reductio ad absurdum)
If there were $q \in \bigcap_{n=1}^{\infty} E_n$ with $q \neq p$,
then $d(p,q) > 0$ (Definition 2.15(a)).
It implies that
$$\mathrm{diam}(E_n)
\geq d(p,q) > 0 \text{ for all } n,$$
contrary to $\lim_{n \to \infty} \mathrm{diam}(E_n) = 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.22 (Baire category theorem).}
\addcontentsline{toc}{subsection}{Exercise 3.22 (Baire category theorem).}
\emph{Suppose $X$ is a complete metric space,
and $\{G_n\}$ is a sequence of dense open subsets of $X$.
Prove Baire's theorem, namely, that $\bigcap^\infty_1{G_n}$ is not empty.
(In fact, it is dense in $X$.)
(Hint: Find a shrinking sequence of neighborhoods $E_n$ such
that $\overline{E_n} \subseteq G_n$, and apply Exercise 3.21.) } \\

\emph{Proof.}
Given any open set $G_0$ in $X$,
will show that $$\bigcap_{n=0}^{\infty} G_n \neq \varnothing.$$
\begin{enumerate}
\item[(1)]
Since $G_1$ is dense, $G_0 \cap G_1$ is nonempty.
Take any one point $p_1$ in the open set $G_0 \cap G_1$,
then there exists a closed neighborhood
$$V_1
= \{ q \in X : d(q,p_1) < r_1 \}$$
of $p_1$ with $r_1 < 1$
such that
$$V_1 \subseteq G_0 \cap G_1.$$
Take $U_1 \subseteq E_1 \subseteq V_1$
such that
\begin{align*}
E_1 &= \left\{ q \in X : d(q,p_1) \leq \frac{r_1}{64} \right\} \subseteq V_1, \\
U_1 &= \left\{ q \in X : d(q,p_1) < \frac{r_1}{89} \right\} \subseteq E_1.
\end{align*}
\item[(2)]
Suppose $V_n, E_n, U_n$ have been constructed,
take any one point $p_{n+1}$ in the open set $U_n \cap G_{n+1}$,
there exists an open neighborhood
$$V_{n+1}
= \{ q \in X : d(q,p_{n+1}) < r_{n+1} \}$$
of $p_{n+1}$ with $r_{n+1}$ with $r_{n+1} < \frac{1}{n+1}$
such that
$$V_{n+1} \subseteq U_n \cap G_{n+1}.$$
Take $U_1 \subseteq E_1 \subseteq V_1$
such that
\begin{align*}
E_{n+1} &= \left\{ q \in X : d(q,p_{n+1}) \leq \frac{r_{n+1}}{64} \right\} \subseteq V_{n+1}, \\
U_{n+1} &= \left\{ q \in X : d(q,p_{n+1}) < \frac{r_{n+1}}{89} \right\} \subseteq E_{n+1}.
\end{align*}
\item[(3)]
Note that
  \begin{enumerate}
  \item[(a)]
  $E_n$ is closed and nonempty (since $p_n \in E_n$).
  \item[(b)]
  $\lim_{n \to \infty} \mathrm{diam}(E_n) = 0$
  (since $\mathrm{diam}(E_n) \leq 2 \cdot \frac{r_n}{64} < r_n < \frac{1}{n}$.)
  \item[(c)]
  $E_1 \supseteq E_2 \supseteq \cdots$
  (since
  $E_{n+1} \subseteq V_{n+1} \subseteq U_n \cap G_{n+1} \subseteq U_n \subseteq E_n$).
  \end{enumerate}
Since $X$ is complete, by Exercise 3.21,
$$\bigcap_{n=1}^{\infty} E_n = \{p\}$$
for some $p \in X$.
\item[(4)]
Hence
\begin{align*}
p \in \bigcap_{n=1}^{\infty} E_n
&\Longleftrightarrow
p \in E_n \text{ for all } n=1,2,3,\ldots \\
&\Longrightarrow
p \in E_1 \subseteq G_0 \cap G_1 \text{ and }
p \in E_{n+1} \subseteq U_n \cap G_{n+1} \subseteq G_{n+1} \\
&\Longrightarrow
p \in G_0 \cap G_1 \cap \cdots = \bigcap_{n=0}^{\infty} G_n \\
&\Longrightarrow
\bigcap_{n=0}^{\infty} G_n \neq \varnothing.
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.23.}
\addcontentsline{toc}{subsection}{Exercise 3.23.}
\emph{Suppose $\{p_n\}$ and $\{q_n\}$ are Cauchy sequences in a metric space $X$.
Show that the sequence $\{d(p_n,q_n)\}$ converges.
(Hint: For any $m, n$,
$$d(p_n,q_n) \leq d(p_n,p_m) + d(p_m,q_m) + d(q_m,q_n);$$
it follows that
$$|d(p_n,q_n) - d(p_m,q_m)|$$
is small if $m$ and $n$ are large.)} \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Since $\{p_n\}$ and $\{q_n\}$ are Cauchy sequences,
there exists $N$ such that
$$d(p_n,p_m) < \frac{\varepsilon}{2} \text{ and }
d(q_m,q_n) < \frac{\varepsilon}{2}$$ whenever $m, n \geq N$.
\item[(2)]
Note that
$$d(p_n,q_n) \leq d(p_n,p_m) + d(p_m,q_m) + d(q_m,q_n).$$
It follows that
$$|d(p_n,q_n) - d(p_m,q_m)|
\leq d(p_n,p_m) + d(q_m,q_n)
< \frac{\varepsilon}{2} + \frac{\varepsilon}{2}
= \varepsilon.$$
Thus $\{d(p_n,q_n)\}$ is a Cauchy sequence in $\mathbb{R}^1$ (not in $X$).
\item[(3)]
Since $\mathbb{R}^1$ is a complete metric space, $\{d(p_n,q_n)\}$ converges.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.24.}
\addcontentsline{toc}{subsection}{Exercise 3.24.}
\emph{Let $X$ be a metric space.}
\begin{enumerate}
\item[(a)]
\emph{Call two Cauchy sequences $\{p_n\}$, $\{q_n\}$ in $X$ equivalent if
$$\lim_{n \to \infty}{d(p_n,q_n)} = 0.$$
Prove that this is an equivalence relation.}
\item[(b)]
\emph{Let $X^*$ be the set of all equivalence classes so obtained.
If $P \in X^*$, $Q \in X^*$, $\{p_n\} \in P$, $\{q_n\} \in Q$, define
$$\Delta(P,Q) = \lim_{n \to \infty} d(p_n,q_n);$$
by Exercise 3.23, this limit exists.
Show that the number
$\Delta(P,Q)$ is unchanged if $\{p_n\}$ and $\{q_n\}$ are replaced by equivalent sequences,
and hence that $\Delta$ is a distance function in $X^*$.}
\item[(c)]
\emph{Prove that the resulting metric space $X^*$ is complete.}
\item[(d)]
\emph{For each $p \in X$, there is a Cauchy sequence all of whose terms are $p$;
let $P_p$ be the element of $X^*$ which contains this sequence.
Prove that
$$\Delta(P_p,P_q) = d(p,q)$$
for all $p,q \in X$.
In other words, the mapping $\varphi$ defined by $\varphi(p) = P_p$
is an isometry
(i.e., a distance-preserving mapping) of $X$ into $X^*$.}
\item[(e)]
\emph{Prove that $\varphi(X)$ is dense in $X^*$, and that $\varphi(X) = X^*$ if $X$ is complete.
By (d), we may identify $X$ and $\varphi(X)$
and thus regard $X$ as embedded in the complete metric space $X^*$.
We call $X^*$ the \textbf{completion} of $X$.} \\
\end{enumerate}



\emph{Proof of (a).}
Given Cauchy sequences $\{p_n\}$, $\{q_n\}$, $\{r_n\}$ in $X$.
\begin{enumerate}
\item[(1)]
\emph{(Reflexivity)}
$$\lim_{n \to \infty}{d(p_n,q_n)} = \lim_{n \to \infty} 0 = 0$$
by the reflexivity of the metric function $d$.
\item[(2)]
\emph{(Symmetry)}
$$\lim_{n \to \infty}{d(p_n,q_n)} = \lim_{n \to \infty}{d(q_n,p_n)} = 0$$
by the symmetry of the metric function $d$.
\item[(3)]
\emph{(Transitivity)}
Suppose that $\lim_{n \to \infty}{d(p_n,q_n)} = \lim_{n \to \infty}{d(q_n,r_n)} = 0$.
By the triangle inequality of the metric function $d$,
we have
$$0 \leq d(p_n,r_n) \leq d(p_n,q_n)+d(q_n,r_n).$$
Take limit to get
\begin{align*}
0
&\leq \lim_{n \to \infty}d(p_n,r_n) \\
&\leq \lim_{n \to \infty}(d(p_n,q_n)+d(q_n,r_n)) \\
&= \lim_{n \to \infty}d(p_n,q_n) + \lim_{n \to \infty}d(q_n,r_n) \\
&= 0
\end{align*}
or $\lim_{n \to \infty}d(p_n,r_n) = 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\Delta$ is well-defined.}
Given any $\{p_n\}, \{p'_n\} \in P$ and $\{q_n\}, \{q'_n\} \in Q$.
  \begin{enumerate}
  \item[(a)]
  $\lim_{n \to \infty}d(p_n,p'_n) = 0$
  since $\{p_n\}$ and $\{p'_n\}$ are in the same equivalence class.
  \item[(b)]
  $\lim_{n \to \infty}d(q_n,q'_n) = 0$ (similar to (a)).
  \item[(c)]
  \emph{Show that $\lim_{n \to \infty}d(p_n,q_n) \leq \lim_{n \to \infty}d(p'_n,q'_n)$.}
  Since $d(p_n,q_n) \leq d(p_n,p'_n)+d(p'_n,q'_n)+d(q'_n,q_n)$,
  take limit to get
  \begin{align*}
    \lim_{n \to \infty} d(p_n,q_n)
    &\leq \lim_{n \to \infty}(d(p_n,p'_n)+d(p'_n,q'_n)+d(q'_n,q_n)) \\
    &= \lim_{n \to \infty}d(p_n,p'_n)
      + \lim_{n \to \infty}d(p'_n,q'_n)
      + \lim_{n \to \infty}d(q'_n,q_n) \\
    &= 0 + \lim_{n \to \infty}d(p'_n,q'_n) + 0 \\
    &= \lim_{n \to \infty}d(p'_n,q'_n)
  \end{align*}
  since (a)(b).
  \item[(d)]
  \emph{Show that $\lim_{n \to \infty}d(p_n,q_n) \geq \lim_{n \to \infty}d(p'_n,q'_n)$.}
  Similar to (c).
  \end{enumerate}
By (c)(d), $\lim_{n \to \infty}d(p_n,q_n) = \lim_{n \to \infty}d(p'_n,q'_n)$,
or $\Delta(P,Q)$ is well-defined.
\item[(2)]
\emph{Show that $\Delta$ is a metric.}
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $\Delta(P,Q) > 0$ if $P \neq Q$; $\Delta(P,P) = 0$.}
  It is the definition of $\Delta$.
  \item[(b)]
  \emph{Show that $\Delta(P,Q) = \Delta(Q,P)$.}
  Similar to the argument in (a)(2).
  \item[(c)]
  \emph{Show that $\Delta(P,Q) \leq \Delta(P,R) + \Delta(R,Q)$.}
  Similar to the argument in (a)(3).
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\emph{Show that $\{P_k\}_{k=1}^{\infty}$ converges to $P$ in $(X^*, \Delta)$
for any given Cauchy sequence $\{P_k\}$.}
\begin{enumerate}
\item[(1)]
Take a Cauchy sequence $\{ p^{(k)}_n \}_{n=1}^{\infty}$ to represent $P_k$ for each $k$.
\emph{We will construct a Cauchy sequence $\{ p_k \}$ in $(X,d)$ such that
$\{P_k\}$ converges to $P$ which is the equivalent class of $\{ p_k \}$.}
\item[(2)]
For each $k$,
there exists $N_k$ such that
$$d\left(p^{(k)}_m,p^{(k)}_n\right) < \frac{1}{k} \text{ whenever } m,n \geq N_k.$$
Especially,
$$d\left(p^{(k)}_m,p^{(k)}_{N_k}\right) < \frac{1}{k} \text{ whenever } m \geq N_k.$$
Let $p_k = p^{(k)}_{N_k}$ and collect all $p_k$ as $\{ p_k \}_{k=1}^{\infty}$.
\item[(3)]
\emph{Show that $\{p_k\}$ is a Cauchy sequence in $(X,d)$.}
Note that for any $k$, we have
  \begin{align*}
    d(p_m,p_n)
    &= d\left(p^{(m)}_{N_m},p^{(n)}_{N_n}\right) \\
    &\leq d\left(p^{(m)}_{N_m},p^{(m)}_k\right)
      + d\left(p^{(m)}_k,p^{(n)}_k\right)
      + d\left(p^{(n)}_k,p^{(n)}_{N_n}\right).
  \end{align*}
Let $k \to \infty$, we have
  \begin{align*}
    d(p_m,p_n)
    &\leq \limsup_{k \to \infty}\left[ d\left(p^{(m)}_{N_m},p^{(m)}_k\right)
      + d\left(p^{(m)}_k,p^{(n)}_k\right)
      + d\left(p^{(n)}_k,p^{(n)}_{N_n}\right) \right] \\
    &\leq \frac{1}{m} + \Delta(P_m,P_n) + \frac{1}{n}
  \end{align*}
for any $m, n$ (by (2)).
Let $m, n \to \infty$, we establish the result (since $\{P_k\}$ is Cauchy).
\item[(4)]
\emph{Show that $\{P_k\}$ converges to $P \ni \{ p_k \}$.}
Given any $\varepsilon > 0$.
Since $\{p_k\}$ is Cauchy (3), there is $N > \frac{2}{\varepsilon}$ such that
$$d(p_m,p_n) < \frac{\varepsilon}{2} \text{ whenever } m,n \geq N.$$
Note that
  \begin{align*}
    d\left(p^{(k)}_n,p_n\right)
    &= d\left(p^{(k)}_n,p^{(n)}_{N_n}\right) \\
    &\leq d\left(p^{(k)}_n,p^{(k)}_{N_k}\right)
      + d\left(p^{(k)}_{N_k},p^{(n)}_{N_n}\right).
  \end{align*}
For any $k \geq N$, let $n \to \infty$ to get
  \begin{align*}
    \Delta(P_k,P)
    &= \lim_{n \to \infty} d\left(p^{(k)}_n,p_n\right) \\
    &\leq \limsup_{n \to \infty} d\left(p^{(k)}_n,p^{(k)}_{N_k}\right)
      + \limsup_{n \to \infty} d\left(p^{(k)}_{N_k},p^{(n)}_{N_n}\right) \\
    &< \frac{1}{k} + \frac{\varepsilon}{2} \\
    &\leq \frac{1}{N} + \frac{\varepsilon}{2} \\
    &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    &< \varepsilon.
  \end{align*}
\end{enumerate}
Hence, $(X^*, \Delta)$ is complete.
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
Define $\{p_n\}$ by $p_n = p$ $(n = 1,2,\ldots)$ for any $p \in X$.
\item[(2)]
\emph{Show that $\{p_n\}$ is a Cauchy sequence.}
$d(p_m,p_n) = d(p,p) = 0$.
\item[(3)]
Take $\{p\} \in P_p$ and $\{q\} \in P_q$.
Then
$$\Delta(P_p,P_q)
= \lim_{n \to \infty} d(p_n,q_n)
= \lim_{n \to \infty} d(p,q)
= d(p,q).$$
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\varphi(X)$ is dense in $X^*$.}
Given any $P \in X^*$, any $\{p_n\} \in P$ and any $\varepsilon > 0$.
Since $\{p_n\}$ is Cauchy, there is $N$ such that
$$d(p_m,p_n) < \frac{\varepsilon}{64} \text{ whenever } m,n \geq N.$$
Note that $p_N \in X$.
Pick $\{p_N\} \in P_{p_N} = \varphi(p_N) \in \varphi(X)$.
So
$$\Delta(P,P_{p_N})
= \lim_{n \to \infty} d(p_n,p_N)
\leq \frac{\varepsilon}{64}
< \varepsilon.$$
Hence $\varphi(X)$ is dense in $X^*$.
\item[(2)]
\emph{Show that $\varphi(X) = X^*$ if $X$ is complete.}
Given any $P \in X^* \ni \{p_n\}$.
Since $X$ is complete, a Cauchy sequence $\{p_n\}$ converges to $p \in X$.
Pick $\{p\} \in P_p = \varphi(p) \in \varphi(X)$.
So
$$\Delta(P,P_p)
= \lim_{n \to \infty} d(p_n,p)
= 0,$$
or $P = P_p$, or $\varphi(X) = X^*$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 3.25.}
\addcontentsline{toc}{subsection}{Exercise 3.25.}
\emph{Let $X$ be the metric space whose points are rational numbers,
with the metric $d(x,y) = |x-y|$.
What is the completion of this space? (Compare Exercise 3.24.)} \\

\emph{Proof.}
By Exercise 3.24, we can identify one completion $(X^*,\Delta)$ with $(\mathbb{R},|\cdot|)$
(Theorem 3.11(c) and Theorem 1.20(b)).
$\Box$ \\



\textbf{Supplement (Uniqueness of completion).}
\emph{Show that a completion of a metric space is unique up to isometry.} \\

\emph{Outline.}
Suppose there are two completions $\{\varphi_i, (X^*_i,d^*_i) \}$ $(i=1,2)$ of $(X,d)$.
Let $$\psi = \varphi_2 \circ \varphi_1^{-1}: \varphi_1(X) \to \varphi_2(X)$$
be an isometry from $\varphi_1(X)$ into $\varphi_2(X)$
The sets $\varphi_i(X)$ $(i=1,2)$ are dense in $X^*_i$.
So we can extend $\psi$ (continuously) to a map $\psi: X^*_1 \to X^*_2$. \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Given any $P \in X^*_1$, there is a Cauchy sequence
  $\{P_{p_n}\} = \{\varphi_1(p_n)\}$ in $\varphi_1(X)$ converging to $P$.
  Define $\psi(P)$ by
  $$\psi(P) = \lim_{n \to \infty} \psi(P_{p_n}).$$
  \item[(2)]
  \emph{Show that $\psi$ is well-defined.}
  Note that
  \begin{align*}
  \Delta_2(\psi(P_{p_m}), \psi(P_{p_n}))
  &= \Delta_2(\psi(\varphi_1(p_m)), \psi(\varphi_1(p_n))) \\
  &= \Delta_2(\varphi_2(p_m), \varphi_2(p_n)) \\
  &= d(p_n,p_m)
    &\text{($\varphi_2$ is isometric)} \\
  &= \Delta_1(\varphi_1(p_m), \varphi_1(p_n))
    &\text{($\varphi_1$ is isometric)} \\
  &= \Delta_1(P_{p_m}, P_{p_n}).
  \end{align*}
  So $\{ \psi(P_{p_n}) \}$ is a Cauchy sequence in $\varphi_2(X)$
  if (and only if) $\{P_{p_n}\}$ is a Cauchy sequence in $\varphi_1(X)$.
  Since $X^*_2$ is complete, $\{ \psi(P_{p_n}) \}$ converges to $\psi(P)$.
  The limit $\psi(P)$ is uniquely determined since $\Delta_2$ is a metric function.
  \item[(3)]
  Since $\psi$ is an isometry from $\varphi_1(X)$ into $\varphi_2(X)$,
  $$\psi^{-1} = \varphi_1 \circ \varphi_2^{-1}: \varphi_2(X) \to \varphi_1(X)$$
  is an isometry from $\varphi_2(X)$ into $\varphi_1(X)$.
  Besides, $\psi^{-1} \circ \psi = 1_{\varphi_1(X)}$
  and $\psi \circ \psi^{-1} = 1_{\varphi_2(X)}$.
  \item[(4)]
  \emph{Show that $\psi$ is surjective.}
  Given any $Q \in X^*_2$, there is a Cauchy sequence
  $\{P_{q_n}\} = \{\varphi_2(q_n)\}$ in $\varphi_2(X)$ converging to $Q$.
  Define
  $$P_{p_n} = \psi^{-1}(P_{q_n}) \in \varphi_1(X).$$
  $\psi(P_{p_n}) = 1_{\varphi_2(X)}(P_{q_n}) = P_{q_n}$.
  Besides, similar to argument in (2),
  $\{P_{p_n}\}$ is a Cauchy sequence in $\varphi_1(X)$.
  Since $X^*_1$ is complete, $\{P_{p_n}\}$ converges to $P \in X^*_1$.
  It is easy to verify that $\psi(P) = Q$.
  \item[(5)]
  \emph{Show that $\psi$ is injective.}
  Given any $P \in X^*_1$ and $Q \in X^*_1$,
  there are Cauchy sequences
  $$\{P_{p_n}\} = \{\varphi_1(p_n)\} \to P \text{ and }
  \{P_{q_n}\} = \{\varphi_1(q_n)\} \to Q.$$
  So
  \begin{align*}
     \psi(P) = \psi(Q)
     &\Longrightarrow
     \lim_{n \to \infty} \psi(P_{p_n}) = \lim_{n \to \infty} \psi(P_{q_n}) \\
     &\Longrightarrow
     0 = \lim_{n \to \infty} \Delta_2(\psi(P_{p_n}),\psi(P_{q_n})) \\
     &\Longrightarrow
     0 = \lim_{n \to \infty} \Delta_2(\psi(\varphi_1(p_n)),\psi(\varphi_1(q_n))) \\
     &\Longrightarrow
     0 = \lim_{n \to \infty} \Delta_2(\varphi_2(p_n),\varphi_2(q_n)) \\
     &\Longrightarrow
     0 = \lim_{n \to \infty} d(p_n,q_n).
       &\text{($\varphi_2$ is isometric)}
  \end{align*}
  Thus $\{p_n\} \in P$ and $\{q_n\} \in Q$ in the same equivalence class.
  Thus $P = Q$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 4: Continuity \\}
\addcontentsline{toc}{section}{Chapter 4: Continuity}



\subsection*{Exercise 4.1.}
\addcontentsline{toc}{subsection}{Exercise 4.1.}
\emph{Suppose $f$ is a real function define on $\mathbb{R}^1$ which satisfies
$$\lim_{h \to 0}[f(x+h)-f(x-h)] = 0$$
for every $x \in \mathbb{R}^1$.
Does this imply that $f$ is continuous?} \\

\emph{Proof.}
$\lim_{h \to 0}[f(x+h)-f(x-h)] = 0$ holds if $f$ is continuous.
But the converse of this statement and is not true.
For example, define $f: \mathbb{R}^1 \to \mathbb{R}^1$ by
\begin{equation*}
  f(x) =
    \begin{cases}
      1 & (x = 0), \\
      0 & (x \neq 0).
    \end{cases}
\end{equation*}
$f$ is not continuous at $x = 0$ but
$$\lim_{h \to 0}[f(x+h)-f(x-h)] = 0$$ for any $x \in \mathbb{R}^1$.
(The identity holds for $x \neq 0$ since $f$ is continuous on $\mathbb{R}^1 - \{0\}$.
Besides, $\lim_{h \to 0}[f(0+h)-f(0-h)] = \lim_{h \to 0}[0 - 0] = 0$.)
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.2.}
\addcontentsline{toc}{subsection}{Exercise 4.2.}
\emph{If $f$ is a continuous mapping of a metric space $X$
into a metric space $Y$,
prove that $f(\overline{E}) \subseteq \overline{f(E)}$
for every set $E \subseteq X$.
($\overline{E}$ denotes the closure of $E$.)
Show, by an example, that $f(\overline{E})$ can be a proper subset of $\overline{f(E)}$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Since $f$ is continuous and $\overline{f(E)}$ is closed,
$f^{-1}(\overline{f(E)})$ is closed.
Hence,
\begin{align*}
f^{-1}(\overline{f(E)})
&\supseteq f^{-1}(f(E))
  & \text{(Monotonicity of $f^{-1}$)} \\
&\supseteq E,
  & \text{(Note in Theorem 4.14)} \\
\overline{E}
&\subseteq f^{-1}(\overline{f(E)}),
  & \text{(Monotonicity of closure)} \\
f(\overline{E})
&\subseteq f(f^{-1}(\overline{f(E)}))
  & \text{(Monotonicity of $f$)} \\
&\subseteq \overline{f(E)}.
  & \text{(Note in Theorem 4.14)}
\end{align*}
\item[(2)]
Let $f: (0, \infty) \to \mathbb{R}$ be a continuous function
defined by $$f(x) = \frac{1}{x}.$$
Consider $E = \mathbb{Z}^+ \subseteq (0, \infty)$.
Then $f(E) = \left\{ \frac{1}{n} : n \in \mathbb{Z}^+ \right\}$,
and thus
\begin{align*}
  f(\overline{E})
  &= \left\{ \frac{1}{n} : n \in \mathbb{Z}^+ \right\}. \\
  \overline{f(E)}
  &= \left\{ \frac{1}{n} : n \in \mathbb{Z}^+ \right\} \bigcup \{0\}.
\end{align*}
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Inverse image).}
\begin{enumerate}
\item[(1)]
\emph{$E \subseteq f^{-1}[f(E)]$ for $E \subseteq X$.}
\begin{align*}
  \forall \: x \in E
  &\Longrightarrow
  f(x) \in f(E)
    & \\
  &\Longleftrightarrow
  x \in f^{-1}[f(E)].
    &\text{(Definition of the inverse image)}
\end{align*}
$\Box$ \\
\item[(2)]
\emph{$f[f^{-1}(E)] \subseteq E$ for $E \subseteq Y$.}
\begin{align*}
  \forall \: y \in f[f^{-1}(E)]
  &\Longleftrightarrow
  \exists \: x \in f^{-1}(E) \text{ such that } y = f(x) \\
  &\Longleftrightarrow
  \exists \: x, f(x) \in E \text{ such that } y = f(x) \\
  &\Longrightarrow
  \exists \: x, y = f(x) \in E.
\end{align*}
$\Box$ \\
\end{enumerate}



\textbf{Supplement (Continuity).}
\emph{Let $f$ be a map from a topological space on $X$
to a topological space on $Y$.
Then, the following statements are equivalent:}
\begin{enumerate}
\item[(1)]
\emph{$f$ is continuous:
For each $x \in X$ and every neighborhood $V$ of $f(x)$,
there is a neighborhood $U$ of $x$ such that $f(U) \subseteq V$.}
\item[(2)]
\emph{For every open set $O$ in $Y$, the inverse image $f^{-1}(O)$
is open in $X$.}
\item[(3)]
\emph{For every closed set $C$ in $Y$, the inverse image $f^{-1}(C)$
is closed in $X$.}
\item[(4)]
\emph{$f(A)^{\circ} \subseteq f(A^{\circ})$ for every subset $A$ of $X$.}
\item[(5)]
\emph{$f^{-1}(B^{\circ}) \subseteq (f^{-1}(B))^{\circ}$ for every subset $B$ of $Y$.}
\item[(6)]
\emph{$f(\overline{A}) \subseteq \overline{f(A)}$ for every subset $A$ of $X$.}
\item[(7)]
\emph{$\overline{f^{-1}(B)} \subseteq f^{-1}(\overline{B})$ for every subset $B$ of $Y$.} \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.3.}
\addcontentsline{toc}{subsection}{Exercise 4.3.}
\emph{Let $f$ be a continuous real function on a metric space $X$.
Let $Z(f)$ (the zero set of $f$) be the set of all $p \in X$ at which $f(p) = 0$.
Prove that $Z(f)$ is closed.} \\

\emph{Proof (Corollary to Theorem 4.8).}
Since $f$ is continuous, $f^{-1}(\{0\}) = Z(f)$ is closed in $X$
for a closed subset $\{0\}$ in $\mathbb{R}^1$.
$\Box$ \\

Denote the complement of any set $E$ by $\widetilde{E}$. \\

\emph{Proof (Theorem 4.8).}
Consider the complement of $Z(f)$ in $X$,
\begin{align*}
\widetilde{Z(f)}
&= \{ x \in X : f(x) \neq 0 \} \\
&= f^{-1}((-\infty, 0) \cup (0, \infty)).
\end{align*}

Since $f$ is continuous, $f^{-1}((-\infty, 0) \cup (0, \infty)) = \widetilde{Z(f)}$
is open in $X$ for a open subset $(-\infty, 0) \cup (0, \infty)$ in $\mathbb{R}^1$.
$\Box$ \\

\emph{Proof (Definition 2.18(d)).}
Given any limit point $p$ of $Z(f)$.
\emph{Show that $f(p) = 0$ or $p \in Z(f)$.}
Since $f$ is continuous, given any $\varepsilon > 0$ there exists a $\delta > 0$
such that $|f(x) - f(p)| < \varepsilon$ for all $x \in X$ for which $d_X(x, p) < \delta$.
Since $p$ is a limit point of $Z(f)$, for such $\delta > 0$ we have a point $q \neq p$
such that $q \in Z(f)$, or $f(q) = 0$. So $|f(p)| < \varepsilon$ for any $\varepsilon > 0$.
$f(p) = 0$.
$\Box$ \\

\emph{Proof (Definition 2.18(f)).}
Consider the complement of $Z(f)$ in $X$,
$$\widetilde{Z(f)} = \{ x \in X : f(x) \neq 0 \} = \{f > 0\} \cup \{f < 0\}$$
where $\{f > 0\} = \{ x \in X : f(x) > 0 \}$ and $\{f < 0\} = \{ x \in X : f(x) < 0 \}$.
It suffices to show $\{f > 0\}$ is open. ($\{f < 0\}$ is similar.)
Given any point $p$ of $\{f > 0\}$ or $f(p) > 0$.
\emph{Want to show $p$ is an interior point of $\{f > 0\}$.}
Since $f$ is continuous, given any $\varepsilon = \frac{f(p)}{2} > 0$
there exists a $\delta > 0$
such that $|f(x) - f(p)| < \frac{f(p)}{2}$ for all $x \in X$ for which $d_X(x, p) < \delta$.
For such $x$ with $d_X(x, p) < \delta$ we have
$$\frac{1}{2}f(p) < f(x) < \frac{3}{2}f(p).$$
That is, $N = \{ x : d_X(x, p) < \delta \}$ is a neighborhood $p$ such that
$N \subseteq \{f > 0\}$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.4.}
\addcontentsline{toc}{subsection}{Exercise 4.4.}
\emph{Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$,
and let $E$ be a dense subset of $X$.
Prove that $f(E)$ is dense in $f(X)$.
If $g(p) = f(p)$ for all $p \in E$, prove that $g(p) = f(p)$ for all $p \in X$.
(In other words, a continuous mapping is determined by
its values on a dense subset of its domain.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $f(E)$ is dense in $f(X)$.}
It suffices to show that every point $y \in f(X)-f(E)$ is a limit point of $f(E)$.
Since $y \in f(X)-f(E)$, there exists a point $x \in X-E$ such that $y = f(x)$.
Since $E$ is dense in $X$, there exists a sequence $\{x_n\}$ in $E$ such that
$x_n \to x$ as $n \to \infty$.
Let $y_n = f(x_n) \in f(E)$.
Take limit and use the continuity of $f$,
$y_n \to y$ as $n \to \infty$, or $y$ is a limit point of $f(E)$.
\item[(2)]
\emph{Show that $g(p) = f(p)$ for all $p \in X$ if $g(p) = f(p)$ for all $p \in E$.}
It suffices to show $g(p) = f(p)$ for all $p \in X-E$.
Given any $p \in X-E$, there exists a sequence $\{p_n\}$ in $E$ such that
$p_n \to p$ as $n \to \infty$.
Notice that $g(p_n) = f(p_n)$ by the assumption.
Take limit and use the continuity of $f$ and $g$,
$g(p) = f(p)$ for $p \in X-E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.5.}
\addcontentsline{toc}{subsection}{Exercise 4.5.}
\emph{If $f$ is a real continuous function defined on a closed set $E \subseteq \mathbb{R}^1$,
prove that there exist continuous real function $g$ on $\mathbb{R}^1$ such that
$g(x) = f(x)$ for all $x \in E$.
(Such functions $g$ are called \textbf{continuous extensions} of $f$ from $E$ to $\mathbb{R}^1$.)
Show that the result becomes false if the word ``closed'' is omitted.
Extend the result to vector valued functions.
(Hint: Let the graph of $g$ be a straight line on each of the segments
which constitute the complement of $E$
(compare Exercise 2.29).
The result remains true if $\mathbb{R}^1$ is replaced by any metric space,
but the proof is not so simple.) } \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Every open set in $\mathbb{R}^1$ is the union of an at most countable collection
of disjoint segments (Exercise 2.29).
\item[(2)]
We need to construct a continuous real function on the complement of $E$.
By (1), write $\widetilde{E} = \bigcup_{i \in \mathscr{C}} (a_i, b_i)$
where $\mathscr{C}$ is at most countable and
$a_i < b_i$. ($a_i, b_i$ could be $\pm \infty$.)
Define $g(x)$ by
\begin{equation*}
  g(x) =
    \begin{cases}
      f(x)
        & (x \in E), \\
      f(a_i) + \frac{f(b_i)-f(a_i)}{b_i-a_i}(x - a_i)
        & (x \in (a_i, b_i):\text{finite interval}), \\
      f(a_i)
        & (x \in (a_i, b_i): a_i:\text{finite}, b_i = +\infty), \\
      f(b_i)
        & (x \in (a_i, b_i): a_i = -\infty, b_i:\text{finite}), \\
      0
        & (x \in (a_i, b_i): a_i = -\infty, b_i = +\infty).
    \end{cases}
\end{equation*}
\emph{Show that $g$ is continuous in $\mathbb{R}^1$,}
or show that $g(x)$ is continuous at $x = p$ for any point $p \in \mathbb{R}^1$.
\begin{enumerate}
  \item[(a)]
  Given a point $p \in \widetilde{E}$. There is an open interval $I = (a_i, b_i)$
  such that $p \in I$.
  Since the graph of $g$ in an open interval $I$ is a straight line,
  $g$ is continuous at $x = p$.
  \item[(b)]
  Given an isolated point $p \in E$.
  There are two open intervals $I = (a_i, b_i)$ and $J = (a_j, b_j)$
  such that $b_i = p = a_j$.
  So $\lim_{x \to p^{-}} g(x) = \lim_{x \to p^{+}} g(x) = f(p)$
  by the construction of $g$, which says $g$ is continuous at $x = p$.
  \item[(c)]
  Given a limit point $p \in E$. So that $g(p) = f(p)$.
  Given $\varepsilon > 0$.
  Consider $\lim_{x \to p^{+}} g(x)$ first.
  (The case $\lim_{x \to p^{-}} g(x)$ is similar.)
    \begin{enumerate}
    \item[(i)]
    For such $\varepsilon > 0$, there is a $\delta' > 0$ such that
    $$f(p)-\varepsilon < f(x) < f(p)+\varepsilon$$
    whenever
    $$x \in E \text{ and } p < x < \delta'.$$
    Since $p$ is a limit point of $E$,
    there is a point $q \neq p$ such that $|q - p| < \delta'$.
    Might assume that $q > p$, and then retake $\delta = \min\{\delta', q-p\} > 0$.
    (If no such $q$, $\lim_{x \to p^{+}} g(x) = f(p)$ trivially.)
    \item[(ii)]
    For any $x$ such that $p < x < q$,
    consider $x \in E$ or else $x \in \widetilde{E}$.
    As $x \in E$, nothing to do by (i).
    \item[(iii)]
    As $x \in \widetilde{E}$, there exists an open interval $I = (a_i, b_i)$
    such that $x \in I \subseteq (p,q)$.
    Therefore,
    $$f(a_i) \leq g(x) \leq f(b_i) \text{ or }
    f(a_i) \geq g(x) \geq f(b_i).$$
    By (i),
    \begin{align*}
      f(p) - \varepsilon &< f(a_i) < f(p) + \varepsilon \text{ and } \\
      f(p) - \varepsilon &< f(b_i) < f(p) + \varepsilon, \\
      f(p) - \varepsilon < f(a_i) &\leq g(x) \leq f(b_i) < f(p) + \varepsilon \text{ or } \\
      f(p) - \varepsilon < f(b_i) &\leq g(x) \leq f(a_i) < f(p) + \varepsilon.
    \end{align*}
    \end{enumerate}
  Hence, given $\varepsilon > 0$
  there is a $\delta > 0$ such that
  $|g(x) - g(p)| < \varepsilon$ whenever $p < x < \delta$ (and $x \in \mathbb{R}^1$),
  or $\lim_{x \to p^{+}} g(x) = g(p)$.
  \end{enumerate}
\item[(3)]
Consider $f(x) = \log(x)$ in $(0, \infty)$.
Since $\lim_{x \to 0} f(x) = -\infty$,
we cannot find any real continuous function $g$ defined on $x = 0$.
\item[(4)]
For a vector-valued function
$\mathbf{f} = (f_1, \ldots, f_k)$,
with each $f_i$ is continuous on a closed set $E \subseteq \mathbb{R}^1$,
extend $f_i$ to a continuous function $g_i$ on $\mathbb{R}^1$ as (2).
Put $\mathbf{g} = (g_1, \ldots, g_k)$.
Clearly $\mathbf{g}$ is an extension of $\mathbf{f}$.
Besides, $\mathbf{g}$ is continuous in $\mathbb{R}^1$ by Theorem 4.10.
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Tietze's Extension Theorem).}
\emph{If $X$ is a normal topological space and
$f: A \to \mathbb{R}$ is a continuous map
from a closed subset $A$ of $X$ into the real numbers carrying the standard topology,
then there exists a continuous map
$g: X \to \mathbb{R}$ with $g(a) = f(a)$ for all $a \in A$.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.6.}
\addcontentsline{toc}{subsection}{Exercise 4.6.}
\emph{If $f$ is defined on $E$, the graph of $f$ is the set of points $(x,f(x))$,
for $x \in E$.
In particular, if $E$ is a set of real numbers, and $f$ is real-valued,
the graph of $f$ is a subset of the plain.
Suppose $E$ is compact, and prove that that $f$ is continuous on $E$
if and only if its graph is compact.} \\

\emph{Proof.}
Let $G = \{ (x,f(x)) : x \in E\}$ be the graph of $f$.
\begin{enumerate}
\item[(1)]
$(\Longrightarrow)$
Let $\mathbf{f}: E \to G$ defined by
$$\mathbf{f}(x) = (x,f(x)).$$
$\mathbf{f}(E) = G$ exactly.
Since $f$ and $x$ are continuous in $E$, $\mathbf{f}$ is continuous (Theorem 4.10).
As $E$ is compact, $\mathbf{f}(E)$ is compact (Theorem 4.14).
\item[(2)]
$(\Longleftarrow)$
Let $\pi: G \to E$ be a projection map defined by
$$\pi(x, f(x)) = x.$$
Notice that $\pi \circ \mathbf{f} = \text{id}_E$ and
$\mathbf{f} \circ \pi = \text{id}_G$.
Besides,
$\pi$ is a continuous one-to-one mapping of a compact set $G$ onto $E$.
Then the inverse mapping $\pi^{-1} = \mathbf{f}$
is a continuous mapping of $E$ onto $G$ (Theorem 4.17).
So $f$ is continuous (Theorem 4.10).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.7.}
\addcontentsline{toc}{subsection}{Exercise 4.7.}
\emph{If $E \subseteq X$ and if $f$ is a function defined on $X$,
the \textbf{restriction} of $f$ to $E$ is the function $g$ whose domain of definition is $E$,
such that $g(p) = f(p)$ for $p \in E$.
Define $f$ and $g$ on $\mathbb{R}^2$ by:}
\begin{equation*}
  f(x,y) =
  \begin{cases}
    0                    & \text{if $(x,y) = (0,0)$} \\
    \frac{xy^2}{x^2+y^4} & \text{if $(x,y) \neq (0,0)$},
  \end{cases}
\end{equation*}
\begin{equation*}
  g(x,y) =
  \begin{cases}
    0                    & \text{if $(x,y) = (0,0)$} \\
    \frac{xy^2}{x^2+y^6} & \text{if $(x,y) \neq (0,0)$},
  \end{cases}
\end{equation*}
\emph{Prove that $f$ is bounded on $\mathbb{R}^2$,
that $g$ is unbounded in every neighborhood of $(0,0)$,
and that $f$ is not continuous at $(0,0)$;
nevertheless, the restrictions of both $f$ and $g$ to every straight line
in $\mathbb{R}^2$ are continuous!} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $f$ is bounded on $\mathbb{R}^2$.}
\begin{align*}
  (|x| - |y^2|)^2 \geq 0
  &\Longleftrightarrow
  |x|^2 - 2|x||y^2| + |y^2|^2 \geq 0 \\
  &\Longleftrightarrow
  |x|^2 + |y^2|^2 \geq 2|x||y^2| \\
  &\Longleftrightarrow
  |x^2 + y^4| \geq 2|xy^2| \\
  &\Longrightarrow
  \frac{1}{2} \geq \abs{ \frac{xy^2}{x^2+y^2} } \text{ whenever } (x,y) \neq (0,0) \\
  &\Longrightarrow
  |f(x,y)| \leq \frac{1}{2} \text{ whenever } (x,y) \neq (0,0).
\end{align*}
Note that $f(0,0) = 0 \leq \frac{1}{2}$.
Hence $f$ is bounded by $\frac{1}{2}$ on $\mathbb{R}^2$.

\item[(2)]
\emph{Show that $g$ is unbounded in every neighborhood of $\mathbb{R}^2$.}
Consider
a sequence $\{\mathbf{p}_n\}_{n \geq 1} \subseteq \mathbb{R}^2$
\[
  \mathbf{p}_n
  = (x_n, y_n)
  = \left( \frac{1}{n^3}, \frac{1}{n} \right)
\]
such that $\mathbf{p}_n \neq \mathbf{0}$ and $\lim \mathbf{p}_n = \mathbf{0}$.
Thus,
\[
  \lim_{n \to \infty} g(\mathbf{p}_n)
  = \lim_{n \to \infty} \frac{ x_n y_n^2 }{ x_n^2 + y_n^6 }
  = \lim_{n \to \infty} \frac{ (\frac{1}{n^3}) (\frac{1}{n})^2 }
    { (\frac{1}{n^3})^2 + (\frac{1}{n})^6 }
  = \lim_{n \to \infty} \frac{n}{2}
  = \infty.
\]
Hence $g$ is unbounded in every neighborhood of $\mathbb{R}^2$.

\item[(3)]
\emph{Show that $f$ is not continuous at $(0,0)$.}
Consider
a sequence $\{\mathbf{p}_n\}_{n \geq 1} \subseteq \mathbb{R}^2$
\[
  \mathbf{p}_n
  = (x_n, y_n)
  = \left( \frac{1}{n^2}, \frac{1}{n} \right)
\]
such that $\mathbf{p}_n \neq \mathbf{0}$ and $\lim \mathbf{p}_n = \mathbf{0}$.
Thus,
\[
  \lim_{n \to \infty} f(\mathbf{p}_n)
  = \lim_{n \to \infty} \frac{ x_n y_n^2 }{ x_n^2 + y_n^4 }
  = \lim_{n \to \infty} \frac{ (\frac{1}{n^2}) (\frac{1}{n})^2 }
    { (\frac{1}{n^2})^2 + (\frac{1}{n})^4 }
  = \frac{1}{2}.
\]
So, $\lim f(\mathbf{p}_n) = \frac{1}{2} \neq 0$.
By Theorem 4.6, $f$ is not continuous at $(0,0)$.

\item[(4)]
\emph{The restrictions of $f$ to every straight line
in $\mathbb{R}^2$ is continuous.}
  \begin{enumerate}
  \item[(a)]
  \emph{The line $L_{\infty} = \{(0,y) : y \in \mathbb{R} \}$.}
  Hence $f|_{L_{\infty}}(x,y) = 0$ for all $(x,y) \in L_{\infty}$
  (including $(0,0) \in L_{\infty}$).
  Therefore $f|_{L_{\infty}}$ is continuous.
  \item[(b)]
  \emph{The line $L_{\alpha} = \{(x,\alpha x) : x \in \mathbb{R} \}$
  for some $\alpha \in \mathbb{R}$.}
  $f|_{L_{\alpha}}(x,y)$ is continuous on $L_{\alpha} - \{(0,0)\}$.
  \begin{equation*}
    f|_{L_{\alpha}}(x,y) =
    f|_{L_{\alpha}}(x,\alpha x) =
    \begin{cases}
      0                                   & \text{if $(x,y) = (0,0)$} \\
      \frac{\alpha^2 x}{1 + \alpha^4 x^2} & \text{if $(x,y) \neq (0,0)$}.
    \end{cases}
  \end{equation*}
  So
  \[
    \lim_{(x,y) \to (0,0)} f|_{L_{\alpha}}(x,y)
    = \lim_{x \to 0} \frac{\alpha^2 x}{1 + \alpha^4 x^2}
    = 0
    = f(0,0),
  \]
  or $f|_{L_{\alpha}}(x,y)$ is continuous at $(0,0)$.
  Therefore, $f|_{L_{\alpha}}(x,y)$ is continuous on $L_{\alpha}$.
  \item[(c)]
  \emph{The line $L$ not passing $(0,0)$.}
  It is clear since $f(x,y)$ is continuous on $\mathbb{R}^2 - \{(0,0)\}$.
  \end{enumerate}

\item[(5)]
\emph{The restrictions of $g$ to every straight line
in $\mathbb{R}^2$ is continuous.}
Similar to (4).
  \begin{enumerate}
  \item[(a)]
  \emph{The line $L_{\infty} = \{(0,y) : y \in \mathbb{R} \}$.}
  Hence $g|_{L_{\infty}}(x,y) = 0$ for all $(x,y) \in L_{\infty}$
  (including $(0,0) \in L_{\infty}$).
  Therefore $g|_{L_{\infty}}$ is continuous.
  \item[(b)]
  \emph{The line $L_{\alpha} = \{(x,\alpha x) : x \in \mathbb{R} \}$
  for some $\alpha \in \mathbb{R}$.}
  $g|_{L_{\alpha}}(x,y)$ is continuous on $L_{\alpha} - \{(0,0)\}$.
  \begin{equation*}
    g|_{L_{\alpha}}(x,y) =
    g|_{L_{\alpha}}(x,\alpha x) =
    \begin{cases}
      0                                   & \text{if $(x,y) = (0,0)$} \\
      \frac{\alpha^2 x}{1 + \alpha^6 x^4} & \text{if $(x,y) \neq (0,0)$}.
    \end{cases}
  \end{equation*}
  So
  \[
    \lim_{(x,y) \to (0,0)} g|_{L_{\alpha}}(x,y)
    = \lim_{x \to 0} \frac{\alpha^2 x}{1 + \alpha^6 x^4}
    = 0
    = g(0,0),
  \]
  or $g|_{L_{\alpha}}(x,y)$ is continuous at $(0,0)$.
  Therefore, $g|_{L_{\alpha}}(x,y)$ is continuous on $L_{\alpha}$.
  \item[(c)]
  \emph{The line $L$ not passing $(0,0)$.}
  It is clear since $g(x,y)$ is continuous on $\mathbb{R}^2 - \{(0,0)\}$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.8.}
\addcontentsline{toc}{subsection}{Exercise 4.8.}
\emph{Let $f$ be a real uniformly continuous function
on the bounded set $E$ in $\mathbb{R}$.
Prove that $f$ is bounded on $E$.
Show that the conclusion is false
if boundedness of $E$ is omitted from the hypothesis.} \\

The conclusion is false if boundedness of $E$ is omitted from the hypothesis.
For example, $f(x) = x$ on $\mathbb{R}$ is uniformly continuous on $\mathbb{R}$
but $f(\mathbb{R}) = \mathbb{R}$ is unbounded. \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
Since $f: E \to \mathbb{R}$ is uniformly continuous,
given any $\varepsilon > 0$, there is $\delta > 0$ such that
$\abs{f(x) - f(y)} < \varepsilon$ whenever $\abs{x - y} < \delta$.
In particular, pick $\varepsilon = 1$.
\item[(2)]
By the boundedness of $E$, there is $M > 0$ such that $\abs{x} < M$ for all $x \in E$.
\item[(3)]
For such $\delta > 0$, we construct a covering of $E \subseteq \mathbb{R}$.
Construct a special collection $\mathscr{C}$ of intervals
$$I_{a} =
  \left[ \frac{\delta}{2}a, \frac{\delta}{2}(a+1) \right]
$$
where $a \in \mathbb{Z}$ satisfying
$$\abs{a} < \frac{2M}{\delta} + 1.$$
By construction, $\mathscr{C}$ is a finite covering of $E$.
\item[(4)]
For every interval $I_a$ of the collection $\mathscr{C}$,
pick a point $x_a \in E \bigcap I_a$ if possible.
This process will terminate eventually since $\mathscr{C}$ is a finite.
Collect these representative points as $\mathscr{D} = \{ x_a \}$.
Notice that $\mathscr{D}$ is finite again.
\item[(5)]
Now for any point $x \in E$, $x$ lies in some $I_a$
containing $x_a$.
Both $x$ and $x_a$ are in the same interval and their distance satisfies
$$\abs{x - x_a}
\leq \frac{\delta}{2}
< \delta$$
and thus by (1)
$$\abs{f(x) - f(x_a)} < 1,
\text{ or }
\abs{f(x)} <  1 + \abs{f(x_a)}.$$
\item[(6)]
Let
$$M = 1 + \max_{x_\mathbf{a} \in \mathscr{D}} \abs{f(x_a)}.$$
So given any $x \in E$, $\abs{f(x)} < M$.
\end{enumerate}
$\Box$ \\

\emph{Proof (Heine-Borel Theorem).}
Heine-Borel theorem provides the finiteness property to construct
the boundedness property of $f$.

\begin{enumerate}
\item[(1)]
\emph{Let $E$ be a bounded subset of a metric space $X$.
Show that the closure of $E$ in $X$ is also bounded in $X$.}
$E$ is bounded if $E \subseteq B_X(a;r)$ for some $r > 0$ and some $a \in X$.
(The ball $B_X(a;r)$ is defined to the set of all $x \in X$ such that
$d_X(x, a) < r$.)
Take the closure on the both sides,
$$\overline{E}
\subseteq \overline{B_X(a;r)}
= \{ x \in X : d_X(x, a) \leq r \}
\subseteq B_X(a;2r),$$
or $\overline{E}$ is bounded.

\item[(2)]
Since $f: E \to \mathbb{R}$ is uniformly continuous,
given any $\varepsilon > 0$, there is $\delta > 0$ such that
$\abs{f(x) - f(y)} < \varepsilon$ whenever $\abs{x - y} < \delta$.
In particular, pick $\varepsilon = 1$.

\item[(3)]
For such $\delta > 0$, we construct an open covering of $\overline{E} \subseteq \mathbb{R}$.
Pick a collection $\mathscr{C}$ of open balls
$B(a;\delta) \subseteq \mathbb{R}$
where $a$ runs over all elements of $E$.
$\mathscr{C}$ covers $\overline{E}$ (by the definition of accumulation points).
Since $\overline{E} $ is closed and bounded
(by applying (1) on the boundedness of $E$),
$\overline{E}$ is compact
(Heine-Borel theorem).
That is, there is a finite subcollection $\mathscr{C}'$ of $\mathscr{C}$
also covers $\overline{E}$, say
$$\mathscr{C}'
= \left\{B(a_1;\delta)), B(a_2;\delta), \ldots, B(a_m;\delta) \right\}.$$

\item[(4)]
Given any $x \in E \subseteq \overline{E}$,
there is some $a_i \in E$ $(1 \leq i \leq m)$ such that $x \in B(a_i;\delta)$.
In such ball, $\abs{x - a_i} < \delta$.
By (2), $\abs{f(x) - f(a_i)} < 1$,
or $\abs{f(x)} < 1 + \abs{f(a_i)}$.
Almost done.
Notice that $a_i$ depends on $x$,
and thus we might use finiteness of $\{ a_1, a_2, \ldots, a_m \}$
to remove dependence of $a_i$.

\item[(5)]
Let
$$M = 1 + \max_{1 \leq i \leq m}{\abs{f(a_i)}}.$$
So given any $x \in E$, $\abs{f(x)} < M$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement.}
Exercise about considering the closure.
(Problem 3.5 in H. L. Royden, Real Analysis, 3rd Edition.)
\emph{Let $A = \mathbb{Q} \cap [0,1]$,
and let $\{ I_n\}$ be a finite collection of open intervals covering $A$.
Then $\sum l(I_n) \geq 1$.} \\

\emph{Proof.}
\begin{align*}
1
= m^{*}[0, 1]
= m^{*}\overline{A}
&\leq m^{*}\left( \overline{\bigcup I_n} \right)
= m^{*}\left( \bigcup \overline{I_n} \right) \\
&\leq \sum m^{*}(\overline{I_n})
= \sum l(\overline{I_n})
= \sum l(I_n).
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.9.}
\addcontentsline{toc}{subsection}{Exercise 4.9.}
\emph{Show that the requirement in the definition of uniformly continuity
can be rephrased as follows, in terms of diameters of sets:
To every $\varepsilon > 0$ there exists a $\delta > 0$ such that
$\mathrm{diam}f(E) < \varepsilon$ for all $E \subseteq X$ with
$\mathrm{diam}E < \delta$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
($\Longrightarrow$)
Given $\varepsilon > 0$.
By Definition 4.18, there exists a $\delta > 0$ such that
\[
  d(f(p),f(q)) < \frac{\varepsilon}{64}
\]
for all $p$ and $q$ in $X$ for which $d(p,q) < \delta$.
Let $E$ be any subset of $X$ satisfying $\mathrm{diam}E < \delta$.
Then for any $p, q \in E$,
\[
  d(p,q) \leq \mathrm{diam}E < \delta.
\]
So that
\[
  d(f(p),f(q)) < \frac{\varepsilon}{64},
\]
or $\frac{\varepsilon}{64}$ is an upper bound of $S = \{d(f(p),f(q)) : p, q \in E\}$.
Hence
\[
  \mathrm{diam}f(E) = \sup S \leq \frac{\varepsilon}{64} < \varepsilon.
\]
(Here we pick ``$\frac{\varepsilon}{64}$'' instead of $\varepsilon$
since we want to get ``$\mathrm{diam}f(E) < \varepsilon$''
instead of $\mathrm{diam}f(E) \leq \varepsilon$.)

\item[(2)]
($\Longleftarrow$)
Easy.
Given $\varepsilon > 0$ there exists a $\delta > 0$ such that
$\mathrm{diam}f(E) < \varepsilon$ for all $E \subseteq X$ with
$\mathrm{diam}E < \delta$.
In particular, for any $p, q \in X$ with $d(p,q) < \delta$,
we can take $E = \{p,q\} \subseteq X$ and its diameter
\[
  \mathrm{diam}E = d(p,q) < \delta.
\]
So that
\[
  d(f(p),f(q)) = \mathrm{diam}f(E) < \varepsilon,
\]
or Definition 4.18 holds.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.10.}
\addcontentsline{toc}{subsection}{Exercise 4.10.}
\emph{Complete the details of the following alternative proof of Theorem 4.19
(Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$.
Then $f$ is uniformly continuous on $X$):
If $f$ is not uniformly continuous,
then for some $\varepsilon > 0$
there are sequences $\{p_n\}$, $\{q_n\}$ in $X$ such that
$d_X(p_n,q_n) \to 0$ but $d_Y(f(p_n),f(q_n)) > \varepsilon$.
Use Theorem 2.37 to obtain a contradiction.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
(Reductio ad absurdum)
If $f$ were not uniformly continuous,
then for some $\varepsilon > 0$
there are sequences $\{p_n\}$, $\{q_n\}$ in $X$ such that
$d_X(p_n,q_n) \to 0$ but $d_Y(f(p_n),f(q_n)) > \varepsilon$.

\item[(2)]
By Theorem 2.37,
there is a subsequence $\{p_{n_k}\}$ of $\{p_n\}$ such that
$\{p_{n_k}\}$ converges to $p \in X$.
Similar argument to $\{q_n\}$,
we have a subsequence $\{q_{n'_k}\}$ of $\{q_n\}$ converging to $q \in X$.

\item[(3)]
Since
\[
  d_X(p,q)
  \leq d_X(p,p_{n_k}) + d_X(p_{n_k},q_{n'_k}) + d_X(q_{n'_k},q)
  \to 0
\]
(by assumption and (2)) and $d_X(p,q)$ is a constant,
$d_X(p,q) = 0$ or $p = q$.

\item[(4)]
Since $f$ is continuous,
\[
  \lim_{k \to \infty} f(p_{n_k})
  = f(p)
  = f(q)
  = \lim_{k \to \infty} f(q_{n'_k})
\]
or $d_Y(f(p_{n_k}),f(q_{n'_k})) \to 0$,
contrary to the assumption.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.11.}
\addcontentsline{toc}{subsection}{Exercise 4.11.}
\emph{Suppose $f$ is a uniformly continuous mapping of a metric space $X$ into a metric space $Y$
and prove that $\{f(x_n)\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence $\{x_n\}$ in $X$.
Use this result to give an alternative proof of the theorem stated in Exercise 4.13.} \\

An alternative proof of Exercise 4.13 will be in Exercise 4.13 itself. \\

\emph{Proof (Definition 4.18).}
Given any Cauchy sequence $\{x_n\}$ in $X$.
\begin{enumerate}
\item[(1)]
Given any $\varepsilon > 0$.
Since $f$ is uniformly continuous, there exists a $\delta > 0$ such that
\[
  d_Y(f(p),f(q)) < \varepsilon
\]
for all $p$ and $q$ in $X$ for which $d_X(p,q) < \delta$.

\item[(2)]
Since $\{x_n\}$ is Cauchy in $X$, for such $\delta > 0$ there is an integer $N$ such that
\[
  d_X(x_n,x_m) < \delta
\]
whenever $n, m \geq N$.

\item[(3)]
By (1)(2),
\[
  d_Y(f(x_n),f(x_m)) < \varepsilon
\]
whenever $n, m \geq N$.
Hence $\{f(x_n)\}$ is Cauchy in $Y$.
\end{enumerate}
$\Box$ \\



\emph{Proof (Exercise 4.9).}
Given any Cauchy sequence $\{x_n\}$ in $X$.
\begin{enumerate}
\item[(1)]
Given any $\varepsilon > 0$.
Since $f$ is uniformly continuous, there exists a $\delta > 0$ such that
$\mathrm{diam}f(E) < \varepsilon$ for all $E \subseteq X$ with
$\mathrm{diam}E < \delta$.

\item[(2)]
Since $\{x_n\}$ is Cauchy in $X$, for such $\delta > 0$ there is an integer $N$ such that
\[
  d_X(x_n,x_m) < \frac{\delta}{64}
\]
whenever $n, m \geq N$.

\item[(3)]
Consider $E = \{ x_N, x_{N+1}, \ldots \}$.
By (2), $\mathrm{diam}E \leq \frac{\delta}{64} < \delta$.
By (1),
\[
  d_Y(f(x_n),f(x_m)) \leq \mathrm{diam}f(E) < \varepsilon
\]
whenever $n, m \geq N$.
Hence $\{f(x_n)\}$ is Cauchy in $Y$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.12.}
\addcontentsline{toc}{subsection}{Exercise 4.12.}
\emph{A uniformly continuous function of a uniformly continuous function
is uniformly continuous. State this more precisely and prove it.} \\

Statement (similar to Theorem 4.7):
\emph{suppose $X$, $Y$, $Z$ are metric space, $E \subseteq X$,
$f$ maps $E$ into $Y$,
$g$ maps the range of $f$, $f(E)$, into $Z$,
and $h$ is the mapping of $E$ into $Z$ defined by
\[
  h(x) = g(f(x)) \qquad (x \in E).
\]
If $f$ is uniformly continuous on $E$ and
$g$ is uniformly continuous on $f(E)$,
then $h$ is uniformly continuous on $E$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Given $\varepsilon > 0$.
Since $g$ is uniformly continuous on $f(E)$,
there exists a $\eta > 0$ such that
\[
  d_Z(g(f(p)),g(f(q))) < \varepsilon
  \:\:\text{ if }\:\: d_Y(f(p),f(q)) < \eta
  \:\:\text{ and }\:\: f(p), f(q) \in f(E).
\]

\item[(2)]
Since $f$ is uniformly continuous on $E$,
there exists a $\delta > 0$ such that
\[
  d_Y(f(p),f(q)) < \eta
  \:\:\text{ if }\:\: d_X(p,q) < \delta
  \:\:\text{ and }\:\: p, q \in E.
\]

\item[(3)]
By (1)(2),
\[
  d_Z(h(p),h(q)) = d_Z(g(f(p)),g(f(q))) < \varepsilon
\]
if $d_X(p,q) < \delta$ and $p, q \in E$.
Hence $h$ is uniformly continuous on $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.13.}
\addcontentsline{toc}{subsection}{Exercise 4.13.}
\emph{Let $E$ be a dense subset of a metric space $X$,
and let $f$ be a uniformly continuous real function defined on $E$.
Prove that $f$ has a continuous extension from $E$ to $X$
(see Exercise 4.5 for terminology).
(Uniqueness follows from Exercise 4.4.)
(Hint:
For each $p \in X$ and each positive integer $n$,
let $V_n(p)$ be the set of all $q \in E$ with $d(p,q) < \frac{1}{n}$.
Use Exercise 4.9 to show that the intersection of the closures of the sets
$f(V_1(p)), f(V_2(p)), \ldots$,
consists of a single point, say $g(p)$, of $\mathbb{R}^1$.
Prove that the function $g$ so define on $X$ is the desired extension of $f$.)
Could the range space $\mathbb{R}^1$ be replaced by $\mathbb{R}^k$?
By any compact metric space?
By any complete metric space?
By any metric space?} \\

\emph{Proof (Hint).}
We prove the case that the range metric space is complete.
\begin{enumerate}
\item[(1)]
Given any $p \in X$. We will extend $f$ on $x = p$.
For any positive integer $n$, let
$V_n(p)$ be the set of all $q \in E$ with $d(p,q) < \frac{1}{n}$.

\item[(2)]
\emph{Show that $\overline{f(V_n(p))} \supseteq \overline{f(V_{n+1}(p))}$.}
By construction, $V_n(p) \supseteq V_{n+1}(p)$.
Thus $f(V_n(p)) \supseteq f(V_{n+1}(p))$
and
\[
  \overline{f(V_n(p))} \supseteq \overline{f(V_{n+1}(p))}.
\]

\item[(3)]
\emph{Show that $\lim_{n \to \infty} \mathrm{diam} \overline{f(V_n(p))} = 0$.}
\begin{enumerate}
  \item[(a)]
  Since $E$ is dense in $X$, $V_n(p) \neq \varnothing$ and thus
  \[
    f(V_n(p)) \neq \varnothing.
  \]
  Especially,
  \[
    \overline{f(V_n(p))} \supseteq f(V_n(p)) \neq \varnothing.
  \]
  Hence $\mathrm{diam}V_n(p)$ and $\mathrm{diam}f(V_n(p))$
  are well-defined.

  \item[(b)]
  By the definition of $V_n(p)$ or $0 \leq \mathrm{diam}V_n(p) \leq \frac{2}{n}$,
  \[
    \lim_{n \to \infty} \mathrm{diam}V_n(p) = 0.
  \]

  \item[(c)]
  By the uniformly continuity of $f$ (Exercise 4.9),
  \[
    \lim_{n \to \infty} \mathrm{diam}f(V_n(p)) = 0.
  \]

  \item[(d)]
  Since $\mathrm{diam} \overline{f(V_n(p))} = \mathrm{diam}f(V_n(p))$ (Theorem 3.10(a)),
  \[
    \lim_{n \to \infty} \mathrm{diam} \overline{f(V_n(p))} = 0.
  \]
\end{enumerate}

\item[(4)]
\emph{Show that there is an integer $N$ such that
$\overline{f(V_n(p))}$ is closed and bounded whenever $n \geq N$.}
\begin{enumerate}
  \item[(a)]
  (Closeness.)
  Each $\overline{f(V_n(p))}$ is closed.

  \item[(b)]
  (Boundedness.)
  Since $\lim_{n \to \infty} \mathrm{diam} \overline{f(V_n(p))} = 0$ by (3),
  there is an integer $N$ such that
  \[
    \mathrm{diam} \overline{f(V_n(p))} \leq \frac{1}{89}
  \]
  whenever $n \geq N$.
  By the definition of diameters of $\overline{f(V_n(p))}$,
  each $\overline{f(V_n(p))}$ is bounded by $\frac{1}{64}$ whenever $n \geq N$.

  \item[(c)]
  \emph{Note.}
  If we apply Exercise 4.8 instead,
  we need extra efforts to generalize Exercise 4.8
  to different range spaces for answering the following questions.
\end{enumerate}

\item[(5)]
By (2)(3)(4) and Exercise 3.21,
\[
  \bigcap_{n=N}^{\infty} \overline{f(V_n(p))}
\]
or
\[
  \bigcap_{n=1}^{\infty} \overline{f(V_n(p))}
\]
consists of exactly one point, say $g(p)$.
This point $g(p)$ is an extension of $f$ at $x = p$.
Clearly, $g(p) = f(p)$ if $p \in E$.

\item[(6)]
\emph{Define
\begin{equation*}
  g(p) =
    \begin{cases}
      \bigcap_{n=1}^{\infty} \overline{f(V_n(p))} = f(p)
        & (p \in E), \\
      \bigcap_{n=1}^{\infty} \overline{f(V_n(p))}
        & (p \not\in E).
    \end{cases}
\end{equation*}
Show that $g$ is uniformly continuous.}
  \begin{enumerate}
  \item[(a)]
  Given any $\varepsilon > 0$.
  Since $f$ is uniformly continuous on $E$,
  there exists a $\delta > 0$ such that
  \[
    d(f(p),f(q)) < \frac{\varepsilon}{3} < \varepsilon
  \]
  whenever $d(p,q) < \delta$ and $p, q \in E$.
  We will show that such $\delta$ also holds for $g$.
  Now given any $p, q \in X$ with $d(p,q) < \delta$.

  \item[(b)]
  Since $\mathrm{diam} f(V_n(p)) = \mathrm{diam} \overline{f(V_n(p))}$
  and $\lim_{n \to \infty} \mathrm{diam} \overline{f(V_n(p))} = 0$
  (whether $p \in E$ or not),
  there is an integer $N_1$ such that
  \[
    \mathrm{diam} f(V_n(p)) < \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_1$.
  Similarly,
  there is an integer $N_2$ such that
  \[
    \mathrm{diam} f(V_n(q)) < \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_2$.

  \item[(c)]
  Take an integer $N_3$ satisfying
  \[
    N_3 > \frac{4}{\delta-d(p,q)} > 0.
  \]
  For any $p' \in V_n(p) \neq \varnothing$ and
  $q' \in V_n(q) \neq \varnothing$ as $n \geq N_3$,
  we have
  \begin{align*}
    d(p',q')
    &\leq d(p',p) + d(p,q) + d(q,q') \\
    &\leq \frac{2}{n} + d(p,q) + \frac{2}{n} \\
    &\leq \frac{2}{N_3} + d(p,q) + \frac{2}{N_3} \\
    &< \frac{2(\delta-d(p,q))}{4} + d(p,q) + \frac{2(\delta-d(p,q))}{4} \\
    &= \delta.
  \end{align*}

  \item[(d)]
  Take $N = \max\{N_1,N_2,N_3\}$.
  For any $p' \in V_N(p)$ and $q' \in V_N(p)$,
  we have
  \begin{align*}
    d(g(p),f(p')) &\leq \mathrm{diam} f(V_N(p)) < \frac{\varepsilon}{3}, \\
    d(f(p'),f(q')) &< \frac{\varepsilon}{3}, \\
    d(f(q'),g(q)) &\leq \mathrm{diam} f(V_N(q)) < \frac{\varepsilon}{3}.
  \end{align*}
  Hence
  \begin{align*}
    d(g(p),g(q))
    &\leq
    d(g(p),f(p')) + d(f(p'),f(q')) + d(f(q'),g(q)) \\
    &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
    &= \varepsilon.
  \end{align*}
  \end{enumerate}

\item[(7)]
\emph{Show that the range space $\mathbb{R}^1$ cannot be replaced by any metric space.}
  \begin{enumerate}
  \item[(a)]
  Take $X = \mathbb{R}$ and $Y = \mathbb{Q}$ with the Euclidean metric.
  Let $E = \mathbb{Q}$ be a dense subset of $X = \mathbb{R}$.
  Define $f: E \to Y$ by
  \[
    f(x) = x.
  \]

  \item[(b)]
  $f$ is uniformly continuous on $E$.

  \item[(c)]
  (Reductio ad absurdum)
  If $f$ were having a continuous extension $g$ on $X$,
  then
  \[
    \lim_{n \to \infty} g(p_n) = g(p)
  \]
  for any sequence $\{p_n\}$ in $X$ such that $p_n \neq p$ and $\lim_{n \to \infty} p_n = p$.

  \item[(d)]
  In particular, for some rational sequence $\{p_n\}$ in $E = \mathbb{Q}$
  converging to $\sqrt{2} \in X$, we have
  \[
    \lim_{n \to \infty} g(p_n)
    = \lim_{n \to \infty} f(p_n)
    = \lim_{n \to \infty} p_n
    = \sqrt{2} = g(p) \in \mathbb{Q},
  \]
  which is absurd.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof (Exercise 4.11).}
We prove the case that the range metric space is complete.
\begin{enumerate}
\item[(1)]
Given any $p \in X$. We will extend $f$ on $x = p$.
Since $E$ is dense in $X$, there exists a sequence $\{p_n\}$ in $E$ converging to $p$
(whether $p \in E$ or not).

\item[(2)]
Since $E$ is dense in $X$, there exists a sequence $\{p_n\}$ in $E$ converging to $p$
(whether $p \in E$ or not).
Hence $\{p_n\}$ is Cauchy in $E$ (Theorem 3.11(a)).
Since $f$ is uniformly continuous, $\{f(p_n)\}$ is Cauchy.
Since the range space is complete, $\{f(p_n)\}$ converges to a point, say $g(p)$.
This point $g(p)$ is an extension of $f$ at $x = p$.
Clearly, $g(p) = f(p)$ if $p \in E$.

\item[(3)]
\emph{Show that $g(p)$ is well-defined.}
If $\{p'_n\}$ is another sequence in $E$ converging to $p$,
we construct a new sequence $\{p''_n\}$ based on $\{p_n\}$ and $\{p'_n\}$ by
\begin{equation*}
  p''_n =
    \begin{cases}
      p_{\frac{n+1}{2}} & (n \equiv 1 \pmod{2}), \\
      p'_{\frac{n}{2}}  & (n \equiv 0 \pmod{2}).
    \end{cases}
\end{equation*}
Clearly $\{p''_n\}$ also converges to $p$.
So $\{f(p''_n)\}$ converges to a single point.
Note that $\{f(p_n)\}$ and $\{f(p'_n)\}$ are two subsequences of $\{f(p''_n)\}$,
and thus both subsequences converge to the same point.

\item[(4)]
\emph{Define
\begin{equation*}
  g(p) =
    \begin{cases}
      f(p) & (p \in E), \\
      \lim_{n \to \infty} f(p_n)
        & (p \not\in E)
    \end{cases}
\end{equation*}
where $\{p_n\}$ is any sequence in $E$ converging to $p$.
Show that $g$ is uniformly continuous.}
  \begin{enumerate}
  \item[(a)]
  Given any $\varepsilon > 0$.
  Since $f$ is uniformly continuous on $E$,
  there exists a $\delta > 0$ such that
  \[
    d(f(p),f(q)) < \frac{\varepsilon}{3} < \varepsilon
  \]
  whenever $d(p,q) < \delta$ and $p, q \in E$.
  We will show that such $\delta$ also holds for $g$.
  Now given any $p, q \in X$ with $d(p,q) < \delta$.

  \item[(b)]
  By (2), there exists a sequence $\{p_n\}$ in $E$ such that $\lim p_n = p$.
  Take an integer $N_1$ such that
  \[
    d(p_n,p) < \frac{\delta - d(p,q)}{2}
  \]
  whenever $n \geq N_1$.
  Similarly, there exists a sequence $\{q_n\}$ in $E$ such that $\lim q_n = q$.
  Take an integer $N_2$ such that
  \[
    d(q_n,q) < \frac{\delta - d(p,q)}{2}
  \]
  whenever $n \geq N_2$.
  Therefore,
  \begin{align*}
    d(p_n,q_n)
    &\leq d(p_n,p) + d(p,q) + d(q,q_n) \\
    &< \frac{\delta - d(p,q)}{2} + d(p,q) + \frac{\delta - d(p,q)}{2} \\
    &= \delta.
  \end{align*}
  whenever $n \geq N_1$ and $n \geq N_2$.

  \item[(c)]
  Since $\lim f(p_n) = g(p)$,
  there is an integer $N_3$ such that
  \[
    d(f(p_n), g(p)) < \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_3$.
  Similarly,
  since $\lim f(q_n) = g(q)$,
  there is an integer $N_4$ such that
  \[
    d(f(q_n), g(q)) < \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_4$.

  \item[(d)]
  Take $N = \max\{N_1,N_2,N_3,N_4\}$, we have
  \begin{align*}
    d(g(p),f(p_N)) & < \frac{\varepsilon}{3}, \\
    d(f(p_N),f(q_N)) &< \frac{\varepsilon}{3}, \\
    d(f(q_N),g(q)) & < \frac{\varepsilon}{3}.
  \end{align*}
  Hence
  \begin{align*}
    d(g(p),g(q))
    &\leq
    d(g(p),f(p_N)) + d(f(p_N),f(q_N)) + d(f(q_N),g(q)) \\
    &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
    &= \varepsilon.
  \end{align*}
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.14 (Brouwer's fixed-point theorem).}
\addcontentsline{toc}{subsection}{Exercise 4.14 (Brouwer's fixed-point theorem).}
\emph{Let $I=[0,1]$ be the closed unit interval.
Suppose $f$ is continuous mapping of $I$ into $I$.
Prove that $f(x) = x$ for at least one $x \in I$.} \\

\emph{Proof (Theorem 4.23).}
Let $g(x) = f(x) - x$ in $I$.

\begin{enumerate}
\item[(1)]
$g(0) = 0$. Take $x = 0$.
\item[(2)]
$g(1) = 0$. Take $x = 1$.
\item[(3)]
Suppose $g(0) \neq 0$ ($f(0) \neq 0$) and $g(1) \neq 0$ ($f(1) \neq 1$).
Since $f: I \to I$, $f(0) > 0$ and $f(1) < 1$. That is, $g(0) > 0$ and $g(1) < 0$.
Applying the intermediate value theorem (Theorem 4.23),
there is a point in $\xi \in (0, 1)$ such that $g(\xi) = 0$.
That is, $f(\xi) = \xi$ for some $\xi \in (0,1)$.
\end{enumerate}
In any case, the conclusion holds.
$\Box$ \\



\textbf{Supplement.} Brouwer's fixed-point theorem.
\begin{enumerate}
  \item[(1)]
    In the $\mathbb{R}^1$, see Exercise 4.14 itself.

  \item[(2)]
    In the $\mathbb{R}^2$, see Exercise 8.29.

  \item[(3)]
    In the $\mathbb{R}^n$,
    every continuous function from a closed ball of a Euclidean space $\mathbb{R}^n$
    into itself has a fixed point (without proof).

  \item[(4)]
    In a Banach space, Schauder fixed-point theorem. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.15.}
\addcontentsline{toc}{subsection}{Exercise 4.15.}
\emph{Call a mapping of $X$ into $Y$ \textbf{open} if $f(V)$ is an open set in $Y$
whenever $V$ is an open set in $X$.
Prove that every continuous open mapping of $\mathbb{R}^1$ into $\mathbb{R}^1$
is monotonic.} \\

In fact, $f$ is strictly monotonic. \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  (Reductio ad absurdum)
  If $f$ were not strictly monotonic,
  then there exist $a < c < b \in \mathbb{R}^1$ such that
  \[
    f(a) \leq f(c) \geq f(b)
  \]
  or
  \[
    f(a) \geq f(c) \leq f(b).
  \]

  \item[(2)]
  In any case, $f$ is a real continuous function on a compact set $[a,b]$.
  By Theorem 4.16,
  there exist $p, q \in [a,b]$ such that
  \begin{align*}
    M &= \sup_{x \in [a,b]}f(x) = f(p), \\
    m &= \inf_{x \in [a,b]}f(x) = f(q).
  \end{align*}

  \item[(3)]
  As $f(a) \leq f(c) \geq f(b)$, we consider where $f$ reaches its maximum value $M$ (by (2)).
  \begin{enumerate}
  \item[(a)]
    $f(a) = M$ or $f(b) = M$. Since $f(a) \leq f(c) \geq f(b)$, by the maximality of $M$,
    $f(c) = M$ or $M \in f((a,b))$.

  \item[(b)]
    $f(a) < M$ and $f(b) < M$. Hence $M \in f((a,b))$ clearly.
  \end{enumerate}
  In any case, $M \in f((a,b))$.
  Note that $f((a,b))$ is open
  since $f$ is an open mapping and $(a,b)$ is open.

  Since $M$ is in an open set $f((a,b))$,
  there exists an open neighborhood $B(M;r) \subseteq f((a,b))$ where $r > 0$.
  Hence
  \[
    M+\frac{r}{64} \in B(M;r) \subseteq f((a,b)),
  \]
  contrary to the maximality of $M$.

  \item[(4)]
  As $f(a) \geq f(c) \leq f(b)$,
  we consider where $f$ reaches its minimum value $m$ (by (2)).
  Similar to (3), we can reach a contradiction again.

  \item[(5)]
  By (3)(4), (1) is absurd, and thus $f$ is strictly monotonic.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.16.}
\addcontentsline{toc}{subsection}{Exercise 4.16.}
\emph{Let $[x]$ denote the largest integer contained in $x$, this is,
$[x]$ is a integer such that $x-1 < [x] \leq x$;
and let $(x) = x - [x]$ denote the fractional part of $x$.
What discontinuities do the function $[x]$ and $(x)$ have?} \\

\emph{Proof.}

\begin{enumerate}
\item[(1)]
\emph{The function $[x]$ only has discontinuities at $x \in \mathbb{Z}$.}
  \begin{enumerate}
  \item[(a)]
  For any $p \not\in \mathbb{Z}$,
  there is an integer $n$ such that $n < p < n+1$.
  Given any $\varepsilon > 0$, there is a $\delta = \min\{p-n, (n+1)-p\} > 0$
  such that $\abs{[x] - [p]} < \varepsilon$
  whenever $\abs{x - p} < \delta$.
  In fact, $\abs{x - p} < \delta$ is equivalent to $n < x < n+1$
  and therefore $\abs{[x] - [p]} = \abs{n - n} = 0 < \varepsilon$.
  \item[(b)]
  For any $p \in \mathbb{Z}$,
  $\lim_{x \to p^{+}}[x] = p$ and $\lim_{x \to p^{-}}[x] = p-1$.
  \end{enumerate}
\item[(2)]
\emph{The function $(x)$ only has discontinuities at $x \in \mathbb{Z}$.}
  \begin{enumerate}
  \item[(a)]
  Since $[x]$ is continuous on $\mathbb{R} - \mathbb{Z}$ and
  $x$ is continuous on $\mathbb{R}$, especially on $\mathbb{R} - \mathbb{Z}$,
  $(x) = x - [x]$ is continuous on $\mathbb{R} - \mathbb{Z}$.
  \item[(b)]
  For any $p \in \mathbb{Z}$,
  $\lim_{x \to p^{+}}(x) = 0$ and $\lim_{x \to p^{-}}(x) = 1$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.17.}
\addcontentsline{toc}{subsection}{Exercise 4.17.}
\emph{Let $f$ be a real function defined on $(a,b)$.
Prove that the set of points at which $f$ has a simple discontinuity is at most countable.
(Hint: Let $E$ be the set on which $f(x-) < f(x+)$.
With each point $x$ of $E$, associate a triple $(p,q,r)$ of rational numbers such that}
\begin{enumerate}
  \item[(a)]
  \emph{$f(x-) < p < f(x+)$,}
  \item[(b)]
  \emph{$a < q < t < x$ implies $f(t) < p$,}
  \item[(c)]
  \emph{$x < t < r < b$ implies $f(t) > p$.}
\end{enumerate}
\emph{The set of all such triples is countable.
Show that each triple is associated with at most one point of $E$.
Deal similarly with the other possible types of simple discontinuities.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
  \item[(1)]
  By Definition 4.26,
  \emph{it suffices to show that each following set is at most countable}
  and thus the union of four sets are at most countable (Corollary to Theorem 2.12).
  \begin{enumerate}
    \item[(a)]
    $E_1 = \{ x : f(x+) > f(x-) \}$,

    \item[(b)]
    $E_2 = \{ x : f(x+) < f(x-) \}$,

    \item[(c)]
    $E_3 = \{ x : f(x) > f(x+) = f(x-) \}$,

    \item[(d)]
    $E_4 = \{ x : f(x) < f(x+) = f(x-) \}$,
  \end{enumerate}

  \item[(2)]
  \emph{Show that with each point $x$ of $E_1$,
  we can associate a triple $(p,q,r)$ of rational numbers such that}
  \begin{enumerate}
    \item[(a)]
    \emph{$f(x-) < p < f(x+)$,}
    \item[(b)]
    \emph{$a < q < t < x$ implies $f(t) < p$,}
    \item[(c)]
    \emph{$x < t < r < b$ implies $f(t) > p$.}
  \end{enumerate}
  \begin{enumerate}
    \item[(i)]
    For (a), as $(f(x-),f(x+))$ is a nonempty open set in $\mathbb{R}$
    and $\mathbb{Q}$ is dense in $\mathbb{R}$,
    there exists a rational number $p \in (f(x-),f(x+))$ or $f(x-) < p < f(x+)$.

    \item[(ii)]
    For (b), since $f(x-)$ exists, for $\varepsilon = p - f(x-) > 0$
    there exists a $\delta > 0$ such that
    \[
      f(x-)-\varepsilon < f(t) < f(x-)+\varepsilon
      \:\:\text{ whenever }\:\:
      a < x - \delta < t < x.
    \]
    So
    \[
      f(t) < p
      \:\:\text{ whenever }\:\:
      a < x - \delta < t < x.
    \]
    Since $(x-\delta,x)$ is a nonempty open set in $\mathbb{R}$
    and $\mathbb{Q}$ is dense in $\mathbb{R}$,
    there exists a rational number $q \in (x-\delta,x)$ or $a < x-\delta < q < x$.
    Hence if $a < q < t < x$ we have $f(t) < p$.

    \item[(iii)]
    For (c), similar to (ii).
    Since $f(x+)$ exists, for $\varepsilon = f(x+) - p > 0$
    there exists a $\delta > 0$ such that
    \[
      f(x+)-\varepsilon < f(t) < f(x+)+\varepsilon
      \:\:\text{ whenever }\:\:
      x < t < x + \delta < b.
    \]
    So
    \[
      f(t) > p
      \:\:\text{ whenever }\:\:
      x < t < x + \delta < b.
    \]
    Since $(x,x+\delta)$ is a nonempty open set in $\mathbb{R}$
    and $\mathbb{Q}$ is dense in $\mathbb{R}$,
    there exists a rational number $r \in (x,x+\delta)$ or $x < r < x+\delta < b$.
    Hence if $x < t < r < b$ we have $f(t) > p$.
  \end{enumerate}

  \item[(3)]
  \emph{Show that each triple in (2) is associated with at most one point of $E_1$.}
  (Reductio ad absurdum)
  If there were another $x' > x$ such that
  \begin{enumerate}
    \item[(a)]
    $f(x'-) < p < f(x'+)$,
    \item[(b)]
    $a < q < t < x'$ implies $f(t) < p$,
    \item[(c)]
    $x' < t < r < b$ implies $f(t) > p$,
  \end{enumerate}
  then there exists a $t_0 \in \mathbb{R}$ satisfying $x < t_0 < x'$
  such that $f(t_0) > p$ (property (c) of $x$) and $f(t_0) < p$ (property (b) of $x'$),
  which is absurd.
  The case $x' < x$ is similar.

  \item[(4)]
  By (2)(3), the set $E_1$ is a subset of a countable set
  \[
    \{ (p,q,r) : (p,q,r) \in \mathbb{Q}^3 \},
  \]
  which is at most countable (Theorem 2.8 and Corollary to Theorem 2.12).
  Similarly, $E_2$ is at most countable.

  \item[(5)]
  Similar to (2), for each point $x$ of $E_3$,
  we can associate a triple $(p,q,r)$ of rational numbers such that
  \begin{enumerate}
    \item[(a)]
    $f(x) > p > f(x+) = f(x-)$,
    \item[(b)]
    $a < q < t < x$ implies $f(t) < p$,
    \item[(c)]
    $x < t < r < b$ implies $f(t) < p$.
  \end{enumerate}

  \item[(6)]
  \emph{Show that each triple in (5) is associated with at most one point of $E_3$.}
  (Reductio ad absurdum)
  If there were another $x' > x$ such that
  \begin{enumerate}
    \item[(a)]
    $f(x') > p > f(x'+) = f(x'-)$,
    \item[(b)]
    $a < q < t < x'$ implies $f(t) < p$,
    \item[(c)]
    $x' < t < r < b$ implies $f(t) < p$,
  \end{enumerate}
  then $f(x) < p$ (property (b) of $x'$), which is absurd.
  The case $x' < x$ is similar.

  \item[(7)]
  By (5)(6), the set $E_3$ is a subset of a countable set
  \[
    \{ (p,q,r) : (p,q,r) \in \mathbb{Q}^3 \},
  \]
  which is at most countable (Theorem 2.8 and Corollary to Theorem 2.12).
  Similarly, $E_4$ is at most countable.

  \item[(8)]
  By (4)(7), all $E_j$ ($j=1,2,3,4$) are at most countable.
  Hence the set of points at which $f$ has a simple discontinuity is at most countable.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.18 (Thomae's function).}
\addcontentsline{toc}{subsection}{Exercise 4.18 (Thomae's function).}
\emph{Every rational $x$ can be written in the form $x = \frac{m}{n}$, where $n > 0$,
and $m$ and $n$ are integers without any common divisors.
When $x = 0$, we take $n = 1$.
Consider the function $f$ defined on $\mathbb{R}^1$ by
\begin{equation*}
  f(x) =
    \begin{cases}
      0           & (\text{$x$ irrational}), \\
      \frac{1}{n} & (x = \frac{m}{n}).
    \end{cases}
\end{equation*}
Prove that $f$ is continuous at every irrational point,
and that $f$ has a simple discontinuity at every rational point.} \\

\emph{Proof.}

\begin{enumerate}
\item[(1)]
\emph{Show that $f$ has period $1$.}
  \begin{enumerate}
  \item[(a)]
  As $x$ is irrational, $x+1$ is irrational too.
  Hence $f(x+1) = 0 = f(x)$.

  \item[(b)]
  As $x = \frac{m}{n} \in \mathbb{Q}$, where $n > 0$,
  and $m$ and $n$ are integers without any common divisors.
  \[
    x+1 = \frac{m+n}{n} \in \mathbb{Q},
  \]
  where $n > 0$, and $\gcd(m+n,n) = \gcd(m,n) = 1$.
  Hence $f(x+1) = \frac{1}{n} = f(x)$.
  \end{enumerate}
  In any case, $f(x+1) = f(x)$ for any $x \in \mathbb{R}$.

\item[(2)]
\emph{Show that $f(p+) = f(p-) = 0$ for any $p \in \mathbb{R}$.}
\begin{enumerate}
  \item[(a)]
  By (1), we might assume $p \in [0,1]$.
  For the edge cases $f(0-)$ and $f(1+)$,
  note that $f(0-) = f(1-)$ and $f(1+) = f(0+)$.

  \item[(b)]
  Now given any point $p \in [0,1]$.
  Given $\varepsilon > 0$,
  it suffices to find $\delta > 0$ such that
  \[
    \abs{ f(x) - 0 } < \varepsilon
    \:\:\text{ whenever }\:\:
    p < x < p + \delta.
  \]
  (The case $f(p-) = 0$ is similar.)

  \item[(c)]
  For such $\varepsilon > 0$,
  there is an integer $N \geq s$ such that
  \[
    \frac{1}{N} < \varepsilon.
  \]
  Hence
  \[
    E = \left\{ \frac{m}{n} \in [0,1] : n \leq N
      \text{ where $\frac{m}{n}$ is defined as this exercise} \right\}
      - \left\{ p \right\}
  \]
  is finite since $m$ and $n$ are satisfying the relation $0 \leq m \leq n \leq N$
  and there are only finitely many pairs of $(m,n)$.

  \item[(d)]
  Now we can take
  \[
    \delta = \min\{ |q - p| : q \in E \} > 0.
  \]
  As $p < x < p + \delta$,
  $\abs{ f(x) - 0 } = 0 < \varepsilon$ if $x$ is irrational;
  \[
    \abs{ f(x) - 0 } < \frac{1}{N} < \varepsilon
  \]
  if $x$ is rational.
  (Note that $x \not\in E$ by the construction of $\delta$
  and thus the denominator of $x$ is $> N$.)
  In any case, $\abs{ f(x) - 0 } < \varepsilon$.
  \end{enumerate}

\item[(3)]
Since $f(x) = 0$ if $x \in \mathbb{R} - \mathbb{Q}$
and $f(x) \neq 0$ if $x \in \mathbb{Q}$,
$f$ is continuous at every irrational point
and $f$ has a simple discontinuity at every rational point (by (2)).
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
\emph{$f$ is nowhere differentiable.}
  \begin{enumerate}
  \item[(a)]
  For any rational number, this follows from discontinuity.

  \item[(b)]
  For any irrational number $x$, given $\varepsilon = \frac{1}{64} > 0$,
  \emph{it suffices to show that for any $\delta > 0$, there exists a $x < t < x + \delta$ such that
  \[
    \frac{f(t)-f(x)}{t-x} > \frac{1}{64}.
  \]}

  \item[(c)]
  It is clear to find $t$ in $\mathbb{Q}$.
  For such $\delta > 0$, there is an integer $n$ such that
  \[
    n > \frac{1}{\delta} > 0.
  \]
  For such $n > 0$, there is an integer $m$ such that
  \[
    m-1 < nx < m.
  \]
  ($m-1 \leq nx < m$ is true trivially and notice that the equality does not hold
  since $x$ is irrational.)
  Take
  \[
    t = \frac{m}{n}.
  \]
  So that
  \[
    x < t < x + \frac{1}{n} < x + \delta.
  \]

  \item[(d)]
  Let $\gcd(m,n) = g \geq 1$.
  Then
  \[
    f(t) = \frac{1}{\frac{n}{g}} = \frac{g}{n} \geq \frac{1}{n}
  \]
  and thus
  \[
    \frac{f(t)-f(x)}{t-x} > \frac{\frac{1}{n}}{\frac{1}{n}} = 1 > \frac{1}{64}.
  \]
  \end{enumerate}
  $\Box$

\item[(2)]
  Another proof for showing that $f$ is nowhere differentiable.
  \begin{enumerate}
  \item[(a)]
  In Proof of (b) in Exercise 4.25, we showed that
  \begin{quote}
    \emph{Given any real $\alpha > 0$ and an integer $N > 1$,
    show that there exist integers $h$ and $k$ with $0 < k \leq N$ such that
    \[
      \abs{ k\alpha - h } < \frac{1}{N}.
    \]}
  \end{quote}

  \item[(b)]
  \emph{Given any irrational $\alpha > 0$,
  show that there are infinitely many rational numbers
  $\frac{h}{k}$ with $k > 0$ such that
  \[
    \abs{ \alpha - \frac{h}{k} } < \frac{1}{k^2}.
  \]}
  (Reductio ad absurdum)
  If there were only a finite numbers
  $\frac{h_1}{k_1}, \ldots, \frac{h_r}{k_r}$,
  write
  \[
    \delta = \min\left\{ \abs{ \alpha - \frac{h_i}{k_i} } : 1 \leq i \leq r \right\}.
  \]
  Here $\delta > 0$ since $\alpha$ is irrational.
  Take $N > \max\{1, \frac{1}{\delta} \}$ and apply (a) to get that
  there exist integers $h$ and $k$ with $0 < k \leq N$ such that
  \[
    \abs{ k\alpha - h } < \frac{1}{N}.
  \]
  Hence
  \[
    \abs{ \alpha - \frac{h}{k} } < \frac{1}{kN} \leq \frac{1}{k^2}
  \]
  and
  \[
    \abs{ \alpha - \frac{h}{k} } < \frac{1}{kN} \leq \frac{1}{N} < \delta,
  \]
  contrary to the construction of $\delta$.

  \item[(c)]
  For any rational number, this follows from discontinuity.
  For any irrational number $x$,
  by (b) there is a sequence
  \[
    \left\{ \frac{h_n}{k_n} \right\}
  \]
  of rational numbers such that
  \[
    \abs{ \frac{h_n}{k_n} - x } < \frac{1}{k_n^2}
    \qquad\text{ and }\qquad
    \lim_{n \to \infty} k_n = \infty.
  \]
  Hence
  \[
    \abs{ \frac{f\left( \frac{h_n}{k_n} \right) - f(x)}{\frac{h_n}{k_n} - x} }
    \geq
    \frac{1}{k_n} \cdot \left( \frac{1}{k_n^2} \right)^{-1}
    = k_n
    \to \infty
  \]
  as $n \to \infty$.
  Therefore, $f$ is not differentiable.
  \end{enumerate}
  $\Box$

\item[(3)]
\emph{$f \in \mathscr{R}$ on any bounded interval $[a,b]$ and $\int_{a}^{b} f dx = 0$.}
To prove this, use the similar argument in Exercise 6.6. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.19.}
\addcontentsline{toc}{subsection}{Exercise 4.19.}
\emph{Suppose $f$ is a real function with domain $\mathbb{R}^1$
which has the intermediate value property:
If $f(a) < c < f(b)$, then $f(x)=c$ for some $x$ between $a$ and $b$.
Suppose also, for every rational $r$,
that the set of all $x$ with $f(x)=r$ is closed.
Prove that $f$ is continuous.
(Hint: If $x_n \to x_0$ but $f(x_n) > r > f(x_0)$ for some $r$ and all $n$,
then $f(t_n) = r$ for some $t_n$ between $x_0$ and $x_n$;
thus $t_n \to x_0$.
Find a contradiction.
(N. J. Fine, Amer. Math. Monthly, vol. 73, 1966, p.782.))} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
(Reductio ad absurdum)
If $f$ were not continuous at $x = x_0$, then there exists a sequence $\{x_n\}$ in $\mathbb{R}^1$
such that $\{f(x_n)\}$ in $\mathbb{R}^1$ not converging to $f(x_0)$.
So there exists a $\varepsilon_0 > 0$ and a subsequence $\{x_{n'}\}$ of $\{x_n\}$
such that
\[
  \abs{ f\left(x_{n'}\right) - f(x_0) } \geq \varepsilon_0
\]
for all $n'$.
Hence
\[
  f\left(x_{n'}\right) \geq f(x_0) + \varepsilon_0
  \:\: \text{ or } \:\:
  f\left(x_{n'}\right) \leq f(x_0) - \varepsilon_0
\]
for all $n'$.

\item[(2)]
Since there are infinitely many $\{x_{n'}\}$,
there are only two possible cases:
  \begin{enumerate}
  \item[(a)]
  There is a subsequence $\{x_{n''}\}$ of $\{x_{n'}\}$
  such that
  $f\left(x_{n''}\right) \geq f(x_0) + \varepsilon_0$
  for all $n''$.

  \item[(b)]
  There is a subsequence $\{x_{n''}\}$ of $\{x_{n'}\}$
  such that
  $f\left(x_{n''}\right) \leq f(x_0) - \varepsilon_0$
  for all $n''$.
  \end{enumerate}
In any case, we may assume that there exists a $\varepsilon_0 > 0$
and a sequence $\{x_n\}$
such that
\[
  f(x_{n}) \geq f(x_0) + \varepsilon_0
\]
for all $n$.
(The case that $f(x_{n}) \leq f(x_0) - \varepsilon_0$ is similar.)

\item[(3)]
Pick any rational number $r \in (f(x_0), f(x_0) + \varepsilon_0)$
since $\mathbb{Q}$ is dense in $\mathbb{R}$.
(In fact, $\mathbb{Q}$ can be replaced by any dense subset of $\mathbb{R}$ in this exercise.)
Since
\[
  f(x_n) \geq f(x_0) + \varepsilon_0 > r > f(x_0),
\]
by the intermediate value property,
there exists some $t_n$ between $x_0$ and $x_n$
such that $f(t_n) = r$.

\item[(4)]
Let $E = \{ t : f(t) = r \}$ be a closed set
(since $r \in \mathbb{Q}$ and the assumption).
Hence $t_n \in E$ for all $n$ (by (3)).

\item[(5)]
$\lim t_n = x_0$ since $\lim x_n = x_0$.
Hence $x_0$ is a limit point of $E$.
(Note that $x_0 \not\in E$ since $f(x_0) < r$ (by (3)).)
Since $E$ is closed, $x_0 \in E$ or $f(x_0) = r$, which is absurd.
\end{enumerate}
$\Box$ \\

\textbf{Supplement.}
\emph{Suppose $f$ is a differentiable real function with domain $\mathbb{R}^1$.
Suppose for every rational $r$,
that the set of all $x$ with $f'(x)=r$ is closed.
Prove that $f'$ is continuous.}
Theorem 5.12 (Darboux's theorem) and Exercise 4.19 itself implies all.
\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.20.}
\addcontentsline{toc}{subsection}{Exercise 4.20.}
\emph{If $E$ is a nonempty subset of a metric space $X$,
define the distance from $x \in X$ to $E$ by
\[
  \rho_E(x) = \inf_{z \in E} d(x,z).
\]}
\begin{enumerate}
  \item[(a)]
  \emph{Prove that $\rho_E(x) = 0$ if and only if $x \in \overline{E}$.}

  \item[(b)]
  \emph{Prove that $\rho_E(x)$ is a uniformly continuous function on $X$,
  by showing that
  \[
    \abs{ \rho_E(x) - \rho_E(y) } \leq d(x,y)
  \]
  for all $x \in X$, $y \in X$.
  (Hint: $\rho_E(x) \leq d(x,z) \leq d(x,y) + d(y,z)$, so that
  \[
    \rho_E(x) \leq d(x,y) + \rho_E(y).)
  \]} \\
\end{enumerate}

\emph{Proof of (a).}
Note that $\rho_E(x) = \inf_{z \in E} d(x,z) \geq 0$ since $d(x,z) \geq 0$.
So
\begin{align*}
  \rho_E(x) > 0
  &\Longleftrightarrow
  \inf_{z \in E} d(x,z) > 0 \\
  &\Longleftrightarrow
  \exists \: r > 0 \text{ such that } B(x;r) \cap E = \varnothing \\
  &\Longleftrightarrow
  x \not\in \overline{E}.
\end{align*}
$\Box$ \\

\emph{Proof of (b) (Hint).}
Given any $x \in X$, $y \in X$.
For any $z \in E$, we have
\[
  \rho_E(x) \leq d(x,z) \leq d(x,y) + d(y,z).
\]
$\rho_E(x) - d(x,y)$ is a lower bound of the set $\{ d(y,z) : z \in E \}$.
So that
\[
  \rho_E(x) \leq d(x,y) + \rho_E(y).
\]
Similarly,
\[
  \rho_E(y) \leq d(x,y) + \rho_E(x).
\]
Hence
\[
  \abs{ \rho_E(x) - \rho_E(y) } \leq d(x,y),
\]
or $\rho_E(x)$ is uniformly continuous on $X$
(by taking $\delta = \varepsilon > 0$ in Definition 4.18).
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.21.}
\addcontentsline{toc}{subsection}{Exercise 4.21.}
\emph{Suppose $K$ and $F$ are disjoint sets in a metric space $X$.
$K$ is compact.
$F$ is closed.
Prove that there exists a $\delta > 0$ such that $d(p,q) > \delta$ if $p \in K$, $q \in F$.
(Hint: $\rho_F$ is a continuous positive function on $K$.)
Show that the conclusion may fail for two disjoint closed sets if neither is compact.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Define $\rho_F: K \to \mathbb{R}$ by
\[
  \rho_F(x) = \inf_{q \in F} d(x,q).
\]

\item[(2)]
$K \cap F = \varnothing$ and $F = \overline{F}$ is closed,
$\rho_F(x) > 0$ (Exercise 4.20(a)).

\item[(3)]
Since $\rho_F(x)$ is continuous (Exercise 4.20(b)) on a compact set $K$,
there exists a $\xi \in K$ such that
\[
  \rho_F(\xi) = \inf_{p \in K} \rho_F(p)
\]
(Theorem 4.16).
By (2), $\rho_F(\xi) > 0$.
Write
\[
  \rho_F(\xi) = \delta > 0.
\]
Hence
\begin{align*}
  &\rho_F(\xi) = \inf_{p \in K} \rho_F(p) = \delta > 0 \\
  \Longleftrightarrow&
  \rho_F(p) = \inf_{q \in F} d(p,q) \geq \delta > 0 \: \forall \: p \in K \\
  \Longleftrightarrow&
  d(p,q) \geq \delta > 0 \: \forall \: p \in K \: \forall \: q \in F.
\end{align*}

\item[(4)]
\emph{Show that the conclusion may fail for two disjoint closed sets if neither is compact.}
\begin{enumerate}
  \item[(a)]
  Let $X = \mathbb{R}$ equipped with the Euclidean metric,
  \[
    K = \{ n : n \in \mathbb{Z}^{+} \},
  \]
  and
  \[
    F = \left\{ n + \frac{1}{64^n} : n \in \mathbb{Z}^{+} \right\}.
  \]
  Both $K$ and $F$ are closed and neither is compact.
  And for any positive integer $n$,
  we have
  \[
    d\left(n, n+\frac{1}{64^n}\right) = \frac{1}{64^n} \to 0
  \]
  as $n \to \infty$.
  (Here $n \in K$ and $n+\frac{1}{64^n} \in F$.)

  \item[(b)]
  Another example.
  Let $X = \mathbb{R}^2$ equipped with the Euclidean metric,
  \[
    K = \{ (x,y) : xy = 1, x, y \in \mathbb{R} \},
  \]
  and
  \[
    F = \{ (x,0) : x \in \mathbb{R} \} \bigcup \{ (0,y) : y \in \mathbb{R} \}.
  \]
\end{enumerate}
\end{enumerate}
$\Box$\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.22 (Urysohn's lemma).}
\addcontentsline{toc}{subsection}{Exercise 4.22 (Urysohn's lemma).}
\emph{Let $A$ and $B$ be disjoint nonempty closed sets in a metric space $X$,
and define
\[
  f(p) = \frac{\rho_A(p)}{\rho_A(p)+\rho_B(p)}, \qquad (p \in X).
\]
Show that $f$ is a continuous function on $X$ whose range lies in $[0,1]$,
that $f(p) = 0$ precisely on $A$ and $f(p) = 1$ precisely on $B$.
This establishes a converse of Exercise 4.3:
Every closed set $A$ contained in $X$ is $Z(f)$ for some continuous real $f$ on $X$.
Setting
\[
  V = f^{-1}{\left(\left[0,\frac{1}{2}\right)\right)},
  \qquad
  W = f^{-1}{\left(\left(\frac{1}{2},1\right]\right)},
\]
show that $V$ and $W$ are open and disjoint,
and that $A$ is contained in $V$, $B$ is contained in $W$.
(Thus pairs of disjoint closed set in a metric space can be covered by pairs of disjoint open sets.
This property of metric spaces is called normality.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $f(p) = \frac{\rho_A(p)}{\rho_A(p)+\rho_B(p)}$ is well-defined.}
It suffices to show that $\rho_A(p)+\rho_B(p) > 0$.
Note that $\rho_A(p)+\rho_B(p) \geq 0$.
Hence
\begin{align*}
  &\rho_A(p)+\rho_B(p) = 0 \\
  \Longleftrightarrow&
  \rho_A(p) = 0 \text{ and } \rho_B(p) = 0 \\
  \Longleftrightarrow&
  p \in \overline{A} \text{ and } p \in \overline{B}
    &\text{(Exercise 4.20(a))} \\
  \Longleftrightarrow&
  p \in A \text{ and } p \in B
    &\text{($A, B$: closed)} \\
  \Longleftrightarrow&
  p \in A \cap B = \varnothing.
    &\text{($A, B$: disjoint)} \\
\end{align*}
implies that $\rho_A(p)+\rho_B(p) > 0$.

\item[(2)]
\emph{Show that $f$ is a continuous function on $X$ whose range lies in $[0,1]$.}
Since $\rho_A$ and $\rho_B$ are continuous and
$\rho_A(p)+\rho_B(p) \neq 0$ for all $p \in X$,
$f$ is continuous (Theorem 4.9).
Also, since $\rho_A(p) \geq 0$ and $\rho_B(p) \geq 0$,
\[
  f(p)
  = \frac{\rho_A(p)}{\rho_A(p)+\rho_B(p)}
  \geq 0
\]
and
\[
  f(p)
  = \frac{\rho_A(p)}{\rho_A(p)+\rho_B(p)}
  \leq \frac{\rho_A(p)+\rho_B(p)}{\rho_A(p)+\rho_B(p)}
  = 1.
\]
So $f(p) \in [0,1]$ for all $p \in X$.

\item[(3)]
\emph{Show that $f(p) = 0$ precisely on $A$ and $f(p) = 1$ precisely on $B$.}
\begin{align*}
  f(p) = 0
  &\Longleftrightarrow
  \frac{\rho_A(p)}{\rho_A(p)+\rho_B(p)} = 0 \\
  &\Longleftrightarrow
  \rho_A(p) = 0 \\
  &\Longleftrightarrow
  p \in \overline{A}
    &\text{(Exercise 4.20(a))} \\
  &\Longleftrightarrow
  p \in A.
    &\text{($A$: closed)}
\end{align*}
Similarly, we apply the same argument to $1 - f$ to get
$f(p) = 1$ if and only if $p \in B$.

\item[(4)]
\emph{Setting
\[
  V = f^{-1}{\left(\left[0,\frac{1}{2}\right)\right)},
  \qquad
  W = f^{-1}{\left(\left(\frac{1}{2},1\right]\right)},
\]
show that $V$ and $W$ are open and disjoint,
and that $A$ is contained in $V$, $B$ is contained in $W$.}
  \begin{enumerate}
  \item[(a)]
  Since $\left[0,\frac{1}{2}\right)$ is open in $[0,1]$,
  $V$ is open (Theorem 4.8).
  Similarly, $W$ is open.

  \item[(b)]
  Since $\left[0,\frac{1}{2}\right)$ and $\left(\frac{1}{2},1\right]$ are disjoint,
  $V$ and $W$ are disjoint.

  \item[(c)]
  By (3), $A = f^{-1}(0) \subseteq V$ and $B = f^{-1}(1) \subseteq W$.
  \end{enumerate}
Hence, any metric space is a normal space.
\end{enumerate}
$\Box$\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.23.}
\addcontentsline{toc}{subsection}{Exercise 4.23.}
\emph{A real-valued function $f$ defined in $(a,b)$
is said to be \textbf{convex} if
$$f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)$$
whenever $a < x < b$, $a < y < b$, $0 < \lambda < 1$.
Prove that every convex function is continuous.
Prove that every increasing convex function of a convex function is convex.
(For example, if $f$ is convex, so is $e^f$.)} \\

\emph{If $f$ is convex in $(a,b)$ and if $a < s < t < u < b$,
show that
$$\frac{f(t)-f(s)}{t-s}
\leq \frac{f(u)-f(s)}{u-s}
\leq \frac{f(u)-f(t)}{u-t}.$$} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
$\frac{f(t)-f(s)}{t-s}
\leq \frac{f(u)-f(s)}{u-s}
\leq \frac{f(u)-f(t)}{u-t}$.}
Since
\begin{align*}
  t
  &= \frac{t-s}{u-s} u + \left( 1-\frac{t-s}{u-s} \right) s \\
  &= \left( 1-\frac{u-t}{u-s} \right) u + \frac{u-t}{u-s} s
\end{align*}
and $0 < \frac{t-s}{u-s}, \frac{u-t}{u-s} < 1$,
by the convexity of $f$ we have
\begin{align*}
  f(t)
  &\leq \frac{t-s}{u-s} f(u) + \left( 1-\frac{t-s}{u-s} \right) f(s), \\
  f(t)
  &\leq \left( 1-\frac{u-t}{u-s} \right) f(u) + \frac{u-t}{u-s} f(s).
\end{align*}
It is equivalent to
$$\frac{f(t)-f(s)}{t-s}
\leq \frac{f(u)-f(s)}{u-s}
\leq \frac{f(u)-f(t)}{u-t}.$$
$\Box$\\
\item[(2)]
\emph{If $x, y, x', y'$ are points of $(a,b)$
with $x \leq x' < y'$ and $x < y \leq y'$,
then the chord over $(x',y')$ has larger slope than the chord over $(x,y)$; that is,
$$\frac{f(y)-f(x)}{y-x} \leq \frac{f(y')-f(x')}{y'-x'}.$$}
It is a corollary to (1).
\item[(3)]
\emph{Show that $f$ is continuous.}
Let $[c,d] \subseteq (a,b)$.
Then by (2),
$$\frac{f(c)-f(a)}{c-a}
\leq \frac{f(y) - f(x)}{y - x}
\leq \frac{f(b)-f(d)}{b-d}$$
for $x, y$ in $[c,d]$.
Thus $|f(y) - f(x)| \leq M|y - x|$ in $[c,d]$
(where $M = \max\left( |\frac{f(c)-f(a)}{c-a}|, |\frac{f(b)-f(d)}{b-d}| \right)$),
and so $f$ is absolutely continuous
on each closed subinterval of $(a,b)$.
Especially, $f$ is continuous.
\item[(4)]
\emph{Let $f$ be a convex function,
$g$ be an increasing convex function,
and $h = g \circ f$.
Show that $h$ is convex.}
\begin{align*}
f(\lambda x + (1-\lambda) y)
&\leq \lambda f(x) + (1-\lambda) f(y),
  &\text{(Convexity of $f$)} \\
g(f(\lambda x + (1-\lambda) y))
&\leq g(\lambda f(x) + (1-\lambda) f(y))
  &\text{(Increasing of $g$)} \\
&\leq \lambda g(f(x)) + (1-\lambda) g(f(y)),
  &\text{(Convexity of $g$)} \\
h(\lambda x + (1-\lambda) y)
&\leq \lambda h(x) + (1-\lambda) h(y).
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.24.}
\addcontentsline{toc}{subsection}{Exercise 4.24.}
\emph{Assume that $f$ is a continuous real function defined in $(a,b)$
such that
$$f\left( \frac{x+y}{2} \right)
\leq \frac{f(x)+f(y)}{2}$$
for all $x, y \in (a,b)$.
Prove that $f$ is convex.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
$$f\left( \frac{x_1 + \cdots + x_n}{n} \right)
\leq \frac{f(x_1) + \cdots + f(x_n)}{n}$$
whenever $a < x_i < b$ $(1 \leq i \leq n)$.}
Apply Cauchy induction and use the same argument in proving the AM-GM inequality.
As $n = 1, 2$, the inequality holds by assumption.
Suppose $n = 2^k$ $(k \geq 1)$ the inequality holds.
As $n = 2^{k+1}$,
\begin{align*}
&f\left( \frac{x_1 + \cdots + x_{2^{k+1}}}{2^{k+1}} \right) \\
=& f\left( \frac{1}{2} \left(\frac{x_1 + \cdots + x_{2^k}}{2^k}
  + \frac{x_{2^k+1} + \cdots + x_{2^{k+1}}}{2^k}\right) \right) \\
\leq& \frac{1}{2}
  \left(
    f\left(\frac{x_1 + \cdots + x_{2^k}}{2^k} \right)
    + f\left(\frac{x_{2^k+1} + \cdots + x_{2^{k+1}}}{2^k} \right)
  \right) \\
\leq& \frac{1}{2}
  \left(
    \frac{f(x_1) + \cdots + f(x_{2^k})}{2^k}
    + \frac{f(x_{2^k+1}) + \cdots + f(x_{2^{k+1}})}{2^k}
  \right) \\
=& \frac{f(x_1) + \cdots + f(x_{2^k}) + f(x_{2^k+1}) + \cdots + f(x_{2^{k+1}})}{2^{k+1}} \\
=& \frac{f(x_1) + \cdots + f(x_{2^{k+1}})}{2^{k+1}}.
\end{align*}
As $n$ is not a power of $2$,
then it is certainly less than some natural power of $2$, say $n < 2^m$ for some $m$.
Let
$$x_{n+1} = \cdots = x_{2^m} = \frac{x_1 + \cdots + x_n}{n} = \alpha.$$
Then by the induction hypothesis,
\begin{align*}
  f(\alpha)
  &= f\left( \frac{x_1 + \cdots + x_n + \alpha + \cdots + \alpha}{2^m} \right) \\
  &\leq \frac{f(x_1) + \cdots + f(x_n) + f(\alpha) + \cdots + f(\alpha)}{2^m} \\
  &\leq \frac{f(x_1) + \cdots + f(x_n) + (2^m - n)f(\alpha)}{2^m}, \\
  2^m f(\alpha)
  &\leq f(x_1) + \cdots + f(x_n) + (2^m - n)f(\alpha), \\
  n f(\alpha)
  &\leq f(x_1) + \cdots + f(x_n),
\end{align*}
or $f\left( \frac{1}{n} (x_1 + \cdots + x_n) \right)
\leq \frac{1}{n}(f(x_1) + \cdots f(x_n))$.
\item[(2)]
Hence,
$$f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)$$
for any rational $\lambda$ in $(0,1)$.
(Given any positive integers $p < q$,
put $n = q$,
$x_1 = \cdots = x_p = x$
and $x_{p+1} = \cdots = x_n = y$ in (1).)
\item[(3)]
Given any real $\lambda \in (0,1)$,
there is a sequence of rational numbers $\{r_n\} \subseteq (0, 1)$
such that $r_n \to \lambda$.
By (2),
$$f(r_n x + (1 - r_n) y) \leq r_n f(x) + (1 - r_n) f(y)$$
for any rational $r_n$ in $(0,1)$.
Taking limit on the both sides and using the continuity of $f$,
we have
$$f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y).$$
\end{enumerate}
$\Box$ \\

\emph{Proof (Reductio ad absurdum).}
If $f$ were not convex,
then there is a subinterval $[c,d] \subseteq (a,b)$
such that
$$\frac{f(d)-f(c)}{d-c} < \frac{f(x_0)-f(c)}{x_0-c}$$
for some $x_0 \in [c, d]$.
Let
$$g(x) = f(x)-f(c) - \frac{f(d)-f(c)}{d-c}(x - c)$$
for $x \in [c,d]$.
Therefore,
\begin{enumerate}
\item[(1)]
$g(x)$ is continuous and midpoint convex.
\item[(2)]
$g(c) = g(d) = 0$.
\item[(3)]
Let $M = \sup\{g(x) : x \in [c,d]\}$.
$\infty > M > 0$ due to the continuity of $g$ and the existence of $x_0$.
And let $\xi = \inf \{ x \in [c,d] : g(x) = M \}$.
By the continuity of $g$, $g(\xi) = M$.
$\xi \in (c,d)$ by (2).
\item[(4)]
Since $(c,d)$ is open, there is $h > 0$ such that $(\xi-h,\xi+h) \subseteq (c,d)$.
By the minimality of $\xi$ and $M$, $g(\xi-h) < g(\xi)$ and $g(\xi+h) \leq g(\xi)$.
\end{enumerate}
Therefore,
\begin{align*}
& \: g(\xi-h) + g(\xi+h) < 2 g(\xi) = 2g\left( \frac{(\xi-h) + (\xi+h)}{2} \right) \\
\Longleftrightarrow& \:
\frac{g(\xi-h) + g(\xi+h)}{2} < g\left( \frac{(\xi-h) + (\xi+h)}{2} \right),
\end{align*}
contrary to the midpoint convexity of $g$.
$\Box$ \\

\emph{Note.}
The result becomes false if ``the continuity of $f$'' is omitted. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.25.}
\addcontentsline{toc}{subsection}{Exercise 4.25.}
\emph{If $A \subseteq \mathbb{R}^k$ and $B \subseteq \mathbb{R}^k$,
define $A+B$ to be the set of all sums $\mathbf{x}+\mathbf{y}$ with
$\mathbf{x} \in A$, $\mathbf{y} \in B$.}

\begin{enumerate}
\item[(a)]
\emph{If $K$ is compact and $C$ is closed in $\mathbb{R}^k$,
prove that $K+C$ is closed.
(Hint: Take $\mathbf{z} \notin K+C$,
put $F = \mathbf{z}-C$,
the set of all $\mathbf{z}-\mathbf{y}$ with $\mathbf{y} \in C$.
Then $K$ and $F$ are disjoint.
Choose $\delta$ as in Exercise 4.21.
Show that the open ball with center $\mathbf{z}$ and radius $\delta$ does not intersect $K+C$.)}
\item[(b)]
\emph{Let $\alpha$ be an irrational real number.
Let $C_1$ be the set of all integers,
let $C_2$ be the set of all $n \alpha$ with $n \in C_1$.
Show that $C_1$ and $C_2$ are closed subsets of $\mathbb{R}^1$
whose sum $C_1 + C_2$ is not closed,
by showing that $C_1 + C_2$ is a countable dense subset of $\mathbb{R}^1$.} \\
\end{enumerate}

\emph{Proof of (a) (Hint).}
\begin{enumerate}
\item[(1)]
If $K+C = \mathbb{R}^k$, the result is established since $\mathbb{R}^k$ is closed.

\item[(2)]
If $K+C \neq \mathbb{R}^k$, we take $\mathbf{z} \notin K+C$
and put
\[
  F = \mathbf{z}-C = \{ \mathbf{z}-\mathbf{y} : \mathbf{y} \in C \}.
\]

\item[(3)]
\emph{Show that $K \cap F = \varnothing$.}
\begin{align*}
  &\mathbf{z}-\mathbf{y} \in K \cap F, \mathbf{z} \not\in K+C, \mathbf{y} \in C \\
  \Longrightarrow&
  \mathbf{z} = (\mathbf{z}-\mathbf{y}) + \mathbf{y} \in K+C,
\end{align*}
contrary to the choice of $\mathbf{z}$.

\item[(4)]
\emph{Show that $F$ is closed.}
\begin{align*}
  &\text{$\mathbf{p}$ is a limit point of $F = \mathbf{z}-C$} \\
  \Longleftrightarrow&
  \text{$\mathbf{z} - \mathbf{p}$ is a limit point of $C$} \\
  \Longrightarrow&
  \text{$\mathbf{z} - \mathbf{p} \in C$}
    &\text{($C$ is closed)} \\
  \Longrightarrow&
  \text{$\mathbf{p} \in \mathbf{z}-C = F$} \\
  \Longrightarrow&
  \text{$F$ is closed}.
\end{align*}

\item[(5)]
Note that $K$ is compact, (3) and (4).
By Exercise 4.21, there exists a $\delta > 0$ such that
$\abs{\mathbf{x} - \mathbf{p}} > \delta$ if $\mathbf{x} \in K$ and $\mathbf{p} \in F$.
Write $\mathbf{p} = \mathbf{z} - \mathbf{y}$ for $\mathbf{y} \in C$.
Hence
\[
  \abs{(\mathbf{x}+\mathbf{y}) - \mathbf{z}} > \delta
\]
if $\mathbf{x} \in K$ and $\mathbf{y} \in C$.
That is, there is an open ball $B(\mathbf{z},\delta)$ does not intersect $K+C$.
\end{enumerate}
$\Box$\\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
$C_1$ and $C_2$ are closed
since all points of $C_1$ or $C_2$ are isolated.

\item[(2)]
\emph{Show that $C_1+C_2$ is countable.}
$C_1 \times C_2$ is countable since both $C_1$ and $C_2$ are countable.
Define $f: C_1 + C_2 \to C_1 \times C_2$ by
\[
  f(m+n\alpha) = (m,n\alpha).
\]
$f$ is well-defined and injective since $\alpha$ is irrational.
So that we can identify $C_1 + C_2$ as a subset of $C_1 \times C_2$.
Besides, $C_1 + C_2$ is infinite since there is an infinite subset $C_1 \subseteq C_1 + C_2$.
Hence $C_1+C_2$ is countable subset of $C_1 \times C_2$.

\item[(3)]
  \emph{Given any real $\alpha > 0$ and an integer $N > 1$,
  show that there exist integers $h$ and $k$ with $0 < k \leq N$ such that
  \[
    \abs{ k\alpha - h } < \frac{1}{N}.
  \]}

  (Pigeonhole principle)
  Let
  $x_k = \frac{i}{N}$ ($0 \leq k \leq N$) and
  \[
    P = \left\{ x_0 = 1, x_1, \ldots, x_{N-1}, x_N = 1\right\}
  \]
  be a partition of $[0,1)$.
  Consider a sequence
  \[
    \{ (n\alpha) \} = \{ n\alpha - [n\alpha] \}
  \]
  in $[0,1)$
  where $n = 1,2,3,\ldots,N+1$.
  The pigeonhole principle implies that there exists a subinterval $[x_{k-1},x_k)$
  ($0 < k \leq N$)
  containing at least two elements of $\{ (n\alpha) \}$,
  say
  \[
    (n_1\alpha), (n_2\alpha) \in [x_{k-1},x_k)
  \]
  for some $n_1 > n_2$.
  Thus
  \begin{align*}
    &\abs{ (n_1\alpha) - (n_2\alpha) } < \frac{1}{N} \\
    \Longleftrightarrow&
    \abs{ ( n_1\alpha - [n_1\alpha] ) - ( n_2\alpha - [n_2\alpha] ) } < \frac{1}{N} \\
    \Longleftrightarrow&
    \abs{ (n_1-n_2)\alpha - ([n_1\alpha]-[n_2\alpha]) } < \frac{1}{N}.
  \end{align*}
  Write $k = n_1-n_2 \in \mathbb{Z}$ and $h = [n_1\alpha]-[n_2\alpha] \in \mathbb{Z}$.
  Here $0 < k \leq N$.

\item[(4)]
\emph{Given any irrational $\alpha > 0$.
Show that $C_1+C_2$ is dense in $\mathbb{R}$.}
  It suffices to show that $C_1+C_2$ is dense in $[0,1)$
  (since we can write $x = [x] + (x)$ where $[x]$ is always in $C_1$).
  \begin{enumerate}
  \item[(a)]
  Given any $x \in [0,1)$, it suffices to show that
  for any given $\varepsilon > 0$, there exists a $m+n\alpha \in C_1+C_2$
  (where $m, n \in \mathbb{Z}$)
  such that $m+n\alpha \in B(x;\varepsilon)$.

  \item[(b)]
  For such $\varepsilon > 0$, take $N > \max\{1, \frac{1}{\varepsilon} \}$
  and apply (3) to get that
  there exist integers $h$ and $k$ with $0 < k \leq N$ such that
  \[
    \abs{ k\alpha - h } < \frac{1}{N} < \varepsilon.
  \]
  Note that $\abs{ k \alpha - h } > 0$ since $\alpha$ is irrational.

  \item[(c)]
  For such $\abs{ k \alpha - h } > 0$, there exists an integer $a \geq 0$
  (since $x \geq 0$)
  such that
  \[
    a\abs{ k \alpha - h } \leq x < (a+1)\abs{ k \alpha - h }.
  \]
  \begin{align*}
    &a\abs{ k \alpha - h } \leq x < (a+1)\abs{ k \alpha - h } \\
    \Longleftrightarrow&
    0 \leq x - a\abs{ k \alpha - h } < \abs{ k \alpha - h } \\
    \Longleftrightarrow&
    0 \leq x - \abs{ ak \alpha - ah } < \abs{ k \alpha - h }
      &(a \geq 0) \\
    \Longrightarrow&
    0 \leq x - \abs{ ak \alpha - ah } < \varepsilon
      &((b)) \\
    \Longrightarrow&
    \abs{ ak \alpha - ah } \in B(x;\varepsilon) \\
    \Longleftrightarrow&
    \pm(ak \alpha - ah) \in B(x;\varepsilon).
  \end{align*}
  \end{enumerate}
  Hence $C_1+C_2$ is dense in $[0,1)$, or in $\mathbb{R}$.
\end{enumerate}
$\Box$\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 4.26.}
\addcontentsline{toc}{subsection}{Exercise 4.26.}
\emph{Suppose $X$, $Y$, $Z$ are metric spaces, and $Y$ is compact.
Let $f$ map $X$ into $Y$,
let $g$ be a continuous one-to-one mapping of $Y$ into $Z$,
and put $h(x) = g(f(x))$ for $x \in X$.} \\

\emph{Prove that $f$ is uniformly continuous if $h$ is uniformly continuous.
(Hint: $g^{-1}$ has compact domain $g(Y)$, and $f(x) = g^{-1}(h(x))$.)} \\

\emph{Prove also that $f$ is continuous if $h$ is continuous.} \\

\emph{Show (by modifying Example 4.21, or by finding a different example) that
the compactness of $Y$ cannot be omitted from the hypotheses,
even when $X$ and $Z$ are compact.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $f$ is uniformly continuous if $h$ is uniformly continuous.}
  \begin{enumerate}
  \item[(a)]
  Since $g$ is a continuous one-to-one mapping of a compact metric space $Y$
  onto a metric space $g(Y)$,
  $g^{-1}$ is a continuous mapping of $g(Y)$ onto $Y$ (Theorem 4.17).

  \item[(b)]
  Since $g(Y)$ is compact (Theorem 4.14) and
  $g^{-1}$ is a continuous mapping of $g(Y)$ onto $Y$,
  $g^{-1}$ is uniformly continuous on $g(Y)$ (Theorem 4.19).

  \item[(c)]
  Since $g^{-1}$ and $h$ are uniformly continuous,
  the composition $f(x) = g^{-1}(h(x))$ is uniformly continuous (Exercise 4.12).
  \end{enumerate}

\item[(2)]
\emph{Show that $f$ is continuous if $h$ is continuous.}
  By (1)(a), $g^{-1}$ is a continuous mapping of $g(Y)$ onto $Y$.
  Since $g^{-1}$ and $h$ are continuous,
  the composition $f(x) = g^{-1}(h(x))$ is continuous (Theorem 4.7).

\item[(3)]
\emph{Show that
the compactness of $Y$ cannot be omitted from the hypotheses,
even when $X$ and $Z$ are compact.}
  \begin{enumerate}
  \item[(a)]
  (Example 4.21)
  Let $X = Z = S^1$ be the unit circle
  ($|\mathbf{x}| = 1$ for all $\mathbf{x} \in \mathbb{R}^2$)
  equipped with the Euclidean metric.
  Let $Y = [0, 2\pi)$ be the half-open interval on the real line
  equipped with the Euclidean metric.
  Let $\mathbf{g}: Y \to X$ defined by
  \[
    \mathbf{g}(x) = (\cos x, \sin x),
  \]
  and $\mathbf{f}: X \to Y$ be the inverse mapping of $\mathbf{g}$.
  (See Example 4.21.)
  Hence
  \[
    \mathbf{h}(x,y) = \mathbf{g}(\mathbf{f}(x,y)) = (x,y)
  \]
  is the identity map of $X$.
  Note that $\mathbf{g}$ and $\mathbf{h}$ are uniformly continuous.
  (Since the map $x \mapsto (\cos x, \sin x)$ is a continuous mapping of
  a compact metric space $[0,2\pi]$ into a metric space $S^1$,
  this map is uniformly continuous on $[0,2\pi]$ (Theorem 4.19).
  Hence $\mathbf{g}$ is uniformly continuous on a subset of $[0,2\pi]$.)
  However, $\mathbf{f}$ is not continuous.

  \item[(b)]
  (Another example)
  Let $X = Z = [0,1]$ equipped with the Euclidean metric.
  Let $Y = \left[0,\frac{1}{2}\right) \cup [1,2]$ equipped with the Euclidean metric.
  Let $f: X \to Y$ defined by
  \begin{equation*}
  f(x) =
    \begin{cases}
      x  & \text{ if } x \in \left[0,\frac{1}{2}\right), \\
      2x & \text{ if } x \in \left[\frac{1}{2},1\right],
    \end{cases}
  \end{equation*}
  $g: Y \to X$ defined by
  \begin{equation*}
  g(x) =
    \begin{cases}
      x           & \text{ if } x \in \left[0,\frac{1}{2}\right), \\
      \frac{x}{2} & \text{ if } x \in \left[1,2\right],
    \end{cases}
  \end{equation*}
  and $h(x) = x$ on $X$. Clearly $h(x) = g(f(x))$.
  $g$ and $h$ are uniformly continuous, but $f$ is not continuous.
  \end{enumerate}
\end{enumerate}
$\Box$\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 5: Differentiation \\}
\addcontentsline{toc}{section}{Chapter 5: Differentiation}



\subsection*{Exercise 5.1.}
\addcontentsline{toc}{subsection}{Exercise 5.1.}
\emph{Let $f$ be defined for all real $x$, and suppose that
$$|f(x) - f(y)| \leq (x - y)^2$$
for all real $x$ and $y$. Prove that $f$ is a constant.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Write
\[
  \left| \frac{f(x) - f(y)}{x - y} \right| \leq |x - y|
\]
if $x \neq y$.

\item[(2)]
Given any $y \in \mathbb{R}$,
\[
  \abs{ \frac{f(x) - f(y)}{x - y} } \to 0 \:\: \text{ as } \:\: x \to y,
\]
or $|f'(y)| = 0$.

\item[(3)]
Or using $\varepsilon$-$\delta$ argument. Fix $y \in \mathbb{R}$.
Given any $\varepsilon > 0$, there exists $\delta = \varepsilon > 0$ such that
$$\left| \frac{f(x) - f(y)}{x - y} - 0 \right| \leq |x - y| < \delta = \varepsilon$$
whenever $|x - y| < \delta$. That is, $|f'(y)| = 0$.

\item[(4)]
So $f'(y) = 0$ for any $y \in \mathbb{R}$.
By Theorem 5.11 (b), $f$ is a constant.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.2.}
\addcontentsline{toc}{subsection}{Exercise 5.2.}
\emph{Suppose $f'(x)>0$ in $(a,b)$.
Prove that $f$ is strictly increasing in $(a,b)$, and let $g$ be its inverse function.
Prove that $g$ is differentiable, and that
\[
  g'(f(x)) = \frac{1}{f'(x)} \qquad (a<x<b).
\]}

\emph{Proof.}
Let $E = (a,b)$.
\begin{enumerate}
\item[(1)]
Theorem 5.10 implies that
for any $a < p < q < b$ there exists $\xi \in (p,q)$ such that
\[
  f(p)-f(q) = (p-q)f'(\xi).
\]
Since $\xi \in (p,q) \subseteq E$, by assumption $f'(\xi) > 0$.
Hence $f(p)-f(q) = (p-q)f'(\xi) < 0$ (here $p-q < 0$), or
\[
  f(p) < f(q)
\]
if $p < q$.
Therefore, $f$ is strictly increasing in $(a,b)$.

\item[(2)]
\emph{Show that $f$ is one-to-one in $E$ if $f$ is strictly increasing in $E$.}
If $f(p) = f(q)$, then it cannot be $p > q$ or $p < q$ ((1)).
So that $p = q$, or $f$ is injective.

\item[(3)]
\emph{Show that $g$ is well-defined.}
Theorem 5.2 and Theorem 4.17.

\item[(4)]
\emph{Show that $ g'(f(x)) = \frac{1}{f'(x)}$.}
Given $y \in f(E)$, say $y = f(x)$ for some $x \in E$.
Given any $s \in f(E)$ with $s \neq y$.
Here $s = f(t)$ for some $t \in E$ and $t \neq x$.
\begin{align*}
  \lim_{s \to y} \frac{g(s) - g(y)}{s - y}
  &= \lim_{f(t) \to f(x)} \frac{g(f(t)) - g(f(x))}{f(t) - f(x)} \\
  &= \lim_{t \to x} \frac{t - x}{f(t) - f(x)} \\
  &= \lim_{t \to x} \frac{1}{\frac{f(t) - f(x)}{t - x}} \\
  &=\frac{1}{f'(x)}.
    &(f' > 0)
\end{align*}
Here $s \to y$ if and only if $t \to x$ since both $f$ and $g$
are continuous and one-to-one.
Hence $g$ is differentiable and $g'(f(x)) = \frac{1}{f'(x)}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.3.}
\addcontentsline{toc}{subsection}{Exercise 5.3.}
\emph{Suppose $g$ is a real function on $\mathbb{R}^1$,
with bounded derivative (say $|g'|\leq M$).
Fix $\varepsilon > 0$, and define $f(x)=x+\varepsilon g(x)$.
Prove that $f$ is one-to-one if $\varepsilon$ is small enough.
(A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Note that
$f'(x) = 1 + \varepsilon g'(x)$ (Theorem 5.3).
Since $|g'|\leq M$,
\[
  1 - \varepsilon M \leq f'(x) \leq 1 + \varepsilon M.
\]

\item[(2)]
Pick
\[
  \varepsilon = \frac{1}{M+1} > 0.
\]
Thus,
\[
  f'(x) \geq \frac{1}{M+1} > 0.
\]
By Exercise 5.2, $f(x)$ is strictly increasing in $\mathbb{R}$ or one-to-one in $\mathbb{R}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.4.}
\addcontentsline{toc}{subsection}{Exercise 5.4.}
\emph{If
$$C_0 + \frac{C_1}{2} + \cdots + \frac{C_{n- 1}}{n} + \frac{C_n}{n + 1} = 0,$$
where $C_0, \ldots, C_n$ are real constants, prove that the equation
$$C_0 + C_1 x + \cdots + C_{n - 1} x^{n - 1} + C_n x^n = 0$$
has at least one real root between $0$ and $1$.} \\

\emph{Proof.}
Let
$$g(x) = C_0 x + \frac{C_1}{2} x^2 + \cdots + \frac{C_{n- 1}}{n} x^n + \frac{C_n}{n + 1} x^{n + 1}
\in \mathbb{R}[x].$$
Then $g(0) = g(1) = 0$, and
$g'(x) = C_0 + C_1 x + \cdots + C_{n - 1} x^{n - 1} + C_n x^n$.
By the mean value theorem (Theorem 5.10), there exists a point $\xi \in (0, 1)$ at which
$$g(1) - g(0) = g'(\xi)(1 - 0),$$
or $g'(\xi) = 0.$ That is, there exists a real root $x = \xi$ between $0$ and $1$
at which $C_0 + C_1 x + \cdots + C_{n - 1} x^{n - 1} + C_n x^n = 0$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.5.}
\addcontentsline{toc}{subsection}{Exercise 5.5.}
\emph{Suppose $f$ is defined and differentiable for every $x > 0$,
and $f'(x) \to 0$ as $x \to +\infty$.
Put $g(x) = f(x+1) - f(x)$.
Prove that $g(x) \to 0$ as $x \to +\infty$.} \\

\emph{Proof.}
Given any $x > 0$.
Since f is differentiable for every $x > 0$,
f is differentiable on $[x,x+1]$.
By Theorem 5.2 and Theorem 5.10 (the mean value theorem),
there is a point $\xi \in (x,x+1)$ at which
\[
  f(x+1) - f(x) = [(x+1) - x ]f'(\xi)
\]
or
\[
  g(x) = f'(\xi).
\]
As $x \to +\infty$, $\xi \to +\infty$.
Hence
\[
  \lim_{x \to +\infty} g(x)
  = \lim_{\xi \to +\infty} f'(\xi) = 0.
\]
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.6.}
\addcontentsline{toc}{subsection}{Exercise 5.6.}
\emph{Suppose}
\begin{enumerate}
  \item[(a)]
  \emph{$f$ is continuous for $x \geq 0$},

  \item[(b)]
  \emph{$f'(x)$ exists for $x > 0$},

  \item[(c)]
  \emph{$f(0) = 0$},

  \item[(d)]
  \emph{$f'$ is monotonically increasing.}
\end{enumerate}
\emph{Put
\[
  g(x) = \frac{f(x)}{x} \qquad (x>0)
\]
and prove that $g$ is monotonically increasing.} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  It suffices to show that $g'(x) \geq 0$ for $x>0$ (Theorem 5.11(a)),
  that is, to show that
  \[
    g'(x) = \frac{x f'(x) - f(x)}{x^2} \geq 0 \qquad (x>0),
  \]
  or
  \[
    x f'(x) - f(x) \geq 0 \qquad (x>0)
  \]
  since $x^2 > 0$ for all nonzero $x$.

  \item[(2)]
  Given $x > 0$.
  By (a)(b), we apply the mean value theorem (Theorem 5.10) on $f$ to get
  \[
    f(x) - f(0) = (x - 0)f'(\xi)
  \]
  for some $\xi \in (0,x)$.
  By (c),
  \[
    f(x) = x f'(\xi).
  \]
  By (d),
  \[
    f(x) = x f'(\xi) \leq x f'(x).
  \]
  Hence $x f'(x) - f(x) \geq 0$,
  or $g$ is monotonically increasing.
\end{enumerate}
$\Box$ \\

\emph{Note.}
$g$ is increasing strictly if $f$ is increasing strictly. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.7.}
\addcontentsline{toc}{subsection}{Exercise 5.7.}
\emph{Suppose $f'(x)$, $g'(x)$ exist, $g'(x) \neq 0$, and $f(x) = g(x) = 0$.
Prove that
\[
  \lim_{t \to x}{\frac{f(t)}{g(t)}} = \frac{f'(x)}{g'(x)}.
\]
(This holds also for complex functions.)} \\

\emph{Proof.}
\begin{align*}
  \frac{f'(t)}{g'(t)}
  &= \frac{\lim_{t \to x}{\frac{f(t)-f(x)}{t-x}}}{\lim_{t \to x}{\frac{g(t)-g(x)}{t-x}}} \\
  &= \lim_{t \to x} \frac{{\frac{f(t)-f(x)}{t-x}}}{{\frac{g(t)-g(x)}{t-x}}}
    &\text{(Both limits exist and $g' \neq 0$)} \\
  &= \lim_{t \to x} \frac{f(t)}{g(t)}.
    &\text{($f(x)=g(x)=0$)}
\end{align*}
This proof is also true for complex functions.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.8.}
\addcontentsline{toc}{subsection}{Exercise 5.8.}
\emph{Suppose $f'(x)$ is continuous on $[a,b]$ and $\varepsilon > 0$.
Prove that there exists $\delta > 0$ such that
\[
  \abs{ \frac{f(t)-f(x)}{t-x} - f'(x) } < \varepsilon
\]
whenever $0 < |t-x| < \delta$, $a \leq x \leq b$, $a \leq t \leq b$.
(This could be expressed by saying $f$ is \textbf{uniformly differentiable}
on $[a,b]$ if $f'$ is continuous on $[a,b]$.)
Does this hold for vector-valued functions too?} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Since $f'(x)$ is continuous on a compact set $[a,b]$,
  $f'(x)$ is uniformly continuous on $[a,b]$.
  So given any $\varepsilon > 0$ there exists $\delta > 0$ such that
  \[
    | f'(t) - f'(x) | < \varepsilon
  \]
  whenever $0 < |t-x| < \delta$, $a \leq x \leq b$, $a \leq t \leq b$.

  \item[(2)]
  For such $t < x$ in (1),
  by the mean value theorem (Theorem 5.10),
  there exists a point $\xi \in (t,x)$
  at which
  \[
    f'(\xi) = \frac{f(t)-f(x)}{t-x}.
  \]
  Note that $\xi$ is also satisfying $0 < |t-\xi| < |t-x| < \delta$
  and $a \leq \xi \leq b$.
  Hence by (1) we also have
  \[
    | f'(\xi) - f'(x) | < \varepsilon,
  \]
  or
  \[
    \abs{ \frac{f(t)-f(x)}{t-x} - f'(x) } < \varepsilon.
  \]

  \item[(3)]
  \emph{Suppose $\mathbf{f}'(x)$ is continuous on $[a,b]$ and $\varepsilon > 0$.
  Prove that there exists $\delta > 0$ such that
  \[
    \abs{ \frac{\mathbf{f}(t)-\mathbf{f}(x)}{t-x} - \mathbf{f}'(x) } < \varepsilon
  \]
  whenever $0 < |t-x| < \delta$, $a \leq x \leq b$, $a \leq t \leq b$.}
  \begin{enumerate}
  \item[(a)]
  Write
  \[
    \mathbf{f}(x) = (f_1(x), \ldots, f_k(x)) \in \mathbb{R}^k.
  \]
  By Remarks 5.16,
  $\mathbf{f}(x)$ is differentiable at a point $x$ if and only if
  each $f_1, \ldots, f_k$ is differentiable at $x$.
  So that
  \[
    \mathbf{f}'(x) = (f_1'(x), \ldots, f_k'(x)) \in \mathbb{R}^k.
  \]
  By Theorem 4.10,
  $\mathbf{f}'(x)$ is continuous if and only if
  each $f_1, \ldots, f_k$ is continuous.

  \item[(b)]
  Similar to (1)(2),
  Since $f_i'(x)$ is continuous on a compact set $[a,b]$ where $1 \leq i \leq k$,
  $f_i'(x)$ is uniformly continuous on $[a,b]$.
  So given any $\varepsilon > 0$ there exists $\delta_i > 0$ such that
  \[
    | f_i'(t) - f_i'(x) | < \frac{\varepsilon}{\sqrt{k}}
  \]
  whenever $0 < |t-x| < \delta_i$, $a \leq x \leq b$, $a \leq t \leq b$.
  Take $\delta = \min_{1 \leq i \leq k} \delta_i > 0$.

  \item[(c)]
  For such $t < x$ in (1),
  by the mean value theorem (Theorem 5.10),
  there exists a point $\xi_i \in (t,x)$
  at which
  \[
    f_i'(\xi_i) = \frac{f_i(t)-f_i(x)}{t-x}.
  \]
  Note that $\xi_i$ is also satisfying $0 < |t-\xi_i| < |t-x| < \delta$
  and $a \leq \xi_i \leq b$.
  Hence by (1) we also have
  \[
    | f_i'(\xi_i) - f_i'(x) | < \frac{\varepsilon}{\sqrt{k}},
  \]
  or
  \[
    \abs{ \frac{f_i(t)-f_i(x)}{t-x} - f_i'(x) } < \frac{\varepsilon}{\sqrt{k}}.
  \]

  \item[(d)]
  Hence
  \[
    \abs{ \frac{\mathbf{f}(t)-\mathbf{f}(x)}{t-x} - \mathbf{f}'(x) }
    = \left( \sum_{i=1}^{k} \abs{ \frac{f_i(t)-f_i(x)}{t-x} - f_i'(x) }^2 \right)^{\frac{1}{2}}
    < \varepsilon.
  \]
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.9.}
\addcontentsline{toc}{subsection}{Exercise 5.9.}
\emph{Let $f$ be a continuous real function on $\mathbb{R}^1$,
of which it is known that $f'(x)$ exists for all $x \neq 0$ and that $f'(x) \to 3$ as $x \to 0$.
Dose it follow that $f'(0)$ exists?} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $f'(0) = 3$.}
  It is equivalent to show that
  \[
    \lim_{x \to 0} \frac{f(x) - f(0)}{x - 0} = 3.
  \]
  Write $F(x) = f(x) - f(0)$ and $G(x) = x - 0$ on $\mathbb{R}^1$.
  So that
  \[
    \lim_{x \to 0} \frac{f(x) - f(0)}{x - 0} = \lim_{x \to 0} \frac{F(x)}{G(x)} = 0.
  \]

  \item[(2)]
  Note that
    \[
      \lim_{x \to 0} \frac{F'(x)}{G'(x)}
      = \lim_{x \to 0} \frac{f'(x)}{1}
      = 3.
    \]

  \item[(3)]
  Since $f$ is continuous on $\mathbb{R}^1$,
  $F$ is continuous on $\mathbb{R}^1$.
  Hence
    \[
      \lim_{x \to 0} F(x) = F(\lim_{x \to 0} x) = F(0) = 0.
    \]
  Also, $G$ is continuous on $\mathbb{R}^1$ implies that
    \[
      \lim_{x \to 0} G(x) = G(\lim_{x \to 0} x) = G(0) = 0.
    \]

  \item[(4)]
  Apply L'Hospital's rule (Theorem 5.13) to (2)(3), we have
  \[
    \lim_{x \to 0} \frac{F(x)}{G(x)} = 3,
  \]
  or $f'(0) = 3$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.10.}
\addcontentsline{toc}{subsection}{Exercise 5.10.}
\emph{Suppose $f$ and $g$ are complex differentiable functions on $(0,1)$,
$f(x) \to 0$, $g(x) \to 0$, $f'(x) \to A$, $g'(x) \to B$ as $x \to 0$,
where $A$ and $B$ are complex numbers, $B \neq 0$.
Prove that
\[
  \lim_{x \to 0}{\frac{f(x)}{g(x)}}= \frac{A}{B}.
\]
Compare with Example 5.18.
(Hint:
\[
  \frac{f(x)}{g(x)}
  = \left( \frac{f(x)}{x} - A \right) \frac{x}{g(x)} +  A\frac{x}{g(x)}.
\]
Apply Theorem 5.13 to
the real and imaginary parts of $\frac{f(x)}{x}$ and $\frac{g(x)}{x}$.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
  \item[(1)]
  Write
  \[
    f(x) = f_1(x) + i f_2(x)
  \]
  for $x \in (0,1)$,
  where both $f_1$ and  $f_2$ are real functions.
  By Remarks 5.16, it is clear that
  \[
    f'(x) = f_1'(x) + i f_2'(x).
  \]

  \item[(2)]
  Write
  \[
    A = A_1 + i A_2
  \]
  where both $A_1$ and $A_2$ are real numbers.
  Then as $x \to 0$, we have
  \begin{enumerate}
    \item[(a)]
    $f(x) \to 0$ if and only if $f_1(x) \to 0$ and $f_2(x) \to 0$.

    \item[(b)]
    $f'(x) \to A$ if and only if $f_1'(x) \to A_1$ and $f_2'(x) \to A_2$.
  \end{enumerate}

  Hence by L'Hospital's rule (Theorem 5.13),
  \[
    \lim_{x \to 0} \frac{f_i(x)}{x}
    = \lim_{x \to 0} \frac{f_i'(x)}{1} = A_i
  \]
  ($i = 1, 2$) or
  \begin{align*}
    \lim_{x \to 0} \frac{f(x)}{x}
    &= \lim_{x \to 0} \frac{f_1(x) + i f_2(x)}{x} \\
    &= \lim_{x \to 0} \frac{f_1(x)}{x} + i \lim_{x \to 0} \frac{f_2(x)}{x} \\
    &= A_1 + i A_2 \\
    &= A.
  \end{align*}
  Similarly,
  \[
    \lim_{x \to 0} \frac{g(x)}{x} = B.
  \]
  Note that $B \neq 0$, and thus
  \[
    \lim_{x \to 0} \frac{x}{g(x)} = \frac{1}{B}.
  \]

  \item[(3)]
  Hence
  \begin{align*}
    \lim_{x \to 0} \frac{f(x)}{g(x)}
    &= \lim_{x \to 0}
      \left[ \left( \frac{f(x)}{x} - A \right) \frac{x}{g(x)} + A\frac{x}{g(x)} \right] \\
    &= \lim_{x \to 0} \left( \frac{f(x)}{x} - A \right) \cdot \lim_{x \to 0} \frac{x}{g(x)}
      + \lim_{x \to 0} A\frac{x}{g(x)} \\
    &= 0 \cdot \frac{1}{B} + \frac{A}{B} \\
    &= \frac{A}{B}.
  \end{align*}

  \item[(4)]
  \emph{Compare with Example 5.18.}
  Define $f(x) = x$ and $g(x) = x + x^2 \exp(\frac{i}{x^2})$ as in Example 5.18.
  Note that $f(x) \to 0$, $g(x) \to 0$, $f'(x) \to 1$ and $g'(x) \to \infty$ as $x \to 0$.
  By Example 5.18
  \[
    \lim_{x \to 0} \frac{f(x)}{g(x)} = 1
    \neq 0 = \frac{1}{\infty} = \lim_{x \to 0} \frac{A}{B}.
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.11.}
\addcontentsline{toc}{subsection}{Exercise 5.11.}
\emph{Suppose $f$ is defined in a neighborhood of $x$, and suppose $f''(x)$ exists.
Show that
\[
  \lim_{h \to 0}{\frac{f(x+h) + f(x-h) - 2f(x)}{h^2}} = f''(x).
\]
Show by an example that the limit may exist even if $f''(x)$ dose not.
(Hint: Use Theorem 5.13.)} \\

\emph{Proof (Theorem 5.13).}
\begin{enumerate}
  \item[(1)]
  Write $F(h) = f(x+h) + f(x-h) - 2f(x)$ and $G(h) = h^2$.
  It is equivalent to show that
  \[
    \lim_{h \to 0}\frac{F(h)}{G(h)} = f''(x).
  \]
  We might apply Theorem 5.13 (L'Hospital rule) to get it.

  \item[(2)]
  \emph{Show that $\lim_{h \to 0} F(h) = 0$ and $\lim_{h \to 0} G(h) = 0$.}
  It is clear that $\lim_{h \to 0} G(h) = \lim_{h \to 0} h^2 = 0$
  since $x \mapsto x^2$ is continuous on $\mathbb{R}^1$.
  Besides, since $f$ is continuous at $x$ (by applying Theorem 5.2 twice),
  \[
    \lim_{h \to 0} F(h)
    = f(x) + f(x) - 2f(x)
    = 0.
  \]

  \item[(3)]
  \emph{Show that
  \[
    \lim_{h \to 0} \frac{F'(h)}{G'(h)} = \lim_{h \to 0} \frac{f'(x+h) - f'(x-h)}{2h}
  \]
  is well-defined.}
  Since $f''(x)$ exists in a neighborhood $B(x;r)$ of $x$ (where $r > 0$),
  $f'(x)$ exists and is continuous in $B(x;r)$ (Theorem 5.2).
  As $0 < |h| < \frac{r}{2}$,
  \[
    x+h \in B\left(x+h;\frac{r}{2}\right) \subseteq B(x;r)
  \]
  and
  \[
    x-h \in B\left(x-h;\frac{r}{2}\right) \subseteq B(x;r).
  \]
  So $f'(x+h)$ and $f'(x-h)$ exist in $B(x;r)$ as $0 < |h| < \frac{r}{2}$.
  Hence
  \[
    \lim_{h \to 0} \frac{F'(h)}{G'(h)}
    = \lim_{h \to 0} \frac{f'(x+h) - f'(x-h)}{2h}
  \]
  is well-defined (Theorem 5.3 and Theorem 5.5 (the chain rule)).

  \item[(4)]
  \emph{Show that
  \[
    \lim_{h \to 0} \frac{f'(x+h) - f'(x-h)}{2h} = f''(x).
  \]}

  Since $f''(x)$ exists, by definition
  \[
    \lim_{h \to 0} \frac{f'(x+h) - f'(x)}{h} = f''(x)
  \]
  and
  \[
    \lim_{h \to 0} \frac{f'(x-h) - f'(x)}{-h} = f''(x).
  \]
  Sum up two expressions to get
  \[
    2 f''(x) = \lim_{h \to 0} \frac{f'(x-h) - f'(x-h)}{h}.
  \]

  \item[(5)]
  By (2)(3)(4) and Theorem 5.13 (L'Hospital rule),
  the result is established.

  \item[(6)]
  \emph{Given $f(x) = x|x|$ on $\mathbb{R}^1$.
  Show that
  \[
    \lim_{h \to 0}{\frac{f(h) + f(-h) - 2f(0)}{h^2}} = 0
  \]
  but $f''(x)$ does not exist at $x = 0$.}
  Clearly,
  \begin{align*}
    \lim_{h \to 0}{\frac{f(h) + f(-h) - 2f(0)}{h^2}}
    &= \lim_{h \to 0}{\frac{h|h| + (-h)|-h| - 2 \cdot 0}{h^2}} \\
    &= \lim_{h \to 0}{\frac{h|h| - h|h| - 0}{h^2}} \\
    &= \lim_{h \to 0}{0} \\
    &= 0.
  \end{align*}
  But $f''(x)$ does not exist by Exercise 5.12.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.12.}
\addcontentsline{toc}{subsection}{Exercise 5.12.}
\emph{If $f(x) = |x|^3$, compute $f'(x)$, $f''(x)$ for all real $x$,
and show that $f^{(3)} (0)$ does not exist.} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Write
  \begin{equation*}
    f(x) =
    \begin{cases}
      x^3 & (x \geq 0), \\
      -x^3 & (x \leq 0).
    \end{cases}
  \end{equation*}

  \item[(2)]
  \emph{Show that $f'(x) = 3x|x|$.}
  It is trivial that
  \begin{equation*}
    f'(x) =
    \begin{cases}
      3x^2 & (x > 0), \\
      -3x^2 & (x < 0).
    \end{cases}
  \end{equation*}
  Note that
  \[
    \lim_{x \to 0} f'(x) = 0.
  \]
  Apply the same argument in Exercise 5.9, we have
  \[
    f'(0) = 0.
  \]
  Hence $f'$ exists and $f'(x) = 3x|x|$ for any $x \in \mathbb{R}$.

  \item[(3)]
  \emph{Show that $f''(x) = 6|x|$.}
  Similar to (2).
  \begin{equation*}
    f''(x) =
    \begin{cases}
      6x & (x > 0), \\
      -6x & (x < 0).
    \end{cases}
  \end{equation*}
  Note that
  \[
    \lim_{x \to 0} f''(x) = 0.
  \]
  Apply the same argument in Exercise 5.9, we have
  \[
    f''(0) = 0.
  \]
  Hence $f''$ exists and $f''(x) = 6|x|$ for any $x \in \mathbb{R}$.

  \item[(4)]
  \emph{Show that $f^{(3)} (0)$ does not exist.}
  \begin{equation*}
    f'''(x) =
    \begin{cases}
      6 & (x > 0), \\
      -6 & (x < 0).
    \end{cases}
  \end{equation*}

  There are some proofs for showing that $f^{(3)} (0)$ does not exist.
  \begin{enumerate}
  \item[(a)]
  Since
  \[
    \lim_{t \to 0+} \frac{f''(t) - f''(0)}{t - 0}
    = \lim_{t \to 0+} \frac{6t}{t} = 6
  \]
  and
  \[
    \lim_{t \to 0-} \frac{f''(t) - f''(0)}{t - 0}
    = \lim_{t \to 0-} \frac{-6t}{t} = -6,
  \]
  $f^{(3)} (0)$ does not exist.

  \item[(b)]
  (Reductio ad absurdum)
  If $f$ were differentiable on $\mathbb{R}^1$,
  then
  \[
    \lim_{t \to 0+} f'''(t) = 6
  \]
  and
  \[
    \lim_{t \to 0-} f'''(t) = -6,
  \]
  or $f'''$ has a simple discontinuity at $x = 0$,
  contrary to Corollary to Theorem 5.12.
  \end{enumerate}
\end{enumerate}
$\Box$ \\

\emph{Note.}
Given $k > 0$.
We can construct one real function $f$ on $\mathbb{R}^1$, say
  \begin{equation*}
    f(x) =
    \begin{cases}
      |x|^k      & (\text{$k$ is odd}), \\
      x|x|^{k-1} & (\text{$k > 0$ is even}),
    \end{cases}
  \end{equation*}
such that
all $f^{(0)}(0) = \cdots = f^{(k-1)}(0) = 0$ exist but $f^{(k)}(0)$ does not exist.
\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.13.}
\addcontentsline{toc}{subsection}{Exercise 5.13.}
\emph{Suppose $a$ and $c$ are real numbers, $c > 0$,
and $f$ is defined on $[-1,1]$ by
  \begin{equation*}
  f(x) =
    \begin{cases}
      x^a \sin{(x^{-c})} & (\text{if } x \neq 0), \\
      0                  & (\text{if } x = 0).
    \end{cases}
  \end{equation*}
Prove the following statements:}
\begin{enumerate}
  \item[(a)]
  \emph{$f$ is continuous if and only if $a > 0$.}

  \item[(b)]
  \emph{$f'(0)$ exists if and only if $a > 1$.}

  \item[(c)]
  \emph{$f'$ is bounded if and only if $a \geq 1+c$.}

  \item[(d)]
  \emph{$f'$ is continuous if and only if $a > 1+c$.}

  \item[(e)]
  \emph{$f''(0)$ exists if and only if $a > 2 + c$.}

  \item[(f)]
  \emph{$f''$ is bounded if and only if $a \geq 2+2c$.}

  \item[(g)]
  \emph{$f''$ is continuous if and only if $a > 2+2c$.} \\
\end{enumerate}

Note that $f$ is not well-defined as a real function if $x < 0$.
Hence we modify the definition of $f$ for the case $x < 0$:
  \begin{equation*}
  f(x) =
    \begin{cases}
      |x|^a \sin{(|x|^{-c})} & (\text{if } x \neq 0), \\
      0                      & (\text{if } x = 0). \\
    \end{cases}
  \end{equation*}

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  Since $|x|^a \sin{(|x|^{-c})}$ is continuous on $\mathbb{R}^1 - \{0\}$,
  $f$ is continuous if and only if
  \[
    \lim_{x \to 0} |x|^a \sin{(|x|^{-c})} = 0.
  \]

  \item[(2)]
  \emph{Given $a > 0$.
  Show that}
  \[
    \lim_{x \to 0} |x|^a \sin{(|x|^{-c})} = 0.
  \]
  Since $|x|^a \to 0$ as $x \to 0$ and $\abs{ \sin{(|x|^{-c})} }$ is bounded by $1$,
  the limit $\lim |x|^a \sin{(|x|^{-c})}$ exists and is equal to $0$.

  \item[(3)]
  \emph{Given $a = 0$.
  Show that
  \[
    \lim_{x \to 0} |x|^a \sin{(|x|^{-c})} = \lim_{x \to 0} \sin{(|x|^{-c})}
  \]
  does not exist although
  $|x|^a \sin{(|x|^{-c})} = \sin{(|x|^{-c})}$ is bounded on $[-1,1] - \{0\}$.}
  \begin{enumerate}
    \item[(a)]
    Take $x_n = \left( \frac{\pi}{2} + 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ x_n \}$ converges to $0$, and
    \[
      \lim_{n \to \infty} f(x_n)
      = \lim_{n \to \infty} \sin{(|x_n|^{-c})}
      = \lim_{n \to \infty} 1
      = 1.
    \]

    \item[(b)]
    Similarly, take $y_n = \left( 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ y_n \}$ converges to $0$, and
    \[
      \lim_{n \to \infty} f(y_n) = 0.
    \]

    \item[(c)]
    By (a)(b), $\lim_{x \to 0} |x|^a \sin{(|x|^{-c})}$ does not exist (Theorem 4.2).

    \item[(d)]
    Clearly, $|\sin{(|x|^{-c})}| \leq 1$ as $\sin{(|x|^{-c})}$ is well-defined.
  \end{enumerate}

  \item[(4)]
  \emph{Given $a < 0$.
  Show that
  \[
    \lim_{x \to 0} |x|^a \sin{(|x|^{-c})}
  \]
  does not exist.}
  Similar to (3), we take the same $\{x_n\}$ and $\{y_n\}$ as (3)
  to get the similar result:
  \begin{align*}
    \lim_{n \to \infty} f(x_n) &= \infty, \\
    \lim_{n \to \infty} f(y_n) &= 0.
  \end{align*}
  By Theorem 4.2, $\lim_{x \to 0} |x|^a \sin{(|x|^{-c})}$ does not exist.

  \item[(5)]
  By (2)(3)(4), $f$ is continuous if and only if $a > 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  By definition,
  \[
    f'(0)
    = \lim_{x \to 0} \frac{f(x) - f(0)}{x - 0}
    = \lim_{x \to 0} \mathrm{sgn}(x) |x|^{a-1} \sin{(|x|^{-c})}.
  \]
  Here $\mathrm{sgn}(x)$ is the sign function defined by
  \begin{equation*}
  \mathrm{sgn}(x) =
    \begin{cases}
      1 & (x > 0), \\
      0 & (x = 0), \\
      -1 & (x < 0).
    \end{cases}
  \end{equation*}

  \item[(2)]
  Similar to (2)(3)(4) in the proof of (a),
  $f'(0) = 0$ exists if and only if $a - 1 > 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
  \item[(1)]
  Write $E = [-1,1] - \{0\}$.
  $f'$ is bounded if and only if $f'(0)$ exists and $f'$ is bounded on $E$.

  \item[(2)]
  Given any $x \in E$,
  \begin{align*}
    f'(x)
    &= \mathrm{sgn}(x)\left( a|x|^{a-1} \sin(|x|^{-c}) + |x|^a \cos(|x|^{-c})(-c)|x|^{-c-1} \right) \\
    &= \mathrm{sgn}(x)|x|^{a-c-1} \left( a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c}) \right).
  \end{align*}

  \item[(3)]
  \emph{Given $a-c-1 \geq 0$.
  Show that $f'$ is bounded on $E$.}
  Since $\mathrm{sgn}(x)$ is bounded by $1$ on $E$,
  $|x|^{a-c-1}$ is bounded by $1$ on $E$ and
  $a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c})$ is bounded by $|a|+|c|$ on $E$,
  $f'$ is bounded on $E$.

  \item[(4)]
  \emph{Given $a-c-1 < 0$.
  Show that $f'$ is unbounded on $E$.}
  Take $x_n = \left( 2n\pi \right)^{-\frac{1}{c}} \neq 0$
  for $n = 1, 2, 3, \ldots$.
  The sequence $\{ x_n \}$ converges to $0$, and
  \[
    \lim_{n \to \infty} f'(x_n)
    = \lim_{n \to \infty} -c (2n\pi)^{-\frac{a-c-1}{c}}
    = -\infty.
  \]

  \item[(5)]
  By (b), $f'(0)$ exists if and only if $a > 1$.
  By (3)(4), $f'$ is bounded on $E$ if and only if $a-c-1 \geq 0$.
  Since $c > 0$, $f'$ is bounded on $[-1,1]$ if and only if $a-c-1 \geq 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
Similar to the proof of (a).
\begin{enumerate}
  \item[(1)]
  Write $E = [-1,1] - \{0\}$.
  By (b)(c),
  \begin{equation*}
  f'(x) =
    \begin{cases}
      0
        & \text{ if } x = 0, \\
      \mathrm{sgn}(x)|x|^{a-c-1} \left( a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c}) \right)
        & \text{ if } x \in E.
    \end{cases}
  \end{equation*}
  Clearly, $f'$ is continuous on $E$.
  Hence, $f'$ is continuous if and only if $\lim_{x \to 0} f'(x) = f'(0) = 0$.

  \item[(2)]
  \emph{Given $a-c-1 > 0$.
  Show that $\lim_{x \to 0} f'(x) = 0$.}
  Since $|x|^{a-c-1} \to 0$ as $x \to 0$,
  $\mathrm{sgn}(x)$ is bounded by $1$ on $E$, and
  $a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c})$ is bounded by $|a|+|c|$ on $E$,
  \[
    \mathrm{sgn}(x) |x|^{a-c-1} \left( a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c}) \right) \to 0
  \]
  as $x \to 0$.
  The result is established.

  \item[(3)]
  \emph{Given $a-c-1 = 0$.
  Show that $\lim_{x \to 0} f'(x)$ does not exist.}
  \begin{enumerate}
    \item[(a)]
    Take $x_n = \left( \frac{\pi}{2} + 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ x_n \}$ converges to $0$, and
    \begin{align*}
      \lim_{n \to \infty} f'(x_n)
      &= \lim_{n \to \infty} \mathrm{sgn}(x_n)
        \left( a|x_n|^{c}\sin(|x_n|^{-c}) - c\cos(|x_n|^{-c}) \right) \\
      &= \lim_{n \to \infty} \frac{a}{\frac{\pi}{2} + 2n\pi} \\
      &= 0.
    \end{align*}

    \item[(b)]
    Similarly, take $y_n = \left( 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ y_n \}$ converges to $0$, and
    \begin{align*}
      \lim_{n \to \infty} f'(y_n)
      &= \lim_{n \to \infty} \mathrm{sgn}(y_n)
        \left( a|y_n|^{c}\sin(|y_n|^{-c}) - c\cos(|y_n|^{-c}) \right) \\
      &= \lim_{n \to \infty} -c \\
      &= -c \neq 0.
    \end{align*}

    \item[(c)]
    By (a)(b), $\lim_{x \to 0} f'(x)$
    does not exist (Theorem 4.2).
  \end{enumerate}

  \item[(4)]
  \emph{Given $a-c-1 < 0$.
  Show that $\lim_{x \to 0} f'(x)$ does not exist.}
  It is the same as (4) in the proof of (c).

  \item[(5)]
  By (2)(3)(4),
  $f'$ is continuous if and only if
  $\lim_{x \to 0} f'(x) = 0$ if and only if
  $a-c-1 > 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
Similar to the proof of (b).
\begin{enumerate}
  \item[(1)]
  Write $E = [-1,1] - \{0\}$.
  By the proof of (d),
  \begin{equation*}
  f'(x) =
    \begin{cases}
      0
        & \text{ if } x = 0, \\
      \mathrm{sgn}(x) |x|^{a-c-1} \left( a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c}) \right)
        & \text{ if } x \in E.
    \end{cases}
  \end{equation*}
  By definition
  \begin{align*}
    f''(0)
    &= \lim_{x \to 0} \frac{f'(x) - f'(0)}{x - 0} \\
    &= \lim_{x \to 0} |x|^{a-c-2} \left( a|x|^{c}\sin(|x|^{-c}) - c\cos(|x|^{-c}) \right).
  \end{align*}
  (Here $\mathrm{sgn}(x)^2 = 1$ if $x \neq 0$.)

  \item[(2)]
  Similar to (2)(3)(4) in the proof of (d),
  $f''(0) = 0$ exists if and only if $(a-c-1) - 1 = a-c-2 > 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
Similar to the proof of (c).
\begin{enumerate}
  \item[(1)]
  Write $E = [-1,1] - \{0\}$.
  $f''$ is bounded if and only if $f''(0)$ exists and $f''$ is bounded on $E$.

  \item[(2)]
  Given any $x \in E$,
  \begin{align*}
    f''(x)
    =& |x|^{a-2c-2} \\
      &\cdot \left[
        ( a(a-1)|x|^{2c} - c^2 ) \sin(|x|^{-c})
        - c(2a-c-1)|x|^c\cos(|x|^{-c})
      \right].
  \end{align*}

  \item[(3)]
  \emph{Given $a-2c-2 \geq 0$.
  Show that $f''$ is bounded on $E$.}
  Since $|x|^{a-2c-2}$ is bounded by $1$ on $E$ and
  \begin{align*}
    &\abs{( a(a-1)|x|^{2c} - c^2 ) \sin(|x|^{-c}) - c(2a-c-1)|x|^c\cos(|x|^{-c})} \\
    \leq&
    |a(a-1)| + |c^2| + |c(2a-c-1)|
  \end{align*}
  is bounded on $E$, $f''$ is bounded on $E$.

  \item[(4)]
  \emph{Given $a-2c-2 < 0$.
  Show that $f''$ is unbounded on $E$.}
  Take
  $x_n = \left( \frac{\pi}{2} + 2n\pi \right)^{-\frac{1}{c}} \neq 0$
  for $n = 1, 2, 3, \ldots$.
  The sequence $\{ x_n \}$ converges to $0$, and
  \begin{align*}
    &\lim_{n \to \infty} f''(x_n) \\
    =& \lim_{n \to \infty}
      \underbrace{\left( a(a-1)
        \left( \frac{\pi}{2} + 2n\pi \right)^{-2} - c^2 \right)}_{\to -c^2 \neq 0}
      \underbrace{\left( \frac{\pi}{2} + 2n\pi \right)^{-\frac{a-2c-2}{c}}}_{\to \infty} \\
    =& - \infty.
  \end{align*}

  \item[(5)]
  By (e), $f''(0)$ exists if and only if $a - c - 2 > 0$.
  By (3)(4), $f''$ is bounded on $E$ if and only if $a - 2c - 2 \geq 0$.
  Since $c > 0$, $f''$ is bounded on $[-1,1]$ if and only if $a-2c-2 \geq 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (g).}
Similar to the proof of (a) or (d).
\begin{enumerate}
  \item[(1)]
  Write $E = [-1,1] - \{0\}$.
  By (e)(f),
  \begin{equation*}
  f''(x) =
    \begin{cases}
      0
        & \text{ if } x = 0, \\
      |x|^{a-2c-2} \left[
        ( a(a-1)|x|^{2c} - c^2 ) \sin(|x|^{-c}) - c(2a-c-1)|x|^c\cos(|x|^{-c})
      \right].
        & \text{ if } x \in E. \\
    \end{cases}
  \end{equation*}
  Clearly, $f''$ is continuous on $E$.
  Hence, $f''$ is continuous if and only if $\lim_{x \to 0} f''(x) = f''(0) = 0$.

  \item[(2)]
  \emph{Given $a-2c-2 > 0$.
  Show that $\lim_{x \to 0} f''(x) = 0$.}
  Since $|x|^{a-2c-2} \to 0$ as $x \to 0$ and
  \[
    ( a(a-1)|x|^{2c} - c^2 ) \sin(|x|^{-c}) - c(2a-c-1)|x|^c\cos(|x|^{-c})
  \]
  is bounded by $|a(a-1)| + |c^2| + |c(2a-c-1)|$ on $E$,
  \begin{align*}
    &|x|^{a-2c-2} \\
    &\cdot \left[
      ( a(a-1)|x|^{2c} - c^2 ) \sin(|x|^{-c}) - c(2a-c-1)|x|^c\cos(|x|^{-c})
    \right] \to 0
  \end{align*}
  as $x \to 0$.
  The result is established.

  \item[(3)]
  \emph{Given $a-2c-2 = 0$.
  Show that $\lim_{x \to 0} f''(x)$ does not exist.}
  \begin{enumerate}
    \item[(a)]
    Take $x_n = \left( \frac{\pi}{2} + 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ x_n \}$ converges to $0$, and
    \begin{align*}
      &\lim_{n \to \infty} f''(x_n) \\
      =& \lim_{n \to \infty}
        ( a(a-1)|x_n|^{2c} - c^2 ) \sin(|x_n|^{-c}) - c(2a-c-1)|x_n|^c\cos(|x_n|^{-c}) \\
      =& \lim_{n \to \infty} \frac{a(a-1)}{\left( \frac{\pi}{2} + 2n\pi \right)^2} - c^2 \\
      =& -c^2
    \end{align*}

    \item[(b)]
    Similarly, take $y_n = \left( \frac{3\pi}{2} + 2n\pi \right)^{-\frac{1}{c}} \neq 0$
    for $n = 1, 2, 3, \ldots$.
    The sequence $\{ y_n \}$ converges to $0$, and
    \begin{align*}
      &\lim_{n \to \infty} f''(y_n) \\
      =& \lim_{n \to \infty}
        ( a(a-1)|y_n|^{2c} - c^2 ) \sin(|y_n|^{-c}) - c(2a-c-1)|y_n|^c\cos(|y_n|^{-c}) \\
      =& \lim_{n \to \infty} -\frac{a(a-1)}{\left( \frac{3\pi}{2} + 2n\pi \right)^2} + c^2 \\
      =& c^2.
    \end{align*}

    \item[(c)]
    By (a)(b), $\lim_{x \to 0} f''(x)$
    does not exist (Theorem 4.2).
  \end{enumerate}

  \item[(4)]
  \emph{Given $a-2c-2 < 0$.
  Show that $\lim_{x \to 0} f''(x)$ does not exist.}
  It is the same as (4) in the proof of (f).

  \item[(5)]
  By (2)(3)(4),
  $f''$ is continuous if and only if
  $\lim_{x \to 0} f''(x) = 0$ if and only if
  $a-2c-2 > 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.14.}
\addcontentsline{toc}{subsection}{Exercise 5.14.}
\emph{Let $f$ be a differentiable real function defined in $(a,b)$.
Prove that $f$ is convex if and only if $f'$ is monotonically increasing.
Assume next $f''(x)$ exists for every $x \in (a,b)$,
and prove that $f$ is convex if and only if $f''(x) \geq 0$ for all $x \in (a,b)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $f'$ is monotonically increasing if $f$ is convex.}
  \begin{enumerate}
  \item[(a)]
  Since $f$ is convex, by definition (Exercise 4.23)
  \[
    f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y)
  \]
  whenever $a < x < b$, $a < y < b$, $0 < \lambda < 1$.

  \item[(b)]
  As $x \neq y$, we have
  \begin{align*}
    f(y) - f(x)
    &\geq \frac{f(x + \lambda(y-x)) - f(x)}{\lambda} \\
    &= \frac{f(x + \lambda(y-x)) - f(x)}{\lambda(y-x)} \cdot (y-x)
  \end{align*}
  and let $\lambda \to 0$ to get
  \[
    f(y) - f(x) \geq f'(x)(y - x)
  \]
  (since $f'(x)$ exists).
  Similarly, we have
  \[
    f(x) - f(y) \geq f'(y)(x - y).
  \]

  \item[(c)]
  Given any $y > x$, we have
  \[
    f'(y)(y - x) \geq f(y) - f(x) \geq f'(x)(y - x).
  \]
  Hence $f'(y) \geq f'(x)$ whenever $y > x$,
  or $f'$ is monotonically increasing.
  \end{enumerate}

\item[(2)]
  \emph{Show that $f$ is convex if $f'$ is monotonically increasing.}
  Given any $y > x$ and any $0 < \lambda < 1$.
  \begin{enumerate}
  \item[(a)]
  By Theorem 5.10 (the mean value theorem), there is a point $x < \xi < y$ such that
  \[
    f(y) - f(x) = f'(\xi)(y - x).
  \]
  Since $f'$ is monotonically increasing,
  \[
    f'(y)(y - x) \geq f(y) - f(x) \geq f'(x)(y - x).
  \]

  \item[(b)]
  Write $z = \lambda x + (1-\lambda)y$.
  Hence
  \begin{align*}
    f(y)-f(z) &\geq f'(z)(y-z), \\
    f(z)-f(x) &\leq f'(z)(z-x),
  \end{align*}
  or
  \begin{align*}
    f(y) &\geq f(z) + f'(z)(y-z), \\
    f(x) &\geq f(z) + f'(z)(x-z),
  \end{align*}
  or
  \begin{align*}
    \lambda f(x) + (1-\lambda)f(y)
    \geq&
    \lambda [f(z) + f'(z)(x-z)] \\
      &+ (1-\lambda)[f(z) + f'(z)(y-z)] \\
    =& f(z) \\
    =& f(\lambda x + (1-\lambda)y).
  \end{align*}
  Hence $f$ is convex.
  \end{enumerate}

\item[(3)]
  \emph{Show that $f''(x) \geq 0$ if $f$ is convex and $f''$ exists.}
  By (1), $f'$ is monotonically increasing since $f$ is convex.
  Given any $x \neq y$, we have
  \[
    \frac{f'(y)-f'(x)}{y - x} \geq 0.
  \]
  Let $y \to x$, we have $f''(x) \geq 0$ if $f''$ exists.

\item[(4)]
  \emph{Show that $f$ is convex if $f''$ exists and $f''(x) \geq 0$.}
  By Theorem 5.11(a), $f'$ is monotonically increasing.
  By (2), $f$ is convex.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.15 (Landau-Kolmogorov inequality on the half-line).}
\addcontentsline{toc}{subsection}{Exercise 5.15 (Landau-Kolmogorov inequality on the half-line).}
\emph{Suppose $a \in \mathbb{R}^1$,
$f$ is a twice-differentiable real function on $(a,\infty)$,
and $M_0$, $M_1$, $M_2$ are the least upper bounds of
$|f(x)|$, $|f'(x)|$, $|f''(x)|$, respectively, on $(a,\infty)$.
Prove that
\[
  M_1^2 \leq 4 M_0 M_2.
\]
(Hint: If $h>0$, Taylor's theorem shows that
\[
  f'(x) = \frac{1}{2h} [ f(x+2h) - f(x) ] - h f''(\xi)
\]
for some $\xi \in (x,x+2h)$.
Hence
\[
  |f'(x)| \leq h M_2 + \frac{M_0}{h}.)
\]}

\emph{To show that $M_1^2=4 M_0 M_2$ can actually happen, take $a=-1$, define
\begin{equation*}
  f(x) =
    \begin{cases}
      2x^2 - 1            & (-1 < x < 0), \\
      \frac{x^2-1}{x^2+1} & (0 \leq x < \infty),
    \end{cases}
\end{equation*}
and show that $M_0 = 1$, $M_1 = 4$, $M_2 = 4$.}
\emph{Does $M_1 ^2 \leq 4 M_0 M_2$ hold for vector-valued functions too?} \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
Write
\[
  M_1 \leq 2 M_0^{\frac{1}{2}} M_2^{\frac{1}{2}}.
\]
$2$ is called the Landau-Kolmogorov constant,
which is the best possible by the above example.

\item[(2)]
In general,
suppose $a \in \mathbb{R}^1$,
$f$ is a $n$th differentiable real function on $(a,\infty)$,
and $M_0$, $M_k$, $M_n$ are the least upper bounds of
$\abs{ f(x) }$, $\abs{ f^{(k)}(x) }$, $\abs{ f^{(n)}(x) }$, respectively, on $(a,\infty)$
where $1 \leq k < n$.
Then
\[
  M_k \leq C(n,k) M_0^{1 - \frac{k}{n}} M_n^{\frac{k}{n}}.
\]
\end{enumerate}



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Consider some trivial cases.
  \begin{enumerate}
  \item[(a)]
  If $M_0 = 0$, then $f(x) = 0$ on $(a,+\infty)$.
  So that $f'(x) = f''(x) = 0$ on $(a,+\infty)$, or $M_1 = M_2 = 0$.
  The inequality holds.

  \item[(b)]
  If $M_2 = 0$, then $f''(x) = 0$ on $(a,+\infty)$.
  So that $f'(x) = \alpha$ for some constant $\alpha \in \mathbb{R}^1$ (Theorem 5.11(b)),
  and $f(x) = \alpha x + \beta$ for some constant $\beta \in \mathbb{R}^1$
  (by applying Theorem 5.11(b) to $x \mapsto f(x) - \alpha x$).
  Hence $M_1 = |\alpha|$ and
  \begin{equation*}
  M_0 =
    \begin{cases}
      +\infty & (\alpha \neq 0), \\
      |\beta| & (\alpha = 0).
    \end{cases}
  \end{equation*}
  In any case, the inequality holds.

  \item[(c)]
  If $M_0 = +\infty$ and $M_2 \neq 0$, there is nothing to do.

  \item[(d)]
  If $M_2 = +\infty$ and $M_0 \neq 0$, there is nothing to do.
  \end{enumerate}

\item[(2)]
By (1), we suppose that $0 < M_0 < +\infty$ and $0 < M_2 < +\infty$.
Given $x \in (a,+\infty)$ and $h>0$.
By Taylor's theorem (Theorem 5.15):
\[
  f(x+2h) = f(x) + 2h f'(x) + 2 h^2 f''(\xi)
\]
for some $\xi \in (x,x+2h) \subseteq (a,+\infty)$.
Thus
\begin{align*}
  2h|f'(x)|
  &\leq |f(x+2h)| + |f(x)| + 2 h^2 |f''(\xi)| \\
  &\leq 2 M_0 + 2 h^2 M_2, \\
  |f'(x)|
  &\leq \frac{M_0}{h} + h M_2
\end{align*}
holds for all $h > 0$.
In particular, take
\[
  h = \sqrt{\frac{M_0}{M_2}}
\]
to get
\[
  |f'(x)| \leq 2 \sqrt{M_0 M_2}.
\]
Thus $2 \sqrt{M_0 M_2}$ is an upper bound of $|f'(x)|$ for all $x \in (a,+\infty)$.
Hence
\[
  M_1 \leq 2 \sqrt{M_0 M_2}
\]
or
\[
  M_1^2 \leq 4 M_0 M_2.
\]

\item[(3)]
\emph{Define
\begin{equation*}
  f(x) =
    \begin{cases}
      2x^2 - 1            & (-1 < x < 0), \\
      \frac{x^2-1}{x^2+1} & (0 \leq x < \infty).
    \end{cases}
\end{equation*}
Show that $M_0 = 1$, $M_1 = 4$, $M_2 = 4$.}
Similar to Exercise 5.12,
\begin{equation*}
  f'(x) =
    \begin{cases}
      4x                   & (-1 < x \leq 0), \\
      \frac{4x}{(x^2+1)^2} & (0 \leq x < \infty).
    \end{cases}
\end{equation*}
(Here $\lim_{x \to 0+} f'(x) = 0$ and $\lim_{x \to 0-} f'(x) = 0$.
So $f'(0) = 0$ by Exercise 5.9.)
Also,
\begin{equation*}
  f''(x) =
    \begin{cases}
      4                          & (-1 < x \leq 0), \\
      \frac{-12x^2+4}{(x^2+1)^3} & (0 \leq x < \infty).
    \end{cases}
\end{equation*}
(Here $\lim_{x \to 0+} f''(x) = 4$ and $\lim_{x \to 0-} f''(x) = 4$.
So $f''(0) = 4$ by Exercise 5.9.)
Hence, $M_0 = 1$, $M_1 = 4$, $M_2 = 4$.

\item[(4)]
\emph{Given
\[
  \mathbf{f}(x) = (f_1(x), \ldots, f_k(x))
\]
be a twice-differentiable vector-valued function from $(a,\infty)$ to $\mathbb{R}^k$.
and $M_0$, $M_1$, $M_2$ are the least upper bounds of
$|\mathbf{f}(x)|$, $|\mathbf{f}'(x)|$, $|\mathbf{f}''(x)|$, respectively, on $(a,\infty)$.
Show that
\[
  M_1^2 \leq 4 M_0 M_2.
\]}

Similar to (1), we suppose that $0 < M_0 < +\infty$ and $0 < M_2 < +\infty$.
Given any $\mathbf{v} = (v_1, \ldots, v_k) \in \mathbb{R}^k$,
$\mathbf{v} \cdot \mathbf{f}$ is a twice-differentiable real function on $(a,\infty)$.
Similar to (2),
Given $x \in (a,+\infty)$ and $h>0$.
By Taylor's theorem (Theorem 5.15):
\[
  (\mathbf{v} \cdot \mathbf{f})(x+2h)
  = (\mathbf{v} \cdot \mathbf{f})(x)
    + 2h (\mathbf{v} \cdot \mathbf{f})'(x)
    + 2 h^2 (\mathbf{v} \cdot \mathbf{f})''(\xi)
\]
for some $\xi \in (x,x+2h) \subseteq (a,+\infty)$.
Thus by the Schwarz inequality (Theorem 1.37(d))
\begin{align*}
  2h|(\mathbf{v} \cdot \mathbf{f})'(x)|
  &\leq |(\mathbf{v} \cdot \mathbf{f})(x+2h)|
    + |(\mathbf{v} \cdot \mathbf{f})(x)|
    + 2 h^2 |(\mathbf{v} \cdot \mathbf{f})''(\xi)| \\
  &\leq |\mathbf{v}| |\mathbf{f}(x+2h)|
    + |\mathbf{v}||\mathbf{f}(x)|
    + 2 h^2 |\mathbf{v}| |\mathbf{f}''(\xi)| \\
  &\leq (2 M_0 + 2 h^2 M_2)|\mathbf{v}|, \\
  |(\mathbf{v} \cdot \mathbf{f})'(x)|
  &\leq \left(\frac{M_0}{h} + h M_2\right)|\mathbf{v}|
\end{align*}
holds for any $\mathbf{v}$ and $h > 0$.
In particular, we take
\[
  \mathbf{v} = \mathbf{f}'(y)
\]
and
\[
  h = \sqrt{\frac{M_0}{M_2}}
\]
to get
\[
  |\mathbf{f}'(x) \cdot \mathbf{f}'(y)|
  \leq 2 \sqrt{M_0 M_2} |\mathbf{f}'(y)|
  \leq 2 M_1 \sqrt{M_0 M_2}.
\]
Note that $x$ and $y$ are arbitrary (in $(a,+\infty)$).
In particular, we take $x = y$ to get
\[
  |\mathbf{f}'(x)|^2 \leq 2 M_1 \sqrt{M_0 M_2}.
\]
Thus $2 M_1 \sqrt{M_0 M_2}$ is an upper bound of $|\mathbf{f}'(x)|^2$ for all $x \in (a,+\infty)$.
Hence
\[
  M_1^2 \leq 2 M_1 \sqrt{M_0 M_2}
\]
or
\[
  M_1^2 \leq 4 M_0 M_2.
\]
\end{enumerate}
$\Box$ \\

\textbf{Supplement (Landau-Kolmogorov inequality on the real line).}
\emph{Suppose $f$ is a twice-differentiable real function on $(-\infty,+\infty)$,
and $M_0$, $M_1$, $M_2$ are the least upper bounds of
$|f(x)|$, $|f'(x)|$, $|f''(x)|$, respectively, on $(-\infty,+\infty)$.
Prove that
\[
  M_1^2 \leq 2 M_0 M_2.
\]}

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Similar to (1) in Landau-Kolmogorov inequality on the half-line,
we suppose that $0 < M_0 < +\infty$ and $0 < M_2 < +\infty$.

\item[(2)]
Similar to (2) in Landau-Kolmogorov inequality on the half-line.
Given $x \in \mathbb{R}^1$ and $h>0$.
By Taylor's theorem (Theorem 5.15):
\begin{align*}
  f(x+2h) &= f(x) + 2h f'(x) + 2 h^2 f''(\xi_1) \tag{I} \\
  f(x-2h) &= f(x) - 2h f'(x) + 2 h^2 f''(\xi_2) \tag{II}
\end{align*}
for some $\xi_1 \in (x,x+2h)$ and $\xi_2 \in (x,x-2h)$.
So (I) subtracts (II):
\[
  f(x+2h)-f(x-2h) = 4h f'(x) + 2 h^2 f''(\xi_1) - 2 h^2 f''(\xi_2).
\]
Thus
\begin{align*}
  4h|f'(x)|
  &\leq |f(x+2h)| + |f(x-2h)| + 2 h^2 |f''(\xi_1)| + 2 h^2 |f''(\xi_2)| \\
  &\leq 2 M_0 + 4 h^2 M_2, \\
  |f'(x)|
  &\leq \frac{M_0}{2h} + h M_2
\end{align*}
holds for all $h > 0$.
In particular, take
\[
  h = \sqrt{\frac{M_0}{2M_2}}
\]
to get
\[
  |f'(x)| \leq \sqrt{2 M_0 M_2}.
\]
Thus $\sqrt{2 M_0 M_2}$ is an upper bound of $|f'(x)|$ for all $x \in \mathbb{R}^1$.
Hence
\[
  M_1 \leq \sqrt{2 M_0 M_2}
\]
or
\[
  M_1^2 \leq 2 M_0 M_2.
\]
\end{enumerate}
$\Box$ \\

\emph{Note.}
\begin{enumerate}
\item[(1)]
Write
\[
  M_1 \leq \sqrt{2} M_0^{\frac{1}{2}} M_2^{\frac{1}{2}}.
\]
$\sqrt{2}$ is called the Landau-Kolmogorov constant,
which is the best possible.

\item[(2)]
In general,
suppose $f$ is a $n$th differentiable real function on $\mathbb{R}^1$,
and $M_0$, $M_k$, $M_n$ are the least upper bounds of
$\abs{ f(x) }$, $\abs{ f^{(k)}(x) }$, $\abs{ f^{(n)}(x) }$, respectively, on $\mathbb{R}^1$
where $1 \leq k < n$.
Then
\[
  M_k \leq C(n,k) M_0^{1 - \frac{k}{n}} M_n^{\frac{k}{n}}.
\] \\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.16.}
\addcontentsline{toc}{subsection}{Exercise 5.16.}
\emph{Suppose $f$ is twice-differentiable on $(0,\infty)$,
$f''$ is bounded on $(0,\infty)$,
and $f(x) \to 0$ as $x \to \infty$.
Prove that $f'(x) \to 0$ as $x \to \infty$.
(Hint: Let $a \to \infty$ in Exercise 5.15.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Write $|f''| \leq M$ for some real $M$ since $f''$ is bounded on $(0,\infty)$.

\item[(2)]
Given any $a > 0$.
As in Exercise 5.15, define $M_0, M_1, M_2$ are the least upper bounds of
$|f(x)|$, $|f'(x)|$, $|f''(x)|$ on $(a,\infty)$.
Note that $M_2 \leq M$ for any $a > 0$ (by (1)).
So that
\[
  M_1^2 \leq 4 M_0 M_2 \leq 4M M_0
\]
for any $a > 0$.

\item[(3)]
By assumption, $M_0 \to 0$ as $a \to \infty$.
(So given any $\varepsilon > 0$, there exists a real $A$ such that
\[
  0 \leq M_0 < \frac{\varepsilon}{4M + 1}
\]
whenever $a \geq A$.
Hence
\[
  M_1^2 \leq 4M M_0 \leq 4M \cdot \frac{\varepsilon}{4M + 1} < \varepsilon.
\]
whenever $a \geq A$.)
Therefore $M_1^2 \to 0$ as $a \to \infty$,
or $f'(x) \to 0$ as $x \to \infty$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.17.}
\addcontentsline{toc}{subsection}{Exercise 5.17.}
\emph{Suppose $f$ is a real, three times differentiable function on $[-1,1]$, such that
\[
  f(-1)=0, \qquad
  f(0)=0, \qquad
  f(1)=1, \qquad
  f'(0)=0.
\]
Prove that $f^{(3)}(x) \geq 3$ for some $x \in (-1,1)$.
Note that equality holds for $\frac{1}{2}(x^3+x^2)$.
(Hint: Use Theorem 5.15, with $\alpha = 0$ and $\beta=\pm 1$,
to show that there exist $s \in (0,1)$ and $t \in (-1,0)$ such that
\[
  f^{(3)}(s) + f^{(3)}(t) = 6.)
\]}

We can drop the assumption that $f(0)=0$ actually. \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Use Taylor's theorem (Theorem 5.15), with $\alpha = 0$ and $\beta=\pm 1$,
\begin{align*}
  f(1) &= f(0) + f'(0) + \frac{f''(0)}{2} + \frac{f'''(s)}{6} \tag{I} \\
  f(-1) &= f(0) - f'(0) + \frac{f''(0)}{2} - \frac{f'''(t)}{6} \tag{II}
\end{align*}
for some $s \in (0,1)$ and $t \in (-1,0)$.

\item[(2)]
(I) subtracts (II) implies that
\[
  f(1) - f(-1) = 2 f'(0) + \frac{f'''(s)}{6} + \frac{f'''(t)}{6}.
\]
By assumption, $f(-1)=0$, $f(1)=1$ and $f'(0)=0$.
Hence
\[
  f^{(3)}(s) + f^{(3)}(t) = 6
\]
for some $s \in (0,1)$ and $t \in (-1,0)$.
So either $f^{(3)}(s) \geq 3$ or $f^{(3)}(t) \geq 3$
for some $s, t \in (-1,1)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.18.}
\addcontentsline{toc}{subsection}{Exercise 5.18.}
\emph{Suppose $f$ is a real function on $[a,b]$, $n$ is a positive integer,
and $f^{(n-1)}$ exists for every $t \in [a,b]$.
Let $\alpha$, $\beta$, and $P$ be as in Taylor's theorem (Theorem 5.15).
Define
\[
  Q(t) = \frac{f(t) - f(\beta)}{t - \beta}
\]
for $t \in [a,b]$, $t \neq \beta$, differentiate
\[
  f(t) - f(\beta) = (t - \beta)Q(t)
\]
$n-1$ times at $t = \alpha$,
and derive the following version of Taylor's theorem:
\[
  f(\beta) = P(\beta)
    + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n.
\]}

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  \emph{Show that
  \[
    f^{(k)}(t) = kQ^{(k-1)}(t) + (t - \beta)Q^{(k)}(t)
  \]
  for $k = 1, 2, \ldots, n$.}
  Induction on $k$.
  \begin{enumerate}
  \item[(a)]
  If $k = 1$, then
  \[
    f'(t) = Q(t) + (t-\beta)Q'(t)
  \]
  (Theorem 5.3(b)).

  \item[(b)]
  Assume the induction hypothesis that for the single case $k = m-1$ holds.
  Apply Theorem 5.3(b) again to get
  \begin{align*}
    f^{(m)}(t)
    &= (f^{(m-1)}(t))' \\
    &= ((m-1)Q^{(m-2)}(t) + (t - \beta)Q^{(m-1)}(t))' \\
    &= (m-1)Q^{(m-1)}(t) + Q^{(m-1)}(t) + (t - \beta)Q^{(m)}(t) \\
    &= mQ^{(m-1)}(t) + (t - \beta)Q^{(m)}(t).
  \end{align*}

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction the result holds.
  \end{enumerate}

  \item[(2)]
  \emph{Show that
  \[
    f(\beta) = P(\beta)
    + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n
  \]
  where}
  \[
    P(t) = \sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k!}(t - \alpha)^k.
  \]
  Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  If $n = 1$, then by the definition of $Q(t)$
  \[
    f(\beta) = f(\alpha) + Q(\alpha)(\beta - \alpha).
  \]

  \item[(b)]
  Assume the induction hypothesis that for the single case $n = m-1$ holds.
  By (1), we have
  \[
    Q^{(m-2)}(\alpha) = \frac{1}{m-1}( f^{(m-1)}(\alpha) + Q^{(m-1)}(\alpha)(\beta - \alpha) ).
  \]
  Hence
  \begin{align*}
    f(\beta)
    =& \sum_{k=0}^{m-2} \frac{f^{(k)}(\alpha)}{k!}(\beta - \alpha)^k
      + \frac{Q^{(m-2)}(\alpha)}{(m-2)!} (\beta - \alpha)^{m-1} \\
    =& \sum_{k=0}^{m-2} \frac{f^{(k)}(\alpha)}{k!}(\beta - \alpha)^k \\
      &+ \frac{ f^{(m-1)}(\alpha) }{(m-1)!} (\beta - \alpha)^{m-1}
      + \frac{Q^{(m-1)}(\alpha)(\beta - \alpha)}{(m-1)!} (\beta - \alpha)^{m-1} \\
    =& \sum_{k=0}^{m-1} \frac{f^{(k)}(\alpha)}{k!}(\beta - \alpha)^k
      + \frac{Q^{(m-1)}(\alpha)}{(m-1)!} (\beta - \alpha)^{m}.
  \end{align*}

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction the result holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\

\emph{Note.}
It is also true for vector-valued functions:
  \emph{Suppose $\mathbf{f}$ is a function of $[a,b]$ into $\mathbb{R}^{m}$,
  $n$ is a positive integer,
  $\mathbf{f}^{(n-1)}$ is continuous on $[a,b]$,
  $\mathbf{f}^{(n)}(t)$ exists for every $t \in (a,b)$.
  Let $\alpha$, $\beta$ be distinct points of $[a,b]$, and define
  \[
    \mathbf{P}(t) = \sum_{k=0}^{n-1} \frac{\mathbf{f}^{(k)}(\alpha)}{k!}(t - \alpha)^k
  \]
  and
  \[
    \mathbf{Q}(t) = \frac{\mathbf{f}(t) - \mathbf{f}(\beta)}{t - \beta}.
  \]
  Then
  \[
    \mathbf{f}(\beta) = \mathbf{P}(\beta)
      + \frac{\mathbf{Q}^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n.
  \]} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.19.}
\addcontentsline{toc}{subsection}{Exercise 5.19.}
\emph{Suppose $f$ is defined in $(-1,1)$ and $f'(0)$ exists.
Suppose $-1 < \alpha_n < \beta_n < 1$, $\alpha_n \to 0$,
and $\beta_n \to 0$ as $n \to \infty$.
Define the difference quotients
\[
  D_n = \frac{f(\beta_n) - f(\alpha_n)}{\beta_n - \alpha_n}
\]
Prove the following statements:}
\begin{enumerate}
  \item[(a)]
  \emph{If $\alpha_n < 0 < \beta_n$, then $\lim{D_n} = f'(0)$.}

  \item[(b)]
  \emph{If $0 < \alpha_n < \beta_n$ and $\left\{\frac{\beta_n}{\beta_n-\alpha_n}\right\}$ is bounded,
  then $\lim{D_n} = f'(0)$.}

  \item[(c)]
  \emph{If $f'$ is continuous in $(-1,1)$, then $\lim{D_n} = f'(0)$}.
\end{enumerate}
\emph{Give an example in which $f$ is differentiable in $(-1,1)$
(but $f'$ is not continuous at $0$) and in which
$\alpha_n$, $\beta_n$ tend to $0$ in such a way that $\lim{D_n}$ exists
but is different from $f'(0)$.} \\

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  Write
  \[
    D_n = \frac{f(\beta_n) - f(0)}{\beta_n - 0} \cdot \frac{\beta_n}{\beta_n - \alpha_n}
      - \frac{f(\alpha_n) - f(0)}{\alpha_n - 0} \cdot \frac{\alpha_n}{\beta_n - \alpha_n}.
  \]
  It is well-defined since $\alpha_n \neq 0$ and $\beta_n \neq 0$.

  \item[(2)]
  Given any $\varepsilon > 0$.
  Since $f'(0)$ exists, there exists a common integer $N$ such that
  \[
    \abs{\frac{f(\alpha_n) - f(0)}{\alpha_n - 0} - f'(0)} < \varepsilon
    \: \text{ and } \:
    \abs{\frac{f(\beta_n) - f(0)}{\beta_n - 0} - f'(0)} < \varepsilon
  \]
  whenever $n \geq N$.

  \item[(3)]
  Thus
  \begin{align*}
    &|D_n - f'(0)| \\
    \leq&
    \frac{\beta_n}{\beta_n - \alpha_n}
      \cdot \abs{\frac{f(\beta_n) - f(0)}{\beta_n - 0} - f'(0)}
      + \frac{-\alpha_n}{\beta_n - \alpha_n}
      \cdot \abs{\frac{f(\alpha_n) - f(0)}{\alpha_n - 0} - f'(0)} \\
    <&
    \frac{\beta_n}{\beta_n - \alpha_n} \varepsilon
    + \frac{-\alpha_n}{\beta_n - \alpha_n} \varepsilon \\
    =& \varepsilon.
  \end{align*}
  whenever $n \geq N$.
  Therefore, $\lim{D_n} = f'(0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  Similar to (1) in the proof of (a).
  Write
  \[
    D_n = \frac{f(\beta_n) - f(0)}{\beta_n - 0} \cdot \frac{\beta_n}{\beta_n - \alpha_n}
      - \frac{f(\alpha_n) - f(0)}{\alpha_n - 0} \cdot \frac{\alpha_n}{\beta_n - \alpha_n}.
  \]
  It is well-defined since $\alpha_n \neq 0$ and $\beta_n \neq 0$.

  \item[(2)]
  Write
  \[
    \abs{ \frac{\beta_n}{\beta_n-\alpha_n} } \leq M
  \]
  for some real $M \geq 0$.
  Hence $\left\{\frac{\alpha_n}{\beta_n-\alpha_n}\right\}$ is bounded too, say
  \[
    \abs{ \frac{\alpha_n}{\beta_n-\alpha_n} }
    = \abs{ \frac{\beta_n}{\beta_n-\alpha_n} - 1 } \leq M + 1.
  \]

  \item[(3)]
  Given any $\varepsilon > 0$.
  Since $f'(0)$ exists, there exists a common integer $N$ such that
  \begin{align*}
    \abs{\frac{f(\alpha_n) - f(0)}{\alpha_n - 0} - f'(0)} &< \frac{\varepsilon}{64(M+1)}, \\
    \abs{\frac{f(\beta_n) - f(0)}{\beta_n - 0} - f'(0)} &< \frac{\varepsilon}{89(M+1)}
  \end{align*}
  whenever $n \geq N$.

  \item[(4)]
  Thus
  \begin{align*}
    &|D_n - f'(0)| \\
    \leq&
    \abs{\frac{\beta_n}{\beta_n - \alpha_n}}
      \cdot \abs{\frac{f(\beta_n) - f(0)}{\beta_n - 0} - f'(0)} \\
      &+ \abs{\frac{-\alpha_n}{\beta_n - \alpha_n}}
      \cdot \abs{\frac{f(\alpha_n) - f(0)}{\alpha_n - 0} - f'(0)} \\
    <&
    \frac{M}{89(M+1)} \varepsilon + \frac{M+1}{64(M+1)} \varepsilon \\
    <&
    \frac{\varepsilon}{89} + \frac{\varepsilon}{64} \\
    <& \varepsilon
  \end{align*}
  whenever $n \geq N$.
  Therefore, $\lim{D_n} = f'(0)$.
\end{enumerate}
$\Box$ \\

\emph{Proof of (c).}
By the mean value theorem (Theorem 5.10),
there is point $\xi_n \in (\alpha_n,\beta_n)$ at which
\[
  f(\beta_n) - f(\alpha_n) = (\beta_n - \alpha_n) f'(\xi_n)
\]
or
\[
  f'(\xi_n) = \frac{f(\beta_n) - f(\alpha_n)}{\beta_n - \alpha_n} = D_n.
\]
Since $\xi_n \in (\alpha_n,\beta_n)$ and $\lim \alpha_n = \lim \beta_n = 0$,
$\lim \xi_n = 0$.
Since $f'$ is continuous at $x = 0$,
\[
  \lim D_n = \lim f'(\xi_n) = f'(\lim \xi_n) = f'(0).
\]
$\Box$ \\



\emph{Note.}
\begin{enumerate}
  \item[(1)]
  \emph{Give an example in which $f$ is differentiable in $(-1,1)$
  (but $f'$ is not continuous at $0$) and in which
  $\alpha_n$, $\beta_n$ tend to $0$ in such a way that $\lim{D_n}$ exists
  but is different from $f'(0)$.}

  \item[(2)]
  Let $f$ be defined by
  \begin{equation*}
    f(x) =
    \begin{cases}
      x^2 \sin \frac{1}{x} & (x \neq 0), \\
      0                    & (x = 0)
    \end{cases}
  \end{equation*}
  as in Examples 5.6(b).
  So
  \begin{equation*}
    f'(x) =
    \begin{cases}
      2x \sin\frac{1}{x} - \cos\frac{1}{x} & (x \neq 0), \\
      0                                    & (x = 0).
    \end{cases}
  \end{equation*}

  \item[(3)]
  Take $\alpha_n = \left(2n\pi \right)^{-1} \neq 0$
  and $\beta_n = \left(\frac{\pi}{2} + 2n\pi \right)^{-1} \neq 0$
  for $n = 1, 2, 3, \ldots$.
  Hence $\lim \alpha_n = \lim \beta_n = 0$, and
  \begin{align*}
    \lim D_n
    &=
    \lim \frac{\left(\frac{\pi}{2} + 2n\pi \right)^{-2}}
      {\left(\frac{\pi}{2} + 2n\pi \right)^{-1} - \left(2n\pi \right)^{-1}} \\
    &=
    \lim \frac{2n\pi}{(2n\pi)\left(\frac{\pi}{2} + 2n\pi \right)
      - \left(\frac{\pi}{2} + 2n\pi \right)^2} \\
    &=
    \lim \frac{2n\pi}{-\frac{\pi}{2}\left(\frac{\pi}{2} + 2n\pi \right)} \\
    &= -\frac{2}{\pi} \neq f'(0).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.20.}
\addcontentsline{toc}{subsection}{Exercise 5.20.}
\emph{Formulate and prove an inequality which follows form Taylor's theorem and
which remains valid for vector-valued function.} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  \emph{Suppose $\mathbf{f}$ is a function of $[a,b]$ into $\mathbb{R}^{m}$,
  $n$ is a positive integer,
  $\mathbf{f}^{(n-1)}$ is continuous on $[a,b]$,
  $\mathbf{f}^{(n)}(t)$ exists for every $t \in (a,b)$.
  Let $\alpha$, $\beta$ be distinct points of $[a,b]$, and define
  \[
    \mathbf{P}(t) = \sum_{k=0}^{n-1} \frac{\mathbf{f}^{(k)}(\alpha)}{k!}(t - \alpha)^k.
  \]
  Then there exists a point $x$ between $\alpha$ and $\beta$ such that}
  \[
    \abs{ \mathbf{f}(\beta) - \mathbf{P}(\beta) }
    \leq (\beta - \alpha)^n \abs{\frac{\mathbf{f}^{(n)}(x)}{n!}}.
  \]

  For $n = 1$, this is just Theorem 5.19.

  \item[(2)]
  Similar to the proof of Theorem 5.19.
  Put
  \[
    \mathbf{z} = \mathbf{f}(\beta) - \mathbf{P}(\beta).
  \]
  Define
  \[
    \varphi(t) = \mathbf{z} \cdot \mathbf{f}(t)
    \qquad (\alpha \leq t \leq \beta).
  \]
  Then $\varphi(t)$ is a function of $[a,b]$ into $\mathbb{R}^1$, and
  \[
    \varphi^{(k)}(t) = \mathbf{z} \cdot \mathbf{f}^{(k)}(t)
  \]
  where $0 \leq k \leq n$.
  Also, $\varphi^{(n-1)}$ is continuous on $[\alpha,\beta]$,
  and $\varphi^{(n)}(t)$ exists for every $t \in (\alpha,\beta)$.

  \item[(3)]
  By Taylor's theorem (Theorem 5.15), there exists $x \in (\alpha,\beta)$ such that
  \[
    \varphi(\beta) = Q(\beta) + \frac{\varphi^{(n)}(x)}{n!}(\beta-\alpha)^n
  \]
  where
  \[
    Q(t) = \sum_{k=0}^{n-1} \frac{\varphi^{(k)}(\alpha)}{k!}(t - \alpha)^k.
  \]
  By (2), we have $Q(t) = \mathbf{z} \cdot \mathbf{P}(t)$ and thus
  \[
    \mathbf{z} \cdot (\mathbf{f}(\beta) - \mathbf{P}(\beta))
    = \mathbf{z} \cdot \frac{\mathbf{f}^{(n)}(x)}{n!}(\beta-\alpha)^n.
  \]
  Note that $\mathbf{z} = \mathbf{f}(\beta) - \mathbf{P}(\beta)$
  and Schwarz inequality (Theorem 1.37(d)).
  Hence
  \begin{align*}
    |\mathbf{f}(\beta) - \mathbf{P}(\beta)|^2
    &=
    \abs{ (\mathbf{f}(\beta) - \mathbf{P}(\beta))
      \cdot \frac{\mathbf{f}^{(n)}(x)}{n!}(\beta-\alpha)^n } \\
    &\leq
      \abs{ \mathbf{f}(\beta) - \mathbf{P}(\beta) }
      \abs{ \frac{\mathbf{f}^{(n)}(x)}{n!} } (\beta-\alpha)^n
  \end{align*}
  or
  \[
    |\mathbf{f}(\beta) - \mathbf{P}(\beta)|
    \leq \abs{ \frac{\mathbf{f}^{(n)}(x)}{n!} } (\beta-\alpha)^n
  \]
  (whether $\mathbf{f}(\beta) - \mathbf{P}(\beta)$ is zero nor not).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.21.}
\addcontentsline{toc}{subsection}{Exercise 5.21.}
\emph{Let $E$ be a closed subset of $\mathbb{R}^1$.
We saw in Exercise 4.22, that there is a real continuous function $f$ on $\mathbb{R}^1$
whose zero set is $E$.
Is it possible, for each closed set $E$,
to find such an $f$ which is differentiable on $\mathbb{R}^1$,
or one which is $n$ times differentiable,
or even one which has derivatives of all orders on $\mathbb{R}^1$?} \\

It is possible by leveraging Exercise 8.1. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Every open set in $\mathbb{R}^1$ is the union of an at most countable collection
of disjoint segments (Exercise 2.29).

\item[(2)]
We need to construct an infinitely differentiable real function $f$ on $\mathbb{R}^1$
such that the zero set $Z(f)$ is $E$.
By (1), write $\widetilde{E}$ as the union of an at most countable collection of disjoint segments,
say
\[
  \widetilde{E} = \bigcup_{(a_i,b_i) \in \mathscr{C}} (a_i,b_i)
\]
where $\mathscr{C}$ is at most countable and all $(a_i,b_i)$ segments are disjoint.

\item[(3)]
For each disjoint segment $(a_i,b_i)$ of
\[
  \widetilde{E} = \bigcup_{(a_i,b_i) \in \mathscr{C}} (a_i,b_i),
\]
define $f(x)$ on $\mathbb{R}^1$ by
  \begin{equation*}
    f(x) =
      \begin{cases}
        1
          & (x \in (-\infty,\infty)), \\
        \exp(-\frac{1}{(x-a_i)^2})
          & (x \in (a_i,\infty), a_i \neq -\infty), \\
        \exp(-\frac{1}{(x-b_i)^2})
          & (x \in (-\infty,b_i), b_i \neq \infty), \\
        \exp(-\frac{1}{(x-a_i)^2(x-b_i)^2})
          & (x \in (a_i,b_i), a_i \neq -\infty, b_i \neq \infty), \\
        0
          & (x \in E).
      \end{cases}
  \end{equation*}
By construction, $f(x) = 0$ if and only if $x \in E$ (Theorem 8.6(c)).
By the same argument in the proof of Exercise 8.1,
$f(x)$ is infinitely differentiable on $\mathbb{R}^1$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.22 (Fixed-point iteration).}
\addcontentsline{toc}{subsection}{Exercise 5.22 (Fixed-point iteration).}
\emph{Suppose $f$ is a real function on $(-\infty,+\infty)$.
Call $x$ a \textbf{fixed point} of $f$ if $f(x)=x$.}
\begin{enumerate}
  \item[(a)]
  \emph{If $f$ is differentiable and $f'(t) \neq 1$ for every real $t$,
  prove that $f$ has at most one fixed point.}

  \item[(b)]
  \emph{Show that the function $f$ defined by
  \[
    f(t) = t+(1+e^t)^{-1}
  \]
  has no fixed point, although $0 < f'(t) < 1$ for all real $t$.}

  \item[(c)]
  \emph{However, if there is a constant $A<1$ such that $|f'(t)| \leq A$ for all real $t$,
  prove that a fixed point $x$ of $f$ exists,
  and that $x = \lim x_n$, where $x_1$ is an arbitrary real number and
  \[
    x_{n+1} = f(x_n)
  \]
  for $n = 1,2,3,\ldots$.}

  \item[(d)]
  \emph{Show that the process describe in (c) can be visualized by the zig-zag path
  \[
    (x_1,x_2) \to (x_2,x_2) \to (x_2,x_3) \to (x_3,x_3) \to (x_3,x_4) \to \ldots.
  \]}
  \end{enumerate}

\emph{Proof of (a).}
(Reductio ad absurdum)
\begin{enumerate}
\item[(1)]
Suppose that there were two different fixed points $x_1 < x_2$.
By the mean value theorem (Theorem 5.10),
there exists $\xi \in (x_1, x_2)$ such that
\[
  f(x_1) - f(x_2) = (x_1 - x_2) f'(\xi).
\]

\item[(2)]
Since $x_1$ and $x_2$ are fixed points, $f(x_1) = x_1$ and $f(x_2) = x_2$ or
\[
  (x_1 - x_2)(f'(\xi) - 1) = 0.
\]
Since $x_1 \neq x_2$, $f'(\xi) = 1$,
contrary to the fact that $f'(t) \neq 1 \: \forall t \in \mathbb{R}^1$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\emph{Show that $f$ has no fixed point.}
\begin{align*}
  f(t) = t
  &\Longleftrightarrow
  t+(1+e^t)^{-1} = t \\
  &\Longleftrightarrow
  (1+e^t)^{-1} = 0,
\end{align*}
which is absurd since $1+e^t > 1$ (Theorem 8.6(c)) and
the multiplicative inverse of $(1+e^t)^{-1}$ is never zero.

\item[(2)]
\emph{Show that $0 < f'(t) < 1$.}
\[
  f'(t) = 1 - \frac{e^t}{(1+e^t)^2} = \frac{1+e^t+e^{2t}}{1+2e^t+e^{2t}}.
\]
Since $e^t > 0$ for all $t \in \mathbb{R}^1$,
$0 < f'(t) < 1$ for all $t \in \mathbb{R}^1$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c)(Banach fixed point theorem).}
Might assume that $A > 0$.
(If $A = 0$, then $f(x) = c$ for some constant $c$ (Theorem 5.11(b))
and thus $x = c$ is the unique fixed point.)
\begin{enumerate}
\item[(1)]
Given any integer $n > 1$.
By the mean value theorem (Theorem 5.10),
there exists $\xi_{n-1}$ between $x_{n-1}$ and $x_{n}$ such that
\[
  f(x_{n}) - f(x_{n-1}) = (x_{n} - x_{n-1}) f'(\xi_{n-1}).
\]
By definition of $\{x_n\}$,
$f(x_{n}) = x_{n+1}$ and $f(x_{n-1}) = x_{n}$.
So that
\begin{align*}
  |x_{n+1} - x_{n}|
  &= |f(x_n) - f(x_{n-1})| \\
  &= |x_{n} - x_{n-1}||f'(\xi_{n-1})| \\
  &\leq A|x_{n} - x_{n-1}|.
\end{align*}

\item[(2)]
Hence by induction
\[
  |x_{n+1} - x_{n}| \leq A^{n-1}|x_2 - x_1|.
\]
So if $m > n$ we have
\begin{align*}
  |x_{m} - x_{n}|
  &\leq \sum_{i=n}^{m-1} \abs{x_{i+1} - x_{i}} \\
  &\leq \sum_{i=n}^{m-1} A^{i-1}|x_2 - x_1| \\
  &\leq \sum_{i=n}^{\infty} A^{i-1} |x_2 - x_1| \\
  &= \frac{A^{n-1}}{1-A} |x_2 - x_1|.
\end{align*}

\item[(3)]
Given $\varepsilon > 0$.
Take an integer $N$ such that
\[
  \frac{A^{n-1}}{1-A} |x_2 - x_1| < \varepsilon
\]
whenever $n \geq N$.
For example,
\[
  N > 1 + \frac{\log \frac{(1-A)\varepsilon}{1 + |x_2-x_1|}}{\log A}.
\]
Hence as $m > n \geq N$, $|x_{m} - x_{n}| < \varepsilon$,
or $\{x_n\}$ is a Cauchy sequence.
Since $\mathbb{R}^1$ is complete (Theorem 3.11(c)),
$\{x_n\}$ converges to $x \in \mathbb{R}^1$.

\item[(4)]
Since $f$ is differentiable, $f$ is continuous (Theorem 5.2).
Take $n \to \infty$ in $x_{n+1} = f(x_n)$
to get
\[
  x = \lim x_{n+1} = \lim f(x_n) = f(\lim x_n) = f(x).
\]
So that $\lim x_n = x$ is a fixed point of $f$.
\end{enumerate}
$\Box$ \\

\emph{Note.}
It is called the contraction principle.
See Definition 9.22 and Theorem 9.23. \\



\emph{Proof of (d).}
Write
\[
  (x_1,x_2) \to (x_2,x_2) \to (x_2,x_3) \to (x_3,x_3) \to \ldots
\]
as
\[
  \underbrace{(x_1,f(x_1))}_{\text{in }y = f(x)}
  \to
  \overbrace{(f(x_1),x_2)}^{\text{in }y = x}
  \to
  \underbrace{(x_2,f(x_2))}_{\text{in }y = f(x)}
  \to
  \overbrace{(f(x_2),x_3)}^{\text{in }y = x} \to \ldots.
\]
Hence the path is zig-zag in the visualization.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.23.}
\addcontentsline{toc}{subsection}{Exercise 5.23.}
\emph{The function $f$ defined by
\[
  f(x) = \frac{x^3 + 1}{3}
\]
has three fixed points, say $\alpha$, $\beta$, $\gamma$, where
\[
  -2 < \alpha < -1, \qquad
  0 < \beta < 1, \qquad
  1 < \gamma < 2.
\]
For arbitrarily chosen $x_1$, define $\{x_n\}$ by setting $x_{n+1} = f(x_n)$.}
\begin{enumerate}
  \item[(a)]
  \emph{If $x_1 < \alpha$, prove that $x_n \to -\infty$ as $n \to \infty$.}

  \item[(b)]
  \emph{$If \alpha < x_1 < \gamma$, prove that $x_n \to \beta$ as $n \to \infty$.}

  \item[(c)]
  \emph{If $\gamma < x_1$, prove that $x_n \to +\infty$ as $n \to \infty$.}
  \end{enumerate}
\emph{Thus $\beta$ can be located by this method,
but $\alpha$ and $\gamma$ cannot.} \\

\emph{Note.}
\begin{enumerate}
\item[(1)]
$f'(x) = x^2$ is unbounded.
So that it does not exist such $\sup|f'(x)| = A < 1$ in Exercise 5.22(c).

\item[(2)]
Cardano's Formula implies that
\begin{align*}
  \alpha &= -2 \cos\frac{\pi}{9}, \\
  \beta &= 2 \sin\frac{\pi}{18}, \\
  \gamma &= \sqrt{3} \sin\frac{\pi}{9} + \cos\frac{\pi}{9}.
\end{align*}
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
Write
\[
  g(x) = f(x) - x = \frac{x^3}{3} - x + \frac{1}{3}.
\]
$f(x) = x$ if and only if $g(x) = 0$.

\item[(2)]
$\alpha$, $\beta$ and $\gamma$ are the only three roots of $g(x)$ (Theorem 8.8).
Hence $\alpha$, $\beta$ and $\gamma$ are the only three fixed points of $f(x)$.

\item[(3)]
\emph{Show that $\{x_n\}$ is strictly decreasing,
or $x_{n+1} < x_n < \alpha$ for $n = 1, 2, 3, \ldots$.}
Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  As $n = 1$, it suffices to show that
  \[
    g(x_1) = f(x_1) - x_1 = x_2 - x_1 < 0.
  \]

  $g'(x) = x^2 - 1$ implies that $g(x)$ is strictly increasing on $(-\infty,-1)$.
  Since $x_1 < \alpha < -1$, $g(x_1) < g(\alpha) = 0$.

  \item[(b)]
  Assume the induction hypothesis that for the single case $n = k$ holds.
  So that $x_{k+1} < x_k < \alpha$.
  Apply the same argument in (a) to get
  \[
    g(x_{k+1}) < g(\alpha) = 0.
  \]
  Hence $x_{k+2} < x_{k+1} < \alpha$.

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction $x_{n+1} < x_n$ for all $n$.
  \end{enumerate}

\item[(4)]
\emph{Show that $\{x_n\}$ is unbounded.}
(Reductio ad absurdum)
If $\{x_n\}$ were bounded,
by (3) $\{x_n\}$ converges to some $\xi \in \mathbb{R}^1$ (Theorem 3.14).
That is, $\xi$ is a fixed point of $f$.
Note that $\xi \leq x_1 < \alpha$, contrary to (2).
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
Consider three possible cases.
\begin{enumerate}
\item[(1)]
The case $x_1 = \beta$.
There is nothing to prove since all $x_n = \beta$.

\item[(2)]
The case $x_1 \in (\alpha,\beta)$.
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $g(x) > 0$ on $(\alpha, \beta)$.}
  $g'(x) = x^2 - 1 = (x-1)(x+1)$ implies that
    \begin{enumerate}
    \item[(i)]
    $g(x)$ is strictly increasing on $(-\infty,-1)$.

    \item[(ii)]
    $g(x)$ is strictly decreasing on $(-1,1)$.

    \item[(iii)]
    $g(x)$ is strictly increasing on $(1, \infty)$.
    \end{enumerate}
  As $x \in (\alpha,-1)$, $g(x) > g(\alpha) = 0$.
  As $x \in (-1,\beta)$, $g(x) > g(\beta) = 0$.
  As $x = -1$, $g(-1) = 1 > 0$.
  Hence $g(x) > 0$ on $(\alpha, \beta)$.

  \item[(b)]
  \emph{Show that $x_n \in (\alpha, \beta)$ for $n = 1, 2, 3, \ldots$.}
  Induction on $n$.
    \begin{enumerate}
    \item[(i)]
    $x_1 \in (\alpha, \beta)$ by assumption.

    \item[(ii)]
    Assume the induction hypothesis that for the single case $n = k$ holds,
    that is, $\beta > x_k > \alpha$.
    Since $x \mapsto x^3$ is strictly increasing,
    \begin{align*}
      \beta > x_k > \alpha
      &\Longrightarrow
      \beta^3 > x_k^3 > \alpha^3 \\
      &\Longrightarrow
      \frac{\beta^3 + 1}{3} > \frac{x_k^3+1}{3} > \frac{\alpha^3+1}{3} \\
      &\Longrightarrow
      \beta > x_{k+1} > \alpha.
    \end{align*}
    By (a),
    \[
      g(x_{k}) > 0.
    \]
    Hence $x_{k+1} > x_{k} > \alpha$.

    \item[(iii)]
    Since both the base case in (i) and
    the inductive step in (ii) have been proved as true,
    by mathematical induction $x_n \in (\alpha, \beta)$ for all $n$.
    \end{enumerate}

  \item[(c)]
  \emph{Show that $\{x_n\}$ is strictly increasing,
  or $\beta > x_{n+1} > x_n > \alpha$ for $n = 1, 2, 3, \ldots$.}
  Induction on $n$.
    \begin{enumerate}
    \item[(i)]
    As $n = 1$, by (a)
    \[
      g(x_1) = f(x_1) - x_1 = x_2 - x_1 > 0.
    \]
    Note that $x_2 \in (\alpha, \beta)$ by (b).

    \item[(ii)]
    Assume the induction hypothesis that for the single case $n = k$ holds,
    that is, $\beta > x_{k+1} > x_k > \alpha$.
    By (a)
    \[
      g(x_{k+1}) > 0.
    \]
    Hence $x_{k+2} > x_{k+1}$.
    Note that $x_{k+2} \in (\alpha, \beta)$ by (b).

    \item[(iii)]
    Since both the base case in (i) and
    the inductive step in (ii) have been proved as true,
    by mathematical induction $\beta > x_{n+1} > x_n > \alpha$ for all $n$.
    \end{enumerate}

  \item[(d)]
  By (b)(c),
  $\{x_n\}$ converges to some $\xi \in \mathbb{R}^1$ (Theorem 3.14).
  That is, $\xi$ is a fixed point of $f$.
  Note that $\beta \geq \xi \geq x_1 > \alpha$.
  By (2) in the proof of (a), $\lim x_n = \xi = \beta$.
  \end{enumerate}

\item[(3)]
The case $x_1 \in (\beta,\gamma)$.
Similar to (2).
  \begin{enumerate}
  \item[(a)]
  \emph{Show that $g(x) < 0$ on $(\beta,\gamma)$.}
  Similar to (2)(a).

  \item[(b)]
  \emph{Show that $x_n \in (\beta,\gamma)$ for $n = 1, 2, 3, \ldots$.}
  Similar to (2)(b).

  \item[(c)]
  \emph{Show that $\{x_n\}$ is strictly decreasing,
  or $\beta < x_{n+1} < x_n < \gamma$ for $n = 1, 2, 3, \ldots$.}
  Similar to (2)(c).

  \item[(d)]
  By (b)(c),
  $\{x_n\}$ converges to some $\xi \in \mathbb{R}^1$ (Theorem 3.14).
  That is, $\xi$ is a fixed point of $f$.
  Note that $\beta \leq \xi \leq x_1 < \gamma$.
  By (2) in the proof of (a), $\lim x_n = \xi = \beta$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
Similar to (a).
Recall $g(x) = f(x) - x = \frac{x^3}{3} - x + \frac{1}{3}$.
\begin{enumerate}
\item[(1)]
\emph{Show that $\{x_n\}$ is strictly increasing,
or $x_{n+1} > x_n > \gamma$ for $n = 1, 2, 3, \ldots$.}
Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  As $n = 1$, it suffices to show that
  \[
    g(x_1) = f(x_1) - x_1 = x_2 - x_1 > 0.
  \]

  $g'(x) = x^2 - 1$ implies that $g(x)$ is strictly increasing on $(1, \infty)$.
  Since $x_1 > \gamma > 1$, $g(x_1) > g(\gamma) = 0$.

  \item[(b)]
  Assume the induction hypothesis that for the single case $n = k$ holds.
  So that $x_{k+1} > x_k > \gamma$.
  Apply the same argument in (a) to get
  \[
    g(x_{k+1}) > g(\gamma) = 0.
  \]
  Hence $x_{k+2} > x_{k+1} > \gamma$.

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction $x_{n+1} > x_n$ for all $n$.
  \end{enumerate}

\item[(2)]
\emph{Show that $\{x_n\}$ is unbounded.}
(Reductio ad absurdum)
If $\{x_n\}$ were bounded,
by (1) $\{x_n\}$ converges to some $\xi \in \mathbb{R}^1$ (Theorem 3.14).
That is, $\xi$ is a fixed point of $f$.
Note that $\xi \geq x_1 > \gamma$, contrary to (2) in the proof of (a).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.24.}
\addcontentsline{toc}{subsection}{Exercise 5.24.}
\emph{The process described in part (c) of Exercise 5.22 can of course also be applied to
functions that map $(0,\infty)$ to $(0,\infty)$.
Fix some $\alpha > 1$, and put
\[
  f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad
  g(x) = \frac{\alpha + x}{1 + x}.
\]
Both $f$ and $g$ have $\sqrt{\alpha}$ as their fixed point in $(0,\infty)$.
Try to explain, on the basis of properties of $f$ and $g$,
why the convergence in Exercise 3.16, is so much more rapid than it is in Exercise 3.17.
(Compare $f'$ and $g'$, draw the zig-zags suggested in Exercise 5.22.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Note that
\begin{align*}
  f'(x) &= \frac{1}{2} \left(1-\frac{\alpha}{x^2}\right) \to 0 \:\: \text{ and } \\
  g'(x) &= \frac{1-\alpha}{(1+x)^2} \to \frac{1-\sqrt{\alpha}}{1+\sqrt{\alpha}} \neq 0
\end{align*}
as $x \to \alpha$.

\item[(2)]
The rate of convergence of $f(x)$ is
at least quadratically geometric (since $A \to 0$ in the sense of Exercise 5.22(c)).

\item[(3)]
The rate of convergence of $g(x)$ is geometric
of the ratio $A = \frac{1-\sqrt{\alpha}}{1+\sqrt{\alpha}}$ in the sense of Exercise 5.22(c).

\item[(4)]
Hence the rate of convergence of $f(x)$ is much more rapid than of $g(x)$.
(Omit drawing two zig-zag paths.)
\end{enumerate}

$\Box$ \\\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.25.}
\addcontentsline{toc}{subsection}{Exercise 5.25.}
\emph{Suppose $f$ is twice differentiable on $[a,b]$,
$f(a) < 0$, $f(b) > 0$, $f'(x) \geq \delta > 0$,
and $0 \leq f''(x) \leq M$ for all $x \in [a,b]$.
Let $\xi$ be the unique point in $(a,b)$ at which $f(\xi) = 0$.
Complete the details in the following outline of
\textbf{Newton's method} for computing $\xi$.}
\begin{enumerate}
\item[(a)]
\emph{Choose $x_1 \in (\xi,b)$, and define $\{ x_n \}$ by
\[
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.
\]
Interpret this geometrically, in terms of a tangent to the graph of $f$.}

\item[(b)]
\emph{Prove that $x_{n+1} < x_n$ and that
\[
  \lim_{n \to \infty}{x_n} = \xi.
\]}
\item[(c)]
\emph{Use Taylor's theorem to show that
\[
  x_{n+1} - \xi = \frac{f''(t_n)}{2 f'(x_n)} (x_n-\xi)^2
\]
for some $t_n \in (\xi,x_n)$.}

\item[(d)]
\emph{(Quadratic convergence)
If $A = \frac{M}{2\delta}$, deduce that
\[
  0 \leq x_{n+1} - \xi \leq \frac{1}{A} [A(x_1 - \xi)]^{2^n}.
\]
(Compare with Exercise 3.16 and 3.18.)}

\item[(e)]
\emph{Show that Newton's method amounts to finding a fixed point of the function $g$ defined by
\[
  g(x) = x - \frac{f(x)}{f'(x)}.
\]
How does $g'(x)$ behave for $x$ near $\xi$?}

\item[(f)]
\emph{Put $f(x) = x^{\frac{1}{3}}$ on $(-\infty, +\infty)$ and try Newton's method.
What happens?} \\
\end{enumerate}



\emph{Proof of (a) (Wikipedia).}
The equation of the tangent line to the curve $y = f(x)$ at $x = x_n$ is
\[
  y = f'(x_n)(x - x_n) + f(x_n).
\]
The $x$-intercept of this line (the value of $x$ which makes $y = 0$)
is taken as the next approximation, $x_{n+1}$, to the root,
so that the equation of the tangent line is satisfied when
$(x,y) = (x_{n+1},0)$:
\[
  0 = f'(x_n)(x - x_n) + f(x_n).
\]
Solving for $x_{n+1}$ gives
\[
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.
\]
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\emph{Show that $x_n \geq \xi$ for all $n$.}
Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  $n = 1$ is clearly true: $x_1 > \xi$ by assumption.

  \item[(b)]
  Assume the induction hypothesis that for the single case $n = k$ holds.
  By the mean value theorem (Theorem 5.10),
  there is a point $\xi_k \in (\xi, x_k)$
  \[
    f(x_k) - f(\xi) = f'(\xi_k)(x_k - \xi),
  \]
  or
  \[
    f(x_k) = f'(\xi_k)(x_k - \xi)
  \]
  (since $f(\xi) = 0$).
  Since $f'' \geq 0$, $f'$ is monotonically increasing (Theorem 5.11(a)).
  Hence $f'(\xi_k) \leq f'(x_k)$ and thus
  \[
    f(x_k)
    = f'(\xi_k)(x_k - \xi)
    \leq f'(x_k)(x_k - \xi).
  \]
  Since $f'(x_k) > 0$ by assumption,
  \[
    \xi \leq x_k - \frac{f(x_k)}{f'(x_k)} = x_{k+1}.
  \]

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction $x_n \geq \xi$ for all $n$.
  \end{enumerate}

\item[(2)]
\emph{Show that $x_{n+1} < x_n$ for all $n$.}
  \begin{enumerate}
  \item[(a)]
  Since $f' > 0$, $f'(x_n) > 0$ for all $n$.

  \item[(b)]
  Since $f' > 0$, $f$ is strictly increasing (Theorem 5.10).
  Hence $f(x_n) > f(\xi) = 0$ for all $n$ (by (1)).

  \item[(c)]
  By (a)(b), $\frac{f(x_n)}{f'(x_n)} > 0$ or
  \[
    x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} < x_n.
  \]
  \end{enumerate}

\item[(3)]
By Theorem 3.14, $\{x_n\}$ converges to some real number $\zeta \geq \xi$.
Note that $f$ and $f'$ are continuous by the existence of $f''$ (Theorem 5.2),
we have
\[
  \lim_{n \to \infty} x_{n+1}
  = \lim_{n \to \infty} x_n - \frac{f(\lim_{n \to \infty} x_n)}{f'(\lim_{n \to \infty} x_n)}
\]
provided $f' \neq 0$ (Theorem 4.9 and Theorem 4.4).
Hence
\[
  \zeta = \zeta - \frac{f(\zeta)}{f'(\zeta)}
\]
or $f(\zeta) = 0$.
By the uniqueness of $\xi$, $\zeta = \xi$ or $\lim x_n = \xi$ as desired.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
By Taylor's theorem (Theorem 5.15),
\[
        f(\xi) = f(x_n) + f'(x_n)(\xi - x_n) + \frac{f''(t_n)}{2}(\xi - x_n)^2
\]
for some $t_n \in (\xi, x_n)$.
Note that $f(\xi) = 0$, $f'(x_n) \neq 0$ and $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$,
we have the desired result.
$\Box$ \\



\emph{Proof of (d).}
Clearly, $0 \leq x_{n+1} - \xi$ for all $n$ (by (b)).
Besides, by (c)
\[
  x_{n+1} - \xi = \frac{f''(t_n)}{2 f'(x_n)} (x_n-\xi)^2
\]
Note that $f'' \leq M$ and $f' \geq \delta > 0$ by assumption, and thus
\[
  x_{n+1} - \xi
  \leq \frac{M}{2\delta} (x_n - \xi)^2 = A (x_n - \xi)^2.
\]
By induction,
\[
  x_{n+1} - \xi \leq \frac{1}{A}(A(x_1-\xi))^{2^n}.
\]
$\Box$ \\

\emph{Note.}
Compare with Exercise 3.16 and Exercise 3.18.
Might assume that $p > 1$.
\begin{enumerate}
\item[(1)]
Fix a positive number $\alpha$.
Let $f(x) = x^p - \alpha$ on $E = (a,b)$
where $a = \frac{1}{2} \alpha^{\frac{1}{p}}$ and
\begin{equation*}
  b =
    \begin{cases}
      2 \alpha^{\frac{1}{p}}
      & (p = 2), \\
      \left( \frac{2(p-1)}{p} \right)^{\frac{1}{p-2}} \alpha^{\frac{1}{p}}
      & (p > 2).
    \end{cases}
\end{equation*}
$E = (a,b)$ is well-defined since $a < b$.
Besides, $\xi = \alpha^{\frac{1}{p}} \in E = (a,b)$.

\item[(2)]
By construction,
\[
  f(a) < 0 \text{ and } f(b) > 0.
\]
By $f'(x) = px^{p-1}$ and $f''(x) = p(p-1)x^{p-2}$,
\begin{align*}
  f'(x) &\geq pa^{p-1} > 0, \\
  0 \leq f''(x) &\leq p(p-1)b^{p-2}.
\end{align*}
on $E$.
Write
\begin{align*}
  \delta &= pa^{p-1} = \frac{p}{2^{p-1}} \alpha^{\frac{p-1}{p}}, \\
  M &= p(p-1)b^{p-2} = 2 (p-1)^2 \alpha^{\frac{p-2}{p}}.
\end{align*}

\item[(3)]
Hence the Newton's method works for $f(x) = x^p - \alpha$.
That is, as we define $\{x_n\}$ by
\[
  x_{n+1}
  = x_n - \frac{f(x_n)}{f'(x_n)}
  = \frac{p-1}{p}x_n + \frac{\alpha}{p} x_n^{-p+1},
\]
we have $\lim x_n = \xi = \alpha^{\frac{1}{p}}$.
And
\[
  0 \leq x_{n+1} - \xi \leq \frac{1}{A}(A(x_1-\xi))^{2^n}.
\]
Here
\[
  A = \frac{M}{2 \delta} = \frac{2^{p-1}(p-1)^2}{p \alpha^{\frac{1}{p}}}.
\]

\item[(4)]
Note that
\[
  \beta
  = \frac{p \alpha^{\frac{1}{p}}}{(p-1)^2}
  \neq \frac{p \alpha^{\frac{1}{p}}}{2^{p-1}(p-1)^2} = \frac{1}{A}.
\]
where $\beta$ is defined in the proof of Exercise 3.18.
Note that $f'(x_n) \geq f'(\xi)$
(since $f'$ is monotonically increasing and all $x_n \geq \xi$),
and thus $A$ can be chosen by a better estimation:
\[
  A = \frac{M}{2 f'(\xi)} = \frac{(p-1)^2}{p \alpha^{\frac{1}{p}}} = \frac{1}{\beta}.
\]
Now it is exactly the same as Exercise 3.16 and Exercise 3.18. \\
\end{enumerate}



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
Define $g(x) = x - \frac{f(x)}{f'(x)}$ on $[a,b]$.
$g(\xi) = \xi$ if and only if $f(\xi) = 0$.

\item[(2)]
By the construction of $g$, $g$ is differentiable and
\[
  g'(x)
  = 1 - \frac{f'(x)^2 - f(x) f''(x)}{f'(x)^2}
  = \frac{f(x) f''(x)}{f'(x)^2}.
\]

\item[(3)]
Hence
\[
  |g'(x)|
  \leq \abs{\frac{f(x) f''(x)}{f'(x)^2}}
  = \frac{|f(x)| |f''(x)|}{|f'(x)|^2}
  \leq \frac{M}{\delta^2} |f(x)|.
\]
As $x \to \xi$, $|f(x)| \to 0$.
Therefore,
$|g'(x)| \to 0$ or $g'(x) \to 0$ as $x \to \xi$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
It is clearly that $f(x) = 0$ if and only if $x = 0$.
Write $\xi = 0$.

\item[(2)]
Note that
\[
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} = -2 x_n,
\]
or
\[
  x_n = (-2)^{n-1} x_1
\]
for any $x_1 \in (\xi, \infty)$ where $n = 1, 2, 3, \ldots$.
Hence, the sequence $\{x_n\}$ does not converge for any choice of $x_1 \in (\xi, \infty)$.
In this case we cannot find $\xi$ satisfying $f(\xi) = 0$ by Newton's method.

\item[(3)]
In fact,
\[
  f'(x) = \frac{1}{3} x^{-\frac{2}{3}} \to 0 \text{ as } x \to \pm \infty.
\]
Hence such $\delta > 0$ satisfying $f'(x) \geq \delta > 0$ does not exist.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.26.}
\addcontentsline{toc}{subsection}{Exercise 5.26.}
\emph{Suppose $f$ is differentiable on $[a,b]$, $f(a) = 0$,
and there is a real number $A$ such that $|f'(x)| \leq A|f(x)|$ on $[a,b]$.
Prove that $f(x)=0$ for all $x \in [a,b]$.
(Hint: Fix $x_0 \in [a,b]$, let
\[
  M_0 = \sup{|f(x)|}, \qquad M_1 = \sup{|f'(x)|}
\]
for $a \leq x \leq x_0$.
For any such $x$,
\[
  |f(x)| \leq M_1(x_0-a) \leq A(x_0-a) M_0.
\]
Hence $M_0 = 0$ if $A(x_0-a) < 1$.
That is, $f = 0$ on $[a,x_0]$.
Proceed.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
If $A = 0$, then $f'(x) = 0$ or $f(x)$ is constant on $[a,b]$ (Theorem 5.11(b)).
Since $f(a) = 0$, $f(x) = 0$ on $[a,b]$.

\item[(2)]
Suppose that $A > 0$.
Fix $x_0 \in [a,b]$, let
\[
  M_0 = \sup{|f(x)|}, \qquad M_1 = \sup{|f'(x)|}
\]
for $a \leq x \leq x_0$.
Since $|f'(x)| \leq A|f(x)|$ on $[a,b]$,
\[
  |f'(x)| \leq A|f(x)| \leq A M_0.
\]
Since $A M_0$ is an upper bound for $|f'(x)|$,
\[
  M_1 \leq A M_0.
\]

\item[(3)]
Given any $x \in [a,x_0]$.
Since $f$ is differentiable on $[a,x_0] \subseteq [a,b]$,
by the mean value theorem (Theorem 5.10), there is $\xi \in (a,x)$
such that
\[
  f(x) - f(a) = f'(\xi)(x - a).
\]
Note that $f(a) = 0$ by assumption.
So that
\begin{align*}
  |f(x)|
  &= |f'(\xi)|(x - a) \\
  &\leq M_1 (x - a)
    &(\text{Definition of $M_1$}) \\
  &\leq A M_0 (x - a)
    &((2)) \\
  &\leq A M_0 (x_0 - a).
    &(x \in [a,x_0])
\end{align*}
Since $A M_0 (x_0 - a)$ is an upper bound for $|f(x)|$,
\[
  M_0 \leq A M_0 (x_0 - a).
\]
Take
\[
  x_0 = \min \left\{ \frac{1}{2A} + a, b \right\}
\]
so that $M_0 \leq A M_0 (x_0 - a) \leq \frac{M_0}{2}$.
$M_0 = 0$ or $f(x) = 0$ on $[a,x_0]$.

\item[(4)]
Take a partition
\[
  P = \{ a = x_{-1}, x_0, \ldots, x_n = b \}
\]
of $[a,b]$ such that each subinterval $[x_{i-1},x_i]$ satisfying
$\Delta x_i = x_i - x_{i-1} < \frac{1}{2A}$.
By (3), $f(x) = 0$ on $[x_{-1},x_0]$.
Apply the same argument in (3), $f(x) = 0$ on $[x_0,x_1]$.
Continue this process, $f(x) = 0$ on each subinterval and thus on the whole interval $[a,b]$.
\end{enumerate}
$\Box$ \\



\emph{Note.}
It holds for vector-valued functions too:
\begin{quote}
\emph{Suppose $\mathbf{f}$ is a vector-valued differentiable function on $[a,b]$, $f(a) = 0$,
and there is a real number $A$ such that $|\mathbf{f}'(x)| \leq A|\mathbf{f}(x)|$ on $[a,b]$.
Prove that $\mathbf{f}(x)=0$ for all $x \in [a,b]$.}
\end{quote}
The proof is similar except using Theorem 5.19
($|\mathbf{f}(b) - \mathbf{f}(a)| \leq (b-a)|\mathbf{f}'(x)|$)
in addition. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.27.}
\addcontentsline{toc}{subsection}{Exercise 5.27.}
\emph{Let $\phi$ be a real function defined on a rectangle $R$ in the plane,
given by $a \leq x \leq b$, $\alpha \leq y \leq \beta$.
A \textbf{solution} of the initial-value problem
\[
  y' = \phi(x,y), \qquad y(a) = c \qquad (\alpha \leq c \leq \beta)
\]
is, by definition, a differentiable function $f$ on $[a,b]$ such that
$f(a) = c$, $\alpha \leq f(x) \leq \beta$, and
\[
  f'(x) = \phi(x,f(x)) \qquad (a \leq x \leq b)
\]
Prove that such a problem has at most one solution if there is a constant $A$ such that
\[
  |\phi(x,y_2) - \phi(x,y_1)| \leq A|y_2 - y_1|
\]
whenever $(x,y_1) \in R$ and $(x,y_2) \in R$.
(Hint: Apply Exercise 26 to the difference of two solutions.)
Note that this uniqueness theorem does not hold for the initial-value problem
\[
  y' = y^{\frac{1}{2}}, \qquad y(0) = 0,
\]
which has two solutions: $f(x) = 0$ and $f(x) = \frac{x^2}{4}$.
Find all other solutions.} \\



\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Suppose $f_1$ and $f_2$ are two solutions of that problem.
Define $f = f_1 - f_2$.
$f$ is differentiable on $[a,b]$, $f(a) = f_1(a) - f_2(a) = c - c = 0$.
And
\begin{align*}
  |f'(x)|
  &= |f_1'(x) - f_2'(x)| \\
  &= \abs{ \phi(x,f_1(x)) - \phi(x,f_2(x)) } \\
  &\leq A | f_1(x) - f_2(x) |
\end{align*}
on $[a,b]$.
By Exercise 5.26, $f(x) = 0$ on $[a,b]$, or $f_1(x) = f_2(x)$ on $[a,b]$.

\item[(2)]
\emph{The initial-value problem
\[
  y' = y^{\frac{1}{2}}, \qquad y(0) = 0,
\]
which has two solutions: $f(x) = 0$ and $f(x) = \frac{x^2}{4}$.
Find all other solutions.} \\

  \emph{Note.}
  It does not exist a real $A$ such that $|\phi(x,y_2) - \phi(x,y_1)| \leq A|y_2 - y_1|$
  in this initial-value problem.

  \begin{enumerate}
  \item[(a)]
  Clearly, $f(x) = 0$ and $f(x) = \frac{x^2}{4}$ are two solutions for the initial-value problem.

  \item[(b)]
  Suppose $f(x) \neq 0$ on $[0,\infty)$.
  Since $f'(x) = f(x)^{\frac{1}{2}}$, $f(x) \geq 0$.
  Since $f(x)$ is continuous (Theorem 5.2), the set
  \[
    E = \{ x \in [0, \infty) : f(x) > 0 \}
  \]
  is open in $\mathbb{R}^1$ (Theorem 4.8).
  By Exercise 2.29
  we write $E$ as the union of an at most countable collection of disjoint segments,
  say
  \[
    E = \bigcup_{(a_i,b_i) \in \mathscr{C}} (a_i,b_i)
  \]
  where $\mathscr{C}$ is at most countable and all $(a_i,b_i)$ segments are disjoint.
  Note that $E$ (or $\mathscr{C}$) is nonempty.

  \item[(c)]
  For any segment $(a_i,b_i)$,
  define $g(x) = f(x)^{\frac{1}{2}}$ on $(a_i,b_i)$.
  (Clearly, $g(a_i) = f(a_i) = 0$ by the definition of $E$.)
  Thus
  \[
    g'(x) = \frac{1}{2} f(x)^{-\frac{1}{2}} f'(x) = \frac{1}{2}.
  \]
  Hence
  \[
    g(x) = \frac{1}{2} x + c
  \]
  for some constant $c \in \mathbb{R}^1$.
  So
  \[
    f(x) = g(x)^2 = \left( \frac{1}{2} x + c \right)^2.
  \]
  $f(a_i) = 0$ implies that $c = -\frac{a_i}{2}$.
  Hence
  \[
    f(x) = \frac{1}{4}(x - a_i)^2
  \]
  on $(a_i,b_i)$.

  \item[(d)]
  By (c), if $b_i < 0$ is defined as a real number,
  then $f(b_i) = 0$ by definition of $E$.
  Note that
  \[
    \lim_{x \to b_i-} f(x) = \frac{1}{4}(b_i - a_i)^2 > 0,
  \]
  which is absurd.
  Hence $b_i = \infty$ and thus $E$ is of the form
  \[
    E = (a,\infty) \qquad (a \geq 0).
  \]
  Therefore,
  \begin{equation*}
  f(x) =
    \begin{cases}
      0                    & (0 \leq x \leq a), \\
      \frac{1}{4}(x - a)^2 & (x > a \geq 0).
    \end{cases}
  \end{equation*}
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.28.}
\addcontentsline{toc}{subsection}{Exercise 5.28.}
\emph{Formulate and prove an analogous uniqueness theorem for
systems of differential equations of the form
\[
  y'_j = \phi_j(x, y_1, \ldots, y_k), \qquad
  y_j(a) = c_j \qquad
  (j = 1, \ldots, k)
\]
Note that this can be rewritten in the form
\[
  \mathbf{y}' = \bm{\phi}(x,\mathbf{y}), \qquad
  \mathbf{y}(a) = \mathbf{c}
\]
where $\mathbf{y} = (y_1, \ldots, y_k)$ ranges over a $k$-cell,
$\bm{\phi}$ is the mapping of a $(k+1)$-cell into the Euclidean $k$-space
whose components are the function $\phi_1, \ldots, \phi_k$,
and $\mathbf{c}$ is the vector $(c_1, \ldots, c_k)$.
Use Exercise 5.26, for vector-valued functions.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{A \textbf{solution} of the initial-value problem
\[
  \mathbf{y}' = \bm{\phi}(x,\mathbf{y}), \qquad
  \mathbf{y}(a) = \mathbf{c}
\]
is, by definition, a differentiable function $\mathbf{f}$ on $[a,b]$
such that $\mathbf{f}(a) = \mathbf{c}$, and
\[
  \mathbf{f}'(x) = \bm{\phi}(x,\mathbf{f}(x)) \qquad (a \leq x \leq b).
\]
Then this problem has at most one solution if there is a constant $A$ such that
\[
  \abs{ \bm{\phi}(x,\mathbf{y}_2) - \bm{\phi}(x,\mathbf{y}_1) }
  \leq A \abs{ \mathbf{y}_2 - \mathbf{y}_1 }
\]
whenever $(x,\mathbf{y}_1) \in R$ and $(x,\mathbf{y}_2) \in R$
where $R$ is a $(k+1)$-cell defined by
\[
  R = [a,b] \times [\alpha_1,\beta_1] \times \cdots \times [\alpha_k,\beta_k].
\]}
\item[(2)]
Similar to Exercise 5.27,
Suppose $\mathbf{f}_1$ and $\mathbf{f}_2$ are two solutions of that problem.
Define $\mathbf{f} = \mathbf{f}_1 - \mathbf{f}_2$.
$\mathbf{f}$ is differentiable on $[a,b]$,
$\mathbf{f}(a) = \mathbf{f}_1(a) - \mathbf{f}_2(a) = \mathbf{c} - \mathbf{c} = 0$.
And
\begin{align*}
  |\mathbf{f}'(x)|
  &= |\mathbf{f}_1'(x) - \mathbf{f}_2'(x)| \\
  &= \abs{ \bm{\phi}(x,\mathbf{f}_1(x)) - \bm{\phi}(x,\mathbf{f}_2(x)) } \\
  &\leq A | \mathbf{f}_1(x) - \mathbf{f}_2(x) |
\end{align*}
on $[a,b]$.
By Note in Exercise 5.26,
$\mathbf{f}(x) = 0$ on $[a,b]$, or $\mathbf{f}_1(x) = \mathbf{f}_2(x)$ on $[a,b]$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 5.29.}
\addcontentsline{toc}{subsection}{Exercise 5.29.}
\emph{Specialize Exercise 5.28 by considering the system
\begin{align*}
  y_j' &= y_{j+1} \qquad (j = 1, \ldots, k-1), \\
  y_k' &= f(x) - \sum_{j=1}^{k}{g_j(x)y_j}
\end{align*}
where $f, g_1, \ldots, g_k$ are continuous real functions on $[a,b]$,
and derive a uniqueness theorem for solutions of the equation
\[
  y^{(k)} + g_k(x)y^{(k-1)} + \cdots + g_2(x)y' + g_1(x)y = f(x),
\]
  subject to initial conditions
\[
  y(a) = c_1, \qquad
  y'(a) = c_1, \qquad
  \ldots, \qquad
  y^{(k-1)}(a) = c_k.
\]}

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Write
\begin{align*}
  \mathbf{y}
  &= (y_1, \ldots, y_k) \\
  &= \left(y, y', y'', \ldots, y^{(k-1)}\right), \\
  \bm{\phi}(x, \mathbf{y})
  &= \left(y_2, y_3, \ldots, y_{k-1}, f(x) - \sum_{j=1}^{k}{g_j(x)y_j}\right) \\
  &= \left(y', y'', \ldots, y^{(k-1)}, f(x) - \sum_{j=1}^{k}{g_j(x)y^{(j-1)}}\right), \\
  \mathbf{c}
  &= (c_1, \ldots, c_k).
\end{align*}
So that
\[
  \mathbf{y}' = \bm{\phi}(x,\mathbf{y}), \qquad
  \mathbf{y}(a) = \mathbf{c}
\]
where $\mathbf{y}$ ranges over a $k$-cell $R$.

\item[(2)]
\emph{To show that the problem has at most one solution, by Exercise 5.28
it suffices to show that there is a constant $A$ such that
\[
  \abs{ \bm{\phi}(x,\mathbf{y}) - \bm{\phi}(x,\mathbf{z}) }
  \leq A \abs{ \mathbf{y} - \mathbf{z} }
\]
whenever $(x,\mathbf{y}) \in R$ and $(x,\mathbf{z}) \in R$.}

\item[(3)]
Since all $g_j$ ($1 \leq j \leq k$) are real continuous functions on a compact set $[a,b]$,
all $g_j$ are bounded (Theorem 4.15),
say $|g_j| \leq M$ on $[a,b]$ for some $M_j \in \mathbb{R}^1$ ($1 \leq j \leq k$).

\item[(4)]
Write $\mathbf{y} = (y_{1}, \ldots, y_{k})$
and $\mathbf{z} = (z_{1}, \ldots, z_{k})$.
So
\begin{align*}
  &\abs{ \bm{\phi}(x,\mathbf{y}) - \bm{\phi}(x,\mathbf{z}) }^2 \\
  =&
  \abs{ \left(y_{2} - z_{2}, y_{3} - z_{3}, \ldots, y_{k-1} - z_{k-1},
    - \sum_{j=1}^{k}{g_j(x)(y_{j} - z_{j})}\right) }^2 \\
  =&
  \sum_{j=2}^{k-1} (y_{j} - z_{j})^2
    + \left( -\sum_{j=1}^{k}{g_j(x)(y_{j} - z_{j})} \right)^2 \\
  \leq&
  \sum_{j=2}^{k-1} (y_{j} - z_{j})^2
    + \sum_{j=1}^{k} g_j(x)^2 \sum_{j=1}^{k}(y_{j} - z_{j})^2
    &(\text{Theorem 1.35}) \\
  \leq&
  \sum_{j=2}^{k-1} (y_{j} - z_{j})^2
    + \sum_{j=1}^{k} M_j^2 \sum_{j=1}^{k}(y_{j} - z_{j})^2
    &((3)) \\
  \leq&
  \sum_{j=1}^{k} (y_{j} - z_{j})^2
    + \sum_{j=1}^{k} M_j^2 \sum_{j=1}^{k}(y_{j} - z_{j})^2
    &(x^2 \geq 0 \: \forall x \in \mathbb{R}^1) \\
  \leq&
   \left( 1 + \sum_{j=1}^{k} M_j^2 \right) |\mathbf{y} - \mathbf{z}|^2.
\end{align*}
Hence
$\abs{ \bm{\phi}(x,\mathbf{y}) - \bm{\phi}(x,\mathbf{z}) }
\leq A \abs{ \mathbf{y} - \mathbf{z} }$
for some $A = \left( 1 + \sum_{j=1}^{k} M_j^2 \right)^{\frac{1}{2}}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 6: The Riemann-Stieltjes Integral \\}
\addcontentsline{toc}{section}{Chapter 6: The Riemann-Stieltjes Integral}



\subsection*{Supplement (Another definition of Riemann-Stieltjes integral).}
\addcontentsline{toc}{subsection}{Supplement (Another definition of Riemann-Stieltjes integral).}
\emph{(Exercise 7.3, 7.4 of the book
T. M. Apostol, Mathematical Analysis, Second Edition.)
Let $P$ be a partition of $[a, b]$.
The norm of a partition $P$ is the length of the largest subinterval $[x_{i-1}, x_i]$
of $P$ and is denoted by $\Vert P \Vert$.} \\

\emph{We say $f \in \mathscr{R}(\alpha)$
if there exists $A \in \mathbb{R}$ having the property that
for any $\varepsilon > 0$, there exists $\delta > 0$ such that
for any partition $P$ of $[a, b]$ with norm $\Vert P \Vert < \delta$
and for any choice of $t_i \in [x_{i-1}, x_i]$,
we have $|\sum_{i = 1}^{n} f(t_i) \Delta \alpha_i - A| < \varepsilon$.} \\\\

\textbf{Claim.}
\emph{$f \in \mathscr{R}$ in the sense of Definition 6.2
implies that
$f \in \mathscr{R}$ in the sense of this another definition.} \\

\emph{Proof of Claim.}
Let $A = \int f dx$, $M > 0$ be one upper bound of $|f|$ on $[a, b]$.
Given $\varepsilon > 0$, there exists a partition
$P_0 = \{a = x_0, x_1, \ldots, x_{N-1}, x_N = b \}$
such that
$U(P_0, f) \leq A + \frac{\varepsilon}{2}$.
Let $\delta = \frac{\varepsilon}{2MN} > 0$.
Then for any partition $P$ with norm $\Vert P \Vert < \delta$, write
$$U(P, f) = \sum_{i = 1}^{n} M_i \Delta x_i = S_1 + S_2,$$
where
$S_1$ is the sum of terms arising from those subintervals of $P$ containing no point of $P_0$,
$S_2$ is the sum of the remaining terms.
Then
\begin{align*}
S_1 &\leq U(P_0, f) < A + \frac{\varepsilon}{2}, \\
S_2 &\leq NM \Vert P \Vert < NM \delta < \frac{\varepsilon}{2}.
\end{align*}
Therefore, $U(P, f) < A + \varepsilon$.
Similarly, $L(P, f) > A - \varepsilon$ whenever $\Vert P \Vert < \delta'$.
Hence, $|\sum_{i = 1}^{n} f(t_i) \Delta x_i - A| < \varepsilon$
whenever $\Vert P \Vert < \min\{\delta, \delta'\}$.
(Copy Apostol's hint and ensure $M > 0$. $M$ in Apostol's hint might be zero if $f = 0$.)
$\Box$ \\



This supplement will be used in computing
$\int_0^{\infty} (\frac{\sin x}{x})^2 dx = \frac{\pi}{2}$ in Exercise 8.12. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.1.}
\addcontentsline{toc}{subsection}{Exercise 6.1.}
\emph{Suppose $\alpha$ increases on $[a, b]$, $a \leq  x_0 \leq b$,
$\alpha$ is continuous at $x_0$, $f(x_0) = 1$, and $f(x) = 0$ if $x \neq x_0$.
Prove that $f \in \mathscr{R}(\alpha)$ and that $\int f d \alpha = 0$.} \\

Given any partition $P = \{a = p_0, p_1, \ldots, p_{n-1}, p_n = b \}$,
where $a = p_0 \leq p_1 \leq \cdots \leq p_{n-1} \leq p_n = b$.
We might compute $L(P, f, \alpha)$ and $U(P, f, \alpha)$ by using $\varepsilon$-$\delta$ argument
since we are hinted by the condition that $\alpha$ is continuous.
A function which is continuous at $x_0$ has a nice property near $x_0$
and this property would help us estimate $U(P, f, \alpha)$ near $x_0$.
On the contrary, if both $f$ and $\alpha$ are discontinuous at $x_0$,
it might be $f \not\in \mathscr{R}(\alpha)$.
Besides, if $f$ has too many points of discontinuity
($f(x) = 0$ if $x \in \mathbb{Q}$ and $f(x) = 1$ otherwise, for example),
then $f$ might not be Riemann-integrable on $[0, 1]$. \\\\



\textbf{Claim 1.}
\emph{$L(P, f, \alpha) = 0$.} \\

\emph{Proof of Claim 1.}
$m_i = 0$ since $\inf f(x) = 0$ on any subinterval of $[a, b]$.
So $L(P, f, \alpha) = \sum m_i \Delta \alpha_i = 0$.
Here we don't need the condition that $\alpha$ is continuous at $x_0$.
$\Box$ \\\\



\textbf{Claim 2.}
\emph{For any $\varepsilon > 0$,
there exists a partition $P$ such that $U(P, f, \alpha) < \varepsilon$.} \\

\emph{Proof of Claim 2.}
Say $x_0 \in [p_{i_0 - 1}, p_{i_0}]$ for some $i_0$.
Then
\begin{equation*}
  M_i = \sup_{p_{i - 1} \leq x \leq p_i} f(x) =
    \begin{cases}
      0 & \text{ if $i \neq i_0$}, \\
      1 & \text{ if $i = i_0$}.
    \end{cases}
\end{equation*}
So
$$U(P, f, \alpha) = \sum M_i \Delta \alpha_i = \Delta \alpha_{i_0}.$$
It is not true for any arbitrary $\alpha$. (For example, $\alpha$ has a jump on $x = x_0$.)
In fact, Exercise 6.3 shows this.
Luckily, $\alpha$ is continuous at $x_0$. So for $\varepsilon > 0$,
there exists $\delta > 0$ such that $|\alpha(x) - \alpha(x_0)| < \frac{\varepsilon}{2}$
whenever $|x - x_0| < \delta$ (and $x \in [a, b]$).
Now we pick a nice partition
$$P = \{ a, x_0 - \delta_1, x_0 + \delta_2, b \},$$
where $\delta_1 = \min\{\delta, x_0 - a\} \geq 0$
and $\delta_2 = \min\{\delta, b - x_0\} \geq 0$.
(It is a trick about resizing ``$\delta$''
to avoid considering the edge cases $x_0 = a$ or $x_0 = b$ or $a = b$.)
Then $x_0 \in [x_0 - \delta_1, x_0 + \delta_2]$
and $\Delta \alpha$ on $[x_0 - \delta_1, x_0 + \delta_2]$ is
\begin{align*}
\alpha(x_0 + \delta_2) - \alpha(x_0 - \delta_1)
&= (\alpha(x_0 + \delta_2) - \alpha(x_0)) + (\alpha(x_0) - \alpha(x_0 - \delta_1)) \\
&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{align*}
Therefore, $U(P, f, \alpha) < \varepsilon$.
$\Box$ \\\\



\emph{Proof (Definition 6.2).}
By Claim 1 and 2 and notice that $U(P, f, \alpha) \geq 0$ for any partition $P$,
\begin{align*}
\upint_a^b f d\alpha &= \inf U(P, f, \alpha) = 0, \\
\lowint_a^b f d\alpha &= \sup L(P, f, \alpha) = 0,
\end{align*}
the inf and sup again being taken over all partitions.
Hence $f \in \mathscr{R}(\alpha)$ and that $\int f d \alpha = 0$ by Definition 6.2.
$\Box$ \\

\emph{Proof (Theorem 6.6).}
By Claim 1 and 2,
$$0 \leq U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon.$$
Hence $f \in \mathscr{R}(\alpha)$ by Theorem 6.6.
Furthermore,
$$\int f d \alpha = \lowint_a^b f d\alpha = \sup L(P, f, \alpha) = 0.$$
$\Box$ \\

\emph{Proof (Theorem 6.10).}
$f \in \mathscr{R}(\alpha)$ by Theorem 6.10.
Thus, by Claim 1
$$\int f d \alpha = \lowint_a^b f d\alpha = \sup L(P, f, \alpha) = 0.$$
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.2.}
\addcontentsline{toc}{subsection}{Exercise 6.2.}
\emph{Suppose $f \geq 0$,
$f$ is continuous on $[a,b]$, and $\int_{a}^{b} f(x) dx = 0$.
Prove that $f(x) = 0$ for all $x \in [a,b]$.
(Compare with Exercise 6.1.)} \\

For one application, see Exercise 7.20. \\

\emph{Proof.}
(Reductio ad absurdum)
If there were $p \in [a,b]$ such that $f(p) > 0$.
Since $f$ is continuous on $[a,b]$, given $\varepsilon = \frac{1}{64}f(p) > 0$
there exists $\delta > 0$
such that
\[
  |f(x) - f(p)| \leq \frac{1}{64}f(p) \text{ whenever } |x-p| \leq \delta, x \in [a,b].
\]
Hence
\[
  f(x) \geq \frac{63}{64}f(p)
\]
whenever $x \in E = [\max\{a,p-\delta\}, \min\{b,p+\delta\}] \subseteq [a,b]$.
Note that the length of $E$ is $|E| > 0$.
So
\[
  0
  = \int_{a}^{b} f(x) dx
  \geq \int_{E} f(x) dx
  \geq \int_{E} \frac{63}{64}f(p) dx
  = \frac{63}{64}f(p)|E| > 0,
\]
which is absurd.
$\Box$ \\

\emph{Note.}
(Lebesgue integral)
\emph{Let $f$ be a nonnegative measurable function.
Then $\int f = 0$ implies $f = 0$ a.e.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.3.}
\addcontentsline{toc}{subsection}{Exercise 6.3.}
\emph{Define three functions $\beta_1$, $\beta_2$, $\beta_3$ as follows:
$\beta_j(x) = 0$ if $x < 0$, $\beta_j(x) = 1$ if $x > 0$ for $j=1,2,3$;
and $\beta_1(0) = 0$, $\beta_2(0) = 1$, $\beta_3(0) = \frac{1}{2}$.
Let $f$ be a bounded functions on $[-1,1]$.}
\begin{enumerate}
\item[(a)]
  \emph{Prove that $f \in \mathscr{R}(\beta_1)$ if and only if $f(0+) = f(0)$ and that then
  \[
    \int f d\beta_1 = f(0).
  \]}
\item[(b)]
  \emph{State and prove a similar result for $\beta_2$.}

\item[(c)]
  \emph{Prove that $f \in \mathscr{R}(\beta_3)$ if and only if $f$ is continuous at $0$.}

\item[(d)]
  \emph{If $f$ is continuous at $0$ prove that
  \[
    \int f d\beta_1 = \int f d\beta_2 = \int f d\beta_3 = f(0).
  \]} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Given any $\delta > 0$,
  we have
  \[
    |f(x) - f(0)| \leq \sup_{x\in[0,\delta]} f(x) - \inf_{x\in[0,\delta]} f(x)
  \]
  if $x \in [0,\delta]$.

\item[(2)]
  Given any $\varepsilon > 0$ and $\delta > 0$.
  \emph{Show that if $f$ is bounded and $|f(x) - f(0)| < \varepsilon$ on $[0,\delta]$ then
  \[
    \sup_{x\in[0,\delta]} f(x) - \inf_{x\in[0,\delta]} f(x) < 2\varepsilon.
  \]}

  Since $f$ is bounded, there exists $x_1,x_2 \in [0,\delta]$
  such that
  \[
    f(x_1) = \sup_{x\in[0,\delta]} f(x) \:\: \text{ and } \:\:
    f(x_2) = \inf_{x\in[0,\delta]} f(x).
  \]
  By assumption,
  \[
    f(x_1) - f(x_2) \leq |f(x_1) - f(0)| + |f(0) - f(x_2)| < 2\varepsilon.
  \]

\item[(3)]
\emph{Show that $f \in \mathscr{R}(\beta_1)$ iff $f(0+) = f(0)$.}
  \begin{align*}
    &f \in \mathscr{R}(\beta_1) \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ such that
      $U(P,f,\beta_1) - L(P,f,\beta_1) < \varepsilon$}
        &\text{(Theorem 6.6)} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $U(P,f,\beta_1) - L(P,f,\beta_1) < \varepsilon$}
        &\text{(Theorem 6.4)} \\
      &\text{where $P = \{-1 = x_0 < x_1 < \ldots < x_k = 0 < \ldots < x_n = 1\}$} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $M_{k+1} - m_{k+1} < \varepsilon$} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $\sup_{x\in[0,\delta]} f(x) - \inf_{x\in[0,\delta]} f(x) < \varepsilon$} \\
      &\text{where $[x_k,x_{k+1}] = [0,\delta]$, $\delta > 0$} \\
      &\text{(Take $P = \{-1,0,\delta,1\}$ in ``$\Longleftarrow$'' direction)} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $\delta > 0$ such that
      $|f(x) - f(0)| < \varepsilon$ whenever $x \in [0,\delta]$}
        &((1)(2)) \\
      &\text{(Replace $\varepsilon$ by $\frac{\varepsilon}{2}$ in ``$\Longleftarrow$'' direction)} \\
    \Longleftrightarrow&
      \lim_{x \to 0+} f(x) = f(0).
  \end{align*}

\item[(4)]
  \emph{Show that $\int f d\beta_1 = f(0)$ if $f \in \mathscr{R}(\beta_1)$.}
  By (3) and Theorem 6.7,
  \[
    \abs{ f(0) - \int_{a}^{b} f d\beta_1 } < \varepsilon.
  \]
  Since $\varepsilon$ is arbitrary, $\int f d\beta_1 = f(0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\emph{Show that $f \in \mathscr{R}(\beta_2)$ if and only if $f(0-) = f(0)$ and that then
\[
  \int f d\beta_2 = f(0).
\]}

Similar to (a).
\begin{enumerate}
\item[(1)]
  Given any $\delta > 0$,
  we have
  \[
    |f(x) - f(0)| \leq \sup_{x\in[-\delta,0]} f(x) - \inf_{x\in[-\delta,0]} f(x)
  \]
  if $x \in [-\delta,0]$.

\item[(2)]
  Given any $\varepsilon > 0$ and $\delta > 0$.
  \emph{Show that if $f$ is bounded and $|f(x) - f(0)| < \varepsilon$ on $[-\delta,0]$ then
  \[
    \sup_{x\in[-\delta,0]} f(x) - \inf_{x\in[-\delta,0]} f(x) < 2\varepsilon.
  \]}

  Since $f$ is bounded, there exists $x_1,x_2 \in [-\delta,0]$
  such that
  \[
    f(x_1) = \sup_{x\in[-\delta,0]} f(x) \:\: \text{ and } \:\:
    f(x_2) = \inf_{x\in[-\delta,0]} f(x).
  \]
  By assumption,
  \[
    f(x_1) - f(x_2) \leq |f(x_1) - f(0)| + |f(0) - f(x_2)| < 2\varepsilon.
  \]

\item[(3)]
\emph{Show that $f \in \mathscr{R}(\beta_1)$ iff $f(0-) = f(0)$.}
  \begin{align*}
    &f \in \mathscr{R}(\beta_2) \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ such that
      $U(P,f,\beta_2) - L(P,f,\beta_2) < \varepsilon$}
        &\text{(Theorem 6.6)} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $U(P,f,\beta_2) - L(P,f,\beta_2) < \varepsilon$}
        &\text{(Theorem 6.4)} \\
      &\text{where $P = \{-1 = x_0 < x_1 < \ldots < x_k = 0 < \ldots < x_n = 1\}$} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $M_{k} - m_{k} < \varepsilon$} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $P$ containing $0$ such that
      $\sup_{x\in[-\delta,0]} f(x) - \inf_{x\in[-\delta,0]} f(x) < \varepsilon$} \\
      &\text{where $[x_{k-1},x_{k}] = [-\delta,0]$, $\delta > 0$} \\
      &\text{(Take $P = \{-1,-\delta,0,1\}$ in ``$\Longleftarrow$'' direction)} \\
    \Longleftrightarrow&
      \text{$\forall \varepsilon > 0$ there is $\delta > 0$ such that
      $|f(x) - f(0)| < \varepsilon$ whenever $x \in [-\delta,0]$}
        &((1)(2)) \\
      &\text{(Replace $\varepsilon$ by $\frac{\varepsilon}{2}$ in ``$\Longleftarrow$'' direction)} \\
    \Longleftrightarrow&
      \lim_{x \to 0-} f(x) = f(0).
  \end{align*}

\item[(4)]
  \emph{Show that $\int f d\beta_2 = f(0)$ if $f \in \mathscr{R}(\beta_2)$.}
  By (3) and Theorem 6.7,
  \[
    \abs{ f(0) - \int_{a}^{b} f d\beta_2 } < \varepsilon.
  \]
  Since $\varepsilon$ is arbitrary, $\int f d\beta_2 = f(0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
Note that $f$ is continuous at $0$ iff $f(0+) = f(0-) = f(0)$.
Apply the same argument in (a) and (b),
we have $f \in \mathscr{R}(\beta_3)$ if and only if $f(0+) = f(0-) = f(0)$.
$\Box$ \\



\emph{Proof of (d).}
\emph{It suffices to show that
\[
  \int_{a}^{b} f d\beta_3 = f(0).
\]}

We can apply Theorem 6.12(d)(e) to $\beta_3 = \frac{1}{2}(\beta_1+\beta_2)$.
That is,
\[
  \int_{a}^{b} f d\beta_3
  = \frac{1}{2}\left[ \int_{a}^{b} f d\beta_1 + \int_{a}^{b} f d\beta_2 \right]
  = \frac{1}{2}[f(0) + f(0)] = f(0).
\]

Or apply the same argument in (a) and (b) to get
\[
  \abs{ f(0) - \int_{a}^{b} f d\beta_3 } < \varepsilon
\]
for any $\varepsilon > 0$, or $\int_{a}^{b} f d\beta_3 = f(0)$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.4.}
\addcontentsline{toc}{subsection}{Exercise 6.4.}
\emph{If
\begin{equation*}
  f(x) =
    \begin{cases}
      0 & \text{ for all irrational $x$}, \\
      1 & \text{ for all rational $x$},
    \end{cases}
\end{equation*}
prove that $f \not\in \mathscr{R}$ on $[a,b]$ for any $a < b$.} \\

\emph{Proof.}
Given any partition
\[
  P = \{a = p_0, p_1, \ldots, p_{n-1}, p_n = b \}
\]
of $[a,b]$ where $a = p_0 \leq p_1 \leq \cdots \leq p_{n-1} \leq p_n = b$.
Since $a < b$, we might assume that $a = p_0 < p_1 < \cdots < p_{n-1} < p_n = b$
by removing duplicated points.
Since $\mathbb{Q}$ and $\mathbb{R} - \mathbb{Q}$ are dense in $\mathbb{R}$, we have
\begin{align*}
  M_i &= \sup_{p_{i-1} \leq x \leq p_i} f(x) = 1, \\
  m_i &= \inf_{p_{i-1} \leq x \leq p_i} f(x) = 0, \\
  U(P,f) &= \sum_{i=1}^{n} M_i \Delta x_i = \sum_{i=1}^{n} \Delta x_i = b - a, \\
  L(P,f) &= \sum_{i=1}^{n} m_i \Delta x_i = \sum_{i=1}^{n} 0 = 0.
\end{align*}
Since $P$ is arbitrary,
\begin{align*}
  \upint_a^b f dx &= \inf U(P,f) = b-a > 0, \\
  \lowint_a^b f dx &= \sup L(P,f) = 0.
\end{align*}
Hence $f \not\in \mathscr{R}$ on $[a,b]$ for any $a < b$.
$\Box$ \\

\emph{Note.}
\begin{enumerate}
\item[(1)]
  (Lebesgue integral)
  $f$ is Lebesgue integrable.

\item[(2)]
  $f \in \mathscr{R}$ on $[a,b]$ iff $a = b$.

\item[(3)]
  (Problem 4.1 in \emph{H. L. Royden, Real Analysis, 3rd edition}.)
  \emph{Construct a sequence $\{f_n\}$ of nonnegative,
  Riemann integrable functions such that $f_n$ increases monotonically to $f$.
  What does this imply about changing the order of integration and the limiting process?}
  (Since $\mathbb{Q}$ is countable, write
  \[
    \mathbb{Q} = \{ r_1, r_2, \ldots \}.
  \]
  Define
  \begin{equation*}
    f_n(x) =
      \begin{cases}
        0 & \text{ if $x \not\in \{ r_1, \ldots, r_n \}$ }, \\
        1 & \text{ if $x \in \{ r_1, \ldots, r_n \}$ }.
      \end{cases}
  \end{equation*}
  By construction, $f_n$ increases monotonically to $f$ pointwise.
  Note that $f_n \to f$ not uniformly.
  Also, $\int_{a}^{b} f_n(x) dx = 0$ by using the same argument in Theorem 6.10.
  Therefore,
  $\lim_{n \to \infty} \int_{a}^{b} f_n(x) dx = 0$
  but $\int_{a}^{b} \lim_{n \to \infty} f_n(x) dx = \int_{a}^{b} f(x) dx$
  does not exist.) \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.5.}
\addcontentsline{toc}{subsection}{Exercise 6.5.}
\emph{Suppose $f$ is a bounded real function on $[a,b]$,
and $f^2 \in \mathscr{R}$ on $[a,b]$.
Does it follow that $f \in \mathscr{R}$?
Does the answer change if we assume that $f^3 \in \mathscr{R}$?} \\

Actually we can omit the boundedness assumption of $f$
since $f^2 \in \mathscr{R}$ or $f^3 \in \mathscr{R}$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $f^2 \in \mathscr{R}$ on $[a,b]$ does not imply that
$f \in \mathscr{R}$ (unless $f \geq 0$ on $[a,b]$).}
Similar to Exercise 6.4,
define
\begin{equation*}
  f(x) =
    \begin{cases}
      -1 & \text{ for all irrational $x$}, \\
      1 & \text{ for all rational $x$}.
    \end{cases}
\end{equation*}
$f^2 = 1 \in \mathscr{R}$ on $[a,b]$ but
$f \not\in \mathscr{R}$ on $[a,b]$ for any $a < b$.
(The proof for the ``unless'' part is similar to (2).)

\item[(2)]
\emph{Show that $f^3 \in \mathscr{R}$ on $[a,b]$ implies that
$f \in \mathscr{R}$.}
Let $\phi(x) = x^{\frac{1}{3}}$ on $\mathbb{R}$.
By Theorem 6.11, $f(x) = \phi(f(x)^3) \in \mathscr{R}$.
(The boundedness condition in Theorem 6.11 is unnecessary.)
\end{enumerate}
$\Box$ \\



\emph{Note.}
(Lebesgue integral)
\emph{Suppose that $f^2$ is Lebesgue integrable.
Does it follow that $f$ is Lebesgue integrable?
Does the answer change if we assume that $f^3$ is Lebesgue integrable?}
Both answers are no. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.6.}
\addcontentsline{toc}{subsection}{Exercise 6.6.}
\emph{Let $P$ be the Cantor set constructed in Sec. 2.44.
Let $f$ be a bounded real function on $[0,1]$ which is continuous at every point outside $P$.
Prove that $f \in \mathscr{R}$ on $[0,1]$.
(Hint: $P$ can be covered by finitely many segments
whose total length can be made as small as desired. Proceed as in Theorem 6.10.)} \\



\emph{Proof (Theorem 6.10).}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Note that in Section 2.44, we have
\[
  P = \bigcap_{n=1}^{\infty} E_n
\]
and each $E_n$ is the union of $2^n$ intervals, each of length $\frac{1}{3^n}$.
For each interval $[u_j,v_j] \subseteq E_n$ of $E_n$ ($1 \leq j \leq 2^n$),
we construct a slightly larger open set
\[
  (u_j-\lambda,v_j+\lambda) \supsetneq [u,v]
\]
where $\lambda = \frac{1}{2}\left( \frac{1}{2.28^n} - \frac{1}{3^n} \right) > 0$.
Each length of $(u_j-\lambda,v_j+\lambda)$ is $\frac{1}{2.28^n}$.
Write
\[
  G_n = \bigcup_{1 \leq j \leq 2^n} (u_j-\lambda,v_j+\lambda).
\]
Hence
\[
  G_n \supsetneq \bigcup_{1 \leq j \leq 2^n} [u_j,v_j] = E_n \supseteq P,
\]
and the total length $|G_n|$ of $G_n$ satisfies
\[
  |G_n|
  \leq \sum_{1 \leq j \leq 2^n} |(u_j-\lambda,v_j+\lambda)|
  = \left(\frac{2}{2.28}\right)^n.
\]
(Two different subintervals might be overlapped.)
As $n \to \infty$, $P$ can be covered by finitely many open segments
whose total length can be made as small as desired.
Now we take an integer $N$ such that
$\left(\frac{2}{2.28}\right)^N < \frac{\varepsilon}{64(M+1)}$.

\item[(2)]
Let $K = [0,1] - G_N$ be a compact set (Theorem 2.35).
By construction, $f$ is continuous on $K$ and thus $f$ is uniformly continuous.
So there is $\delta > 0$ such that $|f(s) - f(t)| < \frac{\varepsilon}{89}$
if $s, t \in K$ and $|s-t| < \delta$.

\item[(3)]
Now we construct a partition $P = \{x_0, x_1, \ldots, x_n\}$ of $[a,b]$,
as the following steps:
  \begin{enumerate}
  \item[(a)]
  Put $\frac{0}{m}, \frac{1}{m}, \ldots, \frac{m}{m}$
  in $P$ for some integer $m \geq \frac{1}{\delta}$.

  \item[(b)]
  Put $u_j-\lambda$ and $v_j+\lambda$ in $P$.

  \item[(c)]
  Remove any points in the segment $(u_j-\lambda,v_j+\lambda)$
  except $0$ and $1$.
  \end{enumerate}

\item[(4)]
  Note that $M_i - m_i \leq 2M$ $(1 \leq i \leq n)$ where $M = \sup|f(x)|$ is defined.
  Hence,
  \[
    U(P,f) - L(P,f)
    \leq \frac{\varepsilon}{89} + 2M \cdot \frac{\varepsilon}{64(M+1)}
    \leq \varepsilon.
  \]
  Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Lebesgue's criterion for Riemann-integrability).}
\emph{(Theorem 11.33.) Let $f$ be a bounded real function on $[a,b]$
and let $D$ be the set of discontinuities of $f$ in $[a,b]$.
Then $f \in \mathscr{R}$ on $[a,b]$ if and only if $D$ has measure zero.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.7.}
\addcontentsline{toc}{subsection}{Exercise 6.7.}
\emph{Suppose $f$ is a real function on $(0,1]$ and $f \in \mathscr{R}$ on $[c,1]$
for every $c > 0$.
Define
\[
  \int_{0}^{1}f(x)dx = \lim_{c \to 0} \int_{c}^{1}f(x)dx
\]
if this limit exists (and is finite).}
\begin{enumerate}
  \item[(a)]
  \emph{If $f \in \mathscr{R}$ on $[0,1]$,
  show that this definition of the integral agrees with the old one.}

  \item[(b)]
  \emph{Construct a function such that the above limit exists,
  although it fails to exist with $|f|$ in place of $f$.} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since $f \in \mathscr{R}$ on $[0,1]$, $f$ is bounded or
  $|f| \leq M$ for some real $M$.

\item[(2)]
For any $0 < c < 1$,
we have
\begin{align*}
  \abs{ \int_{0}^{1} f(x)dx - \int_{c}^{1} f(x)dx }
  &= \abs{ \int_{0}^{c} f(x)dx }
    &\text{(Theorem 6.12(c))}\\
  &\leq Mc.
    &\text{(Theorem 6.12(d))}
\end{align*}

\item[(3)]
Given any $\varepsilon > 0$, there exists $\delta = \frac{\varepsilon}{M+1} > 0$
such that
\[
  \abs{ \int_{0}^{c} f(x)dx - \int_{0}^{1} f(x)dx }
  \leq Mc
  < M \delta
  = M \cdot \frac{\varepsilon}{M+1}
  < \varepsilon
\]
whenever $0 < c < \delta$.
Hence $\lim_{c \to 0} \int_{0}^{c} f(x)dx = \int_{0}^{1} f(x)dx$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b)(Construct by nonabsolutely convergent series).}
\begin{enumerate}
\item[(1)]
Given any nonabsolutely (conditionally) convergent series $\sum_{k=1}^{n} a_k$
(take $\sum \frac{(-1)^n}{n}$ for example and then see Remark 3.46),
we define $f$ on $(0,1]$ by
\[
  f(x) = 2^{n} a_n
\]
if $\frac{1}{2^{n}} < x \leq \frac{1}{2^{n-1}}$ as $n = 1,2,\ldots$.

\item[(2)]
By construction,
\[
  \int_{\frac{1}{2^{n}}}^{\frac{1}{2^{n-1}}} f(x) dx
  = \left( \frac{1}{2^{n-1}} - \frac{1}{2^{n}}\right) 2^n a_n = a_n.
\]
and thus
\[
  \int_{\frac{1}{2^{n}}}^{1} f(x) dx
  = \int_{\frac{1}{2^{n}}}^{\frac{1}{2^{n-1}}} f(x) dx
    + \cdots
    + \int_{\frac{1}{2}}^{1} f(x) dx
  = \sum_{k=1}^{n} a_k.
\]

\item[(3)]
Given any $\varepsilon > 0$.
Since $\sum a_n$ is convergent, there exists a common integer $N$
such that
\[
  |a_n| \leq \frac{\varepsilon}{89}
\]
and
\[
  \abs{ \sum_{k=1}^{n} a_k - A } \leq \frac{\varepsilon}{64}
\]
for some real $A$ whenever $n \geq N$
(Definition 3.21 and Theorem 3.23).
Therefore, for any $0 < c \leq \frac{1}{2^N}$,
say $\frac{1}{2^{n+1}} < c \leq \frac{1}{2^n} \leq \frac{1}{2^N}$ for some $n \geq N$,
we have
\begin{align*}
  \abs{ \int_{c}^{1} f(x) dx - A }
  &= \abs{ \int_{c}^{\frac{1}{2^{n}}} f(x) dx
    + \int_{\frac{1}{2^{n}}}^{1} f(x) dx
    - A } \\
  &\leq \abs{ \left( \frac{1}{2^{n}} - c \right) 2^{n+1} a_{n+1} }
    + \abs{ \sum_{k=1}^{n} a_k - A } \\
  &\leq \abs{ a_{n+1} } + \abs{ \sum_{k=1}^{n} a_k - A } \\
  &\leq \frac{\varepsilon}{89} + \frac{\varepsilon}{64} \\
  &\leq \varepsilon.
\end{align*}
Hence, $\lim_{c \to 0} \int_{c}^{1} f(x) dx = A$ exists.

\item[(4)]
Since
\[
  \int_{\frac{1}{2^{n}}}^{1} |f(x)| dx
  = \int_{\frac{1}{2^{n}}}^{\frac{1}{2^{n-1}}} |f(x)| dx
    + \cdots
    + \int_{\frac{1}{2}}^{1} |f(x)| dx
  = \sum_{k=1}^{n} |a_k| \to \infty
\]
as $n \to \infty$,
$\lim_{c \to 0} \int_{c}^{1}f(x)dx$ does not exist.
(Or show that $\lim_{c \to 0} \int_{c}^{1}f(x)dx = \infty$ by definition directly.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.8.}
\addcontentsline{toc}{subsection}{Exercise 6.8.}
\emph{Suppose $f \in \mathscr{R}$ on $[a,b]$ for every $b > a$ where $a$ is fixed.
Define
\[
  \int_{a}^{\infty} f(x) dx
  = \lim_{b \to \infty} \int^b_a f(x) dx
\]
if this limit exists (and is finite).
In that case, we say that the integral on the left \textbf{converges}.
If it also converges after $f$ has been replaced by $|f|$,
it is said to converge \textbf{absolutely}.
Assume that $f(x) \geq 0$ and that $f$ decreases monotonically on $[1,\infty)$.
Prove that
\[
  \int_1^{\infty} f(x) dx
\]
converges if and only if
\[
  \sum_{n=1}^{\infty} f(n)
\]
converges.
(This is the so-called ``integral test'' for convergence of series.)} \\



\emph{Proof.}
Similar to Exercise 8.9.
\begin{enumerate}
\item[(1)]
Define
\begin{align*}
  a_n &= \int_{1}^{n} f(x) dx, \\
  b_n &= \sum_{k=1}^{n} f(k), \\
  c_n &= b_n - a_n
\end{align*}
for $n = 1,2,3,\ldots$.

\item[(2)]
\emph{Show that $\{c_n\}$ decreases.}
Since $f$ decreases monotonically on $[1,\infty)$, we have
\begin{align*}
  c_n - c_{n+1}
  &= (b_{n} - a_{n}) - (b_{n+1} - a_{n+1}) \\
  &= (a_{n+1} - a_{n}) - (b_{n+1} - b_{n}) \\
  &= \int_{n}^{n+1} f(x) dx - f(n+1) \\
  &\geq \int_{n}^{n+1} f(n+1) dx - f(n+1) \\
  &= f(n+1) - f(n+1) \\
  &= 0.
\end{align*}

\item[(3)]
\emph{Show that $\{c_n\}$ is bounded.}
Since $f$ decreases monotonically on $[1,\infty)$,
\begin{align*}
  c_{n}
  &= b_n - a_n \\
  &= \sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx \\
  &= \sum_{k=1}^{n} f(k) - \sum_{k=1}^{n-1} \int_{k}^{k+1} f(x) dx \\
  &\geq \sum_{k=1}^{n} f(k) - \sum_{k=1}^{n-1} \int_{k}^{k+1} f(k) dx \\
  &= \sum_{k=1}^{n} f(k) - \sum_{k=1}^{n-1} f(k) \\
  &= f(n).
\end{align*}
Since $f(n)$ is nonnegative, $c_n \geq 0$.

\item[(4)]
By (2)(3), $\{c_n\}$ converges (Theorem 3.14).

\item[(5)]
Since $c_n = b_n - a_n$ and $\{c_n\}$ converges,
$\{a_n\}$ converges if and only if $\{b_n\}$ converges,
or $\int_1^{\infty} f(x) dx$ converges if and only if
$\sum_{n=1}^{\infty} f(n)$ converges.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.9.}
\addcontentsline{toc}{subsection}{Exercise 6.9.}
\emph{Show that integration by parts can sometimes be applied to the
``improper'' integrals defined in Exercise 6.7 and 6.8.
(State appropriate hypotheses, formulate a theorem, and prove it.)
For instance show that
\[
  \int_{0}^{\infty} \frac{\cos x}{1+x} dx
  = \int_{0}^{\infty} \frac{\sin x}{(1+x)^2} dx.
\]
Show that one of these integrals converges \textbf{absolutely}, but that
the other does not.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Suppose $F$ and $G$ are differentiable functions on $(0,1]$,
$F' = f \in \mathscr{R}$ on $[c,1]$ and $G' = g \in \mathscr{R}$ on $[c,1]$
for every $c > 0$.
Then
\[
  \int_{0}^{1} F(x)g(x) dx
  = F(1)G(1) - \lim_{c \to 0} F(c)G(c)
  - \int_{0}^{1} f(x)G(x) dx
\]
if any two of $\int_{0}^{1} F(x)g(x) dx$, $\int_{0}^{1} f(x)G(x) dx$
or $\lim_{c \to 0} F(c)G(c)$ exist.}
Theorem 6.22 (integration by parts) implies that
\[
  \int_{c}^{1} F(x)g(x) dx
  = F(1)G(1) - F(c)G(c)
  - \int_{c}^{1} f(x)G(x) dx.
\]
Since any two of $\int_{0}^{1} F(x)g(x) dx$ or $\int_{0}^{1} f(x)G(x) dx$
or $\lim_{c \to 0} F(c)G(c)$ exist,
the rest one exists and satisfies the identity
\[
  \int_{0}^{1} F(x)g(x) dx
  = F(1)G(1) - \lim_{c \to 0} F(c)G(c)
  - \int_{0}^{1} f(x)G(x) dx
\]
by letting $c \to 0$.

\item[(2)]
\emph{Suppose $F$ and $G$ are differentiable functions on $[a,b]$
for every $b > a$ where $a$ is fixed,
$F' = f \in \mathscr{R}$ on $[a,b]$ and $G' = g \in \mathscr{R}$ on $[a,b]$.
Then
\[
  \int_{a}^{\infty} F(x)g(x) dx
  = \lim_{b \to \infty}F(b)G(b) - F(a)G(a)
  - \int_{a}^{\infty} f(x)G(x) dx
\]
if any two of $\int_{a}^{\infty} F(x)g(x) dx$, $\int_{a}^{\infty} f(x)G(x) dx$
or $\lim_{b \to \infty}F(b)G(b)$ exist.}
Theorem 6.22 (integration by parts) implies that
\[
  \int_{a}^{b} F(x)g(x) dx
  = F(b)G(b) - F(a)G(a)
  - \int_{a}^{b} f(x)G(x) dx.
\]
Since any two of $\int_{a}^{\infty} F(x)g(x) dx$ or $\int_{a}^{\infty} f(x)G(x) dx$
or $\lim_{b \to \infty}F(b)G(b)$ exist,
the rest one exists and satisfies the identity
\[
  \int_{a}^{\infty} F(x)g(x) dx
  = \lim_{b \to \infty}F(b)G(b) - F(a)G(a)
  - \int_{a}^{\infty} f(x)G(x) dx
\]
by letting $b \to \infty$.

\item[(3)]
\emph{Show that}
\[
  \int_{0}^{\infty} \frac{\cos x}{1+x} dx
  = \int_{0}^{\infty} \frac{\sin x}{(1+x)^2} dx.
\]
Put $a = 0$, $F(x) = \frac{1}{1+x}$ and $G(x) = \sin x$ in
\[
  \int_{a}^{\infty} F(x)g(x) dx
  = \lim_{b \to \infty}F(b)G(b) - F(a)G(a)
  - \int_{a}^{\infty} f(x)G(x) dx
\]
to get
\[
  \int_{0}^{\infty} \frac{(\sin x)'}{1+x} dx
  = \lim_{b \to \infty}\frac{\sin(b)}{1+b} - \frac{\sin(0)}{1+0}
  - \int_{0}^{\infty} \left(\frac{1}{1+x}\right)' \sin x dx
\]
or
\[
  \int_{0}^{\infty} \frac{\cos x}{1+x} dx
  = \int_{0}^{\infty} \frac{\sin x}{(1+x)^2} dx.
\]

\item[(4)]
\emph{Show that
\[
  \int_{0}^{\infty} \frac{\sin x}{(1+x)^2} dx
\]
converges absolutely.}
Notice that
\begin{align*}
  \int_{0}^{\infty} \abs{\frac{\sin x}{(1+x)^2}} dx
  &\leq \int_{0}^{\infty} \frac{1}{(1+x)^2} dx \\
  &= \lim_{b \to \infty} \left[ -\frac{1}{1+x} \right]_{0}^{b} - (-1) \\
  &= 1.
\end{align*}

\item[(5)]
\emph{Show that
\[
  \int_{0}^{\infty} \frac{\cos x}{1+x} dx
\]
converges conditionally.}
By (3)(4), $\int_{0}^{\infty} \frac{\cos x}{1+x} dx$ converges.
Note that
\[
  \cos x \geq \frac{1}{2}
\]
if $x \in [-\frac{\pi}{3} + 2n\pi, \frac{\pi}{3} + 2n\pi]$ for $n = 1,2,3,\ldots$.
Hence
\begin{align*}
  \int_{0}^{\infty} \abs{\frac{\cos x}{1+x}} dx
  &\geq
  \sum_{n=1}^{\infty} \int_{-\frac{\pi}{3} + 2n\pi}^{\frac{\pi}{3} + 2n\pi}
    \abs{\frac{\cos x}{1+x}} dx \\
  &\geq
  \sum_{n=1}^{\infty} \int_{-\frac{\pi}{3} + 2n\pi}^{\frac{\pi}{3} + 2n\pi}
    \frac{\frac{1}{2}}{1+\frac{\pi}{3} + 2n\pi} dx \\
  &=
  \sum_{n=1}^{\infty} \frac{2\pi}{3} \cdot \frac{\frac{1}{2}}{1+\frac{\pi}{3} + 2n\pi} \\
  &>
  \frac{\pi}{3}
    \sum_{n=1}^{\infty} \frac{1}{\pi + \pi + 2n\pi} \\
  &=
  \frac{1}{6} \sum_{n=1}^{\infty} \frac{1}{n+1}.
\end{align*}
By Theorem 3.28, $\sum_{n=1}^{\infty} \frac{1}{n+1} = \infty$
and thus $\int_{0}^{\infty} \frac{\cos x}{1+x} dx$ does not converge absolutely.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.10.}
\addcontentsline{toc}{subsection}{Exercise 6.10.}
\emph{Let $p$ and $q$ be positive real integers such that
\[
  \frac{1}{p} + \frac{1}{q} = 1.
\]
Prove the following statements.}
\begin{enumerate}
  \item[(a)]
  \emph{If $u \geq 0$ and $v \geq 0$, then
  \[
    uv \leq \frac{u^p}{p} + \frac{v^q}{q}.
  \]
  Equality holds if and only if $u^p = v^q$.}

  \item[(b)]
  \emph{If $f \in \mathscr{R}(\alpha)$, $g \in \mathscr{R}(\alpha)$,
  $f \geq 0$, $g \geq 0$, and
  \[
    \int_{a}^{b} f^p d\alpha = \int_{a}^{b} g^q d\alpha = 1,
  \]
  then
  \[
    \int_{a}^{b} fg d\alpha \leq 1.
  \]}

  \item[(c)]
  \emph{If $f$ and $g$ are complex functions in $\mathscr{R}(\alpha)$, then
  \[
    \abs{ \int_{a}^{b} fg d\alpha }
    \leq
    \left\{ \int_{a}^{b} |f|^p d\alpha \right\}^{\frac{1}{p}}
    \left\{ \int_{a}^{b} |g|^q d\alpha \right\}^{\frac{1}{q}}.
  \]
  This is \textbf{H\"older's inequality}.
  When $p=q=2$ it is usually called the Schwarz inequality.
  (Note that Theorem 1.35 is a very special case of this.)}

  \item[(d)]
  \emph{Show that H\"older's inequality is also true for the ``improper'' integrals
  described in Exercise 6.7 and 6.8.} \\
\end{enumerate}



\emph{Proof of (a)(Young's inequality).}
\begin{enumerate}
  \item[(1)]
  $u = 0$ or $v = 0$ is nothing to do.
  For $u > 0$ and $v > 0$, we give some different proofs.

  \item[(2)]
  First proof.
  \begin{align*}
    uv
    &= \exp(\log(uv)) \\
    &= \exp(\frac{1}{p}\log(u^p) + \frac{1}{q}\log(v^q)) \\
    &\leq \frac{1}{p} \exp(\log(u^p)) + \frac{1}{q}\exp(\log(v^q))
      &\text{(Convexity of $\exp(x)$)} \\
    &= \frac{u^p}{p} + \frac{v^q}{q}.
  \end{align*}
  Here the convexity of $\exp(x)$ can be derived
  by the fact that $(\exp(x))'' > 0$ and Exercise 5.14.
  The fact that the equality holds if and only if $u^p = v^q$
  is derived from the strictly convexity of $\exp(x)$ additionally.
  (For the details about the exponential and logarithmic functions,
  might see Chapter 8.)

  \item[(3)]
  Second proof.
  \begin{align*}
    \log(\frac{u^p}{p} + \frac{v^q}{q})
    &\geq \frac{1}{p} \log(u^p) + \frac{1}{q}\log(v^q)
      &\text{(Concavity of $\log(x)$)} \\
    &= \log(u) + \log(v) \\
    &= \log(uv).
  \end{align*}
  Since $\log(x)$ increases monotonically ($(\log(x))' = \frac{1}{x} > 0$ if $x > 0$),
  $\frac{u^p}{p} + \frac{v^q}{q} \geq uv$
  (or take the exponential function to get the same conclusion).
  Here the concavity of $\log(x)$ can be derived
  by the fact that $(\log(x))'' < 0$ and a statement that
  $f''(x) \leq 0$ if and only if $f$ is concave.
  The fact that the equality holds if and only if $u^p = v^q$
  is derived from the strictly concavity of $\log(x)$ additionally.
  (The proof is analogous to Exercise 5.14.)

  \item[(4)]
  Third proof.
  \emph{Suppose that $f:[0,\infty) \to [0,\infty)$ is a strictly increasing continuous function
  such that $f(0) = 0$ and $\lim_{x \to \infty} f(x) = \infty$.
  Then
  \[
    uv \leq \int_{0}^{u} f(x)dx + \int_{0}^{v} f^{-1}(x)dx
  \]
  for every $u,v \geq 0$, and equality occurs if and only if $v = f(u)$.}
  Define
  \[
    F(x) = -xf(x) + \int_{0}^{x} f(t)dt + \int_{0}^{f(x)} f^{-1}(t)dt.
  \]
  By Theorem 6.20 (the fundamental theorem of calculus) and Theorem 5.5 (chain rule),
  \[
    F'(x) = -(f(x) + xf'(x))+ f(x) + f'(x) f^{-1}(f(x)) = 0.
  \]
  Hence $F(x)$ is a constant on $(0,u)$ (Theorem 5.11(b)).
  Note that $F(x)$ is continuous on $[0,u]$ and $F(0) = 0$,
  so $F(x) = 0$ on $[0,u]$
  or
  \[
    \int_{0}^{x} f(t)dt + \int_{0}^{f(x)} f^{-1}(t)dt = xf(x).
  \]
  Take $x = u$ to get
  \[
    \int_{0}^{u} f(x)dx + \int_{0}^{f(u)} f^{-1}(x)dx = uf(u).
  \]
  Hence
  \begin{align*}
    &\int_{0}^{u} f(x)dx + \int_{0}^{v} f^{-1}(x)dx - uv \\
    =&
    \int_{0}^{u} f(x)dx + \int_{0}^{f(u)} f^{-1}(x)dx + \int_{f(u)}^{v} f^{-1}(x)dx - uv \\
    =&
    uf(u) + \int_{f(u)}^{v} f^{-1}(x)dx - uv \\
    =&
    \int_{f(u)}^{v} [ f^{-1}(x)-f^{-1}(f(u)) ] dx \\
    \geq&
    0.
  \end{align*}
  The last inequality holds since $f$ is strictly increasing
  and thus $f^{-1}$ is strictly increasing too.
  Besides, the equality holds if and only if $f(u) = v$.
  Now the conclusion holds by taking $f(x) = x^{p-1}$ in
  \[
    uv \leq \int_{0}^{u} f(x)dx + \int_{0}^{v} f^{-1}(x)dx
  \]
  and the equality holds if and only if $u^p = v^q$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
Every integral is well-defined (Theorem 6.11 and Theorem 6.13(a)).
Let $u = f \geq 0$ and $v = g \geq 0$ in (a).
Integrate both sides of the inequality
\[
  fg \leq \frac{f^p}{p} + \frac{g^q}{q}
\]
to get
\begin{align*}
  \int_{a}^{b} fg d\alpha
  &\leq \int_{a}^{b} \left( \frac{f^p}{p} + \frac{g^q}{q} \right) d\alpha
    &\text{(Theorem 6.12(b))} \\
  &= \int_{a}^{b} \frac{f^p}{p} d\alpha + \int_{a}^{b} \frac{g^q}{q} d\alpha
    &\text{(Theorem 6.12(a))} \\
  &= \frac{1}{p} \int_{a}^{b} f^p d\alpha + \frac{1}{q} \int_{a}^{b} g^q d\alpha
    &\text{(Theorem 6.12(a))} \\
  &= \frac{1}{p} + \frac{1}{q}
    &\text{(Assumption)} \\
  &= 1.
\end{align*}
The equality holds if $f^p = g^q$.
Note that the equality does not hold only if $f^p = g^q$.
(Consider $\alpha$ is constant on some subinterval $[c,d] \subsetneq [a,b]$.)
Luckily, it is true for the additional assumption that
$\alpha(x) = x$ and $f, g$ are continuous on $[a,b]$.
$\Box$ \\



\emph{Proof of (c).}
There are three possible cases.
\begin{enumerate}
\item[(1)]
  The case $\left\{ \int_{a}^{b} |f|^p d\alpha \right\}^{\frac{1}{p}} = 0$.
  So $\int_{a}^{b} |f|^p d\alpha = 0$.

  \begin{enumerate}
  \item[(a)]
  \emph{Show that $\int_{a}^{b} |f| d\alpha = 0$ if $\int_{a}^{b} |f|^p d\alpha = 0$.}
  (Reductio ad absurdum)
  If $\int_{a}^{b} |f| d\alpha = A > 0$, then given $\varepsilon = \frac{A}{2} > 0$,
  there exists a partition $P_0 = \{a=x_0 \leq \cdots \leq x_n = b \}$ such that
  \[
    \sum_{i=0}^{n} m_i \Delta \alpha_i > \frac{A}{2},
  \]
  where $m_i = \inf_{x \in [x_{i-1},x_i]} |f|$ and
  $\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1})$.
  By the pigeonhole principle,
  there exists $1 \leq i_0 \leq n$
  such that
  \[
    L(P_0,|f|,\alpha) = m_{i_0} \Delta \alpha_{i_0} > \frac{A}{2n} > 0.
  \]
  Especially, $m_{i_0} > 0$ and $\Delta \alpha_{i_0} > 0$.
  Now we consider $L(P,|f|^p,\alpha)$.
  Hence
  \[
    L(P_0,|f|^p,\alpha)
  = \sum_{i=0}^{n} m_i^p \Delta \alpha_i
  \geq m_{i_0}^p \Delta \alpha_{i_0} > 0,
  \]
  or
  \[
    \lowint_{a}^{b}|f|d\alpha = \sup L(P,f,\alpha) \geq m_{i_0}^p \Delta \alpha_{i_0} > 0,
  \]
  which is absurd.

  \item[(b)]
  \emph{Show that $\int_{a}^{b} |fg| d\alpha = 0$ if $\int_{a}^{b} |f| d\alpha = 0$.}
  Since $g \in \mathscr{R}(\alpha)$, $|g|$ is bounded by some real $M$ on $[a,b]$,
  that is, $|g(x)| \leq M$.
  Hence
  \[
    0
  \leq \int_{a}^{b} |fg| d\alpha
  \leq \int_{a}^{b} M|f| d\alpha
  = M \int_{a}^{b}|f| d\alpha
  = 0.
  \]
  Therefore $\int_{a}^{b} |fg| d\alpha = 0$.
  \end{enumerate}
  By (a)(b), $\int_{a}^{b} |fg| d\alpha = 0$
  and thus H\"older's inequality holds for this case.

\item[(2)]
  The case $\left\{ \int_{a}^{b} |g|^q d\alpha \right\}^{\frac{1}{q}} = 0$.
  Similar to (1).

\item[(3)]
  If both
  $\left\{ \int_{a}^{b} |f|^p d\alpha \right\}^{\frac{1}{p}} > 0$
  and
  $\left\{ \int_{a}^{b} |g|^q d\alpha \right\}^{\frac{1}{q}} > 0$,
  then we apply (b)
  to
  \[
    F(x) = \frac{|f(x)|}{\left\{ \int_{a}^{b} |f(x)|^p d\alpha \right\}^{\frac{1}{p}}}
    \qquad
    \text{ and }
    \qquad
    G(x) = \frac{|g(x)|}{\left\{ \int_{a}^{b} |g(x)|^q d\alpha \right\}^{\frac{1}{q}}}.
  \]
  Here $F(x) \geq 0$ and $G(x) \geq 0$ are well-defined and Riemann integrable.
  Thus the conclusion holds.
  The equality holds if $F(x)^p = G(x)^q$
  or
  \[
    \frac{|f|^p}{\int_{a}^{b} |f|^p d\alpha}
    =
    \frac{|g|^q}{\int_{a}^{b} |g|^q d\alpha}.
  \]
  Note that the equality does not hold only if
  $\frac{|f|^p}{\int_{a}^{b} |f|^p d\alpha}
  =
  \frac{|g|^q}{\int_{a}^{b} |g|^q d\alpha}$.
  Luckily, it is true for the additional assumption that
  $\alpha(x) = x$ and $f, g$ are continuous on $[a,b]$.
\end{enumerate}
By (1)(2)(3),
in any case the equality holds if
\[
  |f|^p \int_{a}^{b} |g|^q d\alpha = |g|^q \int_{a}^{b} |f|^p d\alpha.
\]
In addition, if $\alpha(x) = x$ and $f, g$ are continuous on $[a,b]$,
then the equality holds if and only if
\[
  |f|^p \int_{a}^{b} |g|^q d\alpha = |g|^q \int_{a}^{b} |f|^p d\alpha.
\]
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \emph{Suppose $f$ and $g$ are real functions on $(0,1]$
  and $f, g \in \mathscr{R}$ on $[c,1]$ for every $c > 0$.
  Show that
  \[
    \abs{ \int_{0}^{1} fg dx }
    \leq
    \left\{ \int_{0}^{1} |f|^p dx \right\}^{\frac{1}{p}}
    \left\{ \int_{0}^{1} |g|^q dx \right\}^{\frac{1}{q}}.
  \]
  Here $\int_{0}^{1}$ is one improper integral defined in Exercise 6.7.}
  \begin{enumerate}
  \item[(a)]
    By (c), we have
    \[
      \abs{ \int_{c}^{1} fg dx }
      \leq
      \left\{ \int_{c}^{1} |f|^p dx \right\}^{\frac{1}{p}}
      \left\{ \int_{c}^{1} |g|^q dx \right\}^{\frac{1}{q}}
    \]
    for any $c \in (0,1]$.
    Here every integral is well-defined (Theorem 6.11 and Theorem 6.13).

  \item[(b)]
    Since every integral is $\geq 0$, by taking the limit in the right hand side
    we have
    \begin{align*}
      \abs{ \int_{c}^{1} fg dx }
      &\leq
      \left\{ \int_{c}^{1} |f|^p dx \right\}^{\frac{1}{p}}
      \left\{ \int_{c}^{1} |g|^q dx \right\}^{\frac{1}{q}} \\
      &\leq
      \left\{ \int_{0}^{1} |f|^p dx \right\}^{\frac{1}{p}}
      \left\{ \int_{0}^{1} |g|^q dx \right\}^{\frac{1}{q}}.
    \end{align*}
    It is possible that $\left\{ \int_{0}^{1} |f|^p dx \right\}^{\frac{1}{p}} = \infty$
    or $\left\{ \int_{0}^{1} |g|^q dx \right\}^{\frac{1}{q}} = \infty$.

  \item[(c)]
    Now $\abs{ \int_{c}^{1} fg dx }$ is bounded by
    $\left\{ \int_{0}^{1} |f|^p dx \right\}^{\frac{1}{p}}
    \left\{ \int_{0}^{1} |g|^q dx \right\}^{\frac{1}{q}}$.
    Take limit to get
    \[
      \abs{ \int_{0}^{1} fg dx }
      \leq
      \left\{ \int_{0}^{1} |f|^p dx \right\}^{\frac{1}{p}}
      \left\{ \int_{0}^{1} |g|^q dx \right\}^{\frac{1}{q}}
    \]
    even if some limit is divergent.
  \end{enumerate}

  \item[(2)]
  \emph{Suppose $f$ and $g$ are real functions on $[a,b]$
  and $f, g \in \mathscr{R}$ on $[a,b]$ for every $b > a$ where $a$ is fixed.
  Show that
  \[
    \abs{ \int_{a}^{\infty} fg dx }
    \leq
    \left\{ \int_{a}^{\infty} |f|^p dx \right\}^{\frac{1}{p}}
    \left\{ \int_{a}^{\infty} |g|^q dx \right\}^{\frac{1}{q}}.
  \]
  Here $\int_{a}^{\infty}$ is one improper integral defined in Exercise 6.8.}
  Same as (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.11.}
\addcontentsline{toc}{subsection}{Exercise 6.11.}
\emph{Let $\alpha$ be a fixed increasing function on $[a,b]$.
For $u \in \mathscr{R}(\alpha)$, define
\[
  \norm{u}_2 = \left\{ \int_{a}^{b} |u|^2 d\alpha \right\}^{\frac{1}{2}}.
\]
Suppose $f,g,h \in \mathscr{R}(\alpha)$, and prove the triangle inequality
\[
  \norm{f-h}_2 \leq \norm{f-g}_2 + \norm{g-h}_2
\]
as a consequence of the Schwarz inequality, as in the proof of Theorem 1.37.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
By Exercise 6.10(c) with $p=q=2$, we have
\begin{align*}
  \int_{a}^{b} |f-g||g-h| d\alpha
  &=
    \abs{ \int_{a}^{b} |f-g||g-h| d\alpha } \\
  &\leq
    \left\{ \int_{a}^{b} |f-g|^2 dx \right\}^{\frac{1}{2}}
    \left\{ \int_{a}^{b} |g-h|^2 dx \right\}^{\frac{1}{2}} \\
  &= \norm{f-g}_2 \norm{g-h}_2.
\end{align*}
Every integral is well-defined (Theorem 6.12 and Theorem 6.13 (or Theorem 6.11)).

\item[(2)]
Since
\begin{align*}
  \norm{f-h}_2^2
  &= \int_{a}^{b} |f-h|^2 d\alpha \\
  &\leq \int_{a}^{b} (|f-g|+|g-h|)^2 d\alpha
    &\text{(Triangle inequality)} \\
  &= \int_{a}^{b} (|f-g|^2 + 2|f-g||g-h| + |g-h|^2) d\alpha \\
  &= \int_{a}^{b} |f-g|^2 d\alpha
    + 2\int_{a}^{b} |f-g||g-h| d\alpha
    + \int_{a}^{b} |g-h|^2 d\alpha \\
  &\leq
    \norm{f-g}_2^2 + 2 \norm{f-g}_2 \norm{g-h}_2 + \norm{g-h}_2^2
    &\text{((1))} \\
  &= (\norm{f-g}_2 + \norm{g-h}_2)^2,
\end{align*}
we have
\[
  \norm{f-h}_2 \leq \norm{f-g}_2 + \norm{g-h}_2.
\]
Here every integral is well-defined (Theorem 6.12 and Theorem 6.13 (or Theorem 6.11)).

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.12.}
\addcontentsline{toc}{subsection}{Exercise 6.12.}
\emph{With the notations of Exercise 6.11,
suppose $f \in \mathscr{R}(\alpha)$ and $\varepsilon > 0$.
Prove that there exists a continuous function $g$ on $[a,b]$
such that $\norm{f-g}_2 < \varepsilon$.
(Hint: Let $P = \{a=x_0 \leq \cdots \leq x_n = b\}$ be a suitable partition of $[a,b]$,
define
\[
  g(t) = \frac{x_i-t}{\Delta x_i} f(x_{i-1}) + \frac{t-x_{i-1}}{\Delta x_i} f(x_i)
\]
if $x_{i-1} \leq t \leq x_i$.)} \\

\emph{Proof.}
Given $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
There are some real numbers $m$ and $M$
such that $m \leq f(x) \leq M$ if $x \in [a,b]$ since $f \in \mathscr{R}(\alpha)$
or $f$ is bounded on $[a,b]$.
By Theorem 6.6, there exists a partition
$P = \{a=x_0 \leq \cdots \leq x_n = b\}$ such that
\[
  U(P,f,\alpha) - L(P,f,\alpha) < \frac{\varepsilon^2}{M - m + 1}.
\]
Here
\begin{align*}
  U(P,f,\alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i
    \text{ where } M_i = \sup_{x_{i-1} \leq x \leq x_i} f(x) \\
  L(P,f,\alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i
    \text{ where } m_i = \inf_{x_{i-1} \leq x \leq x_i} f(x).
\end{align*}

\item[(2)]
For such partition $P$, define $g$ on $[a,b]$ by
\[
  g(t) = \frac{x_i-t}{\Delta x_i} f(x_{i-1}) + \frac{t-x_{i-1}}{\Delta x_i} f(x_i)
\]
if $x_{i-1} \leq t \leq x_i$.
So that
\begin{align*}
  \abs{ f(t) - g(t) }
  &= \abs{ \left( \frac{x_i-t}{\Delta x_i} + \frac{t-x_{i-1}}{\Delta x_i} \right) f(t)
    - \frac{x_i-t}{\Delta x_i} f(x_{i-1}) + \frac{t-x_{i-1}}{\Delta x_i} f(x_i) } \\
  &= \abs{ \frac{x_i-t}{\Delta x_i}(f(t) - f(x_{i-1}))
    +  \frac{t-x_{i-1}}{\Delta x_i}(f(t) - f(x_i)) } \\
  &\leq \frac{x_i-t}{\Delta x_i}\abs{ f(t) - f(x_{i-1}) }
    +  \frac{t-x_{i-1}}{\Delta x_i}\abs{ f(t) - f(x_i) } \\
  &\leq \frac{x_i-t}{\Delta x_i} (M_i - m_i) + \frac{t-x_{i-1}}{\Delta x_i}(M_i - m_i) \\
  &= M_i - m_i
\end{align*}
if $x_{i-1} \leq t \leq x_i$.
Especially,
\[
  \abs{ f(t) - g(t) } \leq M - m
\]
if $a \leq t \leq b$.

\item[(3)]
Note that the integral $\int_{a}^{b} |f-g|^2 d\alpha$ is well-defined
(Theorem 6.8, Theorem 6.11 and Theorem 6.12).
So that
\begin{align*}
  \int_{a}^{b} |f-g|^2 d\alpha
  =& \sum_{i=1}^n \int_{x_{i-1}}^{x_i} |f-g|^2 d\alpha \\
  \leq& \sum_{i=1}^n \int_{x_{i-1}}^{x_i} (M-m)(M_i - m_i) d\alpha \\
  =& (M-m) \sum_{i=1}^n \int_{x_{i-1}}^{x_i} (M_i - m_i) \Delta \alpha_i \\
  =& (M-m) [ U(P,f,\alpha) - L(P,f,\alpha) ] \\
  \leq& (M-m) \cdot \frac{\varepsilon^2}{M - m + 1} \\
  <& \varepsilon^2.
\end{align*}
Hence,
\[
  \norm{f-g}_2
  = \left\{ \int_{a}^{b} |f-g|^2 d\alpha \right\}^{\frac{1}{2}}
  < \varepsilon.
\]
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
  Apply the same argument we can prove the following statement:
  \begin{quote}
    \emph{Suppose $f \in \mathscr{R}(\alpha)$ and $\varepsilon > 0$.
    Prove that there exists a continuous function $g$ on $[a,b]$
    such that $\int_{a}^{b} |f-g| d\alpha < \varepsilon$.}
  \end{quote}

\item[(2)]
  (Lebesgue integral)
  \begin{enumerate}
    \item[(a)]
    \emph{Let $f$ be Lebesgue integrable over $E$.
    Then, given $\varepsilon > 0$,
    there is a simple function $\varphi$ such that
    \[
      \int_E |f-\varphi| < \varepsilon.
    \]}

    \item[(b)]
    \emph{Under the same hypothesis there is a step function $\psi$ such that
    \[
      \int_E |f-\psi| < \varepsilon.
    \]}

    \item[(c)]
    \emph{Under the same hypothesis there is a continuous function $g$
    vanishing outside a finite interval such that
    \[
      \int_E |f-g| < \varepsilon.
    \]} \\\\
  \end{enumerate}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.13.}
\addcontentsline{toc}{subsection}{Exercise 6.13.}
\emph{Define
\[
  f(x) = \int_{x}^{x+1} \sin(t^2)dt.
\]}
\begin{enumerate}
  \item[(a)]
  \emph{Prove that $|f(x)| < \frac{1}{x}$ if $x > 0$.
  (Hint: Put $t^2 = u$ and integrate by parts, to show that $f(x)$ is equal to
  \[
    \frac{\cos(x^2)}{2x} - \frac{\cos[(x+1)^2]}{2(x+1)}
    - \int_{x^2}^{(x+1)^2} \frac{\cos u}{4u^{\frac{3}{2}}} du.
  \]
  Replace $\cos u$ by $-1$.)}

  \item[(b)]
  \emph{Prove that
  \[
    2xf(x) = \cos(x^2) - \cos[(x+1)^2] + r(x)
  \]
  where $|r(x)| < \frac{c}{x}$ and $c$ is a constant.}

  \item[(c)]
  \emph{Find the upper and lower limits of $xf(x)$, as $x \to \infty$.}

  \item[(d)]
  \emph{Does $\int_{0}^{\infty} \sin(t^2)dt$ converges?} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
  \item[(1)]
  Put $t^2 = u$ and integrate by parts to get
  \begin{align*}
    f(x)
    &= \int_{x}^{x+1} \sin(t^2)dt \\
    &= \int_{x^2}^{(x+1)^2} \frac{\sin u}{2u^{\frac{1}{2}}} du \\
    &= - \frac{\cos[(x+1)^2]}{2(x+1)}
      + \frac{\cos(x^2)}{2x}
      - \int_{x^2}^{(x+1)^2} \frac{\cos u}{4u^{\frac{3}{2}}} du.
  \end{align*}

  \item[(2)]
  \begin{align*}
    |f(x)|
    &\leq \abs{ \frac{\cos[(x+1)^2]}{2(x+1)} }
      + \abs{ \frac{\cos(x^2)}{2x} }
      + \abs{ \int_{x^2}^{(x+1)^2} \frac{\cos u}{4u^{\frac{3}{2}}} du } \\
    &\leq \abs{ \frac{\cos[(x+1)^2]}{2(x+1)} }
      + \abs{ \frac{\cos(x^2)}{2x} }
      + \int_{x^2}^{(x+1)^2} \frac{|\cos u|}{4u^{\frac{3}{2}}} du \\
    &\leq \frac{1}{2(x+1)}
      + \frac{1}{2x}
      + \int_{x^2}^{(x+1)^2} \frac{1}{4u^{\frac{3}{2}}} du \\
    &= \frac{1}{2(x+1)}
      + \frac{1}{2x}
      + \left[ \frac{1}{2x} - \frac{1}{2(x+1)} \right] \\
    &= \frac{1}{x}.
  \end{align*}

  \item[(3)]
  The equality in (2) holds only if
  $\abs{ \cos[(x+1)^2] } = 1$, $\abs{ \cos(x^2) } = 1$,
  and
  \[
    \abs{ \int_{x^2}^{(x+1)^2} \frac{\cos u}{4u^{\frac{3}{2}}} du }
    = \int_{x^2}^{(x+1)^2} \frac{\abs{\cos u}}{4u^{\frac{3}{2}}} du
    = \int_{x^2}^{(x+1)^2} \frac{1}{4u^{\frac{3}{2}}} du.
  \]
  Since $\cos u$ has two absolute minimums or maximums at two different points
  $u = x^2$ and $u = (x+1)^2$, by the property of $\cos(u)$ there is some
  $u_0 \in [x^2,(x+1)^2]$ such that $\cos(u_0) = 0$.
  Hence given $\varepsilon = \frac{1}{2} > 0$ there exists $\delta > 0$
  such that
  \[
    |\cos(u)| \leq \frac{1}{2}
  \]
  whenever
  \[
    u \in E = [\max\{u_0-\delta,x^2\},\min\{u_0+\delta,(x+1)^2\}]
    \subseteq [x^2,(x+1)^2].
  \]
  Here $|E| > 0$.
  So that
  \[
    \int_{x^2}^{(x+1)^2} \frac{\abs{\cos u}}{4u^{\frac{3}{2}}} du
    \leq \int_{x^2}^{(x+1)^2} \frac{1}{4u^{\frac{3}{2}}} du
      - \frac{1}{2} \int_{E} \frac{1}{4u^{\frac{3}{2}}} du
    < \int_{x^2}^{(x+1)^2} \frac{1}{4u^{\frac{3}{2}}} du,
  \]
  which is absurd.
  Hence the equality in (2) does not hold.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  By (a),
  \[
    2xf(x) = \cos(x^2) - \cos[(x+1)^2] + r(x)
  \]
  where
  \[
    r(x)
    = \frac{\cos[(x+1)^2]}{x+1}
    - 2x\int_{x^2}^{(x+1)^2} \frac{\cos u}{4u^{\frac{3}{2}}} du.
  \]

  \item[(2)]
  Similar to (a),
  \begin{align*}
    |r(x)|
    &\leq \frac{1}{x+1}
      + 2x \int_{x^2}^{(x+1)^2} \frac{1}{4u^{\frac{3}{2}}} du \\
    &= \frac{1}{x+1}
      + 2x \left[ \frac{1}{2x} - \frac{1}{2(x+1)} \right] \\
    &= \frac{2}{x+1} \\
    &< \frac{2}{x}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\emph{Show that}
\[
  \limsup_{x \to \infty} x f(x) = 1.
\]
The case $\liminf_{x \to \infty} x f(x) = -1$ is similar.
\begin{enumerate}
  \item[(1)]
  By (b), \emph{it suffices to show that}
  \[
    \limsup_{x \to \infty} \left[ \cos(x^2) - \cos(x+1)^2 \right] = 2.
  \]
  Take $x_n = 2n \sqrt{\pi}$ for $n = 1, 2, 3, \ldots$.
  So
  \[
    \cos(x_n^2) - \cos(x_n+1)^2
    = 1 - \cos(4n\sqrt{\pi} + 1).
  \]
  \emph{It suffices to show that}
  \[
    \liminf_{n \to \infty} \cos(4n\sqrt{\pi} + 1) = -1.
  \]

  \item[(2)]
  $x \mapsto \cos(x)$ is uniformly continuous by the mean value theorem (Theorem 5.10)
  and $x \mapsto -\sin(x)$ is bounded by $1$.
  So given any $\varepsilon > 0$, there exists $\delta > 0$
  such that $\abs{ \cos(x) - \cos(y) } < \varepsilon$
  whenever $|x-y| < \delta$

  \item[(3)]
  Define $\alpha = \frac{1}{\sqrt{\pi}}$ and $x = \frac{\pi-1}{4\pi}$.
  Note that $\pi$ is irrational and thus $\alpha$ is irrational.
  (See
  \href{https://en.wikipedia.org/wiki/Proof_that_%CF%80_is_irrational}{Wikipedia}
  for the irrationality of $\pi$.)
  Exercise 4.25(b) implies that
  there exist integers $n > 0$ and $m$ such that
  \[
    \abs{ n \alpha - m - x } < \frac{\delta}{4\pi},
  \]
  or
  \[
    \abs{ (4n\sqrt{\pi} + 1) - (4m\pi - \pi) } < \delta.
  \]
  By (2),
  \[
    \abs{ \cos(4n\sqrt{\pi} + 1) - (-1) } < \varepsilon.
  \]
  Hence $\liminf_{n \to \infty} \cos(4n\sqrt{\pi} + 1) = -1$.

\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
Yes. $\int_{0}^{\infty} \sin(t^2) dt$ converges.
\begin{enumerate}
  \item[(1)]
  Given any integer $N > 0$.
  Write
  \begin{align*}
    \int_{0}^{N} \sin(t^2) dt
    &= \sum_{n=0}^{N-1} \int_{n}^{n+1} \sin(t^2) dt \\
    &= \sum_{n=0}^{N-1} f(n) \\
    &= f(0)
      + \sum_{n=1}^{N-1}
      \frac{\cos(n^2)}{2n} - \frac{\cos[(n+1)^2]}{2n} + \frac{r(n)}{2n} \\
    &= f(0)
      + \sum_{n=1}^{N-1} \frac{\cos(n^2)}{2n}
      - \sum_{n=1}^{N-1} \frac{\cos[(n+1)^2]}{2n}
      + \sum_{n=1}^{N-1} \frac{r(n)}{2n} \\
    &= f(0)
      + \sum_{n=1}^{N-1} \frac{\cos(n^2)}{2n}
      - \sum_{n=2}^{N} \frac{\cos(n^2)}{2(n-1)}
      + \sum_{n=1}^{N-1} \frac{r(n)}{2n} \\
    &= f(0) + \frac{\cos(1)}{2} - \frac{\cos(N^2)}{2(N-1)}
      - \frac{1}{2} \sum_{n=2}^{N-1} \frac{\cos(n^2)}{n(n-1)}
      + \sum_{n=1}^{N-1} \frac{r(n)}{2n}
  \end{align*}
  where $|r(n)| \leq \frac{2}{n}$ (by (b)).

  \item[(2)]
  $\frac{\cos(N^2)}{2(N-1)} \to 0$ as $N \to \infty$
  since $\cos(N^2)$ is bounded by $1$ and $\frac{1}{N-1} \to 0$ as $N \to \infty$.

  \item[(3)]
  Since $\cos(n^2)$ is bounded by $1$ and
  $\sum \frac{1}{n(n-1)} < \sum \frac{1}{(n-1)^2}$ converges,
  \[
    \frac{1}{2} \sum_{n=2}^{\infty} \frac{\cos(n^2)}{n(n-1)}
  \]
  converges absolutely.

  \item[(4)]
  Since $|r(n)| \leq \frac{2}{n}$ and
  $\sum \frac{1}{n^2}$ converges,
  \[
    \sum_{n=1}^{\infty} \frac{|r(n)|}{2n}
    \leq \sum_{n=1}^{\infty} \frac{1}{n^2}
  \]
  converges.
  So $\sum_{n=1}^{\infty} \frac{r(n)}{2n}$ converges absolutely.

  \item[(5)]
  By (1)(2)(3)(4),
  \[
    \lim_{N \to \infty} \int_{0}^{N} \sin(t^2) dt
  \]
  exists.
  Note that
  \[
    \abs{ \int_{x}^{y} \sin(t^2) dt } < \frac{1}{x}
  \]
  if $y \geq x > 0$ (by applying the same argument in (a)(2)).
  So
  \[
    \lim_{\substack{x \to \infty \\ y \to \infty \\ y \geq x}}
      \int_{x}^{y} \sin(t^2) dt = 0.
  \]
  Therefore,
  \begin{align*}
    \int_{0}^{\infty} \sin(t^2) dt
    &= \lim_{b \to \infty} \int_{0}^{b} \sin(t^2) dt \\
    &= \lim_{b \to \infty} \int_{0}^{[b]} \sin(t^2) dt + \int_{[b]}^{b} \sin(t^2) dt \\
    &= \lim_{b \to \infty} \int_{0}^{[b]} \sin(t^2) dt
      + \lim_{b \to \infty} \int_{[b]}^{b} \sin(t^2) dt \\
    &= \lim_{N \to \infty} \int_{0}^{N} \sin(t^2) dt
      + \lim_{\substack{[b] \to \infty \\ b \to \infty \\ b \geq [b]}}
        \int_{[b]}^{b} \sin(t^2) dt \\
    &= \lim_{N \to \infty} \int_{0}^{N} \sin(t^2) dt
  \end{align*}
  converges.
\end{enumerate}
$\Box$ \\



\emph{Note.}
\[
  \int_{0}^{\infty} \sin(t^2)dt
  = \int_{0}^{\infty} \cos(t^2)dt
  = \frac{\sqrt{\pi}}{2\sqrt{2}}.
\] \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.14.}
\addcontentsline{toc}{subsection}{Exercise 6.14.}
\emph{Deal similarly with
\[
  f(x) = \int_{x}^{x+1} \sin(e^t) dt.
\]
Show that
\[
  e^x|f(x)| < 2
\]
and that
\[
  e^x f(x) = \cos(e^x) - e^{-1} \cos(e^{x+1}) + r(x)
\]
where $|r(x)| < Ce^{-x}$ for some constant $C$.} \\



\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Put $e^t = u$ and integrate by parts to get
  \begin{align*}
    f(x)
    &= \int_{x}^{x+1} \sin(e^t)dt \\
    &= \int_{\exp(x)}^{\exp(x+1)} \frac{\sin u}{u} du \\
    &= - \frac{\cos(e^{x+1})}{e^{x+1}}
      + \frac{\cos(e^x)}{e^x}
      - \int_{\exp(x)}^{\exp(x+1)} \frac{\cos u}{u^2} du.
  \end{align*}

  \item[(2)]
  \emph{Show that $e^x|f(x)| \leq 2$.}
  \begin{align*}
    |f(x)|
    &\leq \abs{ \frac{\cos(e^{x+1})}{e^{x+1}} }
      + \abs{ \frac{\cos(e^x)}{e^x} }
      + \abs{ \int_{\exp(x)}^{\exp(x+1)} \frac{\cos u}{u^2} du } \\
    &\leq \abs{ \frac{\cos(e^{x+1})}{e^{x+1}} }
      + \abs{ \frac{\cos(e^x)}{e^x} }
      + \int_{\exp(x)}^{\exp(x+1)} \frac{|\cos u|}{u^2} du \\
    &\leq \frac{1}{e^{x+1}}
      + \frac{1}{e^x}
      + \int_{\exp(x)}^{\exp(x+1)} \frac{1}{u^2} du \\
    &= \frac{1}{e^{x+1}}
      + \frac{1}{e^x}
      + \left[ \frac{1}{e^x} - \frac{1}{e^{x+1}} \right] \\
    &= \frac{2}{e^x}.
  \end{align*}
  Hence $e^x|f(x)| \leq 2$.

  \item[(3)]
  \emph{Show that $e^x|f(x)| < 2$.}
  Similar to (b)(3) in the proof of Exercise 6.13.

  \item[(4)]
  \emph{Show that
  \[
    e^x f(x) = \cos(e^x) - e^{-1} \cos(e^{x+1}) + r(x)
  \]
  where $|r(x)| < Ce^{-x}$ for some constant $C$.}
  By (1),
  \[
    e^x f(x) = \cos(e^x) - e^{-1}\cos(e^{x+1})
      - e^x \int_{\exp(x)}^{\exp(x+1)} \frac{\cos u}{u^2} du.
  \]
  So that
  \[
    r(x) = - e^x \int_{\exp(x)}^{\exp(x+1)} \frac{\cos u}{u^2} du.
  \]
  By integration by parts (Theorem 6.22),
  \begin{align*}
    \int_{\exp(x)}^{\exp(x+1)} \frac{\cos u}{u^2} du
    &= \left[ \frac{\sin u}{u^2} \right]_{u=\exp(x)}^{u=\exp(x+1)}
      - \int_{\exp(x)}^{\exp(x+1)} \frac{-2 \sin u}{u^3} du \\
    &= \frac{\sin e^{x+1}}{e^{2x+2}} - \frac{\sin e^{x}}{e^{2x}}
      + 2 \int_{\exp(x)}^{\exp(x+1)} \frac{\sin u}{u^3} du.
  \end{align*}
  So that
  \begin{align*}
    |r(x)|
    &\leq \abs{ \frac{\sin e^{x+1}}{e^{x+2}} }
      + \abs{ \frac{\sin e^{x}}{e^{x}} }
      + 2 e^x \int_{\exp(x)}^{\exp(x+1)} \frac{\abs{\sin u}}{u^3} du \\
    &\leq \frac{1}{e^{x+2}}
      + \frac{1}{e^{x}}
      + 2 e^x \int_{\exp(x)}^{\exp(x+1)} \frac{du}{u^3} \\
    &= \frac{1}{e^{x+2}}
      + \frac{1}{e^{x}}
      + 2 e^x \left[ -\frac{1}{2} u^{-2} \right]_{u=\exp(x)}^{u=\exp(x+1)} \\
    &= \frac{2}{e^{x}}.
  \end{align*}
  The equality does not hold as in (3), or $|r(x)| < 2 e^{-x}$.

  \item[(5)]
  \emph{Show that $\int_{0}^{\infty} \sin(e^t) dt$ converges.}
  Similar to (d) in Exercise 6.13.
  Given any integer $N > 0$, write
  \[
    \int_{0}^{N} \sin(e^t) dt
    = f(0) + \frac{\cos(e)}{e} - \underbrace{\frac{\cos(e^N)}{e^N}}_{\to 0}
      + \underbrace{\sum_{n=1}^{N-1} \frac{r(n)}{e^n}}_{< \infty}
  \]
  where $|r(n)| \leq \frac{2}{e^n}$ (by (4)).
  So $\lim \int_{0}^{N} \sin(e^t) dt$ exists.
  Also note that
  \[
    \abs{ \int_{x}^{y} \sin(e^t) dt } < \frac{2}{e^x}
  \]
  if $y \geq x > 0$ (by applying the same argument in (2)).
  Therefore
  \[
    \int_{0}^{\infty} \sin(e^t) dt
    = \lim_{b \to \infty} \int_{0}^{b} \sin(e^t) dt
    = \lim_{N \to \infty} \int_{0}^{N} \sin(e^t) dt
  \]
  converges.
\end{enumerate}
$\Box$ \\



\emph{Note.}
Similar to Exercise 6.13(c), we have
\[
  \limsup_{x \to \infty} e^x f(x) = 1 + e^{-1}
\]
and
\[
  \liminf_{x \to \infty} e^x f(x) = -1-e^{-1}
\]
by the fact that $e$ is irrational (Theorem 3.32). \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.15.}
\addcontentsline{toc}{subsection}{Exercise 6.15.}
\emph{Suppose $f$ is a real, continuously differentiable function on $[a,b]$,
$f(a)=f(b)=0$, and
\[
  \int_{a}^{b} f(x)^2 dx = 1.
\]
Prove that
\[
  \int_{a}^{b} xf(x)f'(x) dx = -\frac{1}{2}
\]
and that
\[
  \int_{a}^{b} [f'(x)]^2 dx \int_{a}^{b} x^2f(x)^2 dx > \frac{1}{4}.
\]
} \\

\emph{Proof.}
Every integral is well-defined (Theorem 4.9 and Theorem 6.8).
\begin{enumerate}
  \item[(1)]
  By Theorem 6.22 (integration by parts),
  \[
    \int_{a}^{b} x \left( \frac{f(x)^2}{2} \right)' dx
    =
    \left[ x \cdot \frac{f(x)^2}{2} \right]_{x=a}^{x=b}
      - \int_{a}^{b} \frac{f(x)^2}{2} dx,
  \]
  or
  \[
    \int_{a}^{b} xf(x)f'(x) dx
    =
      \left[b \cdot \frac{f(b)^2}{2} - a \cdot \frac{f(a)^2}{2} \right]
      - \frac{1}{2} \int_{a}^{b} f(x)^2 dx
    = - \frac{1}{2}.
  \]

  \item[(2)]
  By Exercise 6.10(c),
  \[
    \int_{a}^{b} [f'(x)]^2 dx \int_{a}^{b} x^2f(x)^2 dx
    \geq \left( \int_{a}^{b} xf(x)f'(x) dx \right)^2 = \frac{1}{4}.
  \]

  \item[(3)]
  (Reductio ad absurdum)
  If the equality were holding, then by Exercise 6.10(c)
  \[
    (f'(x))^2 \int_{a}^{b} x^2f(x)^2 dx
    = x^2f(x)^2 \int_{a}^{b} [f'(x)]^2 dx
  \]
  on $[a,b]$
  (since $x$, $f(x)$ and $f'(x)$ are continuous on $[a,b]$).

  \begin{enumerate}
  \item[(a)]
    \emph{Show that both integrals are nonzero.}
    (Reductio ad absurdum)
    If $\int_{a}^{b} x^2f(x)^2 dx = 0$,
    then $x^2f(x)^2 = 0$ or $xf(x) = 0$ on $[a,b]$ (Exercise 6.2).
    So that
    \[
      \int_{a}^{b} xf(x)f'(x) dx = 0 \neq -\frac{1}{2},
    \]
    which is absure.
    Similarly, $\int_{a}^{b} [f'(x)]^2 dx \neq 0$.

  \item[(b)]
    By (a), we write
    \[
      C
      =
      \left\{ \frac{\int_{a}^{b} [f'(x)]^2 dx}{\int_{a}^{b} x^2f(x)^2 dx} \right\}^{\frac{1}{2}}
      > 0
    \]
    be a positive constant.
    Hence
    \[
      f'(x) = \pm C x f(x).
    \]
    Here the sign ``$\pm$'' is not necessary unchanged on $[a,b]$.
    Luckily, we can show that the sign ``$\pm$'' is unchanged on some subinterval of $[a,b]$.

  \item[(c)]
    To find such subinterval of $[a,b]$,
    we consider the zero set $Z(f')$ and $Z(xf)$ on $[a,b]$.
    Since $f'(x) = \pm C x f(x)$ with $C > 0$, we have
    \[
      Z(f') = Z(xf).
    \]
    Note that $Z(f') = Z(xf)$ is closed (Exercise 4.3) and not equal to $[a,b]$
    (by applying the same argument in (a)).
    Hence the complement of $Z(f') = Z(xf)$ is open and nonempty,
    which can be written as the union of an at most countable collection
    of disjoint segments (Exercise 2.29).

  \item[(d)]
    Consider any nonempty open interval in (c), say
    \[
      (c,d) \subseteq [a,b].
    \]
    By construction, $f'(x) \neq 0$ for all $x \in (c,d)$.
    Since $f'(x)$ is continuous, by Theorem 4.23
    there are only two mutually exclusive possible cases:
    \begin{enumerate}
      \item[(i)]
      $f'(x) > 0$ for all $x \in (c,d)$,

      \item[(ii)]
      $f'(x) < 0$ for all $x \in (c,d)$.
    \end{enumerate}
    Similar result for $xf(x)$.
    Therefore, the sign ``$\pm$'' of $f'(x) = \pm C x f(x)$ are unchanged on $(c,d)$,
    that is,
    \begin{enumerate}
      \item[(i)]
      $f'(x) = C x f(x)$ for all $x \in (c,d)$,

      \item[(ii)]
      $f'(x) = -C x f(x)$ for all $x \in (c,d)$,
    \end{enumerate}

  \item[(e)]
    Suppose $f'(x) = C xf(x)$ on $(c,d)$.
    Since $f'(x)$ and $xf(x)$ are both vanishing at $x = c$ and $x = d$,
    $f'(x) = C xf(x)$ at $x = c$ and $x = d$.
    So
    \[
      f'(x) = C xf(x) \:\text{ if }\: x \in [c,d].
    \]
    Define
    \[
      \phi(x,y) = Cxy
    \]
    be a real function on $R = [c,d] \times \mathbb{R}$.
    And consider the initial-value problem
    \[
      y' = \phi(x,y)
      \qquad
      \text{ with }
      \qquad
      y(c) = 0.
    \]
    Then
    \[
      |\phi(x,y_2) - \phi(x,y_1)|
      = Cx|y_2-y_1|
      \leq A|y_2-y_1|
    \]
    where $A = C \cdot \max\{|c|,|d|\}$ is a constant.
    By Exercise 5.27, this initial-value problem has at most one solution.
    Clearly, $y = f(x) = 0$ on $[c,d]$ is one solution of this initial-value problem,
    contrary to the construction of $[c,d]$.
    Similar result for the case $f'(x) = -C xf(x)$.
  \end{enumerate}
  Therefore, the equality does not hold.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.16.}
\addcontentsline{toc}{subsection}{Exercise 6.16.}
\emph{For $1 < s < \infty$, define
\[
  \zeta(s) = \sum_{n=1}^{\infty} \frac{1}{n^s}.
\]
(This is Riemann's zeta function, of great importance in the study of the
distribution of prime numbers.)
Prove that}
\begin{enumerate}
  \item[(a)]
  \[
    \zeta(s) = s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} dx
  \]
\end{enumerate}
\emph{and that}
\begin{enumerate}
  \item[(b)]
  \[
    \zeta(s) = \frac{s}{s-1} - s \int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx,
  \]
\end{enumerate}
\emph{where $[x]$ denotes the greatest integer $\leq x$.
Prove that the integral in (b) converges for all $s > 0$.
(Hint: To prove (a), compute the difference between the integral over $[1,N]$
and the $N$th partial sum of the series that defines $\zeta(s)$.)} \\

\emph{Proof of (a) (Hint).}
\begin{enumerate}
  \item[(1)]
  Define
  \[
    a_N = s \int_{1}^{N} \frac{[x]}{x^{s+1}} dx - \sum_{n=1}^{N} \frac{1}{n^s}.
  \]
  Hence
  \begin{align*}
    s \int_{1}^{N} \frac{[x]}{x^{s+1}} dx
    &=
    \sum_{n=1}^{N-1} s \int_{n}^{n+1} \frac{[x]}{x^{s+1}} dx \\
    &=
    \sum_{n=1}^{N-1} s \int_{n}^{n+1} \frac{n}{x^{s+1}} dx \\
    &=
    \sum_{n=1}^{N-1} n \left( \frac{1}{n^s} - \frac{1}{(n+1)^s} \right) \\
    &=
    \left( \sum_{n=1}^N \frac{1}{n^s} \right) - \frac{1}{N^{s-1}},
  \end{align*}
  or
  \[
    a_N = -\frac{1}{N^{s-1}}.
  \]

  \item[(2)]
  So
  \[
    \lim_{N \to \infty} a_N = 0
  \]
  (since $s - 1 > 0$).
  By Theorem 3.28, $\zeta(s)$ converges if $s > 1$.
  Hence
  \[
    \lim_{N \to \infty} s \int_{1}^{N} \frac{[x]}{x^{s+1}} dx = \zeta(s)
  \]
  converges.

  \item[(3)]
  Hence given any real $b > 1$, there exists an integer $N$ such that $N \leq b < N+1$.
  Since $x \mapsto \frac{[x]}{x^{s+1}} \geq 0$ on $[1,\infty)$,
  \[
    s \int_{1}^{N} \frac{[x]}{x^{s+1}} dx
    \leq
    s \int_{1}^{b} \frac{[x]}{x^{s+1}} dx
    \leq
    s \int_{1}^{N+1} \frac{[x]}{x^{s+1}} dx.
  \]
  Since $b \to \infty$ if and only if $N \to \infty$,
  \begin{align*}
    &\lim_{N \to \infty} s \int_{1}^{N} \frac{[x]}{x^{s+1}} dx
      \leq
      \lim_{b \to \infty} s \int_{1}^{b} \frac{[x]}{x^{s+1}} dx
      \leq
      \lim_{N \to \infty} s \int_{1}^{N+1} \frac{[x]}{x^{s+1}} dx \\
    \Longrightarrow&
      \zeta(s)
      \leq
      \lim_{b \to \infty} s \int_{1}^{b} \frac{[x]}{x^{s+1}} dx
      \leq
      \zeta(s).
  \end{align*}
  Hence
  \[
    \lim_{b \to \infty} s \int_{1}^{b} \frac{[x]}{x^{s+1}} dx
    = s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} dx
    = \zeta(s)
  \]
  (in the sense of Exercise 6.8).
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that}
  \[
    s \int_{1}^{\infty} \frac{1}{x^{s}} dx = \frac{s}{s-1}.
  \]
  Given any real $b > 1$. By the fundamental theorem of calculus (Theorem 6.21),
  \[
    s \int_{1}^{b} \frac{1}{x^{s}} dx
    = \frac{s}{s-1} - \frac{s}{(s-1)b^{s-1}}.
  \]
  Hence
  \[
    \lim_{b \to \infty} s \int_{1}^{b} \frac{1}{x^{s}} dx = \frac{s}{s-1}
  \]
  since $\frac{1}{b^{s-1}} \to 0$ as $b \to \infty$ (in the sense of Exercise 6.8).

  \item[(2)]
  By (a) and (1),
  $s \int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx$ exists
  and equal to
  \[
    s \int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx
    = s \int_{1}^{\infty} \frac{1}{x^{s}} dx - s \int_{1}^{\infty} \frac{[x]}{x^{s+1}} dx
    = \frac{s}{s-1} - \zeta(s).
  \]
  The result is established.

  \item[(3)]
  \emph{Show that
  \[
    \int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx
  \]
  converges for all $s > 0$.}
  Note that $0 \leq x - [x] < 1$ on $[1,\infty)$.
  So
  \[
    \int_{1}^{b} \frac{x - [x]}{x^{s+1}} dx
    \leq
    \int_{1}^{b} \frac{1}{x^{s+1}} dx
    = \frac{1}{s} - \frac{1}{sb^s}.
  \]
  Since $\frac{1}{sb^{s}} \to 0$ as $b \to \infty$,
  \[
    \int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx
    = \lim_{b \to \infty} \int_{1}^{b} \frac{x - [x]}{x^{s+1}} dx
    \leq \lim_{b \to \infty} \frac{1}{s} - \frac{1}{sb^s}
    = \frac{1}{s}.
  \]
  Note that $\frac{1}{s}$ is finite, and thus
  the integral $\int_{1}^{\infty} \frac{x - [x]}{x^{s+1}} dx$
  converges.
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
The integral
$\int_{1}^{\infty} \frac{[x]}{x^{s+1}} dx$ does not converge for all $1 \geq s > 0$.

\item[(2)]
Compare with Exercise 8.9.

\item[(3)]
\textbf{Euler's summation formula.}
(Theorem 7.13 in the textbook:
\emph{Tom. M. Apostol, Mathematical Analysis, 2nd edition.})
\emph{If $f$ has a continuous derivative $f'$ on $[a,b]$, then we have
\[
  \sum_{a < n \leq b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\{x\}dx + f(a)\{a\} - f(b)\{b\},
\]
where $\sum_{a < n \leq b}$ means the sum from $n=[a]+1$ to $n=[b]$.
When $a$ and $b$ are integers, this becomes}
\[
  \sum_{n=a}^{b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\left( \{x\}-\frac{1}{2} \right)dx
  + \frac{f(a)+f(b)}{2}.
\]
By taking $f(x) = \frac{1}{x^s}$ we can get (a) as well. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.17.}
\addcontentsline{toc}{subsection}{Exercise 6.17.}
\emph{Suppose $\alpha$ increases monotonically on $[a,b]$,
$g$ is continuous,
and $g(x) = G'(x)$ for $a \leq x \leq b$.
Prove that
\[
  \int_{a}^{b} \alpha(x)g(x)dx
  = G(b)\alpha(b) - G(a)\alpha(a) - \int_{a}^{b}G d\alpha.
\]
(Hint: Take $g$ real, without loss of generality.
Given $P = \{a = x_0, x_1, \ldots, x_n = b\}$,
choose $t_i \in (x_{i-1},x_i)$ so that $g(t_i)\Delta x_i = G(x_i) - G(x_{i-1})$.
Show that
\[
  \sum_{i=1}^{n} \alpha(x_i)g(t_i)\Delta x_i
  = G(b)\alpha(b) - G(a)\alpha(a) - \sum_{i=1}^{n} G(x_{i-1})\Delta \alpha_i.)
\]} \\

\emph{Proof (Hint).}
Given $\varepsilon > 0$.
\begin{enumerate}
  \item[(1)]
  Take $g$ real, without loss of generality.
  Given any partition
  \[
    P = \{a = x_0 < x_1 < \ldots < x_n = b\}
  \]
  of $[a,b]$.

  \item[(2)]
  By the mean value theorem (Theorem 5.10), there is $t_i \in (x_{i-1},x_i)$
  such that
  \[
    G(x_i) - G(x_{i-1}) = (x_i - x_{i-1})G'(t_i) = g(t_i)\Delta x_i.
  \]

  \item[(3)]
  Hence,
  \begin{align*}
    \sum_{i=1}^{n} \alpha(x_i)g(t_i)\Delta x_i
    &= \sum_{i=1}^{n} \alpha(x_i)(G(x_i) - G(x_{i-1})) \\
    &= \sum_{i=1}^{n} \alpha(x_i)G(x_i) - \sum_{i=1}^{n} \alpha(x_i)G(x_{i-1}) \\
    &= \underbrace{G(b)\alpha(b) - G(a)\alpha(a)
      + \sum_{i=1}^{n} \alpha(x_{i-1})G(x_{i-1})}_{\text{
        adjust the index of $\sum_{i=1}^{n} \alpha(x_i)G(x_i)$}}
      - \sum_{i=1}^{n} \alpha(x_i)G(x_{i-1}) \\
    &= G(b)\alpha(b) - G(a)\alpha(a) - \sum_{i=1}^{n} G(x_{i-1}) \Delta\alpha_i.
  \end{align*}

  \item[(4)]
  Since $G(x)$ is differentiable on $[a,b]$,
  $G(x)$ is continuous (Theorem 5.2) and thus $G \in \mathscr{R}(\alpha)$ (Theorem 6.8).
  So there is a partition $P_1$ such that
  \[
    \abs{\sum_{j=1}^{n} G(t_j)\Delta\alpha_j - \int_{a}^{b}G d\alpha }
    < \varepsilon
  \]
  whenever $t_j \in [x_{j-1},x_j]$ (Theorem 6.7).
  In particular, we pick $t_j = x_{j-1} \in [x_{j-1},x_j]$ for all $j$, that is,
  \[
    \abs{\sum_{j=1}^{n} G(x_{j-1})\Delta\alpha_j - \int_{a}^{b}G d\alpha }
    < \varepsilon.
  \]
  Note that if $P^{*}$ is a refinement of $P$, the result is true too (Theorem 6.4).

  \item[(5)]
  Since $\alpha$ increases monotonically, $\alpha \in \mathscr{R}$ (Theorem 6.9).
  Since $g$ is continuous, $g \in \mathscr{R}$ (Theorem 6.8).
  Hence $\alpha g \in \mathscr{R}$ (Theorem 6.13).
  So there is a partition $P_2$ such that
  \[
    \abs{\sum_{k=1}^{m} \alpha(t_k)g(t_k) \Delta x_k - \int_{a}^{b} \alpha g dx }
    < \varepsilon
  \]
  whenever $t_k \in [x_{k-1},x_k]$ (Theorem 6.7).
  In particular, we pick $t_k = x_k \in [x_{k-1},x_k]$ for all $k$, that is,
  \[
    \abs{\sum_{k=1}^{m} \alpha(x_k)g(x_k) \Delta x_k - \int_{a}^{b} \alpha g dx }
    < \varepsilon.
  \]
  Note that if $P^{*}$ is a refinement of $P$, the result is true too (Theorem 6.4).

  \item[(6)]
  Since $g$ is continuous on a compact set $[a,b]$,
  $g$ is uniformly continuous.
  Hence there exists $\delta > 0$ such that
  \[
    |g(y) - g(x)| < \varepsilon
  \]
  whenever $|y - x| < \delta$ and $x, y \in [a,b]$.
  For such $\delta$, we construct a partition $P_3$ such that
  \[
    |g(t_l) - g(x_l)| < \varepsilon
  \]
  whenever $t_l \in [x_{l-1},x_l]$.
  (For example, we might take
  \[
    P_3 = \left\{
      a, a + \frac{1}{N}(b-a), a + \frac{2}{N}(b-a), \ldots,
      a + \frac{N-1}{N}(b-a), b
    \right\}
  \]
  where $N$ is an integer $\geq \frac{b-a}{\delta}$.)
  Hence
  \begin{align*}
    &\abs{ \sum_{l=1}^{N} \alpha(x_l)g(t_l) \Delta x_l
      - \sum_{l=1}^{N} \alpha(x_l)g(x_l) \Delta x_l } \\
    =&
    \abs{ \sum_{l=1}^{N} \alpha(x_l)[g(t_l)-g(x_l)] \Delta x_l } \\
    \leq&
    \sum_{l=1}^{N} |\alpha(x_l)| \cdot |g(t_l)-g(x_l)| \cdot \Delta x_l \\
    \leq&
    M \varepsilon \sum_{l=1}^{N} \Delta x_l \\
    =&
    M(b-a) \varepsilon.
  \end{align*}
  Note that if $P^{*}$ is a refinement of $P$, the result is true too
  (by the uniformly convergence of $g$).

  \item[(7)]
  Let $P = \{a = x_0 < x_1 < \ldots < x_n = b\}$
  be a common refinement of $P_1$, $P_2$ and $P_3$.
  By (3)(4)(5)(6) we have
  \begin{align*}
    &\abs{ \int_{a}^{b} \alpha(x)g(x)dx
      - G(b)\alpha(b) + G(a)\alpha(a) + \int_{a}^{b}G d\alpha } \\
    =& \abs{
      \int_{a}^{b} \alpha(x)g(x)dx - \sum_{i=1}^{n} \alpha(x_i)g(t_i)\Delta x_i
      +
      \int_{a}^{b}G d\alpha - \sum_{i=1}^{n} G(x_{i-1}) \Delta\alpha_i } \\
    \leq& \abs{
      \int_{a}^{b} \alpha(x)g(x)dx - \sum_{i=1}^{n} \alpha(x_i)g(t_i)\Delta x_i }
      +
      \abs{ \int_{a}^{b}G d\alpha - \sum_{i=1}^{n} G(x_{i-1}) \Delta\alpha_i } \\
    \leq& \abs{
      \int_{a}^{b} \alpha(x)g(x)dx - \sum_{i=1}^{n} \alpha(x_i)g(x_i)\Delta x_i }
      +
      \abs{ \sum_{i=1}^{n} \alpha(x_i)g(x_i)\Delta x_i
        - \sum_{i=1}^{n} \alpha(x_i)g(t_i)\Delta x_i } \\
      &+
      \abs{ \int_{a}^{b}G d\alpha - \sum_{i=1}^{n} G(x_{i-1}) \Delta\alpha_i } \\
    \leq& \varepsilon + M(b-a)\varepsilon + \varepsilon \\
    =& (M(b-a) + 2)\varepsilon.
  \end{align*}
  Since $\varepsilon$ is arbitrary,
  \[
    \abs{ \int_{a}^{b} \alpha(x)g(x)dx
      - G(b)\alpha(b) + G(a)\alpha(a) + \int_{a}^{b}G d\alpha }=0,
  \]
  or
  \[
    \int_{a}^{b} \alpha(x)g(x)dx
      - G(b)\alpha(b) + G(a)\alpha(a) + \int_{a}^{b}G d\alpha = 0,
  \]
  or
  \[
    \int_{a}^{b} \alpha(x)g(x)dx
    = G(b)\alpha(b) - G(a)\alpha(a) - \int_{a}^{b}G d\alpha.
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.18.}
\addcontentsline{toc}{subsection}{Exercise 6.18.}
\emph{Let $\gamma_1$, $\gamma_2$, $\gamma_3$ be curves in the complex plane,
defined on $[0,2\pi]$ by
\begin{align*}
  \gamma_1 &= \exp(it), \\
  \gamma_2 &= \exp(2it), \\
  \gamma_3 &= \exp(2\pi it \sin(\frac{1}{t})).
\end{align*}
Show that these three curves have the same range,
that $\gamma_1$ and $\gamma_2$ are rectifiable,
that the length of $\gamma_1$ is $2\pi$,
that the length of $\gamma_2$ is $4\pi$,
and that $\gamma_3$ is not rectifiable.} \\

Might assume that $\gamma_3(0) = 1$. \\



\emph{Proof.}
Write $S^1 = \{ z \in \mathbb{C} : |z| = 1 \}$.
\begin{enumerate}
  \item[(1)]
  \emph{Show that $\gamma_1$ has the range $S^1$.}
  Given any $z \in S^1$.
  Theorem 8.7(d) implies that there is a unique $t \in [0,2\pi)$
  such that $\exp(it) = z$.

  \item[(2)]
  \emph{Show that $\gamma_1$ is rectifiable and its length is $2\pi$.}
  By the definition of $\exp(z)$,
  \[
    \gamma_1'(t) = i\exp(it),
  \]
  which is continuous on $[0,2\pi]$.
  Hence $\gamma_1$ is rectifiable is rectifiable, and its length is
  \[
    \Lambda(\gamma_1)
    = \int_{0}^{2\pi}|\gamma_1'(t)|dt
    = \int_{0}^{2\pi} dt
    = 2\pi
  \]
  (Theorem 6.27).

  \item[(3)]
  \emph{Show that $\gamma_2$ has the range $S^1$.}
  Similar to (1).
  Given any $z \in S^1$.
  Theorem 8.7(d) implies that there is a unique $t \in [0,2\pi)$
  such that $\exp(it) = z$.
  Write $\exp(it) = \exp(2i\left(\frac{t}{2}\right))$
  where $\frac{t}{2} \in [0,\pi) \subseteq [0,2\pi)$.

  \item[(4)]
  \emph{Show that $\gamma_2$ is rectifiable and its length is $4\pi$.}
  Similar to (2).
  \[
    \gamma_2'(t) = 2i\exp(2it),
  \]
  and
  \[
    \Lambda(\gamma_2)
    = \int_{0}^{2\pi}|\gamma_2'(t)|dt
    = \int_{0}^{2\pi} 2 dt
    = 4\pi.
  \]

  \item[(5)]
  \emph{Show that $\gamma_3$ has the range $S^1$.}
  Define
  \begin{equation*}
  f(t) =
    \begin{cases}
      0
      & (t = 0), \\
      t \sin\frac{1}{t}
      & (t \neq 0).
    \end{cases}
  \end{equation*}
  It suffices to show that $f(I) \supseteq J$
  for for some segment $I \subseteq [0,2\pi]$ and
  some segment $J$ in $\mathbb{R}$ of the length $\geq 1$ (Theorem 8.7(a)).
  Define $I = \left[\frac{6}{7\pi}, \frac{6}{\pi} \right] \subseteq [0,2\pi]$
  and $J = \left[-\frac{3}{7\pi}, \frac{3}{\pi} \right]$ of the length $\frac{24}{7\pi} > 1$.
  Hence $f(I)$ is connected since $I$ is connected (Theorem 4.22).
  Since
  \begin{align*}
    f\left(\frac{6}{7\pi}\right) &= \frac{6}{7\pi} \sin\frac{7\pi}{6} = -\frac{3}{7\pi}, \\
    f\left(\frac{6}{\pi}\right) &= \frac{6}{\pi} \sin\frac{\pi}{6} = \frac{3}{\pi},
  \end{align*}
  $f(I) \supseteq J$ (Theorem 2.47).
  The result is established.

  \item[(6)]
  \emph{Show that $\gamma_3$ is not rectifiable.}
  \begin{enumerate}
    \item[(a)]
    Since
    \[
      \gamma_3'
      = 2\pi i \left(\sin\frac{1}{t} - \frac{1}{t}\cos\frac{1}{t} \right)
        \exp(2\pi it \sin(\frac{1}{t}))
    \]
    is continuous on $[c,2\pi]$ for any $c > 0$,
    $\gamma_3$ is rectifiable on $[c,2\pi]$ (not on $[0,2\pi]$),
    and
    \[
      \Lambda_{[c,2\pi]}(\gamma_3)
      = \int_{c}^{2\pi} |\gamma_3'(t)| dt
    \]
    on $[c,2\pi]$.

    \item[(b)]
    \begin{align*}
      \int_{c}^{2\pi} |\gamma_3'(t)| dt
      &= 2\pi \int_{c}^{2\pi} \abs{\sin\frac{1}{t} - \frac{1}{t}\cos\frac{1}{t}} dt \\
      &\geq 2\pi \int_{c}^{2\pi} \abs{\frac{1}{t}\cos\frac{1}{t}} - 1 dt \\
      &= 2\pi \int_{c}^{2\pi} \abs{\frac{1}{t}\cos\frac{1}{t}} dt - 4\pi^2.
    \end{align*}

    \item[(c)]
    For any integer $n > 0$, we have
    \begin{align*}
      &\int_{\left(2n\pi + \frac{\pi}{3}\right)^{-1}}^{\left(2n\pi - \frac{\pi}{3}\right)^{-1}}
        \abs{\frac{1}{t}\cos\frac{1}{t}} dt \\
      \geq&
      \int_{\left(2n\pi + \frac{\pi}{3}\right)^{-1}}^{\left(2n\pi - \frac{\pi}{3}\right)^{-1}}
         \left(2n\pi - \frac{\pi}{3}\right) \cdot \frac{1}{2} dt \\
      =&
      \left[ \left(2n\pi - \frac{\pi}{3}\right)^{-1}
        - \left(2n\pi + \frac{\pi}{3}\right)^{-1} \right]
        \cdot \left(2n\pi - \frac{\pi}{3}\right) \cdot \frac{1}{2} \\
      =&
      \frac{\frac{\pi}{3}}{2n\pi + \frac{\pi}{3}} \\
      >&
      \frac{1}{6} \cdot \frac{1}{n+1}
    \end{align*}
    since both
    $t \mapsto \frac{1}{t} \geq 2n\pi - \frac{\pi}{3}$
    and
    $t \mapsto \cos t \geq \frac{1}{2}$ on
    $\left[\left(2n\pi + \frac{\pi}{3}\right)^{-1}, \left(2n\pi - \frac{\pi}{3}\right)^{-1} \right]$.

    \item[(d)]
    As $c \geq \frac{1}{2N\pi - \frac{\pi}{3}}$ for some integer $N$,
    by (b)(c) we have
    \begin{align*}
      \int_{c}^{2\pi} |\gamma_3'(t)| dt
      &\geq
      2\pi \int_{c}^{2\pi} \abs{\frac{1}{t}\cos\frac{1}{t}} dt - 4\pi^2 \\
      &\geq
      2\pi \sum_{n=1}^{n=N}
        \int_{\left(2n\pi + \frac{\pi}{3}\right)^{-1}}^{\left(2n\pi - \frac{\pi}{3}\right)^{-1}}
          \abs{\frac{1}{t}\cos\frac{1}{t}} dt - 4\pi^2 \\
      &\geq
      2\pi \sum_{n=1}^{n=N}
        \frac{1}{6} \cdot \frac{1}{n+1} - 4\pi^2 \\
      &=
      \frac{\pi}{3} \sum_{n=1}^{n=N} \frac{1}{n+1} - 4\pi^2.
    \end{align*}

    \item[(e)]
    Hence
    \[
      \Lambda(\gamma_3)
      \geq
      \Lambda_{\left[\left(2N\pi - \frac{\pi}{3}\right)^{-1},2\pi\right]}(\gamma_3)
      \geq
      \frac{\pi}{3} \sum_{n=1}^{n=N} \frac{1}{n+1} - 4\pi^2.
    \]
    Let $N \to \infty$, and thus $\Lambda(\gamma_3)$ cannot be bounded (Theorem 3.28).
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 6.19.}
\addcontentsline{toc}{subsection}{Exercise 6.19.}
\emph{Let $\gamma_1$ be a curve in $\mathbb{R}^k$, defined on $[a,b]$;
let $\phi$ be a continuous $1$-$1$ mapping of $[c,d]$ onto $[a,b]$, such that
$\phi(c) = a$; and define $\gamma_2(s) = \gamma_1(\phi(s))$.
Prove that $\gamma_2$ is an arc, a closed curve, or a rectifiable curve
if and only if the same is true of $\gamma_1$.
Prove that $\gamma_2$ and $\gamma_1$ have the same length.} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $\phi$ is strictly monotonic.}
  Similar to Exercise 4.15.
  \begin{enumerate}
    \item[(a)]
    (Reductio ad absurdum)
    If $\phi$ were not strictly monotonic,
    then there exist $a < c < b \in \mathbb{R}^1$ such that
    \[
      \phi(a) \leq \phi(c) \geq \phi(b)
    \]
    or
    \[
      \phi(a) \geq \phi(c) \leq \phi(b).
    \]
    Since $\phi$ is one-to-one, all equalities does not hold.
    Hence
    \[
      \phi(a) < \phi(c) > \phi(b)
    \]
    or
    \[
      \phi(a) > \phi(c) < \phi(b).
    \]

    \item[(b)]
    The case $\phi(a) < \phi(c) > \phi(b)$.
    Take
    \[
      t = \frac{\max\{\phi(a),\phi(b)\} + \phi(c)}{2}
    \]
    so that $\phi(c) > t > \phi(a)$ and $\phi(c) > t > \phi(b)$.
    By Theorem 4.23 there exist $\xi_1 \in (a,c)$ and $\xi_2 \in(c,b)$
    such that $\phi(\xi_1) = \phi(\xi_2) = t$.
    Here $\xi_1 \neq \xi_2$, contrary to the injectivity of $\phi$.

    \item[(c)]
    The case $\phi(a) > \phi(c) < \phi(b)$.
    The proof is similar to (b).

    \item[(d)]
    By (b)(c), $\phi$ is strictly monotonic.
  \end{enumerate}

  \item[(2)]
  $\phi(d) = b$
  since $\phi$ is strictly monotonic (by (1)), surjective and $\phi(c) = a$.

  \item[(3)]
  The inverse mapping $\phi^{-1}$ is a continuous and injective mapping of $[a,b]$ onto $[c,d]$
  since $\phi$ is continuous and injective on a compact set $[c,d]$ (Theorem 4.17).

  \item[(4)]
  \emph{Show that $\gamma_2$ is an arc if and only if $\gamma_1$ is an arc.}
  Note the the composition of two injective maps is injective.
  Hence the result is established since
  $\gamma_2 = \gamma_1 \circ \phi$ and $\gamma_1 = \gamma_2 \circ \phi^{-1}$.

  \item[(5)]
  \emph{Show that $\gamma_2$ is a closed curve if and only if $\gamma_1$ is a closed curve.}
  Since $\gamma_2 = \gamma_1 \circ \phi$ and $\gamma_1 = \gamma_2 \circ \phi^{-1}$
  (as in (4)),
  $\gamma_1(a) = \gamma_1(b)$ if and only if $\gamma_2(c) = \gamma_2(d)$.

  \item[(6)]
  \emph{Show that $\gamma_2$ is a rectifiable curve if and only if
  $\gamma_1$ is a rectifiable curve.}
  Given any partition $P_1 = \{x_0, \ldots, x_n\}$ of $[a,b]$,
  there is a corresponding partition
  $P_2 = \{ \phi^{-1}(x_0), \ldots, \phi^{-1}(x_n) \}$ of $[c,d]$,
  and vice versa.
  (Given a partition $P_2 = \{x_0, \ldots, x_n\}$ of $[c,d]$,
  there is a corresponding partition
  $P_1 = \{ \phi(x_0), \ldots, \phi(x_n) \}$ of $[a,b]$.)
  Again, since $\gamma_2 = \gamma_1 \circ \phi$ and $\gamma_1 = \gamma_2 \circ \phi^{-1}$
  (as in (4)),
  \[
    \Lambda(P_1, \gamma_1) = \Lambda(P_2, \gamma_2).
  \]
  Hence $\gamma_2$ is rectifiable if and only if $\gamma_1$ is rectifiable.

  \item[(7)]
  \emph{Show that $\gamma_2$ and $\gamma_1$ have the same length.}
  Take the supremum over all partitions $P_1$ of $[a,b]$ to get
  \[
    \Lambda(P_1, \gamma_1) = \Lambda(P_2, \gamma_2) \leq \Lambda(\gamma_1).
  \]
  Hence $\Lambda(\gamma_1)$ is an upper of $\Lambda(P_2, \gamma_2)$.
  So
  \[
    \Lambda(\gamma_2) \leq \Lambda(\gamma_1).
  \]
  Similarly, $\Lambda(\gamma_1) \leq \Lambda(\gamma_2)$.
  Therefore $\Lambda(\gamma_1) = \Lambda(\gamma_2)$
  (whether $\Lambda(\gamma_1)$ or $\Lambda(\gamma_2)$ is finite or not).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 7: Sequences and Series of Functions \\}
\addcontentsline{toc}{section}{Chapter 7: Sequences and Series of Functions}



\subsection*{Exercise 7.1.}
\addcontentsline{toc}{subsection}{Exercise 7.1.}
\emph{Prove that every uniformly convergent sequence of bounded functions
is uniformly bounded.} \\

\emph{Proof (Cauchy criterion).}
Let $\{f_n\}$ be a uniformly convergent sequence of bounded functions.
\begin{enumerate}
\item[(1)]
Since $f_n$ is bounded, there exists a $M_n$ such that $|f_n(x)| \leq M_n$.

\item[(2)]
Since $\{f_n\}$ converges uniformly, given $1 > 0$ there exists an integer $N$
such that
\[
  |f_n(x) - f_m(x)| \leq 1 \text{ whenever } n, m \geq N
\]
(Theorem 7.8 (Cauchy criterion for uniformly convergence)).
Especially,
\[
  |f_n(x)| \leq |f_n(x) - f_N(x)| + |f_N(x)| \leq 1 + M_N \text{ whenever } n \geq N.
\]

\item[(3)]
Thus, $\{f_n\}$ is uniformly bounded by $M = \max\{ M_1, \ldots, M_{N-1}, M_{N}+1 \}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.2.}
\addcontentsline{toc}{subsection}{Exercise 7.2.}
\emph{If $\{f_n\}$ and $\{g_n\}$ converge uniformly on a set $E$,
prove that $\{f_n + g_n\}$ converge uniformly on $E$.
If, in addition, $\{f_n\}$ and $\{g_n\}$ are sequences of bounded functions,
prove that $\{f_n g_n\}$ converges uniformly on $E$.} \\

\emph{Proof.}
Let $f_n \to f$ uniformly and $g_n \to g$ uniformly.
\begin{enumerate}
  \item[(1)]
  \emph{Show that $\{f_n + g_n\}$ converges uniformly.}
    Given $\varepsilon > 0$.
    Since $f_n \to f$ uniformly and $g_n \to g$ uniformly,
    there exist two integers $N_1$ and $N_2$ such that
    \begin{align*}
      |f_n(x) - f(x)| \leq \frac{\varepsilon}{2}
      &\text{ whenever }
      n \geq N_1, x \in E \\
      |g_n(x) - g(x)| \leq \frac{\varepsilon}{2}
      &\text{ whenever }
      n \geq N_2, x \in E.
    \end{align*}
    Take $N = \max\{N_1,N_2\}$, we have
    \begin{align*}
      &|(f_n(x)+ g_n(x)) - (f(x) + g(x))| \\
      =& |(f_n(x) - f(x)) + (g_n(x) - g(x))| \\
      \leq& |f_n(x) - f(x)| + |g_n(x) - g(x)| \\
      \leq& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
      =& \varepsilon
    \end{align*}
    whenever $n \geq N$, $x \in E$.
    Hence $f_n + g_n \to f+g$ uniformly on $E$.

  \item[(2)]
  \emph{Show that $\{f_n g_n\}$ converges uniformly
  if, in addition, $\{f_n\}$ and $\{g_n\}$ are sequences of bounded functions.}
  Given $\varepsilon > 0$.
  \begin{enumerate}
    \item[(a)]
    By Exercise 7.1, both $\{f_n\}$ and $\{g_n\}$ are uniformly bounded.
    So there exist $M_1$ and $M_2$
    such that
    \[
      |f_n(x)| \leq M_1 \text{ and } |g_n(x)| \leq M_2
    \]
    for all $n$ and $x \in E$.
    Also, $|f(x)| \leq M_1 + 1$ and $|g(x)| \leq M_2 + 1$.

    \item[(b)]
    Since $f_n \to f$ uniformly and $g_n \to g$ uniformly,
    there exist two integers $N_1$ and $N_2$ such that
    \begin{align*}
      |f_n(x) - f(x)| \leq \frac{\varepsilon}{2(M_2 + 1)}
      &\text{ whenever }
      n \geq N_1, x \in E \\
      |g_n(x) - g(x)| \leq \frac{\varepsilon}{2(M_1 + 1)}
      &\text{ whenever }
      n \geq N_2, x \in E.
    \end{align*}
    (Note that each denominator of $\frac{\varepsilon}{2(M_j + 1)}$ $(j=1,2)$
    is well-defined and positive!)
    Take $N = \max\{N_1,N_2\}$, we have
    \begin{align*}
      &|f_n(x)g_n(x) - f(x)g(x)| \\
      =& |[f_n(x) - f(x)]g_n(x) + f(x)[g_n(x) - g(x)]| \\
      \leq& |f_n(x) - f(x)||g_n(x)| + |f(x)||g_n(x) - g(x)| \\
      \leq& \frac{\varepsilon}{2(M_2 + 1)} \cdot M_2
        + (M_1 + 1) \cdot \frac{\varepsilon}{2(M_1 + 1)} \\
      \leq& \varepsilon
    \end{align*}
    whenever $n \geq N$, $x \in E$.
    Hence $f_n g_n \to fg$ uniformly on $E$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\

\emph{Proof (Cauchy criterion).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $\{f_n + g_n\}$ converges uniformly.}
    Given $\varepsilon > 0$.
    Since $\{f_n\}$ and $\{g_n\}$ converge uniformly,
    there exist two integers $N_1$ and $N_2$ such that
    \begin{align*}
      |f_n(x) - f_m(x)| \leq \frac{\varepsilon}{2}
      &\text{ whenever }
      n,m \geq N_1, x \in E \\
      |g_n(x) - g_m(x)| \leq \frac{\varepsilon}{2}
      &\text{ whenever }
      n,m \geq N_2, x \in E.
    \end{align*}
    Take $N = \max\{N_1,N_2\}$, we have
    \begin{align*}
      &|(f_n(x)+ g_n(x)) - (f_m(x) + g_m(x))| \\
      =& |(f_n(x) - f_n(x)) + (g_n(x) - g_m(x))| \\
      \leq& |f_n(x) - f_n(x)| + |g_n(x) - g_m(x)| \\
      \leq& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
      =& \varepsilon
    \end{align*}
    whenever $n,m \geq N$, $x \in E$.
    Hence $\{f_n + g_n\}$ converges uniformly on $E$.

  \item[(2)]
  \emph{Show that $\{f_n g_n\}$ converges uniformly
  if, in addition, $\{f_n\}$ and $\{g_n\}$ are sequences of bounded functions.}
  Given $\varepsilon > 0$.
  \begin{enumerate}
    \item[(a)]
    By Exercise 7.1, both $\{f_n\}$ and $\{g_n\}$ are uniformly bounded.
    So there exist $M_1$ and $M_2$
    such that
    \[
      |f_n(x)| \leq M_1 \text{ and } |g_n(x)| \leq M_2
    \]
    for all $n$ and $x \in E$.
    Also, $|f(x)| \leq M_1 + 1$ and $|g(x)| \leq M_2 + 1$.

    \item[(b)]
    Since $\{f_n\} \to f$ uniformly and $\{g_n\} \to g$ uniformly,
    there exist two integers $N_1$ and $N_2$ such that
    \begin{align*}
      |f_n(x) - f_m(x)| \leq \frac{\varepsilon}{2(M_2 + 1)}
      &\text{ whenever }
      n,m \geq N_1, x \in E \\
      |g_n(x) - g_m(x)| \leq \frac{\varepsilon}{2(M_1 + 1)}
      &\text{ whenever }
      n,m \geq N_2, x \in E.
    \end{align*}
    Take $N = \max\{N_1,N_2\}$, we have
    \begin{align*}
      &|f_n(x)g_n(x) - f_m(x)g_m(x)| \\
      =& |[f_n(x) - f_m(x)]g_n(x) + f_m(x)[g_n(x) - g_m(x)]| \\
      \leq& |f_n(x) - f_m(x)||g_n(x)| + |f_m(x)||g_n(x) - g_m(x)| \\
      \leq& \frac{\varepsilon}{2(M_2 + 1)} \cdot M_2
        + M_1 \cdot \frac{\varepsilon}{2(M_1 + 1)} \\
      \leq& \varepsilon
    \end{align*}
    whenever $n \geq N$, $x \in E$.
    Hence $\{f_n g_n\}$ converges uniformly on $E$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\

\emph{Note.} It proved that $f_n g_n \to fg$ in Theorem 7.29.
\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.3.}
\addcontentsline{toc}{subsection}{Exercise 7.3.}
\emph{Construct sequences $\{f_n\}$, $\{g_n\}$ which converge uniformly on some set $E$,
but such that $\{f_n g_n\}$ does not converge uniformly on $E$
(of course, $\{f_n g_n\}$ must converge on $E$).} \\

We provides some examples here. \\

\emph{Proof ($f_n(x) = x + \frac{1}{n}$).}
\begin{enumerate}
  \item[(1)]
  Define $\{f_n(x)\}$ on $E = \mathbb{R}$ by $f_n(x) = x + \frac{1}{n}$ and $f(x) = x$.
  Clearly, $\{f_n(x)\}$ converges to $f(x)$ pointwise.

  \item[(2)]
  \emph{Show that $\{f_n\}$ converges uniformly.}
  Given $\varepsilon > 0$.
  There exists an integer $N \geq \frac{1}{\varepsilon}$ such that
  \[
    |f_n(x) - f(x)| = \frac{1}{n} \leq \frac{1}{N} \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
  Hence $\{f_n\} \to f$ uniformly.

  \item[(3)]
  \emph{Show that $\{f_n^2\}$ does not converge uniformly.}
  Clearly, $\{f_n(x)^2\}$ converges to $f(x)^2$ pointwise.
  Hence
  \[
    \sup_{x \in E} |f_n(x)^2 - f(x)^2|
    = \sup_{x \in E} \abs{\frac{2x}{n} + \frac{1}{n^2}}
    \to \infty
  \]
  as $n \to \infty$ (by considering $x = n^2 \in E$).
  Hence $\{f_n^2 \}$ does not converge uniformly (Theorem 7.9).
\end{enumerate}
$\Box$ \\

\emph{Proof ($f_n(x) = \frac{1}{x}$, $g_n(x) = \frac{1}{n}$).}
\begin{enumerate}
  \item[(1)]
  Let $E = (0,1)$.
  Let $\{f_n(x)\}$ on $E$ be $f_n(x) = \frac{1}{x}$
  and $\{g_n(x)\}$ on $E$ be $g_n(x) = \frac{1}{n}$.
  Clearly, $\{f_n(x)\}$ converges to $f(x) = \frac{1}{x}$ pointwise
  and $\{g_n(x)\}$ converges to $g(x) = 0$ pointwise.

  \item[(2)]
  \emph{Show that $\{f_n\}$ converges uniformly.}
  Given $\varepsilon > 0$.
  There exists an integer $N = 1$ such that
  \[
    |f_n(x) - f(x)| = 0 \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
  Hence $\{f_n\} \to f$ uniformly.

  \item[(3)]
  \emph{Show that $\{g_n\}$ converges uniformly.}
  Given $\varepsilon > 0$.
  There exists an integer $N \geq \frac{1}{\varepsilon}$ such that
  \[
    |g_n(x) - g(x)| = \frac{1}{n} \leq \frac{1}{N} \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
  Hence $\{g_n\} \to g$ uniformly.

  \item[(4)]
  \emph{Show that $\{f_n g_n\}$ does not converge uniformly.}
  Clearly, $\{f_n(x) g_n(x) \}$ converges to $f(x) g(x) = 0$ pointwise.
  Hence
  \[
    \sup_{x \in E} |f_n(x) g_n(x) - 0|
    = \sup_{x \in E} \abs{\frac{1}{nx}}
    \to \infty
  \]
  as $n \to \infty$ (by considering $x = \frac{1}{n^2} \in E$).
  Hence $\{f_n g_n \}$ does not converge uniformly (Theorem 7.9).
\end{enumerate}
$\Box$ \\

\emph{Proof (Exercise 9.2 in Tom M. Apostol, Mathematical Analysis, 2nd edition).}
\begin{enumerate}
  \item[(1)]
  Let $E = [\alpha,\beta] \subseteq \mathbb{R}$ be a bounded interval.
  Define two sequences $\{f_n\}$ and $\{g_n\}$ on $E$ as follows:
  \[
    f_n(x) = x \left( 1+\frac{1}{n} \right)
    \text{ if $x \in \mathbb{R}$, $n = 1,2,\cdots$},
  \]
  \begin{equation*}
  g_n(x) =
    \begin{cases}
      \frac{1}{n}   & \text{ if $x=0$ or if $x$ is irrational}, \\
      b+\frac{1}{n} & \text{ if $x$ is rational $\neq 0$, say $x=\frac{a}{b}$, $b>0$}.
    \end{cases}
  \end{equation*}
  Here we assume that $\mathrm{gcd}(a,b) = 1$.
  Clearly, $f(x) = x$ and
  \begin{equation*}
  g(x) =
    \begin{cases}
      0 & \text{ if $x=0$ or if $x$ is irrational}, \\
      b & \text{ if $x$ is rational $\neq 0$, say $x=\frac{a}{b}$, $b>0$}.
    \end{cases}
  \end{equation*}
  Let $M = \max\{|\alpha|,|\beta|\} \geq 0$.

  \item[(2)]
  \emph{Show that $\{f_n\}$ converges uniformly.}
  Given $\varepsilon > 0$.
  There exists an integer $N \geq \frac{M}{\varepsilon}$ such that
  \[
    |f_n(x) - f(x)| = \frac{|x|}{n} \leq \frac{M}{N} \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
  Hence $\{f_n\} \to f$ uniformly.

  \item[(3)]
  \emph{Show that $\{g_n\}$ converges uniformly.}
  Given $\varepsilon > 0$.
  There exists an integer $N \geq \frac{1}{\varepsilon}$ such that
  \[
    |g_n(x) - g(x)| = \frac{1}{n} \leq \frac{1}{N} \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
  Hence $\{g_n\} \to g$ uniformly.

  \item[(4)]
  \emph{Show that $\{f_n g_n\}$ does not converge uniformly.}
  \begin{enumerate}
    \item[(a)]
      Clearly, $\{f_n(x) g_n(x) \}$ converges to $f(x)g(x)$ pointwise
      where
      \begin{equation*}
      f(x) g(x) =
        \begin{cases}
          0 & \text{ if $x=0$ or if $x$ is irrational}, \\
          a & \text{ if $x=\frac{a}{b}$ is rational $\neq 0$, $b>0$}.
        \end{cases}
      \end{equation*}

    \item[(b)]
      Note that
      \begin{equation*}
      f_n(x) g_n(x) =
        \begin{cases}
          \frac{x}{n} \left( 1 + \frac{1}{n} \right)
            & \text{ if $x=0$ or if $x$ is irrational}, \\
          \left( a + \frac{x}{n} \right)\left( 1 + \frac{1}{n} \right)
            & \text{ if $x=\frac{a}{b}$ is rational $\neq 0$, $b>0$}.
        \end{cases}
      \end{equation*}
      Therefore,
      \begin{equation*}
      f_n(x) g_n(x) - f(x) g(x) =
        \begin{cases}
          \frac{x}{n} \left( 1 + \frac{1}{n} \right)
            & \text{ if $x=0$ or if $x$ is irrational}, \\
          \frac{x}{n} \left( 1 + b + \frac{1}{n} \right)
            & \text{ if $x=\frac{a}{b}$ is rational $\neq 0$, $b>0$}.
        \end{cases}
      \end{equation*}

    \item[(c)]
      Hence
      \begin{align*}
        \sup_{x \in E} |f_n(x) g_n(x) - f(x) g(x)|
        &\geq \sup_{x \in E \cap \mathbb{Q}} |f_n(x) g_n(x) - f(x) g(x)| \\
        &= \sup_{x \in E \cap \mathbb{Q}}
          \abs{a} \left( \frac{1}{n} + \frac{1}{bn} + \frac{1}{bn^2} \right) \\
        &\geq \sup_{x \in E \cap \mathbb{Q}}
          \abs{a} \left( \frac{1}{n} \right) \\
        &= \sup_{x \in E \cap \mathbb{Q}} \frac{\abs{a}}{n}.
      \end{align*}

    \item[(d)]
      \emph{Given any irrational number $\gamma \in E$,
      there exists a sequence
      \[
        \left\{ r_m = \frac{a_m}{b_m} \right\}
      \]
      of nonzero rational numbers in $E$ such that $\lim r_m = \gamma$.
      Show that $\{a_m\}$ is unbounded.}
      If it is true, we can find $x_n = r_{m_n} = \frac{a_{m_n}}{b_{m_n}}$
      such that $|a_{m_n}| \geq n^2$ and
      \[
        \sup_{x \in E} |f_n(x) g_n(x) - f(x) g(x)|
        \geq \sup_{x \in E \cap \mathbb{Q}} \frac{\abs{a}}{n}
        \geq \frac{n^2}{n}
        = n \to \infty
      \]
      as $n \to \infty$.

    \item[(e)]
      (Reductio ad absurdum)
      If $\{a_m\}$ were bounded, then there exists
      a \textbf{constant} subsequence of $\{a_{m_k}\}$
      such that $\lim a_{m_k} = a \in \mathbb{Z}$.
      Since $\lim_{m \to \infty} r_m = \gamma$, $\lim_{k \to \infty} r_{m_k} = \gamma$ or
      \[
        \lim_{k \to \infty} b_{m_k}
        = \lim_{k \to \infty} \frac{a_{m_k}}{r_{m_k}}
        = \frac{a}{\gamma}
      \]
      (it is well-defined since $r_{m_k}$ and $\gamma$ cannot be zero).
      Since all $b_{m_k}$ are positive integers,
      the limit $\lim b_{m_k} = b$ is a positive integer too,
      or $b = \frac{a}{\gamma} \in \mathbb{Z}^+$, or $\gamma = \frac{a}{b} \in \mathbb{Z}$,
      which is absurd.
  \end{enumerate}
  Therefore, $\{f_n g_n\}$ does not converge uniformly.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.4.}
\addcontentsline{toc}{subsection}{Exercise 7.4.}
\emph{Consider
\[
  f(x) = \sum_{n=1}^{\infty} \frac{1}{1+n^2x}.
\]
For what values of $x$ does the series converge absolutely?
On what intervals does it converge uniformly?
On what intervals does it fail to converge uniformly?
Is $f$ continuous whenever the series converges?
Is $f$ bounded?} \\

\emph{Note.}
In the sense of Definition 2.17, the interval is always closed.
We might consider all closed intervals, open intervals, and half-open intervals
together. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  $f(x)$ is well-defined on
  \[
    E = \mathbb{R} - \left\{ -\frac{1}{m^2} : m = 1,2,3,\ldots \right\}.
  \]

\item[(2)]
  \emph{Show that $f(x)$ converges absolutely if and only if $x \in E - \{0\}$.}
  \begin{enumerate}
    \item[(a)]
    The case $x > 0$.
    Consider
    \[
      \sum \abs{\frac{1}{1+n^2x}}
      = \sum \frac{1}{1+n^2x}
      = x^{-1} \sum \frac{1}{n^2 + x^{-1}}.
    \]
    Since $\frac{1}{n^2 + x^{-1}} < \frac{1}{n^2}$ and $\sum \frac{1}{n^2} < \infty$,
    $\sum \abs{\frac{1}{1+n^2x}}$ converges
    (Theorem 3.25(a)(the comparison test) and Theorem 3.28).

    \item[(b)]
    The case $x = 0$.
    $f(0) = \sum 1 = \infty$ diverges.

    \item[(c)]
    The case $x < 0$ and $x \neq -\frac{1}{m^2}$ for any integer $m > 0$.
    There is an integer $N > 0$ such that
    \[
      \frac{1}{n^2 + x^{-1}} > 0
    \]
    whenever $n \geq N$.
    (We might take $N > (-x^{-1})^{\frac{1}{2}}$.)
    Now consider
    \begin{align*}
      \sum_{n=1}^{\infty} \abs{\frac{1}{1+n^2x}}
      &= \sum_{n=1}^{N-1} \abs{\frac{1}{1+n^2x}}
        + \sum_{n=N}^{\infty} \abs{\frac{1}{1+n^2x}} \\
      &= \sum_{n=1}^{N-1} \abs{\frac{1}{1+n^2x}}
        + \abs{x^{-1}} \sum_{n=N}^{\infty} \frac{1}{n^2 + x^{-1}}.
    \end{align*}
    Here $\sum_{n=1}^{N-1} \abs{\frac{1}{1+n^2x}}$ is a fixed number
    and $\sum_{n=N}^{\infty} \abs{\frac{1}{1+n^2x}}$ converges
    (as in (a)).
    Hence $\sum_{n=1}^{\infty} \abs{\frac{1}{1+n^2x}}$ converges.
  \end{enumerate}

\item[(3)]
  \emph{Show that $f$ is unbounded on $(0,b) \subseteq E$ where $b > 0$.}
  For any integer $N > 0$,
  \begin{align*}
    f\left(\frac{1}{N^2}\right)
    &= \sum_{n=1}^{\infty} \frac{N^2}{n^2 + N^2} \\
    &> \sum_{n=1}^{N} \frac{N^2}{n^2 + N^2} \\
    &\geq \sum_{n=1}^{N} \frac{N^2}{N^2 + N^2} \\
    &= \frac{N}{2}.
  \end{align*}
  So $\frac{1}{N^2} \in (0,b)$ as $N \to \infty$ and
  $f\left(\frac{1}{N^2}\right) \to \infty$ as $N \to \infty$.

\item[(4)]
  To find on what intervals, say $I$, $f(x)$ converges uniformly, by (1)(2)
  there are only three possible cases:
  \begin{enumerate}
    \item[(a)]
      $I \subseteq (0,\infty)$.
      \begin{enumerate}
        \item[(i)]
          \emph{Show that $f$ does not converge uniformly on $(0,b)$ if $b > 0$.}
          Consider the $N$th partial sum
          \[
            s_N(x) = \sum_{n=1}^{N} \frac{1}{1+n^2x}
          \]
          on $(0,b)$.
          $s_N(x)$ is bounded by $N$ since each term $\frac{1}{1+n^2x}$ is less than $1$.
          Since $f$ is unbounded on $(0,b)$ (by (3)),
          $f$ does not converge uniformly on $(0,b)$ (Exercise 7.1).

        \item[(ii)]
          \emph{Show that $f$ converges uniformly on $[a,\infty)$ if $a > 0$.}
          For each term $\frac{1}{1+n^2x}$ of $f(x)$ on $[a,\infty)$, we have
          \[
            \abs{\frac{1}{1+n^2x}}
            \leq \frac{1}{1+n^2a}
            < \frac{1}{n^2a}.
          \]
          Since $\sum \frac{1}{n^2a} = a \sum \frac{1}{n^2}$ converges,
          $f$ converges uniformly (Theorem 7.10).
      \end{enumerate}

    \item[(b)]
      $I \subseteq \left(-\frac{1}{m^2}, -\frac{1}{(m+1)^2}\right)$
      for some integer $m > 0$.
      \emph{Show that $f$ converges uniformly on
      $\left(-\frac{1}{m^2}, -\frac{1}{(m+1)^2}\right)$ for all integer $m > 0$.}
      For $n$th term $\frac{1}{1+n^2x}$ of $f(x)$ on $(-\infty,-1)$, we have
      \[
        \abs{\frac{1}{1+n^2x}}
        \leq \frac{(m+1)^2}{n^2 - (m+1)^2}
        \leq \frac{(m+1)^2}{(n-(m+1))^2}
      \]
      if $n > m+1$.
      Since
      $\sum_{n=m+2}^{\infty} \frac{(m+1)^2}{(n-(m+1))^2} = \frac{(m+1)^2\pi^2}{6}$
      converges,
      \[
        f(x) = \sum_{n=1}^{m+1} \frac{1}{1+n^2x} + \sum_{n=m+2}^{\infty} \frac{1}{1+n^2x}
      \]
      converges uniformly (Theorem 7.10).

    \item[(c)]
      $I \subseteq (-\infty,-1)$.
      \emph{Show that $f$ converges uniformly on $(-\infty,-1)$.}
      Similar to (b).
      For $n$th term $\frac{1}{1+n^2x}$ of $f(x)$ on $(-\infty,-1)$, we have
      \[
        \abs{\frac{1}{1+n^2x}}
        \leq \frac{1}{n^2 - 1}
        \leq \frac{1}{(n-1)^2}
      \]
      if $n > 1$.
      Since $\sum_{n=2}^{\infty} \frac{1}{(n-1)^2} = \frac{\pi^2}{6}$ converges,
      \[
        f(x) = \frac{1}{1+x} + \sum_{n=2}^{\infty} \frac{1}{1+n^2x}
      \]
      converges uniformly (Theorem 7.10).

  \end{enumerate}

\item[(5)]
  \emph{Show that $f$ is continuous whenever the series converges.}
  By (2), $f$ converges if and only if $x \in E - \{0\}$.
  As $x \in E - \{0\}$, each term $\frac{1}{1+n^2x}$ of $f$ is continuous
  on $E - \{0\}$.
  By (4), $f$ converges uniformly on some open neighborhood of $x$,
  and thus $f$ is continuous at $x \in  E - \{0\}$ (Theorem 7.12).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.5.}
\addcontentsline{toc}{subsection}{Exercise 7.5.}
\emph{Let
\begin{equation*}
  f_n(x) =
    \begin{cases}
      0                    & (x < \frac{1}{n+1}), \\
      \sin^2 \frac{\pi}{x} & (\frac{1}{n+1} \leq x \leq \frac{1}{n}), \\
      0                    & (\frac{1}{n} < x).
    \end{cases}
\end{equation*}
Show that $\{f_n\}$ converges to a continuous function, but not uniformly.
Use the series $\sum f_n$ to show that absolute convergence, even for all $x$,
does not imply uniform convergence.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $\lim_{n \to \infty} f_n(x) = 0$.
Hence $\{f_n\}$ converges to a continuous function $0$ pointwise.}
Clearly, $f_n(x) = 0$ for all $x \not\in (0,1)$.
Next, for any fixed $x \in (0,1)$, there exists an integer $N > \frac{1}{x}$
such that
\[
  x > \frac{1}{N} \geq \frac{1}{n}
\]
whenever $n \geq N$.
Hence $f_n(x) = 0$ whenever $n \geq N$.

\item[(2)]
\emph{Show that $f_n \to f = 0$ not uniformly.}
Let
\[
  x_n = \frac{1}{n+\frac{1}{2}} \to 0
\]
for all $n=1,2,3,\ldots$.
Thus, $f_m(x_n) = \delta_{mn}$, where $\delta_{mn}$ is Kronecker delta.
  \begin{enumerate}
  \item[(a)]
  \emph{(Definition 7.7.)}
  (Reductio ad absurdum)
  If $\{f_n\}$ were convergent uniformly, then
  given $\varepsilon = \frac{1}{64} > 0$,
  there exists an integer $N$ such that $n \geq N$ implies
  \[
    |f_n(x) - f(x)| \leq \frac{1}{64}
  \]
  for all real $x$.
  However,
  \[
    |f_N(x_N) - f(x_N)| =  1 > \frac{1}{64},
  \]
  which is absurd.

  \item[(b)]
  \emph{(Theorem 7.8)}
  (Reductio ad absurdum)
  If $\{f_n\}$ were convergent uniformly, then
  given $\varepsilon = \frac{1}{64} > 0$,
  there exists an integer $N$ such that $n,m \geq N$ implies
  \[
    |f_n(x) - f_m(x)| \leq \frac{1}{64}
  \]
  for all real $x$.
  However,
  \[
    |f_N(x_{N}) - f_{N+1}(x_{N})| =  1 > \frac{1}{64},
  \]
  which is absurd.

  \item[(c)]
  \emph{(Theorem 7.9)}
  Since
  \[
    M_n
    = \sup_{x \in \mathbb{R}}|f_n(x) - f(x)|
    \geq |f_n(x_n) - f(x_n)| = 1,
  \]
  $f_n \to f$ not uniformly.

  \item[(d)]
  \emph{(Exercise 7.9.)}
  Since
  each $f_n$ is continuous and
  \[
    \lim_{n \to \infty} f_n(x_n) = \lim_{n \to \infty} 1 = 1 \neq 0 = f(0),
  \]
  $f_n \to f = 0$ not uniformly.
  \end{enumerate}

\item[(3)]
\emph{Show that $\sum f_n$ converges absolutely.}
Write $F_n = \sum_{k=1}^{n} f_k$ and $F = \sum f_n$.
Clearly,
\begin{equation*}
  F(x) =
    \begin{cases}
      0                    & (x \leq 0), \\
      \sin^2 \frac{\pi}{x} & (0 < x \leq 1), \\
      0                    & (x \geq 1).
    \end{cases}
\end{equation*}
Note that $f_n \geq 0$ for each $n$.
Hence $\sum f_n$ converges absolutely.

\item[(4)]
\emph{Show that $\sum f_n$ does not converge uniformly.}
Similar to (2).
Let
\[
  x_n = \frac{1}{n+\frac{1}{2}} \to 0
\]
for all $n=1,2,3,\ldots$.
Thus
\begin{equation*}
  F_m(x_n) =
    \begin{cases}
      1 & (m \geq n), \\
      0 & (m < n).
    \end{cases}
\end{equation*}

  \begin{enumerate}
  \item[(a)]
  \emph{(Definition 7.7.)}
  (Reductio ad absurdum)
  If $\{F_n\}$ were convergent uniformly, then
  given $\varepsilon = \frac{1}{64} > 0$,
  there exists an integer $N$ such that $n \geq N$ implies
  \[
    |F_n(x) - F(x)| \leq \frac{1}{64}
  \]
  for all real $x$.
  However,
  \[
    |F_N(x_{N+1}) - F(x_{N+1})| =  1 > \frac{1}{64},
  \]
  which is absurd.

  \item[(b)]
  \emph{(Theorem 7.8)}
  (Reductio ad absurdum)
  If $\{F_n\}$ were convergent uniformly, then
  given $\varepsilon = \frac{1}{64} > 0$,
  there exists an integer $N$ such that $n,m \geq N$ implies
  \[
    |F_n(x) - F_m(x)| \leq \frac{1}{64}
  \]
  for all real $x$.
  However,
  \[
    |F_N(x_{N+1}) - F_{N+1}(x_{N+1})| =  1 > \frac{1}{64},
  \]which is absurd.

  \item[(c)]
  \emph{(Theorem 7.9)}
  Since
  \[
    M_n
    = \sup_{x \in \mathbb{R}}|F_n(x) - F(x)|
    \geq |F_n(x_{n+1}) - F(x_{n+1})| = 1,
  \]
  $F_n \to F$ not uniformly.

  \item[(d)]
  \emph{(Exercise 7.9.)}
  Since
  each $F_n$ is continuous and
  \[
    \lim_{n \to \infty} F_n(x_{n+1}) = \lim_{n \to \infty} 0 \neq 1 = F(x_{n+1}),
  \]
  $F_n \to F$ not uniformly.

  \item[(e)]
  \emph{(Theorem 7.12.)}
  (Reductio ad absurdum)
  If $\{F_n\}$ were converging to $F$ uniformly, then
  $F$ were continuous since each $F_n$ is continuous by Theorem 7.12.
  However, $F$ is not continuous at $x = 0$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.6.}
\addcontentsline{toc}{subsection}{Exercise 7.6.}
\emph{Prove that the series
\[
  \sum_{n=1}^{\infty} (-1)^n \frac{x^2+n}{n^2}
\]
converges uniformly in every bounded interval,
but does not converge absolutely for any value of $x$.} \\

\emph{Proof (Dirichlet's test).}
Given any bounded interval $E = [\alpha,\beta] \subseteq \mathbb{R}$.
Write $f_n(x) = (-1)^n$ on $E$ and $g_n(x) = \frac{x^2+n}{n^2}$ on $E$.
\begin{enumerate}
  \item[(1)]
  The partial sums $F_n(x)$ of $\sum f_n(x)$ form a uniformly bounded sequence.

  \item[(2)]
  $g_1(x) \geq g_2(x) \geq \cdots$ since
  \[
    g_{n+1}(x)
    = \frac{x^2}{(n+1)^2} + \frac{1}{n+1}
    < \frac{x^2}{n^2} + \frac{1}{n}
    = g_n(x).
  \]

  \item[(3)]
  Write $M = \max\{|\alpha|,|\beta|\}$.
  Since
  \[
    |g_n(x)|
    = \frac{x^2}{n^2} + \frac{1}{n}
    \leq \frac{M^2}{n^2} + \frac{1}{n} \to \infty
  \]
  as $n \to \infty$,
  $\lim_{n \to \infty} g_n(x) = 0$.
  By Dirichlet's test (Exercise 7.11),
  $\sum_{n=1}^{\infty} f_n(x) g_n(x) = \sum_{n=1}^{\infty} (-1)^n \frac{x^2+n}{n^2}$
  converges.

  \item[(4)]
  \begin{align*}
    \sum |f_n(x)|
    &= \sum \frac{x^2+n}{n^2} \\
    &\geq \sum \frac{n}{n^2} \\
    &= \sum \frac{1}{n} \to \log n + \gamma
  \end{align*}
  (Exercise 8.9).
  Hence $\sum (-1)^n \frac{x^2+n}{n^2}$ does not converge absolutely for any value of $x$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.7.}
\addcontentsline{toc}{subsection}{Exercise 7.7.}
\emph{For $n=1,2,3,\ldots$, $x$ real, put
\[
  f_n(x) = \frac{x}{1+nx^2}.
\]
Show that $\{f_n\}$ converges uniformly to a function $f$,
and that the equation
\[
  f'(x) = \lim_{n \to \infty} f_n'(x)
\]
is correct if $x \neq 0$, but false if $x = 0$.} \\

$f_n(x)$ is defined on $\mathbb{R}$. \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Since
  \[
    \abs{ f_n(x) }
    = \abs{ \frac{x}{1+nx^2} }
    \leq \frac{|x|}{\sqrt{n}|x|}
    = \frac{1}{\sqrt{n}} \to \infty
  \]
  as $n \to \infty$, $f_n \to 0$ uniformly (Theorem 7.9).

  \item[(2)]
  Clearly, $f'(x) = 0$.
  Since
  \[
    f_n'(x) = \frac{1-nx^2}{(1+nx^2)^2},
  \]
  \begin{equation*}
  \lim_{n \to \infty} f_n'(x) =
    \begin{cases}
      1 & (x = 0), \\
      0 & (x \neq 0).
    \end{cases}
  \end{equation*}
  So that the equation
  \[
    f'(x) = \lim_{n \to \infty} f_n'(x)
  \]
  is correct if $x \neq 0$, but false if $x = 0$.
\end{enumerate}
$\Box$ \\

\emph{Note.}
$f_n'(x)$ does not converge uniformly by considering
\[
  \lim_{n \to \infty} f_n'\left(\frac{1}{n}\right)
  = \lim_{n \to \infty} \frac{1-\frac{1}{n}}{(1+\frac{1}{n})^2}
  = 1.
\]
\\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.8.}
\addcontentsline{toc}{subsection}{Exercise 7.8.}
\emph{If
  \begin{equation*}
  I(x) =
    \begin{cases}
      0 & (x \leq 0), \\
      1 & (x > 0),
    \end{cases}
  \end{equation*}
if $\{ x_n \}$ is a sequence of distinct points of $(a,b)$,
and if $\sum|c_n|$ converges,
prove that the series
\[
  f(x) = \sum_{n=1}^{\infty} c_n I(x-x_n)
  \qquad
  (a \leq x \leq b)
\]
converges uniformly,
and that $f$ is continuous for every $x \neq x_n$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Define $f_n(x) = c_n I(x-x_n)$ on $(a,b)$. So
\[
  |f_n(x)| = |c_n| |I(x-x_n)| \leq |c_n|
  \qquad
  (x \in (a,b), n = 1,2,3,\ldots).
\]
Since $\sum|c_n|$ converges, $f = \sum f_n$ converges uniformly (Theorem 7.10).

\item[(2)]
Given any $p \in (a,b)$ with $p \neq x_n$ for all $n=1,2,3,\ldots$.
So each $I(x-x_n)$ is continuous at $x=p$, and thus
each partial sum $\sum_{n=1}^{N} f_n(x)$ is continuous.

\item[(3)]
By Theorem 7.11
\begin{align*}
  \lim_{x \to p} f(x)
  &= \lim_{x \to p} \sum_{n=1}^{\infty} f_n(x) \\
  &= \lim_{N \to \infty} \left( \lim_{x \to p} \sum_{n=1}^{N} f_n(x) \right) \\
  &= \lim_{N \to \infty} \sum_{n=1}^{N} f_n(p) \\
  &= \sum_{n=1}^{\infty} f_n(p) \\
  &= f(p).
\end{align*}
$f(x)$ is continuous at $x=p$ too.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.9.}
\addcontentsline{toc}{subsection}{Exercise 7.9.}
\emph{Let $\{f_n\}$ be a sequence of continuous functions
which converges uniformly to a function $f$ on a set $E$.
Prove that
\[
  \lim_{n \to \infty} f_n(x_n) = f(x)
\]
for every sequence of points $x_n \in E$ such that $x_n \to x$,
and $x \in E$.
Is the converse of this true?} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  Given any $x \in E$ and any $\varepsilon > 0$.
  Since each $f_n$ is continuous and $f_n \to f$ uniformly,
  $f$ is continuous (Theorem 7.12).
  Hence as $x_n \to x$, there exists an integer $N_1$
  such that
  \[
    |f(x_n) - f(x)| \leq \frac{\varepsilon}{2}
    \text{ whenever } n \geq N_1
  \]
  (Theorem 4.2).
  Also, $f_n \to f$ uniformly implies that there exists an integer $N_2$
  such that
  \[
    |f_n(x_n) - f(x_n)| \leq \frac{\varepsilon}{2}
    \text{ whenever } n \geq N_2.
  \]
  Let $N = \max\{N_1,N_2\}$ be an integer.
  Then
  \[
    |f_n(x_n) - f(x)|
    \leq |f_n(x_n) - f(x_n)| + |f(x_n) - f(x)|
    \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2}
    = \varepsilon
  \]
  whenever $n \geq N$.
  Therefore, $\lim_{n \to \infty} f_n(x_n) = f(x)$.

  \item[(2)]
  \emph{Show that the converse is false.}
  Let $E = (0,1)$ and $f_n = \frac{1}{nx}$ on $E$.
  Given any $x \in E$.
  First,
  \[
    f(x) = \lim_{n \to \infty} f_n = \lim_{n \to \infty} \frac{1}{nx} = 0
  \]
  Next, for each sequence of points $x_n \in E$ such that $x_n \to x$
  (note that each $x_n \neq 0$ and $x \neq 0$), we have
  \[
    \lim_{n \to \infty} f_n(x_n)
    = \lim_{n \to \infty} \frac{1}{nx_n}
    = \lim_{n \to \infty} \frac{1}{n} \lim_{n \to \infty} \frac{1}{x_n}
    = 0 \cdot \frac{1}{x}
    = 0.
  \]
  Hence $\lim_{n \to \infty} f_n(x_n) = f(x) = 0$.
  However, $\{f_n\}$ does not converge uniformly.
  (See \emph{Proof ($f_n(x) = \frac{1}{x}$, $g_n(x) = \frac{1}{n}$)} in Exercise 7.3.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.10.}
\addcontentsline{toc}{subsection}{Exercise 7.10.}
\emph{Letting $(x)$ denote the fractional part of the real number $x$
(see Exercise 4.16 for the definition),
consider the function
\[
  f(x) = \sum_{n=1}^{\infty} \frac{(nx)}{n^2}
  \qquad
  (x \in \mathbb{R}).
\]
Find all discontinuities of $f$, and show that they form a countable dense set.
Show that $f$ is nevertheless Riemann-integrable on every bounded interval.} \\

\emph{Proof.}
Let $f_n(x) = \frac{(nx)}{n^2}$ on $\mathbb{R}$,
$F_n(x) = \sum_{k=1}^{n} f_k(x)$ on $\mathbb{R}$.
\begin{enumerate}
\item[(1)]
Since
\[
  \abs{f_n(x)} = \abs{ \frac{(nx)}{n^2} } \leq \frac{1}{n^2}
\]
for all $x \in \mathbb{R}$ and $n=1,2,3,\ldots$
and $\sum \frac{1}{n^2}$ converges (to $\frac{\pi^2}{6}$),
$F_n = \sum f_k$ converges uniformly to $f$ on $\mathbb{R}$ (Theorem 7.10).

\item[(2)]
Note that $(x)$ is continuous on $\mathbb{R} - \mathbb{Z}$
and not continuous on $\mathbb{Z}$ (Exercise 4.16).
Now we define $E_n = \{ x \in \mathbb{R} : nx \in \mathbb{Z}\}$.
So $E_1 = \mathbb{Z}$, and
\[
  \bigcup_{n=1}^{\infty} E_n = \mathbb{Q}.
\]
So $f_n$ is continuous on $\mathbb{R} - E_n$
and not continuous on $E_n$.
So $F_n = \sum f_k$ is continuous on
$\mathbb{R} - \bigcup_{k=1}^{n} E_k
\supseteq \mathbb{R}$ - $\mathbb{Q}$.

\item[(3)]
\emph{Show that $f(x)$ is continuous on $\mathbb{R}$ - $\mathbb{Q}$.}
Since
$\{F_n\}$ is a sequence of continuous functions on $\mathbb{R}$ - $\mathbb{Q}$ (by (2))
and $F_n \to f$ uniformly (by (1)),
$f$ is continuous on $\mathbb{R}$ - $\mathbb{Q}$ (Theorem 7.12).

\item[(4)]
\emph{Show that $f(x)$ is not continuous on $\mathbb{Q}$,
which is a countable dense set of $\mathbb{R}$.}
  \begin{enumerate}
  \item[(a)]
  (Reductio ad absurdum)
  If there were $p = \frac{a}{b} \in \mathbb{Q}$
  with $a,b \in \mathbb{Z}$, $(a,b) = 1$ and $b > 0$
  such that $f(x)$ is continuous at $x = p$,
  then
  \[
    \lim_{x \to p^{-}} f(x) = \lim_{x \to p^{+}} f(x).
  \]

  \item[(b)]
  As $b \mid n$, say $n = bq$ for some $q \in \mathbb{Z}^{+}$, we have
  \begin{align*}
    \lim_{x \to p^{-}} f_n(x)
    &= \lim_{x \to p^{-}} \frac{1}{b^2 q^2}
    = \frac{1}{b^2 q^2}, \\
    \lim_{x \to p^{+}} f_n(x)
    &= \lim_{x \to p^{+}} \frac{0}{b^2 q^2}
    = 0.
  \end{align*}
  As $b \nmid n$,
  \[
    \lim_{x \to p^{-}} f_n(x) = \lim_{x \to p^{+}} f_n(x) = f_n(p).
  \]
  Thus,
  \[
    \lim_{x \to p^{-}} F_n(x) - \lim_{x \to p^{+}} F_n(x)
    = \frac{1}{b^2} \sum_{q = 1}^{[\frac{n}{b}]} \frac{1}{q^2}.
  \]

  \item[(c)]
  Since $F_n \to f$ uniformly, given $\varepsilon = \frac{64}{1989 b^2} > 0$,
  there exists an integer $N'$ such that
  \[
    \abs{ \sum_{n=m}^{\infty} f_n(x) }
    = \sum_{n=m}^{\infty} f_n(x)
    \leq \frac{64}{1989 b^2}
  \]
  whenever $m \geq N'$.

  \item[(d)]
  Take $N = \max\{N', b\}$.
  \begin{align*}
    &\abs{ \underbrace{\lim_{x \to p^{-}} f(x)}_{\text{exists}}
      - \underbrace{\lim_{x \to p^{+}} f(x)}_{\text{exists}} } \\
    =& \abs{
      \underbrace{\lim_{x \to p^{-}} F_N(x)}_{\text{exists}}
      - \underbrace{\lim_{x \to p^{+}} F_N(x)}_{\text{exists}}
      + \underbrace{\lim_{x \to p^{-}} \sum_{n=N+1}^{\infty} f_n(x)}_{\text{exists}}
      - \underbrace{\lim_{x \to p^{+}} \sum_{n=N+1}^{\infty} f_n(x)}_{\text{exists}} } \\
    \geq& \abs{ \lim_{x \to p^{-}} F_N(x) - \lim_{x \to p^{+}} F_N(x) }
      - \abs{  \lim_{x \to p^{-}} \sum_{n=N+1}^{\infty} f_n(x) }
      - \abs{  \lim_{x \to p^{+}} \sum_{n=N+1}^{\infty} f_n(x) } \\
    \geq& \frac{1}{b^2} \sum_{q = 1}^{[\frac{n}{b}]} \frac{1}{q^2}
      - \frac{64}{1989 b^2} - \frac{64}{1989 b^2} \\
    \geq& \frac{1}{q^2} - \frac{64}{1989 b^2} - \frac{64}{1989 b^2} \\
    =& \frac{1861}{1989 b^2} \\
    >& 0,
  \end{align*}
  which is absurd.
  \end{enumerate}

  \item[(4)]
  \emph{Show that $f$ is nevertheless Riemann-integrable on every bounded interval.}
  Since each $f_n \in \mathscr{R}$ on every bounded interval,
  $F_n \in \mathscr{R}$ on every bounded interval.
  Since $F_n \to f$ uniformly,
  $f \in \mathscr{R}$ on every bounded interval by Theorem 7.16.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.11 (Dirichlet's test).}
\addcontentsline{toc}{subsection}{Exercise 7.11 (Dirichlet's test).}
\emph{Suppose $\{f_n\}$, $\{g_n\}$ are defined on $E$, and}
\begin{enumerate}
  \item[(a)]
  \emph{$\sum f_n(x)$ has uniformly bounded partial sums;}
  \item[(b)]
  \emph{$g_n(x) \to 0$ uniformly on $E$;}
  \item[(b)]
  \emph{$g_1(x) \geq g_2(x) \geq g_3(x) \geq \cdots$ for every $x \in E$.}
\end{enumerate}
\emph{Prove that $\sum f_n(x) g_n(x)$ converges uniformly on $E$.
(Hint: Compare with Theorem 3.42.)} \\



\emph{Theorem 3.42 (Dirichlet's test).}
Suppose
\begin{enumerate}
  \item[(a)]
  the partial sums $A_n$ of $\sum a_n$ form a bounded sequence;
  \item[(b)]
  $b_0 \geq b_1 \geq b_2 \geq \cdots$;
  \item[(c)]
  $\lim_{n \to \infty} b_n = 0$.
\end{enumerate}
Then $\sum a_n b_n$ converges. \\



\emph{Proof (Theorem 3.42).}
Let $F_n(x) = \sum_{k=1}^{n} f_k(x)$.
Choose $M$ such that $|F_n(x)| \leq M$ for all $n$, all $x \in E$.
Given $\varepsilon > 0$,
there is an integer $N$ such that $g_N(x) \leq \frac{\varepsilon}{2(M+1)}$ for all $x \in E$.
For $N \leq p \leq q$, we have
\begin{align*}
  &\abs{\sum_{n=p}^{q} f_n(x) g_n(x)} \\
  =& \abs{\sum_{n=p}^{q-1} F_n(x)(g_n(x)-g_{n+1}(x)) + F_q(x)g_q(x) - F_{p-1}(x)g_p(x)} \\
  \leq& M \abs{\sum_{n=p}^{q-1}(g_n(x)-g_{n+1}(x)) + g_q(x) + g_p(x)} \\
  =& 2M g_p(x) \\
  \leq& 2M g_N(x) \\
  \leq& \varepsilon
\end{align*}
for all $x \in E$.
Uniformly convergence now follows from the Cauchy criterion (Theorem 7.8).
Note that the first inequality in the above chain depends of course on the fact that
$g_n(x) - g_{n+1}(x) \geq 0$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.12.}
\addcontentsline{toc}{subsection}{Exercise 7.12.}
\emph{Suppose $g$ and $f_n$ ($n=1,2,3\ldots$) are defined on $(0,\infty)$,
are Riemann-integrable on $[t,T]$ whenever $0 < t < T < \infty$,
$|f_n| \leq g$, $f_n \to f$ uniformly on every compact subset of $(0,\infty)$),
and
\[
  \int_{0}^{\infty} g(x)dx < \infty.
\]
Prove that
\[
  \lim_{n \to \infty} \int_{0}^{\infty} f_n(x)dx = \int_{0}^{\infty} f(x)dx.
\]
(See Exercises 6.7 and 6.8 for the relevant definitions.)
This is a rather weak form of Lebesgue's dominated convergence theorem (Theorem 11.32).
Even in the context of the Riemann integral,
uniform convergence can be replaced by pointwise convergence if
it is assumed that $f \in \mathscr{R}$.
(See the articles by F. Cunningham in Math. Mag., vol. 40, 1967, pp. 179-186,
and by H. Kestelman in Amer. Math. Monthly, vol. 77, 1970, pp. 182-187.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
It is equivalent to show that
\[
  \lim_{n \to \infty} \int_{0}^{1} f_n(x)dx = \int_{0}^{1} f(x)dx
\]
and
\[
  \lim_{n \to \infty} \int_{1}^{\infty} f_n(x)dx = \int_{1}^{\infty} f(x)dx
\]
in the sense of Exercises 6.7 and 6.8.

\item[(2)]
\emph{Show that $\int_{0}^{1} f_n(x)dx$ ($n=1,2,3\ldots$) and $\int_{0}^{1} f(x)dx$ are
convergent (well-defined) in in the sense of Exercises 6.7.}
By assumption, as $0 < t < 1$ we have
\[
  \abs{\int_{t}^{1} f_n(x)dx}
  \leq \int_{t}^{1} \abs{f_n(x)}dx
  \leq \int_{t}^{1} g(x)dx.
\]
Note that
\[
  \lim_{t \to 0} \int_{t}^{1} g(x)dx = \int_{0}^{1} g(x)dx < \infty
\]
(Exercises 6.7).
Hence
\[
  \lim_{t \to 0}\abs{\int_{t}^{1} f_n(x)dx}
  = \abs{\lim_{t \to 0} \int_{t}^{1} f_n(x)dx}
  \leq \int_{0}^{1} g(x)dx
  < \infty.
\]
Also,
since $|f_n(x)| \leq g(x)$ and $f_n \to f$ uniformly, $f(x) \leq g(x)$ pointwise.
Apply the same argument to get
\[
  \abs{\lim_{t \to 0} \int_{t}^{1} f(x)dx}
  < \infty.
\]
Here $\int_{t}^{1} f(x)dx$ exists by Theorem 7.16.

\item[(3)]
Given any integer $n > 0$ and $t \in (0,1]$, we have
\begin{align*}
  \abs{ \int_{0}^{1} f_n(x)dx - \int_{0}^{1} f(x)dx }
  \leq&
  \abs{ \int_{0}^{1} f_n(x)dx - \int_{t}^{1} f_n(x)dx } \\
    &+ \abs{ \int_{t}^{1} f_n(x)dx - \int_{t}^{1} f(x)dx } \\
    &+ \abs{ \int_{t}^{1} f(x)dx - \int_{0}^{1} f(x)dx } \\
  \leq&
  \abs{ \int_{0}^{t} f_n(x)dx } \\
    &+ \int_{t}^{1} \abs{ f_n(x) - f(x) } dx \\
    &+ \abs{ \int_{0}^{t} f(x)dx } \\
\end{align*}

\item[(4)]
Given $\varepsilon > 0$.
Apply the similar argument in (2),
we have
\begin{align*}
  \abs{ \int_{0}^{t} f_n(x)dx }
  &\leq \int_{0}^{t} |f_n(x)|dx
  \leq \int_{0}^{t} g(x)dx, \\
  \abs{ \int_{0}^{t} f(x)dx }
  &\leq \int_{0}^{t} |f(x)|dx
  \leq \int_{0}^{t} g(x)dx.
\end{align*}
Since $\int_{0}^{t} g(x)dx < \infty$,
there exists a real number $c \in (0,1)$ such that
\[
  \int_{0}^{t} g(x)dx < \frac{\varepsilon}{3}
\]
whenever $0 < t \leq c$.
In particular, for any integer $n > 0$ we have
\begin{align*}
  \abs{ \int_{0}^{c} f_n(x)dx }
  &\leq \frac{\varepsilon}{3}, \\
  \abs{ \int_{0}^{c} f(x)dx }
  &\leq \frac{\varepsilon}{3}.
\end{align*}

\item[(5)]
For such $c \in (0,1)$ in (4), there is an integer $N$ such that
\[
  \abs{f_n(x) - f(x)} < \frac{\varepsilon}{3(1-c)}
\]
whenever $n \geq N$ and $x \in [c,1]$
since $f_n \to f$ uniformly on a compact set $[c,1]$.

\item[(6)]
By (3)(4)(5),
\begin{align*}
  &\abs{ \int_{0}^{1} f_n(x)dx - \int_{0}^{1} f(x)dx } \\
  \leq&
  \abs{ \int_{0}^{c} f_n(x)dx }
    + \int_{c}^{1} \abs{ f_n(x) - f(x) } dx
    + \abs{ \int_{0}^{c} f(x)dx } \\
  <&
  \frac{\varepsilon}{3}
    + (1-c) \cdot \frac{\varepsilon}{3(1-c)}
    + \frac{\varepsilon}{3} \\
  =&
  \varepsilon
\end{align*}
whenever $n \geq N$.
Therefore
\[
  \lim_{n \to \infty} \int_{0}^{1} f_n(x)dx = \int_{0}^{1} f(x)dx.
\]
Similarly,
\[
  \lim_{n \to \infty} \int_{1}^{\infty} f_n(x)dx = \int_{1}^{\infty} f(x)dx.
\]
Hence
\[
  \lim_{n \to \infty} \int_{0}^{\infty} f_n(x)dx = \int_{0}^{\infty} f(x)dx.
\]
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Tannery's convergence theorem for Riemann integrals).}
\emph{(Exercise 10.7 of the book
T. M. Apostol, Mathematical Analysis, Second Edition.)
Prove Tannery's convergence theorem for Riemann integrals:
Given a sequence of functions $\{f_n\}$ and an increasing sequence $\{p_n\}$
of real numbers such that $p_n \to +\infty$ as $n \to \infty$.
Assume that}
\begin{enumerate}
\item[(a)]
\emph{$f_n \to f$ uniformly on $[a,b]$ for every $b \geq a$.}

\item[(b)]
\emph{$f_n$ is Riemann-integrable on $[a,b]$ for every $b \geq a$.}

\item[(c)]
\emph{$|f_n(x)| \leq g(x)$ on $[a,+\infty)$,
where $g$ is improper Riemann-integrable on $[a,+\infty)$.}
\end{enumerate}

\emph{Then both $f$ and $|f|$ are improper Riemann-integrable on $[a,+\infty)$,
the sequence $\{\int_{a}^{p_n}f_n(x)dx\}$ converges, and
\[
  \int_{a}^{\infty} f(x)dx = \lim_{n \to \infty}\int_{a}^{p_n}f_n(x)dx.
\]}
\begin{enumerate}
\item[(d)]
\emph{Use Tannery's theorem to prove that
\[
  \lim_{n \to \infty} \int_{0}^{n} \left( 1-\frac{x}{n} \right)^n x^p dx
  = \int_{0}^{\infty} e^{-x}x^p dx,
\]
if $p > -1$.} \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.13.}
\addcontentsline{toc}{subsection}{Exercise 7.13.}
\emph{Assume that $\{f_n\}$ is a sequence of monotonically increasing functions on $\mathbb{R}^1$
with $0 \leq  f_n(x) \leq 1$ for all $x$ and all $n$.}
\begin{enumerate}
\item[(a)]
  \emph{Prove that there is a function $f$ and a sequence $\{n_k\}$ such that
  \[
    f(x) = \lim_{k \to \infty} f_{n_k}(x)
  \]
  for every $x \in \mathbb{R}^1$.
  (The existence of such a pointwise convergent subsequence is usually
  called \textbf{Helly's selection theorem}.)}

\item[(b)]
  \emph{If, moreover, $f$ is continuous, does $f_{n_k} \to f$ uniformly on $\mathbb{R}^1$
  or on any bounded subset $E$ of $\mathbb{R}^1$?}
\end{enumerate}

\emph{(Hint:}
\begin{enumerate}
\item[(i)]
\emph{Some subsequence $\{f_{n_i}\}$ converges at all rational points $r$, say, to $f(r)$.}

\item[(ii)]
\emph{Define $f(x)$, for any $x \in \mathbb{R}^1$, to be $\sup f(r)$,
the sup being taken over all $r \leq x$.}

\item[(iii)]
\emph{Show that $f_{n_i}(x) \to f(x)$ at every $x$ at which $f$ is continuous.
(This is where monotonicity is strongly used.)}

\item[(iv)]
\emph{A subsequence of $\{f_{n_i}\}$ converges at every point of discontinuity of $f$
since there are at most countably many such points.}

\end{enumerate}
\emph{This proves (a).
To prove (b), modify your proof of (iii) appropriately.)} \\

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that there is a subsequence $\{f_{n_i}\}$ converges at all rational points $r$,
  say, to $f(r)$.}
  Let $E = \mathbb{Q}$ be a countable subset of $\mathbb{R}^1$ in Theorem 7.23.

\item[(2)]
  Define $f(x)$, for any $x \in \mathbb{R}^1$, to be $\sup f(r)$,
  the sup being taken over all $r \leq x$.
  It is well-defined since $f(x) = \sup f(r) \leq 1$
  and the construction of $\mathbb{R}^1$ (Theorem 1.19).
  Note that $f$ is monotonically increasing.

\item[(3)]
  \emph{Show that $f_{n_i}(x) \to f(x)$ at every $x$ at which $f$ is continuous.}
  \begin{enumerate}
  \item[(a)]
    Given any $x$ at which $f$ is continuous.
    Given any $\varepsilon > 0$.
    Since $f$ is continuous at $x$, there exists a $\delta > 0$
    such that
    \[
      f(x)-\frac{\varepsilon}{89} < f(r) < f(x)+\frac{\varepsilon}{89}
    \]
    whenever $r \in (x-\delta,x+\delta)$.

  \item[(b)]
    Given any $r \in \mathbb{Q}$.
    By (1), there is an integer $N$ such that
    \[
      f(r)-\frac{\varepsilon}{64} < f_{n_i}(r) < f(r)+\frac{\varepsilon}{64}
    \]
    whenever $i \geq N$.

  \item[(c)]
    As $r \in (x,x+\delta) \bigcap \mathbb{Q} \neq \varnothing$
    (since $\mathbb{Q}$ is dense in $\mathbb{R}$)
    and $i \geq N$, we have
      \begin{align*}
        f_{n_i}(x)
        &\leq f_{n_i}(r)
          &\text{($f_{n_i}$: increasing)} \\
        &< f(r) + \frac{\varepsilon}{64}
          &\text{((b))} \\
        &< f(x)+\frac{\varepsilon}{89} + \frac{\varepsilon}{64}
          &\text{((a))} \\
        &< f(x) + \varepsilon.
      \end{align*}
    Similarly,
    \[
      f_{n_i}(x) > f(x) - \varepsilon.
    \]
    Therefore
    \[
      \abs{ f_{n_i}(x) - f(x) } < \varepsilon
    \]
    whenever $i \geq N$.
  \end{enumerate}

\item[(4)]
  \emph{Show that there is a subsequence of $\{f_{n_i}\}$ converging
  at every point of discontinuity of $f$
  since there are at most countably many such points.}
  \begin{enumerate}
  \item[(a)]
    By construction of $f$, $f$ is monotonically increasing and $0 \leq f(x) \leq 1$.

  \item[(b)]
    Theorem 4.30 implies that there are at most countably many discontinuity points of $f$.

  \item[(c)]
    Apply Theorem 7.23 again to get there is
    a subsequence of $\{f_{n_i}\}$ converging
    at every point of discontinuity of $f$.
  \end{enumerate}

\item[(5)]
  Since any subsequence of $\{f_{n_i}\}$ converges at every point of continuity of $f$,
  there exists a subsequence $\{f_{n_k}\}$ of $\{f_n\}$ such that
  \[
    \lim_{k \to \infty} f_{n_k}(x) = f(x)
  \]
  for $x \in \mathbb{R}^1$ (by (3)(4)).
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \emph{Show that the result does not hold on $\mathbb{R}^1$.}
  (Using sigmoid functions.)
  \begin{enumerate}
  \item[(a)]
    Define
    \begin{equation*}
    f_n(x) =
      \begin{cases}
        \frac{1}{2(1+e^{-x})} & \text{ if $x < n$}, \\
        1 & \text{ if $x \geq n$}.
      \end{cases}
    \end{equation*}

  \item[(b)]
    $\{f_n\}$ is a sequence of monotonically increasing functions on $\mathbb{R}^1$
    with $0 \leq f_n(x) \leq 1$ for all $x$ and all $n$.

  \item[(c)]
    Define a continuous function $f(x)$ on $\mathbb{R}^1$ by
    \[
      f(x) = \frac{1}{2(1+e^{-x})}.
    \]
    So for every subsequence $\{f_{n_k}\}$ of $\{f_n\}$, we have
    \[
      \lim_{k \to \infty} f_{n_k}(x) = f(x)
    \]
    for all $x \in \mathbb{R}^1$, but
    \[
      \abs{ f_{n}(n) - f(n) }
      = \abs{ 1 - \frac{1}{2(1+e^{-n})} }
      \geq 1 - \frac{1}{2}
      = \frac{1}{2}.
    \]
    So that no subsequence can converge uniformly on $\mathbb{R}^1$
    (by using the similar argument in Example 7.21).
  \end{enumerate}

\item[(2)]
  \emph{Show that the result holds on any bounded subset $E$ of $\mathbb{R}^1$.}
  Might assume that $E = [a,b]$ with $a \neq -\infty$ and $b \neq \infty$.
  \begin{enumerate}
  \item[(a)]
    Given any $\varepsilon > 0$.
    Since $f$ is continuous on a compact set $E$,
    $f$ is continuous uniformly on $E$,
    and thus there exists a $\delta > 0$ such that
    \[
      |f(x) - f(y)| < \frac{\varepsilon}{89}
    \]
    whenever $x,y \in K$ and $|x-y| < \delta$.

  \item[(b)]
    For such $\delta > 0$,
    define a partition $P = \{x_0, x_1, \ldots, x_m\}$ of $[a,b]$ such that
    \[
      \Delta x_j = x_j - x_{j-1} < \frac{\delta}{64}
    \]
    for all $1 \leq j \leq m$.

  \item[(c)]
    Since $f_{n_k} \to f$ (pointwise), for each $x_j$ in the partition $P$
    there exist integers $N_j$ such that
    \[
      \abs{ f_{n_k}(x_j) - f(x_j) } < \frac{\varepsilon}{1989}
    \]
    whenever $k \geq N_j$.
    Take an integer $N = \max\{N_0, N_1, \ldots, N_m\}$. Thus
    \[
      \abs{ f_{n_k}(x_j) - f(x_j) } < \frac{\varepsilon}{1989}
    \]
    whenever $0 \leq j \leq m$ and $k \geq N$.

  \item[(d)]
    As $0 \leq j \leq m$ and $k \geq N$, we have
      \begin{align*}
        &\abs{f_{n_k}(x_j) - f_{n_k}(x_{j-1})} \\
        \leq&
        \abs{f_{n_k}(x_j) - f(x_j)}
          + \abs{f(x_j) - f(x_{j-1})}
          + \abs{f(x_{j-1}) - f_{n_k}(x_{j-1})} \\
        <&
        \frac{\varepsilon}{1989}
          + \frac{\varepsilon}{89}
          + \frac{\varepsilon}{1989}
      \end{align*}
    by (a)(c).

  \item[(e)]
    Now given any $x \in [a,b]$, by (b) there is a subinterval $[x_{i-1},x_i]$
    such that $x \in [x_{i-1},x_i]$.
    Hence
    \begin{align*}
      \abs{f_{n_k}(x) - f(x)}
      \leq&
      \abs{f_{n_k}(x) - f_{n_k}(x_{i-1})} \\
        &+ \abs{f_{n_k}(x_{i-1}) - f(x_{i-1})} \\
        &+ \abs{f(x_{i-1}) - f(x)} \\
      \leq&
      \abs{f_{n_k}(x_i) - f_{n_k}(x_{i-1})}
          &\text{($f_{n_k}$: increasing)} \\
        &+ \abs{f_{n_k}(x_{i-1}) - f(x_{i-1})} \\
        &+ \abs{f(x_{i-1}) - f(x_i)}
          &\text{($f$: increasing)} \\
      <&
      \frac{\varepsilon}{1989}
          + \frac{\varepsilon}{89}
          + \frac{\varepsilon}{1989}
          &\text{((d))} \\
        &+ \frac{\varepsilon}{1989}
          &\text{((c))} \\
        &+ \frac{\varepsilon}{89}
          &\text{((a))} \\
      <& \varepsilon
    \end{align*}
    whenever $k \geq N$.
    The above inequality holds for any $x \in [a,b]$
    and thus $f_{n_k} \to f$ uniformly.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.14.}
\addcontentsline{toc}{subsection}{Exercise 7.14.}
\emph{Let $f$ be a continuous real function on $\mathbb{R}^1$
with the following properties:
$0 \leq f(t) \leq 1$, $f(t+2)=f(t)$ for every $t$, and
\begin{equation*}
  f(t) =
    \begin{cases}
      0 & \left(0 \leq t \leq \frac{1}{3}\right) \\
      1 & \left(\frac{2}{3} \leq t \leq 1\right).
    \end{cases}
\end{equation*}
Put $\Phi(t) = (x(t),y(t))$, where
\begin{align*}
  x(t) &= \sum_{n=1}^{\infty} 2^{-n} f(3^{2n-1}t), \\
  y(t) &= \sum_{n=1}^{\infty} 2^{-n} f(3^{2n}t).
\end{align*}
Prove that $\Phi(t)$ is continuous and
that $\Phi(t)$ maps $I = [0,1]$ onto the unit square $I^2 \subseteq \mathbb{R}^2$.
If fact, show that $\Phi(t)$ maps the Cantor set onto $I^2$.}
\emph{(Hint: Each $(x_0,y_0) \in I^2$ has the form
\begin{align*}
  x_0 &= \sum_{n=1}^{\infty} 2^{-n} a_{2n-1}, \\
  y_0 &= \sum_{n=1}^{\infty} 2^{-n} a_{2n}
\end{align*}
where each $a_i$ is $0$ or $1$.
If
\[
  t_0 = \sum_{i=1}^{\infty} 3^{-i-1}(2a_i)
\]
show that $f(3^kt_0) = a_k$, and hence that $x(t_0) = x_0$, $y(t_0) = y_0$.}
\emph{(This simple example of a so-called ``space-filling curve''
is due to I. J. Schoenberg, Bull. A.M.S., vol. 44, 1938, pp. 519.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\Phi(t)$ is continuous.}
  $x(t)$ and $y(t)$ is well-defined (Theorem 7.10).
  Thus $\Phi(t)$ is well-defined too.
  Note that each term of $x(t)$ or $y(t)$ is continuous
  (since $f$ itself is continuous),
  $x(t)$ or $y(t)$ is continuous (Theorem 7.11).
  Hence $\Phi(t)$ is continuous (Theorem 4.10).

\item[(2)]
  For each $(x_0,y_0) \in I^2$, we can write
  \begin{align*}
    x_0 &= \sum_{n=1}^{\infty} 2^{-n} a_{2n-1}, \\
    y_0 &= \sum_{n=1}^{\infty} 2^{-n} a_{2n}
  \end{align*}
  where each $a_i$ is $0$ or $1$
  (by the same argument in Decimals 1.22).
  For such $\{a_i\}$ define
  \[
    t_0
    = \sum_{i=1}^{\infty} 3^{-i-1}(2a_i).
  \]
  By Exercise 3.19, $t_0$ is precisely in the Cantor set.

\item[(3)]
  \emph{Show that $f(3^k t_0) = a_k$.}
  Write
  \begin{align*}
    3^k t_0
    &= \sum_{i=1}^{\infty} 3^{k-i-1}(2a_i) \\
    &= \sum_{i=1}^{k-1} 3^{k-i-1}(2a_i)
      + 3^{-1}(2a_k)
      + \sum_{i=k+1}^{\infty} 3^{k-i-1}(2a_i) \\
    &= \underbrace{2 \sum_{i=1}^{k-1} 3^{k-i-1} a_i}_{\text{define as } \alpha(k)}
      + \underbrace{\frac{2a_k}{3}}_{\text{define as } \beta(k)}
      + \underbrace{\sum_{i=k+1}^{\infty} \frac{2a_i}{3^{i-k+1}}}_{\text{define as } \gamma(k)}.
  \end{align*}
  Here $\alpha(k)$ is an even integer, and $\gamma(k) \in \left[0,\frac{1}{3}\right]$.
  Since $f$ is of a period $2$, we have
  \[
    f(3^k t_0)
    = f(\alpha(k) + \beta(k) + \gamma(k))
    = f(\beta(k) + \gamma(k)).
  \]
  Now we consider two possible cases of $a_k$.
  \begin{enumerate}
    \item[(a)]
      If $a_k = 0$, then $\beta(k) = 0$ and
      $\beta(k) + \gamma(k) \in \left[0,\frac{1}{3}\right]$ and
      $f(3^k t_0) = 0$.
    \item[(b)]
      If $a_k = 1$, then $\beta(k) = \frac{2}{3}$ and
      $\beta(k) + \gamma(k) \in \left[\frac{2}{3}, 1\right]$ and
      $f(3^k t_0) = 1$.
  \end{enumerate}
  In any case, $f(3^k t_0) = a_k$.

\item[(4)]
  \emph{Show that $x(t_0) = x_0$, $y(t_0) = y_0$,
  and $\Phi(t)$ maps the Cantor set onto $I^2$.}
  By (2)(3),
  \begin{align*}
    x(t_0)
    &= \sum_{n=1}^{\infty} 2^{-n} f(3^{2n-1}t_0)
    = \sum_{n=1}^{\infty} 2^{-n} a_{2n-1}
    = x_0, \\
    y(t_0)
    &= \sum_{n=1}^{\infty} 2^{-n} f(3^{2n}t_0)
    = \sum_{n=1}^{\infty} 2^{-n} a_{2n}
    = y_0.
  \end{align*}
  Hence $\Phi(t)$ maps the Cantor set onto $I^2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.15.}
\addcontentsline{toc}{subsection}{Exercise 7.15.}
\emph{Suppose that $f$ is a real continuous function on $\mathbb{R}^1$,
$f_n(t)= f(nt)$ for $n=1,2,3,\ldots$,
and $\{f_n\}$ is equicontinuous on $[0,1]$.
What conclusion can you draw about $f$?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $f$ is constant on $[0,\infty)$.}

\item[(2)]
  Given any $\varepsilon > 0$.
  Since $\{f_n\}$ is equicontinuous on $[0,1]$,
  there exists a $1 > \delta > 0$ such that
  \[
    |f_n(x)-f_n(y)| < \varepsilon
  \]
  whenever $n \in \mathbb{Z}^{+}$, $|x-y| < \delta < 1$, $x \in [0,1]$ and $y \in [0,1]$.
  Take $x = t \in [0,1]$ and $y = 0$.
  Note that $f_n(t) = f(nt)$ for any $n \in \mathbb{Z}^{+}$ and
  $t \in \mathbb{R}^1$.
  Hence
  \[
    |f(nt) - f(0)| < \varepsilon
  \]
  for all integer $n > 0$ and $0 \leq t < \delta < 1$.

\item[(3)]
  Given any $x \in [0,\infty)$.
  There is an integer $N > 0$ such that $0 \leq x < N\delta$
  (by taking $N > \frac{\delta}{x}$).
  Let $t = \frac{x}{N}$.
  So that $0 \leq t < \delta$.
  Hence
  \[
    |f(x) - f(0)| = |f(Nt) - f(0)| < \varepsilon.
  \]
  Since $\varepsilon > 0$ is arbitrary,
  $f(x) = f(0)$ for any $x \in [0,+\infty)$.
  Therefore $f$ is constant on $[0,\infty)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.16.}
\addcontentsline{toc}{subsection}{Exercise 7.16.}
\emph{Suppose $\{f_n\}$ is an equicontinuous sequence of functions on a compact set $K$,
and $\{f_n\}$ converges pointwise on $K$.
Prove that $\{f_n\}$ converges uniformly on $K$.} \\

(Assume that $\{f_n\}$ is a sequence of complex-valued functions.) \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
  \item[(1)]
  Since $\{f_n\}$ is equicontinuous, there is $\delta > 0$ such that
  \[
    |f_n(x) - f_n(y)| < \frac{\varepsilon}{3}
  \]
  whenever $x,y \in K$, $|x-y| < \delta$, $n = 1,2,3,\ldots$
  (where $d$ is the metric function).

  \item[(2)]
  (Similar to Exercise 4.8.)
  For such $\delta > 0$, we construct an open covering of $K$.
  Pick a collection $\mathscr{C}$ of open balls
  $B(a;\delta)$
  where $a$ runs over all elements of $K$.
  Since $\mathscr{C}$ is an open covering of a compact set $K$,
  there is a finite subcollection $\mathscr{C}'$ of $\mathscr{C}$
  also covers $K$, say
  \[
    \mathscr{C}'
    = \left\{B(a_1;\delta), B(a_2;\delta), \ldots, B(a_m;\delta) \right\}.
  \]

  \item[(3)]
  Since $f_n$ converges pointwise on $K$,
  for each $i$ there is an integer $N_i$ such that
  \[
    |f_n(a_i)-f_m(a_i)| < \frac{\varepsilon}{3}
  \]
  whenever $n,m \geq N_i$.

  \item[(4)]
  Now given any $x \in K$, by (2) there exist $a_j$ $(1 \leq j \leq m)$
  such that $x \in B(a_j;\delta)$.
  Take $N = \max\{N_1,\ldots,N_m\}$.
  Hence
  \begin{align*}
    |f_n(x)-f_m(x)|
    &\leq
    |f_n(x)-f_n(a_j)| + |f_n(a_j)-f_m(a_j)| + |f_m(a_j)-f_m(x)| \\
    &<
    \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
    &=
    \varepsilon.
  \end{align*}
  whenever $n,m \geq N$.
  Hence $\{f_n\}$ converges uniformly (Theorem 7.8).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.17.}
\addcontentsline{toc}{subsection}{Exercise 7.17.}
\emph{Define the notions of uniform convergence and equicontinuous
for mappings into any metric space.
Show that Theorems 7.9 and 7.12 are valid for mappings into any metric space,
that Theorems 7.8 and 7.11 are valid for mappings into any complete metric space,
and that Theorems 7.10, 7.16, 7.17, 7.24, and 7.25 hold for
vector-valued functions, that is, for mappings into any $\mathbb{R}^k$.} \\



\textbf{Definition 7.7 over metric spaces.}
\emph{Suppose $(X,d_X)$ and $(Y,d_Y)$ are metric spaces, $E \subseteq X$.
We say that a sequence of functions $\{f_n\}$ of $f_n: E \to Y$,
$n = 1,2,3,\ldots$,
\textbf{converges uniformly} on $E$ to a function $f$ mapping from $E$ to $Y$
if for every $\varepsilon > 0$ there is an integer $N$ such that $n \geq N$ implies
\[
  d_Y(f_n(x),f(x)) \leq \varepsilon
\]
for all $x \in E$.}
$\Box$ \\



\textbf{Theorem 7.8 over complete metric spaces.}
\emph{Suppose $(X,d_X)$ is a metric space,
$(Y,d_Y)$ is a complete metric spaces, $E \subseteq X$.
The sequence of functions $\{f_n\}$ of $f_n: E \to Y$,
converges uniformly on $E$ if and only if
for every $\varepsilon > 0$
there is an integer $N$ such that $m \geq N$, $n \geq N$, $x \in E$ implies}
\[
  d_Y(f_n(x),f_m(x)) \leq \varepsilon.
\]

\emph{Proof (Theorem 7.8).}
\begin{enumerate}
  \item[(1)]
  Suppose $\{f_n\}$ converges uniformly on $E$, and let $f$ be the limit function.
  Then there is an integer $N$ such that $n \geq N$, $x \in E$ implies
  \[
    d_Y(f_n(x),f(x)) \leq \frac{\varepsilon}{2},
  \]
  so that
  \[
    d_Y(f_n(x),f_m(x))
    \leq d_Y(f_n(x),f(x)) + d_Y(f(x),f_m(x))
    \leq \varepsilon
  \]
  if $n \geq N$, $m \geq N$, $x \in E$.

  \item[(2)]
  Conversely, suppose the Cauchy condition holds.
  By Theorem 3.11,
  the sequence $\{f_n(x)\}$ converges, for every $x$, to a limit which we may call $f(x)$.
  Thus the sequence $\{f_n\}$ converges on $E$, to $f$.
  We have to prove that the convergence is uniformly.
  Let $\varepsilon > 0$ be given, and choose $N$ such that
  \[
    d_Y(f_n(x),f_m(x)) \leq \varepsilon.
  \]
  Fix $n$, and let $m \to \infty$.
  Since $f_m(x) \to f(x)$ as $m \to \infty$, this gives
  \[
    d_Y(f_n(x),f(x)) \leq \varepsilon
  \]
  for every $n \geq N$ and every $x \in E$, which completes the proof.
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.9 over metric spaces.}
\emph{Suppose $(X,d_X)$ and $(Y,d_Y)$ are metric spaces, $E \subseteq X$.
Let $\{f_n\}$ be a sequence of functions of $f_n: E \to Y$.
Suppose
\[
  \lim_{n \to \infty}f_n(x) = f(x)
  \qquad
  (x \in E).
\]
Put
\[
  M_n = \sup_{x \in E} d_Y(f_n(x),f(x)).
\]
Then $f_n \to f$ uniformly on $E$ if and only if $M_n \to 0$ as $n \to \infty$.} \\

\emph{Proof (Theorem 7.9).}
Given any $\varepsilon > 0$.
\begin{enumerate}
  \item[(1)]
  Suppose $\{f_n\}$ converges uniformly on $E$.
  Then there is an integer $N$ such that $n \geq N$, $x \in E$ implies
  \[
    d_Y(f_n(x),f(x)) \leq \varepsilon.
  \]
  Take sup over all $x \in E$ to get
  \[
    M_n = \sup_{x \in E} d_Y(f_n(x),f(x)) \leq \varepsilon
  \]
  whenever $n \geq N$.

  \item[(2)]
  Conversely, suppose $M_n \to 0$ as $n \to \infty$.
  Then there is an integer $N$ such that $n \geq N$ implies
  \[
    M_n = \sup_{x \in E} d_Y(f_n(x),f(x)) \leq \varepsilon
  \]
  whenever $n \geq N$.
  Hence
  \[
    d_Y(f_n(x),f(x)) \leq \sup_{x \in E} d_Y(f_n(x),f(x)) \leq \varepsilon
  \]
  whenever $n \geq N$ and $x \in E$.
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.10 over $\mathbb{R}^k$.}
\emph{Suppose $E \subseteq \mathbb{R}^1$,
$\{\mathbf{f}_n\}$ is a sequence of functions of $\mathbf{f}_n: E \to \mathbb{R}^k$,
and
\[
  |\mathbf{f}_n(x)| \leq M_n
  \qquad
  (x \in E, n = 1,2,3,\ldots).
\]
Then $\sum \mathbf{f}_n$ converges uniformly on $E$ if $\sum M_n$ converges.} \\

\emph{Proof (Brute-force).}
If $\sum M_n$ converges, then for arbitrary $\varepsilon > 0$,
\[
  \abs{\sum_{i=n}^{m}\mathbf{f}_i(x)} \leq \sum_{i=n}^{m}M_i \leq \varepsilon
  \qquad
  (x \in E),
\]
provided $m$ and $n$ are large enough.
Uniform convergence now follows from Theorem 7.8 over complete metric spaces
(since $\mathbb{R}^1$ is complete).
$\Box$ \\



\textbf{Theorem 7.11 over complete metric spaces.}
\emph{Suppose $(X,d_X)$ is a metric space,
$(Y,d_Y)$ is a complete metric spaces, $E \subseteq X$.
Suppose the sequence of functions $\{f_n\}$ of $f_n: E \to Y$
converges uniformly on $E$.
Let $x$ be a limit point of $E$, and suppose that
\[
  \lim_{t \to x} f_n(t) = A_n
  \qquad
  (n=1,2,3,\ldots).
\]
Then $\{A_n\}$ converges, and
\[
  \lim_{t \to x}f(t) = \lim_{n \to \infty} A_n.
\]
In other words, the conclusion is that}
\[
  \lim_{t \to x} \lim_{n \to \infty} f_n(t)
  = \lim_{n \to \infty} \lim_{t \to x} f_n(t).
\]

\emph{Proof (Theorem 7.11).}
\begin{enumerate}
\item[(1)]
  Let $\varepsilon > 0$ be given.
  By the uniform convergence of $\{f_n\}$, there exists an integer $N$
  such that $n \geq N$, $m \geq N$, $t \in E$ imply
  \[
    d_Y(f_n(t),f_m(t)) \leq \varepsilon.
  \]
  Letting $t \to x$, we obtain
  \[
    d_Y(A_n,A_m) \leq \varepsilon
  \]
  for $n \geq N$, $m \geq N$, so that $\{A_n\}$ is a Cauchy sequence in $Y$
  and therefore converges, say to $A$ (since $Y$ is complete).

\item[(2)]
  Next,
  \[
    d_Y(f(t),A)
    \leq
    d_Y(f(t),f_n(t)) + d_Y(f_n(t),A_n) + d_Y(A_n,A).
  \]

\item[(3)]
  We choose $N_1$ such that
  \[
    d_Y(f(t),f_n(t)) \leq \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_1$ and $t \in E$
  (by the uniform convergence of $\{f_n\}$).

\item[(4)]
  We choose $N_2$ such that
  \[
    d_Y(A_n,A) \leq \frac{\varepsilon}{3}
  \]
  whenever $n \geq N_2$.

\item[(5)]
  In particular, we choose $n = \max\{N_1,N_2\}$ in (3)(4).
  For such $n$, there is an open neighborhood $B(x)$ of $x$ such that
  \[
    d_Y(f_n(t),A_n) \leq \frac{\varepsilon}{3}
  \]
  if $t \in B(x) \bigcap E - \{x\}$.

\item[(6)]
  Substituting the inequalities (3)(4)(5) into (2), we see that
  \[
    d_Y(f(t),A) \leq \varepsilon,
  \]
  provided $t \in B(x) \bigcap E - \{x\}$.
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.12 over metric spaces.}
\emph{Suppose $(X,d_X)$ and $(Y,d_Y)$ are metric spaces, $E \subseteq X$.
If $\{f_n\}$ is a sequence of continuous functions of $f_n: E \to Y$,
and if $f_n \to f$ uniformly on $E$,
then $f$ is continuous on $E$.} \\

\emph{Note.}
It is not a corollary of Theorem 7.11 over complete metric spaces
since $Y$ might not be complete. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Suppose $x \in E$.
  If $x$ is an isolated point of $E$, there is nothing to do.
  We might assume that $x$ is a limit point of $E$.
  Let $\varepsilon > 0$ be given.

\item[(2)]
  Next, write
  \[
    d_Y(f(y),f(x))
    \leq
    d_Y(f(y),f_n(y)) + d_Y(f_n(y),f_n(x)) + d_Y(f_n(x),f(x))
  \]
  if $y \in E$.

\item[(3)]
  By the uniform convergence of $\{f_n\}$, there exists an integer $N$
  such that $n \geq N$, $t \in E$ imply
  \[
    d_Y(f_n(t),f(t)) \leq \frac{\varepsilon}{3}.
  \]

\item[(4)]
  In particular, we choose $n = N$ in (3).
  For such $n$, by the continuity of $f_n$
  there is a $\delta > 0$ such that
  \[
    d_Y(f_n(t),f_n(x)) < \frac{\varepsilon}{3}
  \]
  if $0 < d_X(t,x) < \delta$ and $t \in E$.

\item[(5)]
  Substituting the inequalities (3)(4) into (2), we see that
  \[
    d_Y(f(y),f(x)) < \varepsilon,
  \]
  provided $0 < d_X(y,x) < \delta$ and $y \in E$.
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.16 over $\mathbb{R}^k$.}
\emph{Let $\alpha$ be monotonically increasing on $[a,b]$.
Suppose $\mathbf{f}_n \in \mathscr{R}(\alpha)$ on $[a,b]$,
for $n = 1,2,3,\ldots$,
and suppose $\mathbf{f}_n \to \mathbf{f}$ uniformly on $[a,b]$.
Then $\mathbf{f} \in \mathscr{R}(\alpha)$ on $[a,b]$,
and
\[
  \int_{a}^{b} \mathbf{f} d\alpha
  = \lim_{n \to \infty} \int_{a}^{b} \mathbf{f}_n d\alpha.
\]
(The existence of the limit is part of the conclusion.)} \\

\emph{Proof (Theorem 7.16).}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \mathbf{f}_n = \left(f_{n(1)}, \ldots, f_{n(k)}\right)
    \qquad
    \text{ and }
    \qquad
    \mathbf{f} = \left(f_{(1)}, \ldots, f_{(k)}\right)
  \]
  be the corresponding mappings of $[a,b]$ into $\mathbb{R}^k$.
  Since $\mathbf{f}_n \to \mathbf{f}$ uniformly on $[a,b]$,
  $f_{n(j)} \to f_{(j)}$ uniformly on $[a,b]$ for $j = 1,\ldots,k$
  by noting that
  \[
    \abs{ f_{n(j)}(x) - f_{(j)}(x) }
    \leq
    \abs{ \mathbf{f}_n(x) - \mathbf{f}(x) }
  \]
  for all $j$.

\item[(2)]
  By Definition 6.23,
  $\mathbf{f}_n \in \mathscr{R}(\alpha)$ on $[a,b]$
  means that
  $f_{n(j)} \in \mathscr{R}(\alpha)$ on $[a,b]$ for all $j$.
  By Theorem 7.16, $f_{(j)} \in \mathscr{R}(\alpha)$ on $[a,b]$ and
  \[
    \int_{a}^{b} f_{(j)} d\alpha
    = \lim_{n \to \infty} \int_{a}^{b} f_{n(j)} d\alpha
  \]
  for all $j$.

\item[(3)]
  By Definition 6.23, $\mathbf{f} \in \mathscr{R}(\alpha)$ on $[a,b]$, and
  \begin{align*}
    \int_{a}^{b} \mathbf{f} d\alpha
    &= \left(
        \int_{a}^{b} f_{(1)} d\alpha,
        \ldots,
        \int_{a}^{b} f_{(k)} d\alpha
      \right) \\
    &= \left(
        \lim_{n \to \infty} \int_{a}^{b} f_{n(1)} d\alpha,
        \ldots,
        \lim_{n \to \infty} \int_{a}^{b} f_{n(k)} d\alpha
      \right) \\
    &= \lim_{n \to \infty} \left(
        \int_{a}^{b} f_{n(1)} d\alpha,
        \ldots,
        \int_{a}^{b} f_{n(k)} d\alpha
      \right) \\
    &= \lim_{n \to \infty} \int_{a}^{b} \mathbf{f}_n d\alpha.
  \end{align*}
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.17 over $\mathbb{R}^k$.}
\emph{Suppose $\{\mathbf{f}_n\}$ is a sequence of vector-valued functions,
differentiable on $[a,b]$ and such that
$\{\mathbf{f}_n(x_0)\}$ converges for some point $x_0 \in [a,b]$.
If $\{\mathbf{f}'_n\}$ converges uniformly on $[a,b]$,
then $\{\mathbf{f}_n\}$ converges uniformly on $[a,b]$ to a function $\mathbf{f}$,
and}
\[
  \mathbf{f}'(x) = \lim_{n \to \infty} \mathbf{f}'_n(x)
  \qquad
  (a \leq x \leq b).
\]
\emph{Proof (Theorem 7.17).}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \mathbf{f}_n = \left(f_{n(1)}, \ldots, f_{n(k)}\right)
  \]
  be the corresponding mappings of $[a,b]$ into $\mathbb{R}^k$.

\item[(2)]
  $\{\mathbf{f}_n\}$ is a sequence of differentiable functions on $[a,b]$
  if and only if
  $\{f_{n(j)}\}$ is a sequence of differentiable functions on $[a,b]$ for all $j=1,\ldots,k$
  (Remarks 5.16).

\item[(3)]
  $\{\mathbf{f}_n(x_0)\}$ converges for some point $x_0 \in [a,b]$
  if and only if
  $\{f_{n(j)}(x_0)\}$ converges for some point $x_0 \in [a,b]$ and all $j=1,\ldots,k$.

\item[(4)]
  $\{\mathbf{f}'_n\}$ converges uniformly on $[a,b]$
  if and only if
  $\{ f'_{n(j)} \}$ converges uniformly on $[a,b]$ for all $j=1,\ldots,k$.

\item[(5)]
  By Theorem 7.17, $\{f_{n(j)}\}$ converges uniformly on $[a,b]$, to a function $f_{(j)}$,
  and
  \[
    f'_{(j)}(x) = \lim_{n \to \infty} f'_{n(j)}(x)
    \qquad
    (a \leq x \leq b).
  \]
  for all $j$.
  Define
  \[
    \mathbf{f} = \left(f_{(1)}, \ldots, f_{(k)}\right).
  \]
  Hence $\{\mathbf{f}_n\}$ converges uniformly on $[a,b]$ to $\mathbf{f}$,
  and
  \begin{align*}
    \mathbf{f}'(x)
    &= \left( f'_{(1)}(x), \ldots, f'_{(k)}(x) \right) \\
    &= \left( \lim_{n \to \infty} f'_{n(1)}(x), \ldots, \lim_{n \to \infty} f'_{n(k)}(x) \right) \\
    &= \lim_{n \to \infty} \left( f'_{n(1)}(x), \ldots, f'_{n(k)}(x) \right) \\
    &= \lim_{n \to \infty} \mathbf{f}'_n(x).
  \end{align*}
\end{enumerate}
$\Box$ \\



\textbf{Definition 7.22 over metric spaces.}
\emph{Suppose $(X,d_X)$ and $(Y,d_Y)$ are metric spaces, $E \subseteq X$.
A family $\mathscr{F}$ of functions $f: E \to Y$ is
said to be \textbf{equicontinuous} on $E$
if for every $\varepsilon > 0$ there is a $\delta > 0$ such that
\[
  d_Y(f(y),f(x)) < \varepsilon
\]
whenever $d_X(x,y) < \delta$, $x \in E$, $y \in E$, and $f \in \mathscr{F}$.}
$\Box$ \\



\textbf{Theorem 7.24 over $\mathbb{R}^k$.}
\emph{If $K$ is a compact metric space,
if $\mathbf{f}_n \in \mathscr{C}(K)$ for $n=1,2,3,\ldots$,
and if $\{\mathbf{f}_n\}$ converges uniformly on $K$,
then $\{\mathbf{f}_n\}$ is equicontinuous on $K$.} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Note that Definition 7.14 works for vector-valued functions.

\item[(2)]
  Similar to the proof of Theorem 7.24.
  Let $\varepsilon > 0$ be given.
  Since $\{\mathbf{f}_n\}$ converges uniformly, there is an integer $N$ such that
  \[
    \norm{ \mathbf{f}_n - \mathbf{f}_N } < \frac{\varepsilon}{3}
    \qquad
    (n > N).
  \]

\item[(3)]
  Since continuous functions are uniformly continuous on compact set,
  there is a $\delta > 0$ such that
  \[
    \abs{ \mathbf{f}_i(x) - \mathbf{f}_i(y) } < \frac{\varepsilon}{3}
  \]
  if $1 \leq i \leq N$ and $d(x,y) < \delta$.

\item[(4)]
  If $n > N$ and $d(x,y) < \delta$, it follows that
  \begin{align*}
    &\abs{\mathbf{f}_n(x)-\mathbf{f}_n(y)} \\
    \leq& \abs{\mathbf{f}_n(x)-\mathbf{f}_N(x)}
     + \abs{\mathbf{f}_N(x)-\mathbf{f}_N(y)}
     + \abs{\mathbf{f}_N(y)-\mathbf{f}_n(y)} \\
    \leq& \norm{ \mathbf{f}_n - \mathbf{f}_N }
     + \abs{\mathbf{f}_N(x)-\mathbf{f}_N(y)}
     + \norm{ \mathbf{f}_n - \mathbf{f}_N } \\
    <& \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
    =& \varepsilon.
  \end{align*}

\item[(5)]
By (3)(4), the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof (Theorem 7.24).}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \mathbf{f}_n = \left(f_{n(1)}, \ldots, f_{n(k)}\right)
  \]
  be the corresponding mappings of $K$ into $\mathbb{R}^k$.
  Each $f_{n(j)} \in \mathscr{C}(K)$ too.

\item[(2)]
  $\{\mathbf{f}_n\}$ converges uniformly on $K$
  if and only if
  $\{ f_{n(j)} \}$ converges uniformly on $K$ for all $j=1,\ldots,k$.

\item[(3)]
  By Theorem 7.24,
  $\{ f_{n(j)} \}$ is equicontinuous on $K$ for all $j=1,\ldots,k$.
  Let $\varepsilon > 0$ be given.
  There exist $\delta_j > 0$ such that
  \[
    \abs{f_{n(j)}(x) - f_{n(j)}(y)} < \frac{\varepsilon}{\sqrt{k}}
  \]
  whenever $d(x,y) < \delta_j$, $x \in K$ and $y \in K$.
  Take $\delta = \min\{\delta_1, \ldots, \delta_k\} > 0$.
  Hence
  \begin{align*}
    &\abs{\mathbf{f}_n(x) - \mathbf{f}(y)} \\
    =& \left\{ \abs{f_{n(1)}(x) - f_{n(1)}(y)}^2
      + \ldots
      + \abs{f_{n(k)}(x) - f_{n(k)}(y)}^2 \right\}^{\frac{1}{2}} \\
    <& \left\{ k \cdot \left(\frac{\varepsilon}{\sqrt{k}}\right)^2 \right\}^{\frac{1}{2}} \\
    =& \varepsilon.
  \end{align*}
\end{enumerate}
$\Box$ \\



\textbf{Theorem 7.25 over $\mathbb{R}^k$ (Arzel\`{a}-Ascoli theorem).}
\emph{If $K$ is compact, if $\mathbf{f}_n \in \mathscr{C}(K)$ for $n = 1,2,3,\ldots$,
and if $\{\mathbf{f}_n\}$ is pointwise bounded and equicontinuous on $K$, then}
\begin{enumerate}
\item[(a)]
  \emph{$\{\mathbf{f}_n\}$ is uniformly bounded on $K$.}

\item[(b)]
  \emph{$\{\mathbf{f}_n\}$ contains a uniformly convergent subsequence.} \\
\end{enumerate}

\emph{Proof of (a)(Theorem 7.25).}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \mathbf{f}_n = \left(f_{n(1)}, \ldots, f_{n(k)}\right)
  \]
  be the corresponding mappings of $K$ into $\mathbb{R}^k$.
  Each $f_{n(j)} \in \mathscr{C}(K)$ too.

\item[(2)]
  $\{\mathbf{f}_n\}$ is pointwise bounded on $K$
  if and only if
  $\{ f_{n(j)} \}$ is pointwise bounded on $K$ for all $j=1,\ldots,k$.

\item[(3)]
  $\{\mathbf{f}_n\}$ is equicontinuous on $K$
  if and only if
  $\{ f_{n(j)} \}$ is equicontinuous on $K$ for all $j=1,\ldots,k$.

\item[(4)]
  By Theorem 7.25(a),
  $\{ f_{n(j)} \}$ is uniformly bounded on $K$,
  say $\abs{f_{n(j)}(x)} \leq M_j$ ($x \in K$), for all $j=1,\ldots,k$.
  So
  \begin{align*}
    \abs{\mathbf{f}_n(x)}
    &= \left\{ f_{n(1)}(x)^2 + \cdots + f_{n(k)}(x)^2 \right\}^{\frac{1}{2}} \\
    &\leq \left\{ M_1^2 + \cdots + M_k^2 \right\}^{\frac{1}{2}}.
  \end{align*}
  Here $\left\{ M_1^2 + \cdots + M_k^2 \right\}^{\frac{1}{2}}$ is a constant
  and thus $\{\mathbf{f}_n\}$ is uniformly bounded on $K$.
  \end{enumerate}
$\Box$ \\



\emph{Proof of (b)(Theorem 7.25).}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \mathbf{f}_n = \left(f_{n(1)}, \ldots, f_{n(k)}\right)
  \]
  be the corresponding mappings of $K$ into $\mathbb{R}^k$.
  Each $f_{n(j)} \in \mathscr{C}(K)$ too.

\item[(2)]
  $\{\mathbf{f}_n\}$ is pointwise bounded on $K$
  if and only if
  $\{ f_{n(j)} \}$ is pointwise bounded on $K$ for all $j=1,\ldots,k$.

\item[(3)]
  $\{\mathbf{f}_n\}$ is equicontinuous on $K$
  if and only if
  $\{ f_{n(j)} \}$ is equicontinuous on $K$ for all $j=1,\ldots,k$.

\item[(4)]
  By Theorem 7.25(b),
  $\{ f_{n(1)} \}$ contains a uniformly convergent subsequence,
  say
  \[
    \left\{ f_{n_{m(1)}(1)} \right\}.
  \]

\item[(5)]
  Again,
  $\{ f_{n_{m(1)}(2)} \}$ contains a uniformly convergent subsequence,
  say
  \[
    \left\{ f_{n_{m(1),m(2)}(2)} \right\}.
  \]
  Note that
  \[
    \left\{ f_{n_{m(1),m(2)}(1)} \right\}
  \]
  is also a subsequence of $\left\{ f_{n_{m(1)}(1)} \right\}$ or $\{f_{n(1)}\}$
  and thus is uniformly convergent.

\item[(6)]
  Continue this process again and again.
  So we can get a uniformly convergent subsequence of $\{ f_{n(j)} \}$,
  say
  \[
    \left\{ f_{n_{m(1),\ldots,m(k)}(j)} \right\},
  \]
  which is uniformly convergent for all $j=1,\ldots,k$.
  Therefore,
  $\{\mathbf{f}_n\}$ contains a uniformly convergent subsequence
  \[
    \left\{ \mathbf{f}_{n_{m(1),\ldots,m(k)}} \right\}.
  \]
  \end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.18.}
\addcontentsline{toc}{subsection}{Exercise 7.18.}
\emph{Let $\{f_n\}$ be a uniformly bounded sequence of functions which are
Riemann-integrable on $[a,b]$, and put
\[
  F_n(x) = \int_{a}^{x} f_n(t)dt
  \qquad
  (a \leq x \leq b).
\]
Prove that there exists a subsequence $\{F_{n_k}\}$ which
converges uniformly on $[a,b]$.} \\

\emph{Proof (Theorem 7.25).}
\begin{enumerate}
\item[(1)]
  Since $\{f_n\}$ is uniformly bounded,
  there exists a $M$ such that $|f_n(x)| \leq M$ for all $n$ and $x \in [a,b]$.
  Note that $[a,b]$ is compact.

\item[(2)]
  \emph{Show that $\{F_n\}$ is uniformly bounded
  (and thus pointwise bounded) on $[a,b]$.}
  \begin{align*}
    \abs{F_n(x)}
    &= \abs{ \int_{a}^{x} f_n(t)dt } \\
    &\leq \int_{a}^{x} \abs{f_n(t)}dt \\
    &\leq \int_{a}^{x} M dt \\
    &= (x-a)M \\
    &\leq (b-a)M.
  \end{align*}
  Here $(b-a)M$ is constant.

\item[(3)]
  \emph{Show that $\{F_n\}$ is equicontinuous on $[a,b]$.}
  Similar to (2).
  Given any $\varepsilon > 0$,
  there is a $\delta = \frac{\varepsilon}{M+1} > 0$ such that
  \begin{align*}
    \abs{F_n(x)-F_n(y)}
    &= \abs{ \int_{a}^{x} f_n(t)dt - \int_{a}^{y} f_n(t)dt } \\
    &= \abs{ \int_{y}^{x} f_n(t)dt } \\
    &\leq \int_{\min\{x,y\}}^{\max\{x,y\}} \abs{f_n(t)}dt \\
    &\leq \int_{\min\{x,y\}}^{\max\{x,y\}} M dt \\
    &\leq |x-y|M \\
    &\leq \frac{\varepsilon}{M+1} \cdot M \\
    &< \varepsilon
  \end{align*}
  whenever $|x-y| < \delta$, $x \in [a,b]$ and $y \in [a,b]$.
  Hence $\{F_n\}$ is equicontinuous on $[a,b]$.

\item[(4)]
  By Theorem 7.25(b)(Arzel\`{a}-Ascoli theorem),
  $\{F_n\}$ contains a uniformly convergent sequence.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.19.}
\addcontentsline{toc}{subsection}{Exercise 7.19.}
\emph{Let $K$ be a compact metric space,
let $S$ be a subset of $\mathscr{C}(K)$.
Prove that $S$ is compact (with respect to the metric defined in Section 7.14)
if and only if
$S$ is uniformly closed, pointwise bounded, and equicontinuous.
(If $S$ is not equicontinuous,
then $S$ contains a sequence which has no equicontinuous subsequence,
hence has no subsequence that converges uniformly on $K$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $S$ is compact
  if $S$ is uniformly closed, pointwise bounded, and equicontinuous.}
  \begin{enumerate}
  \item[(a)]
    By Exercise 2.26, it suffices to
    \emph{show that every infinite subset $E$ of $S$ has a limit point.}
    From such infinite subset $E$ we can take
    $f_n \in E \subseteq S \subseteq \mathscr{C}(K)$ for $n = 1,2,3,\ldots$.
    Consider the sequence $\{f_n\}$.

  \item[(b)]
    $\{f_n\}$ is uniformly (pointwise) bounded since $S$ is uniformly bounded.

  \item[(c)]
    $\{f_n\}$ is equicontinuous since $S$ is equicontinuous.

  \item[(d)]
    Since $K$ is compact, we can apply Theorem 7.25(b)(Arzel\`{a}-Ascoli theorem)
    to (b)(c) to get that
    $\{f_n\}$ contains a uniformly convergent subsequence
    $\{f_{n_k}\}$ converging to a limit point $f$.
    Since $f_{n_k} \in S$ and $S$ is uniformly closed, $f \in S$.
  \end{enumerate}

\item[(2)]
  \emph{Show that $S$ is uniformly closed if $S$ is compact.}
  $S$ is closed since $S$ is compact (Theorem 2.34).
  By Definition 7.14, $S$ is also called uniformly closed.

\item[(3)]
  \emph{Show that $S$ is uniformly (pointwise) bounded if $S$ is compact.}
  Let
  \[
    G_n = \{ f \in S : \norm{f} < n \}
  \]
  for $n = 1,2,3,\ldots$.
  Note that
  \[
    \mathscr{C} = \{ G_n \}_{n=1,2,3,\ldots}
  \]
  is an open covering of $S$ since each $f \in S \subseteq \mathscr{C}(K)$ is bounded.
  Since $S$ is compact, there exists a finite subcovering $\mathscr{C}'$ of $\mathscr{C}$,
  say
  \[
    \mathscr{C}' = \left\{ G_{n_1}, \ldots, G_{n_k} \right\}.
  \]
  Let $N = \max\{n_1, \ldots, n_k\}$.
  Then
  \[
    S \subseteq G_{n_1} \cup \cdots \cup G_{n_k} = G_N,
  \]
  or $S$ is uniformly (pointwise) bounded by $N$.

\item[(4)]
  \emph{Show that $S$ is equicontinuous if $S$ is compact.}
  (Reductio ad absurdum)
  If $S$ were not equicontinuous,
  then $S$ contains a sequence $\{f_n\}$ which has no equicontinuous subsequence,
  hence has no subsequence that converges uniformly on $K$.
  However, $\{f_n\} \subseteq S$ and the compactness of $S$ imply that
  some subsequence of $\{f_n\}$ converges to a point of $S$ (Theorem 3.6(a)),
  which is absurd.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.20.}
\addcontentsline{toc}{subsection}{Exercise 7.20.}
\emph{If $f$ is continuous on $[0,1]$ and if
\[
  \int_{0}^{1} f(x) x^n dx = 0
  \qquad
  (n=0,1,2,\ldots),
\]
prove that $f(x) = 0$ on $[0,1]$.
(Hint: The integral of the product of $f$ with any polynomial is zero.
Use the Weierstrass theorem to show that
$\int_{0}^{1} f^2(x) dx = 0$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Since $\int_{0}^{1} f(x) x^n dx = 0$ for all $n = 0,1,2,\ldots$,
\[
  \int_{0}^{1} f(x) P(x) dx = 0 \text{ for all } P(x) \in \mathbb{R}[x].
\]

\item[(2)]
By Theorem 7.26 (Stone-Weierstrass Theorem),
there exists a sequence of $P_n(x) \in \mathbb{R}[x]$ such that
\[
  P_n(x) \to f(x)
\]
uniformly on $[0,1]$.
Since $f(x)$ is continuous on the compact set $[0,1]$, $f(x)$ is bounded on $[0,1]$.
Hence
\[
  f(x) P_n(x) \to f^2(x)
\]
uniformly on $[0,1]$.

\item[(3)]
Since each $f(x) P_n(x)$ is continuous,
$f(x) P_n(x) \in \mathscr{R}$ on $[0,1]$ (Theorem 6.8).
By Theorem 7.16,
\[
  \int_{0}^{1} f^2(x) dx
  = \lim_{n \to \infty} \int_{0}^{1} f(x) P_n(x) dx
  = \lim_{n \to \infty} 0
  = 0.
\]

\item[(4)]
Since $f^2(x)$ is continuous,
$f^2(x) = 0$ or $f(x) = 0$ by (3) and Exercise 6.2.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.21.}
\addcontentsline{toc}{subsection}{Exercise 7.21.}
\emph{Let $K$ be the unit circle in the complex plane
(i.e., the set of all $z$ with $|z|=1$),
and let $\mathscr{A}$ be the algebra of all functions of the form
\[
  f(e^{i\theta}) = \sum_{n=0}^{N} c_n e^{in\theta}.
  \qquad
  (\theta \text{ real}).
\]
Then $\mathscr{A}$ separates points on $K$ and
$\mathscr{A}$ vanishes at no point of $K$.
But nevertheless there are continuous functions on $K$ which
are not in the uniform closure of $\mathscr{A}$.
(Hint: For every $f \in \mathscr{A}$
\[
  \int_{0}^{2\pi} f(e^{i\theta})e^{i\theta}d\theta = 0,
\]
and this is also true for every $f$ in the clousure of $\mathscr{A}$.)} \\

\emph{Note.}
$\mathscr{A}$ is not a self-adjoint algebra of complex continuous functions
on $K$, and thus Theorem 7.33 does not hold. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\mathscr{A}$ separates points on $K$ and
  $\mathscr{A}$ vanishes at no point of $K$.}
  Take $f \in \mathscr{A}$ defined by
  \[
    f(e^{i\theta}) = e^{i\theta}.
  \]
  Since $f(z) = z$ for every $z \in K$,
  $f(z_1) \neq f(z_2)$ for every pair of distinct points $z_1, z_2 \in K$.
  Hence $\mathscr{A}$ separates points on $K$.
  Besides, $f(z) \neq 0$ for all $z \in K = \{ z \in \mathbb{C} : |z|=1 \}$.
  Hence $\mathscr{A}$ vanishes at no point of $K$.

\item[(2)]
  \emph{Show that for every $f \in \mathscr{A}$}
  \[
    \int_{0}^{2\pi} f(e^{i\theta})e^{i\theta}d\theta = 0.
  \]
  Similar to Definition 8.9. Given $f \in \mathscr{A}$.
  \begin{align*}
    \int_{0}^{2\pi} f(e^{i\theta})e^{i\theta}d\theta
    &= \int_{0}^{2\pi} \sum_{n=0}^{N} c_n e^{in\theta}e^{i\theta}d\theta \\
    &= \sum_{n=0}^{N} c_n \int_{0}^{2\pi} e^{i(n+1)\theta} d\theta \\
    &= \sum_{n=0}^{N} c_n \cdot 0 \\
    &= 0.
  \end{align*}

\item[(3)]
  \emph{Show that for every $f$ in the clousure of $\mathscr{A}$}
  \[
    \int_{0}^{2\pi} f(e^{i\theta})e^{i\theta}d\theta = 0.
  \]
  Given $f$ in the clousure of $\mathscr{A}$.
  Then $f$ is the limit function of some uniformly convergent sequences $\{f_n\}$
  of members of $\mathscr{A}$.
  Note that $e^{i\theta}$ is bounded on $K$
  and thus $f_n(e^{i\theta})e^{i\theta} \to f(e^{i\theta})e^{i\theta}$ uniformly (Theorem 7.9).
  So Theorem 7.16 implies that
  \[
    \int_{0}^{2\pi} f(e^{i\theta})e^{i\theta}d\theta
    = \lim_{n \to \infty} \int_{0}^{2\pi} f_n(e^{i\theta})e^{i\theta}d\theta
    = \lim_{n \to \infty} 0
    = 0.
  \]

\item[(4)]
  Consider $g(e^{i\theta}) = e^{-i\theta}$ on $K$, or $g(z) = \frac{1}{z}$.
  By definition, $g$ is continuous on $K$ but
  \[
    \int_{0}^{2\pi} g(e^{i\theta})e^{i\theta}d\theta
    = \int_{0}^{2\pi} d\theta
    = 2\pi
    \neq 0.
  \]
  Hence there are continuous functions on $K$ which
  are not in the uniform closure of $\mathscr{A}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.22.}
\addcontentsline{toc}{subsection}{Exercise 7.22.}
\emph{Assume $f \in \mathscr{R}(\alpha)$ on $[a,b]$,
and prove that there are polynomials $P_n$ such that
\[
  \lim_{n \to \infty} \int_{a}^{b} |f-P_n|^2 d\alpha = 0.
\]
(Compare with Exercise 6.12.)} \\

\emph{Notation.}
For $u \in \mathscr{R}(\alpha)$ on $[a,b]$, define
  \[
    \norm{u}_2 = \left\{ \int_{a}^{b}|u|^2 d\alpha \right\}^{\frac{1}{2}}.
  \] \\

\emph{Proof.}
Given any $\varepsilon = \frac{1}{n} > 0$ $(n=1,2,3,\ldots$).
\begin{enumerate}
\item[(1)]
  By Exercise 6.12, there exists a continuous function $g_n$ on $[a,b]$
  such that
  \[
    \norm{f-g_n}_2 < \frac{1}{n}.
  \]

\item[(2)]
  By Theorem 7.26 (Stone-Weierstrass Theorem),
  there is a polynomial $P_n$ such that
  \[
    |g_n(x)-P_n(x)| < \frac{1}{n}
  \]
  for all $x \in [a,b]$.
  Thus
  \[
    \norm{g_n-P_n}_2
    \leq
    \left\{ \int_{a}^{b}\left(\frac{1}{n}\right)^2 d\alpha \right\}^{\frac{1}{2}}
    =
    \frac{(\alpha(b)-\alpha(a))^{\frac{1}{2}}}{n}.
  \]

\item[(3)]
  By Exercise 6.11,
  \[
    \norm{f-P_n}_2
    \leq
    \norm{f-g_n}_2 + \norm{g_n-P_n}_2
    \leq
    \frac{1+(\alpha(b)-\alpha(a))^{\frac{1}{2}}}{n},
  \]
  or
  \[
    0
    \leq
    \int_{a}^{b} |f-P_n|^2 d\alpha
    \leq
    \frac{[1+(\alpha(b)-\alpha(a))^{\frac{1}{2}}]^2}{n^2}.
  \]
  As $n \to \infty$, $\int_{a}^{b} |f-P_n|^2 d\alpha \to 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.23.}
\addcontentsline{toc}{subsection}{Exercise 7.23.}
\emph{Put $P_0 = 0$, and define, for $n = 0,1,2,\ldots$,
\[
  P_{n+1}(x) = P_n(x) + \frac{x^2-P_n^2(x)}{2}.
\]
Prove that
\[
  \lim_{n \to \infty} P_n(x) = |x|,
\]
uniformly on $[-1,1]$.
(This makes it possible to prove the Stone-Weierstrass theorem without
first proving Theorem 7.26.)
(Hint: Use the identity
\[
  |x| - P_{n+1} = [|x| - P_n(x)]\left[1-\frac{|x|+P_n(x)}{2}\right]
\]
to prove that $0 \leq P_n(x) \leq P_{n+1}(x) \leq |x|$ if $|x| \leq 1$, and that
\[
  |x| - P_n(x) \leq |x| \left(1-\frac{|x|}{2}\right)^n < \frac{2}{n+1}
\]
if $|x| \leq 1$.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
\begin{align*}
  |x| - P_{n+1}(x)
  &= |x| - P_n(x) - \frac{|x|^2 - P_n^2(x)}{2} \\
  &= |x| - P_n(x) - \frac{(|x| + P_n(x))(|x| - P_n(x))}{2} \\
  &= [|x| - P_n(x)]\left[1-\frac{|x|+P_n(x)}{2}\right].
\end{align*}

\item[(2)]
\emph{Show that $0 \leq P_n(x) \leq |x|$ if $|x| \leq 1$.}
Induction on $n$.
  \begin{enumerate}
  \item[(a)]
  If $n = 0$, then $P_n(x) = P_0(x) = 0$ and thus $0 \leq P_0(x) \leq |x|$.

  \item[(b)]
  Assume the induction hypothesis that for the single case $n = k$ holds,
  and thus $0 \leq P_k(x) \leq |x|$ if $|x| \leq 1$.
  So
  \begin{align*}
    0 &\leq |x|-P_k(x) \leq |x|, \\
    0 \leq 1-|x| &\leq 1-\frac{|x|+P_k(x)}{2} \leq 1-\frac{|x|}{2} \leq 1
  \end{align*}
  if $|x| \leq 1$.
  Hence
  \[
    0 \leq [|x| - P_k(x)]\left[1-\frac{|x|+P_k(x)}{2}\right] \leq |x|.
  \]
  By (1),
  \[
    0 \leq |x| - P_{k+1}(x) \leq |x|
  \]
  or $0 \leq P_{k+1}(x) \leq |x|$ if $|x| \leq 1$

  \item[(c)]
  Since both the base case in (a) and
  the inductive step in (b) have been proved as true,
  by mathematical induction the result holds.
  \end{enumerate}

\item[(3)]
\emph{Show that $0 \leq P_n(x) \leq P_{n+1}(x) \leq |x|$ if $|x| \leq 1$.}
By (2), it suffices to show that $P_{n}(x) \leq P_{n+1}(x)$.
By (1)(2), we have
\begin{align*}
  |x| - P_{n+1}(x)
  &= [|x| - P_n(x)]\left[1-\frac{|x|+P_n(x)}{2}\right] \\
  &\leq |x| - P_n(x)
\end{align*}
or $P_{n}(x) \leq P_{n+1}(x)$.

\item[(4)]
\emph{Define $f_n(t) = t(1-t)^n$ on $\left[0,\frac{1}{2}\right]$ for $n=1,2,3,\ldots$.
Show that $f_n(t) \leq \frac{1}{n+1}$.}
Since
\[
  f'_n(t) = (1-t)^{n-1}(1 - (n+1)t)
\]
$f'_n(t) = 0$ on $\left[0,\frac{1}{2}\right]$ if and only if $t = \frac{1}{n+1}$.
By Theorem 5.11, $f_n(t)$ reaches its maximum at $t = \frac{1}{n+1}$.
Hence
\[
  f_n(t)
  \leq f_n\left(\frac{1}{n+1}\right)
  = \frac{1}{n+1} \left(\frac{n}{n+1}\right)^n
  < \frac{1}{n+1}.
\]

\item[(5)]
\emph{Show that
\[
  |x| - P_n(x) \leq |x| \left(1-\frac{|x|}{2}\right)^n < \frac{2}{n+1}
\]
if $|x| \leq 1$.}
Note that
\begin{align*}
  |x| - P_n(x)
  &\leq [ |x| - P_0(x) ] \prod_{k=0}^{n-1}\left[1-\frac{|x|+P_k(x)}{2}\right]
    & ((1)) \\
  &\leq |x| \prod_{k=0}^{n-1}\left[1-\frac{|x|}{2}\right]
    & ((2)) \\
  &\leq |x| \left[1-\frac{|x|}{2}\right]^n \\
  & < \frac{2}{n+1}
    & \text{(Put $t=\frac{|x|}{2}$ in (4))}.
\end{align*}

\item[(6)]
(5) implies that
\[
  \lim_{n \to \infty} P_n(x) = |x|
\]
and
\[
  \lim_{n \to \infty} \sup_{x \in [-1,1]}\abs{ P_n(x) - |x| } = 0.
\]
By Theorem 7.9, $P_n(x) \to |x|$ uniformly on $[-1,1]$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.24.}
\addcontentsline{toc}{subsection}{Exercise 7.24.}
\emph{Let $X$ be a metric space, with metric $d$.
Fix a point $a \in X$.
Assign to each $p \in X$ the function $f_p$ defined by
\[
  f_p(x) = d(x,p) - d(x,a)
  \qquad
  (x \in X).
\]
Prove that $\abs{ f_p(x) } \leq d(a,p)$ for all $x \in X$,
and that therefore, $f_p \in \mathscr{C}(X)$.
Prove that
\[
  \norm{ f_p - f_q } = d(p,q)
\]
for all $p,q \in X$.
If $\Phi(p) = f_p$ it follows that $\Phi$ is an \textbf{isometry}
(a distance-preserving mapping)
of $X$ onto $\Phi(X) \subseteq \mathscr{C}(X)$.
Let $Y$ be the closure of $\Phi(X)$ in $\mathscr{C}(X)$.
Show that $Y$ is complete.
Conclusion: $X$ is isometric to a dense subset of a complete metric space $Y$.
(Exercise 3.24 contains a different proof of this.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\abs{ f_p(x) } \leq d(a,p)$ for all $x \in X$.}
  Since $d$ is a metric,
  \begin{align*}
    d(x,p) - d(x,a) &\leq d(a,p),
    d(x,a) - d(x,p) &\leq d(a,p)
  \end{align*}
  for any $x \in X$.
  Thus $\abs{ f_p(x) } \leq d(a,p)$ for any $x \in X$.

\item[(2)]
  \emph{Show that $f_p \in \mathscr{C}(X)$, i.e, $f_p$ is continuous on $X$.}
  Given any $\varepsilon > 0$, there exists a $\delta = \frac{\varepsilon}{2}$ such that
  \begin{align*}
    \abs{ f_p(x) - f_p(y) }
    &= \abs{ (d(x,p) - d(x,a)) - (d(y,p) - d(y,a)) } \\
    &= \abs{ (d(x,p) - d(y,p)) - (d(x,a) - d(y,a)) } \\
    &\leq \abs{ d(x,p) - d(y,p) } + \abs{ d(x,a) - d(y,a) } \\
    &\leq d(x,y) + d(x,y) \\
    &< \delta + \delta \\
    &= \varepsilon
  \end{align*}
  whenever $d(x,y) < \delta$ and $x,y \in X$.
  Hence $f_p$ is (uniformly) continuous on $X$.

\item[(3)]
  \emph{Show that
  \[
    \norm{ f_p - f_q } = d(p,q)
  \]
  for all $p,q \in X$.}
  \begin{enumerate}
  \item[(a)]
    \emph{Show that $\norm{ f_p - f_q } \leq d(p,q)$.}
    Given any $x \in X$, we have
    \begin{align*}
      \abs{f_p(x)-f_q(x)}
      &= \abs{(d(x,p) - d(x,a))-(d(x,q) - d(x,a))} \\
      &= \abs{d(x,p) - d(x,q)} \\
      &\leq d(p,q).
    \end{align*}
    $d(p,q)$ is an upper bound of $\left\{ \abs{f_p(x)-f_q(x)} : x \in E \right\}$.
    Take sup over all $x \in E$ to get $\norm{ f_p - f_q } \leq d(p,q)$.

  \item[(b)]
    \emph{Show that $\norm{ f_p - f_q } \geq d(p,q)$.}
    Note that
    \begin{align*}
      \norm{ f_p - f_q }
      &= \sup_{x \in E} \abs{f_p(x)-f_q(x)} \\
      &\geq \sup_{x \in \{p,q\}} \abs{f_p(x)-f_q(x)} \\
      &= \sup_{x \in \{p,q\}} d(p,q) \\
      &= d(p,q).
    \end{align*}
  \end{enumerate}

\item[(4)]
  \emph{Show that $Y$ is complete.}
  By Theorem 3.11 and Definition 3.12,
  every closed subset of a complete metric space is complete.
  Note that $Y$ is a closed subset (by construction)
  of a complete metric space $\mathscr{C}(X)$ (Theorem 7.15).
  Therefore $Y$ is complete.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.25.}
\addcontentsline{toc}{subsection}{Exercise 7.25.}
\emph{Suppose $\phi$ is a continuous bounded real function in the strip defined by
$0 \leq x \leq 1$, $-\infty < y < \infty$.
Prove that the initial-value problem
\[
  y' = \phi(x, y), \qquad y(0) = c
\]
has a solution.
(Note that the hypothesis of the existence theorem are less stringent than
those of the corresponding uniqueness theorem; see Exercise 5.27.)}
\emph{(Hint: Fix $n$.
For $i = 0,\ldots,n$, put $x_i = \frac{i}{n}$.
Let $f_n$ be a continuous function on $[0, 1]$ such that $f_n(0) = c$,
\[
  f'_n(t) = \phi(x_i, f_n(x_i))
  \qquad \text{if } x_i < t < x_{i+1}
\]
and put
\[
  \Delta_n(t) = f_n'(t) - \phi(t, f_n(t)),
\]
except at the points $x_i$, where $\Delta_n(t) = 0$.
Then
\[
  f_n(x) = c + \int_{0}^{x} [\phi(t, f_n(t)) + \Delta_n(t)] dt.
\]
Choose $M < \infty$ so that $|\phi| \leq M$.
Verify the following assertions.}
\begin{enumerate}
  \item[(a)]
  \emph{$|f_n'| \leq M$, $|\Delta_n| \leq 2M$, $\Delta_n \in \mathscr{R}$,
  and $|f_n| \leq |c| + M = M_1$, say, on $[0, 1]$, for all $n$.}

  \item[(b)]
  \emph{$\{f_n\}$ is equicontinuous on $[0, 1]$, since $|f_n'| \leq M$.}

  \item[(c)]
  \emph{Some $\{f_{n_k}\}$ converges to some $f$, uniformly on $[0, 1]$.}

  \item[(d)]
  \emph{Since $\phi$ is uniformly continuous on the rectangle $0 \leq x \leq 1$, $|y| \leq M_1$,
  \[
    \phi(t, f_{n_k}(t)) \to \phi(t, f(t))
  \]
  uniformly on $[0, 1]$.}

  \item[(e)]
  \emph{$\Delta_n(t) \to 0$ uniformly on $[0, 1]$,
  since
  \[
    \Delta_n(t) = \phi(x_i, f_n(x_i)) - \phi(t, f_n(t))
  \]
  in $(x_i, x_{i+1})$.}

  \item[(f)]
  \emph{Hence}
  \[
    f(x) = c + \int_{0}^{x} \phi(t, f(t)) dt.
  \]
\end{enumerate}
\emph{This $f$ is a solution of the given problem.)} \\

Same as the hint. \\

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\abs{f_n'} \leq M$.}
  $\abs{f_n'(t)} = \abs{\phi(x_i, f_n(x_i))} \leq M$ if $x_i < t < x_{i+1}$
  by assumption.
  (Here $f_n'(x_i)$ is undefined, and that is why we define $\Delta_n(t)$ on $[0,1]$.)

\item[(2)]
  \emph{Show that $\abs{\Delta_n(t)} \leq 2M$.}
  \begin{enumerate}
  \item[(a)]
    If $t = x_i$ for some $i$,
    then $\abs{\Delta_n(t)} = 0 \leq 2M$.

  \item[(b)]
    If $x_i < t < x_{i+1}$ for some $i$,
    then
    \[
      \abs{\Delta_n(t)} \leq \abs{f_n'(t)} + \abs{\phi(t, f_n(t))} \leq 2M.
    \]
  \end{enumerate}
  In any case, $\abs{\Delta_n(t)} \leq 2M$ on $[0,1]$.

\item[(3)]
  \emph{Show that $\Delta_n(t) \in \mathscr{R}$.}
  Since $\phi$ is continuous,
  by construction $\Delta_n(t)$ is continuous on $[0,1]$
  except finitely many points $x_0, \ldots, x_n$.
  By (2), $\Delta_n(t)$ is bounded.
  So Theorem 6.10 implies that $\Delta_n(t) \in \mathscr{R}$.

\item[(4)]
  \emph{Show that $|f_n| \leq |c| + M = M_1$, say, on $[0, 1]$, for all $n$.}
  Since $\phi$ is continuous and bounded, $f_n(x)$ is well-defined.
  Let
  \begin{equation*}
    g(t) = \phi(t, f_n(t)) + \Delta_n(t) =
    \begin{cases}
      f_n'(t)         & (x \neq x_i \text{ for all } 0 \leq i \leq n) \\
      \phi(t, f_n(t)) & (x = x_i \text{ for some } 0 \leq i \leq n).
    \end{cases}
  \end{equation*}
  By (1) and $|\phi| \leq M$, $|g| \leq M$.
  As $x \in [0,1]$,
  \begin{align*}
    |f_n(x)|
    &= \abs{ c + \int_{0}^{x} g(t) dt } \\
    &\leq |c| + \abs{\int_{0}^{x} g(t) dt} \\
    &\leq |c| + \int_{0}^{x} \abs{ g(t) } dt
      & (\text{Theorem 6.13}) \\
    &\leq |c| + x M
      & (x \in [0,1]) \\
    &\leq |c| + M.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
  Recall the definition of $g(x)$ in (a)(4).
  Given any $\varepsilon > 0$, there exists a $\delta = \frac{\varepsilon}{M+1} > 0$
  such that
  \[
    \abs{f_n(x) - f_n(y)}
    = \abs{ \int_{y}^{x} g(t) dt }
    < M |x-y|
    < \varepsilon
  \]
  whenever $|x-y| < \delta$, and $x, y \in [0,1]$.
  Hence $\{f_n\}$ is equicontinuous on $[0,1]$.
$\Box$ \\



\emph{Proof of (c).}
Apply Theorem 7.25(b)(Arzel\`{a}-Ascoli theorem) to (a)(b).
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  Given any $\varepsilon > 0$.
  Since $\phi$ is continuous on a compact set $[0,1] \times [-M_1,M_1]$,
  $\phi$ is continuous uniformly.
  So there is a $\delta > 0$ such that
  \[
    \abs{ \phi(t,y_1) - \phi(t,y_2) } < \varepsilon
  \]
  whenever $\abs{ y_1 - y_2 } < \delta$, $t \in [0,1]$, and $y_1, y_2 \in [-M_1,M_1]$.

\item[(2)]
  By (c), $\{f_{n_k}\}$ converges to some $f$, uniformly on $[0, 1]$.
  So there is an integer $N$ such that
  \[
    \abs{ f_{n_k}(t) - f(t) } < \delta
  \]
  whenever $k \geq N$ and $t \in [0,1]$.

\item[(3)]
  Take $y_1 = f_{n_k}(t) \in [-M_1,M_1]$
  and $y_2 = f(t) \in [-M_1,M_1]$.
  (It is possible since $\abs{f_{n_k}} \leq M_1$ and its limit function $f$
  is also satisfying $\abs{f} \leq M_1$.)
  Hence
  \[
    \abs{ \phi(t,f_{n_k}(t)) - \phi(t,f(t)) } < \varepsilon
  \]
  whenever $k \geq N$ and $t \in [0,1]$.
  Hence
  $\phi(t,f_{n_k}(t)) \to \phi(t,f(t))$ uniformly.
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
  Given any $\varepsilon > 0$.
  It suffices to show that there exists an integer $N$ such that
  \[
    \abs{ \Delta_n(t) } \leq \varepsilon
  \]
  whenever $n \geq N$ and $t \in [0,1]$.
  Since $\Delta_n(x_i) = 0$ if $0 \leq i \leq n$,
  it suffices to \emph{show that there exists an integer $N$ such that
  \[
    \abs{ \Delta_n(t) } \leq \varepsilon
  \]
  whenever $n \geq N$, $t \in [0,1]$ and $t \neq x_i$ for all $0 \leq i \leq n$.}

\item[(2)]
  Given any $t \in [0,1]$ with $t \neq x_j$ ($0 \leq j \leq n$).
  There exists some $i$ such that $x_i < t < x_{i+1}$ for $0 \leq i \leq n$.
  Write
  \begin{align*}
    \abs{ \Delta_n(t) }
    &= \abs{ f_n'(t) - \phi(t, f_n(t)) } \\
    &= \abs{ \phi(x_i, f_n(x_i)) - \phi(t, f_n(t)) } \\
    &\leq \abs{ \phi(x_i, f_n(x_i)) - \phi(x_i, f_n(t)) }
      + \abs{ \phi(x_i, f_n(t)) - \phi(t, f_n(t)) } \\
  \end{align*}

\item[(3)]
  \emph{Show that there is an integer $N_1$ such that
  \[
    \abs{ \phi(x_i, f_n(x_i)) - \phi(x_i, f_n(t)) } < \frac{\varepsilon}{89}
  \]
  whenever $n \geq N_1$.}
  Since $\phi$ is uniformly continuous,
  there exists a $\delta > 0$ such that
  \[
    \abs{ \phi(x_i, y_1)) - \phi(x_i, y_2) } < \frac{\varepsilon}{89}
  \]
  whenever $|y_1 - y_2| < \delta$, and $y_1, y_2 \in [-M_1,M_1]$.
  Since $\{f_n\}$ is equicontinuous on $[0,1]$ (by (b)),
  there exists an integer $N_1 > 0$ such that
  \[
    \abs{ f_n(x_i) - f_n(t) } < \delta
  \]
  whenever $n \geq N_1$.
  (Here by the construction of $\{x_0, \ldots, x_n\}$ we have
  $|x_i - t| < \frac{1}{n} \leq \frac{1}{N_1}$ for $n \geq N_1$.)
  Since $|f_n| \leq M_1$, we can take $y_1 = |f_n(x_i)|$ and $y_2 = |f_n(t)|$
  to get the conclusion.

\item[(4)]
  \emph{Show that there is an integer $N_2$ such that
  \[
    \abs{ \phi(x_i, f_n(t)) - \phi(t, f_n(t)) } < \frac{\varepsilon}{64}
  \]
  whenever $n \geq N_2$.}
  Similar to (3).
  Since $\phi$ is uniformly continuous,
  there exists a $\delta > 0$ such that
  \[
    \abs{ \phi(w_1, f_n(t)) - \phi(w_2 ,f_n(t)) } < \frac{\varepsilon}{64}
  \]
  whenever $|w_1 - w_2| < \delta$, and $w_1, w_2 \in [0,1]$.
  By the construction of $\{x_0, \ldots, x_n\}$,
  there is an integer $N_2 > 0$ such that
  $|x_i - t| < \delta$ for $n \geq N_2$.
  Take $w_1 = x_i$ and $w_2 = t$ to get the conclusion.

\item[(5)]
  By (2)(3)(4), we take an integer $N = \max\{N_1, N_2\}$
  such that
  \begin{align*}
    \abs{ \Delta_n(t) }
    &\leq \abs{ \phi(x_i, f_n(x_i)) - \phi(x_i, f_n(t)) }
      + \abs{ \phi(x_i, f_n(t)) - \phi(t, f_n(t)) } \\
    &< \frac{\varepsilon}{89} + \frac{\varepsilon}{64} \\
    &< \varepsilon
  \end{align*}
  whenever $n \geq N$.
  Combine (1) and this result to get that
  $\Delta_n(t) \to 0$ uniformly on $[0,1]$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{align*}
  f(x)
  &= \lim_{k \to \infty} f_{n_k}(x)
    &((c)) \\
  &= \lim_{k \to \infty}
    \left\{ c + \int_{0}^{x}[ \phi(t,f_{n_k}(t)) + \Delta_{n_k}(t) ]dt \right\} \\
  &= c + \int_{0}^{x} \left[ \lim_{k \to \infty} \phi(t,f_{n_k}(t))
    + \lim_{k \to \infty} \Delta_{n_k}(t) \right] dt
    &(\text{Theorem 7.16}, (d)(e)) \\
  &= c + \int_{0}^{x} \left[ \phi(t,f(t)) + 0 \right] dt \\
  &= c + \int_{0}^{x} \phi(t,f(t)) dt.
\end{align*}
This $f$ satisfies that $f(c) = 0$ and $f(t)' = \phi(t,f(t))$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 7.26.}
\addcontentsline{toc}{subsection}{Exercise 7.26.}
\emph{Prove an analogous existence theorem for the initial-value problem
\[
  \mathbf{y}' = \bm{\Phi}(x, \mathbf{y}), \qquad \mathbf{y}(0) = \mathbf{c},
\]
where now $\mathbf{c} \in \mathbb{R}^k$, $\mathbf{y} \in \mathbb{R}^k$,
and $\bm{\Phi}$ is a continuous bounded mapping of the part of $\mathbb{R}^{k+1}$
defined by $0 \leq x \leq 1$, $\mathbf{y} \in \mathbb{R}^k$ into $\mathbb{R}^k$.
(Compare Exercise 5.28.)
(Hint: Use the vector-valued version of Theorem 7.25.)} \\

\emph{Outline.}
\emph{Fix $n$.
For $i = 0,\ldots,n$, put $x_i = \frac{i}{n}$.
Let $\mathbf{f}_n$ be a continuous function on $[0, 1]$ such that
$\mathbf{f}_n(0) = \mathbf{c}$,
\[
  \mathbf{f}'_n(t) = \bm{\Phi}(x_i, \mathbf{f}_n(x_i))
  \qquad \text{if } x_i < t < x_{i+1}
\]
and put
\[
  \mathbf{\Delta}_n(t) = \mathbf{f}_n'(t) - \bm{\Phi}(t, \mathbf{f}_n(t)),
\]
except at the points $x_i$, where $\mathbf{\Delta}_n(t) = \mathbf{0}$.
Then
\[
  \mathbf{f}_n(x)
  = \mathbf{c} + \int_{0}^{x} [\bm{\Phi}(t, \mathbf{f}_n(t)) + \mathbf{\Delta}_n(t)] dt.
\]
Choose $M < \infty$ so that $|\bm{\Phi}| \leq M$.
Verify the following assertions.}
\begin{enumerate}
  \item[(a)]
  \emph{$|\mathbf{f}_n'| \leq M$, $|\mathbf{\Delta}_n| \leq 2M$,
  $\mathbf{\Delta}_n \in \mathscr{R}$,
  and $|\mathbf{f}_n| \leq |\mathbf{c}| + M = M_1$, say, on $[0, 1]$, for all $n$.}

  \item[(b)]
  \emph{$\{\mathbf{f}_n\}$ is equicontinuous on $[0, 1]$,
  since $|\mathbf{f}_n'| \leq M$.}

  \item[(c)]
  \emph{Some $\{\mathbf{f}_{n_h}\}$ converges to some $\mathbf{f}$, uniformly on $[0, 1]$.}

  \item[(d)]
  \emph{Since $\bm{\Phi}$ is uniformly continuous on the rectangle
  $0 \leq x \leq 1$, $|\mathbf{y}| \leq M_1$,
  \[
    \bm{\Phi}(t, \mathbf{f}_{n_h}(t)) \to \bm{\Phi}(t, \mathbf{f}(t))
  \]
  uniformly on $[0, 1]$.}

  \item[(e)]
  \emph{$\mathbf{\Delta}_n(t) \to 0$ uniformly on $[0, 1]$,
  since
  \[
    \mathbf{\Delta}_n(t)
    = \bm{\Phi}(x_i, \mathbf{f}_n(x_i)) - \bm{\Phi}(t, \mathbf{f}_n(t))
  \]
  in $(x_i, x_{i+1})$.}

  \item[(f)]
  \emph{Hence}
  \[
    \mathbf{f}(x) = \mathbf{c} + \int_{0}^{x} \bm{\Phi}(t, \mathbf{f}(t)) dt.
  \]
\end{enumerate}
\emph{This $\mathbf{f}$ is a solution of the given problem.} \\

Similar to Exercise 7.25. \\

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\abs{\mathbf{f}_n'} \leq M$.}
  Similar to Exercise 7.25(a)(1).

\item[(2)]
  \emph{Show that $\abs{\mathbf{\Delta}_n(t)} \leq 2M$.}
  Similar to Exercise 7.25(a)(2).

\item[(3)]
  \emph{Show that $\mathbf{\Delta}_n(t) \in \mathscr{R}$.}
  $\bm{\Phi}$ is continuous if and only if each component of $\bm{\Phi}$ is continuous.
  By Exercise 7.25(a)(3),
  each component of $\mathbf{\Delta}_n(t) \in \mathscr{R}$.
  Therefore $\mathbf{\Delta}_n(t) \in \mathscr{R}$ (Definition 6.23).

\item[(4)]
  \emph{Show that $|\mathbf{f}_n| \leq |\mathbf{c}| + M = M_1$, say, on $[0, 1]$, for all $n$.}
  Similar to Exercise 7.25(a)(4).
  (Note that Theorem 6.25 is the vector-valued version of Theorem 6.13.)
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
Similar to Exercise 7.25(b).
$\Box$ \\



\emph{Proof of (c).}
Use the vector-valued version of Theorem 7.25(b)(Arzel\`{a}-Ascoli theorem)
(in Exercise 7.17) to (a)(b).
$\Box$ \\



\emph{Proof of (d).}
Apply Exercise 7.25(d) to each component of $\bm{\Phi}(t,\mathbf{f}_{n_h}(t))$ repeatedly.
$\Box$ \\



\emph{Proof of (e).}
Apply Exercise 7.25(e) to each component of $\mathbf{\Delta}_n(t)$ repeatedly.
$\Box$ \\



\emph{Proof of (f).}
Apply Exercise 7.25(f) to each component of $\mathbf{f}$ repeatedly.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 8: Some Special Functions \\}
\addcontentsline{toc}{section}{Chapter 8: Some Special Functions}



\subsection*{Supplement (Fourier coefficients in Definition 8.9).}
\addcontentsline{toc}{subsection}{Supplement (Fourier coefficients in Definition 8.9).}
\begin{enumerate}
\item[(1)]
Write $$f(x) = a_0 + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}$$
(as the textbook Rudin, Principles of Mathematical Analysis, Third Edition).
Then
\begin{align*}
a_0 &= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx. \\
a_n &= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(nx) dx, n \in \mathbb{Z}^+. \\
b_n &= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(nx) dx, n \in \mathbb{Z}^+.
\end{align*}

\item[(2)]
One might write in one different form,
$$f(x) = \frac{a_0}{2} + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
The only difference between the new one and the old one is $a_0$,
so $a_0$ should be
$$a_0 = \frac{1}{\pi} \int_{-\pi}^\pi f(x) dx.$$

\item[(3)]
Again, one might write in one different form,
$$f(x) = \frac{a_0}{\sqrt{2}} + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$ Similarly, $a_0$ should be
$$a_0 = \frac{1}{\pi} \int_{-\pi}^\pi \frac{f(x)}{\sqrt{2}} dx.$$

\item[(4)]
Recall $f(x) = \sum_{-N}^{N} c_n e^{inx}$ ($x \in \mathbb{R}$) where
$$c_n = \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) e^{-inx} dx.$$
The relations among $a_n$, $b_n$ of this textbook and $c_n$ are
\begin{align*}
c_0 &= a_0 \\
c_n &= \frac{1}{2} \left( a_n + i b_n \right), n \in \mathbb{Z}^+.
\end{align*}

\item[(5)]
In some textbooks (Henryk Iwaniec, Topics in Classical Automorphic Forms),
it is convenient to consider periodic functions $f$ of period $1$.
Define $$e(n) = e^{2 \pi i x} = \cos(2 \pi x) + i \sin(2 \pi x).$$
Any periodic and piecewise continuous function $f$ has the Fourier series representation
$$f(x) = \sum_{-\infty}^{\infty} a_n e(nx)$$
with coefficients given by
$$a_n = \int_{0}^{1} f(x) e(-nx) dx.$$
Here is one exercise for this representation.
\emph{Show that the fractional part of $x$, $\{x\} = x - [x]$, is given by
$$\{x\} = \frac{1}{2} - \sum_{n = 1}^{\infty} \frac{\sin(2 \pi nx)}{\pi n}.$$}
\end{enumerate}



\textbf{Supplement.} Parseval's theorem 8.16.
\begin{enumerate}
\item[(1)]
Given $$f(x) = a_0 + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= 2 a_0^2 + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$
\item[(2)]
Given $$f(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= \frac{a_0^2}{2} + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$
\item[(3)]
Given $$f(x) = \frac{a_0}{\sqrt{2}} + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= a_0^2 + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$ \\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.1.}
\addcontentsline{toc}{subsection}{Exercise 8.1.}
\emph{Define
\begin{equation*}
  f(x) =
    \begin{cases}
      e^{-\frac{1}{x^2}} & (x \neq 0), \\
      0                  & (x = 0).
    \end{cases}
\end{equation*}
Prove that $f$ has derivatives of all orders at $x = 0$,
and that $f^{(n)}(0) = 0$ for $n = 1,2,3,\ldots$} \\

$f(x)$ is an example of non-analytic smooth function, that is,
infinitely differentiable functions are not necessarily analytic.
In this exercise, we will show that Taylor series of $f$ at the origin
converges everywhere to the zero function.
So the Taylor series does not equal $f(x)$ for $x \neq 0$.
Consequently, $f$ is not analytic at $x = 0$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
\[
  \lim_{x \rightarrow 0} g(x) e^{-\frac{1}{x^2}} = 0
\]
for any rational function $g(x) \in \mathbb{R}(x)$.}
  \begin{enumerate}
  \item[(a)]
  Write $g(x) = \frac{p(x)}{q(x)}$ for some $p(x), q(x) \in \mathbb{R}[x]$,
  $g(x) \neq 0$.

  \item[(b)]
  Write $q(x) = b_m x^m + b_{m - 1} x^{m - 1} + \cdots + b_0$.
  $q(x)$ is not identically zero, that is, there exists the unique coefficient
  of the least power of $x$ in $q(x)$ which is non-zero, say $b_M \neq 0$.

  \item[(c)]
  Thus,
  \[
    g(x) = \frac{p(x)/x^M}{q(x)/x^M}.
  \]
  The denominator of $g(x)$ tends to $b_M \neq 0$ as $x \rightarrow 0$.
  By the similar argument in Theorem 8.6(f), we have
  \[
    \frac{p(x)}{x^M} e^{-\frac{1}{x^2}} \rightarrow 0 \text{ as } x \rightarrow 0.
  \]
  Hence, $\lim_{x \rightarrow 0} g(x) e^{-\frac{1}{x^2}} = 0$
  for any $g(x) \in \mathbb{R}(x)$.
  \end{enumerate}

\item[(2)]
\emph{Given any real $x \neq 0$, show that
\[
  f^{(n)}(x) = g_n(x) e^{-\frac{1}{x^2}}
\]
for some rational function $g(x) \in \mathbb{R}(x)$.}
  \begin{enumerate}
  \item[(a)]
  Say $g_0(x) = 1 \in \mathbb{R}(x)$.

  \item[(b)]
  $\mathbb{R}(x)$ is a field.
  \emph{Show that $g'(x) \in \mathbb{R}(x)$ for any $g(x) \in \mathbb{R}(x)$.}
  Write $g(x) = \frac{p(x)}{q(x)}$ for some $p(x), q(x) \in \mathbb{R}[x]$, $q(x) \neq 0$.
  Thus
  \[
    g'(x) = \frac{p'(x)q(x) - p(x)q'(x)}{q(x)^2}.
  \]
  The numerator of $g'(x)$ is in $\mathbb{R}[x]$ since
  the differentiation operator on $\mathbb{R}[x]$ is closed in $\mathbb{R}[x]$.
  Also, the denominator of $g'(x) = q(x)^2 \neq 0$
  since $\mathbb{R}[x]$ is an integral domain.
  Therefore, $g'(x) \in \mathbb{R}(x)$.

  \item[(c)]
  Induction on $n$.
  For $n = 1$, we have
  \begin{align*}
    f'(x)
    &= g_0'(x) e^{-\frac{1}{x^2}}
      + g_0(x) \cdot \left( -\frac{1}{x^2} \right)' e^{-\frac{1}{x^2}} \\
    &= \left( g_0'(x) + g_0(x) \cdot \left( -\frac{1}{x^2} \right)' \right) e^{-\frac{1}{x^2}} \\
    &= g_1(x) e^{-\frac{1}{x^2}}
  \end{align*}
  where
  \[
    g_1(x) = g_0'(x) + g_0(x) \cdot \left(-\frac{1}{x^2}\right)' \in \mathbb{R}(x).
  \]
  Now assume that the conclusion holds for $n = k$.
  As $n = k + 1$, similar to the case $n = 1$,
  \[
    f^{(k + 1)}(x) = g_{k + 1}(x) e^{-\frac{1}{x^2}}
  \]
  where
  \[
    g_{k + 1}(x)
    = g_k'(x) + g_k(x) \cdot \left( -\frac{1}{x^2} \right)' \in \mathbb{R}(x).
  \]
  By induction, the conclusion is true.
  \end{enumerate}

\item[(3)]
Induction on $n$.
For $n = 1$, by (1) we have
\[
  f'(0) = \lim_{t \rightarrow 0} \frac{e^{- \frac{1}{t^2}} - 0}{t} = 0.
\]
Now assume that the statement holds for $n = k$.
As $n = k + 1$, by (1)(2) we have
\[
  f^{(k + 1)}(0)
  = \lim_{t \rightarrow 0} \frac{f^{(k)}(t) - f^{(k)}(0)}{t}
  = \lim_{t \rightarrow 0} \frac{g_k(t) e^{- \frac{1}{t^2}} - 0}{t} = 0.
\]
Thus, $f^{(n)}(0) = 0$ for $n \in \mathbb{Z}^+$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.2.}
\addcontentsline{toc}{subsection}{Exercise 8.2.}
\emph{Let $a_{ij}$ be the number in the $i$th row and $j$th column of the array
\[
\begin{array}{rrrrr}
           -1 &           0 &           0 &      0 & \cdots \\
  \frac{1}{2} &          -1 &           0 &      0 & \cdots \\
  \frac{1}{4} & \frac{1}{2} &          -1 &      0 & \cdots \\
  \frac{1}{8} & \frac{1}{4} & \frac{1}{2} &     -1 & \cdots \\
       \vdots &      \vdots &      \vdots & \vdots & \ddots
\end{array}
\]
so that
\begin{equation*}
  a_{ij} =
    \begin{cases}
      0       & (i < j), \\
      -1      & (i = j), \\
      2^{j-i} & (i > j).
    \end{cases}
\end{equation*}
Prove that
\[
  \sum_{i} \sum_{j} a_{ij} = -2, \qquad \sum_{j} \sum_{i} a_{ij} = 0.
\]
} \\

Also see Theorem 8.3. \\

\emph{Proof (Brute-force).}
\begin{align*}
  \sum_{i} \sum_{j} a_{ij}
  &= \sum_{i=1}^{\infty} \left( \sum_{j = i} a_{ij} + \sum_{j < i} a_{ij} \right) \\
  &= \sum_{i=1}^{\infty} \left( -1 + \sum_{j=1}^{i-1} 2^{j-i} \right) \\
  &= \sum_{i=1}^{\infty} ( -1 + (1 - 2^{1-i}) ) \\
  &= \sum_{i=1}^{\infty} -2^{1-i} \\
  &= -2.
\end{align*}
\begin{align*}
  \sum_{j} \sum_{i} a_{ij}
  &= \sum_{j=1}^{\infty} \left( \sum_{i = j} a_{ij} + \sum_{i > j} a_{ij} \right) \\
  &= \sum_{j=1}^{\infty} \left( -1 + \sum_{i=j+1}^{\infty} 2^{j-i} \right) \\
  &= \sum_{j=1}^{\infty} ( -1 + 1 ) \\
  &= \sum_{j=1}^{\infty} 0 \\
  &= 0.
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.3.}
\addcontentsline{toc}{subsection}{Exercise 8.3.}
\emph{Prove that
\[
  \sum_{i} \sum_{j} a_{ij} = \sum_{j} \sum_{i} a_{ij}
\]
if $a_{ij} \geq 0$ for all $i$ and $j$ (the case $+\infty = +\infty$ may occur).} \\

\emph{Note.}
It can be proved by Theorem 8.3 if both summations are finite. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let $\mathscr{F}(I)$ be the collection of all finite subsets of $I$.

\item[(2)]
Let
\[
  s = \sup \left\{
    \sum_{(i,j) \in E} a_{ij} : E \in \mathscr{F}(\mathbb{N}^2)
  \right\}
\]

(the case $s = +\infty$ may occur).
\emph{It suffices to show that $\sum_{i}\sum_{j} a_{ij} = s$.}
The case $\sum_{j}\sum_{i} a_{ij} = s$ is similar, and thus
$\sum_{i}\sum_{j} a_{ij} = \sum_{j}\sum_{i} a_{ij}.$

\item[(3)]
\emph{Show that $\sum_{i}\sum_{j} a_{ij} \geq s$.}
Given any $E \in \mathscr{F}(\mathbb{N}^2)$.
It is clear that
\[
  \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij} \geq \sum_{(i,j) \in E} a_{ij}
\]

(since $a_{ij} \geq 0$).
Thus,
\[
  \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij}
  \geq
  \sup \left\{
    \sum_{(i,j) \in E} a_{ij} : E \in \mathscr{F}(\mathbb{N}^2)
  \right\}
  = s.
\]

\item[(4)]
\emph{Show that $\sum_{i}\sum_{j} a_{ij} \leq s$.}
(Reductio ad absurdum)
If $\sum_{i}\sum_{j} a_{ij} > s$,
especially $s < \infty$,
then there exists $\varepsilon > 0$ such that
\[
  \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{ij} > s + \varepsilon,
\]
or
\[
  \sum_{i=1}^{n} \sum_{j=1}^{\infty} a_{ij} > s + \varepsilon
\]
for some integer $n$.
Consider two possible cases.
  \begin{enumerate}
  \item[(a)]
  If there is some $1 \leq i_0 \leq n$
  such that
  \[
    \sum_{j=1}^{\infty} a_{i_0 j} = \infty,
  \]
  then there is some $m$ such that
  \[
    \sum_{j=1}^{m} a_{i_0 j} > s.
  \]
  For $E = \{ (i_0,1),\ldots,(i_0,m) \} \in \mathscr{F}(\mathbb{N}^2)$,
  \[
    \sum_{(i,j) \in E} a_{ij} = \sum_{j=1}^{m} a_{i_0 j} > s,
  \]
  contrary to the supremum of $s$.

  \item[(b)]
  Otherwise, for each  $1 \leq i \leq n$
  we have
  \[
    \sum_{j=1}^{\infty} a_{ij} < \infty,
  \]
  or there exists some $m_i$ such that
  \[
    \sum_{j=1}^{m_i} a_{ij} > \sum_{j=1}^{\infty} a_{ij} - \frac{\varepsilon}{n}.
  \]
  For $E = \bigcup_{1 \leq i \leq n} \{ (i,1),\ldots,(i,m_i) \} \in \mathscr{F}(\mathbb{N}^2)$,
  \begin{align*}
    \sum_{(i,j) \in E} a_{ij}
    &= \sum_{i=1}^{n} \sum_{j=1}^{m_i} a_{ij} \\
    &> \sum_{i=1}^{n} \left( \sum_{j=1}^{\infty} a_{ij} - \frac{\varepsilon}{n} \right) \\
    &= \sum_{i=1}^{n}\sum_{j=1}^{\infty} a_{ij} - \sum_{i=1}^{n}\frac{\varepsilon}{n} \\
    &> s + \varepsilon - \varepsilon \\
    &= s,
  \end{align*}
  contrary to the supremum of $s$.
  \end{enumerate}
  Therefore, $\sum_{i}\sum_{j} a_{ij} \leq s$.

  \item[(5)]
  By (3)(4), $\sum_{i}\sum_{j} a_{ij} = s$.
  Similarly, $\sum_{j}\sum_{i} a_{ij} = s$.
  Hence, $\sum_{i}\sum_{j} a_{ij} = \sum_{j}\sum_{i} a_{ij}$
  (including the case $+\infty = +\infty$).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.4.}
\addcontentsline{toc}{subsection}{Exercise 8.4.}
\emph{Prove the following limit relations:}
\begin{enumerate}
  \item[(a)]
  $\lim_{x \to 0} \frac{b^x - 1}{x} = \log b \qquad (b > 0).$
  \item[(b)]
  $\lim_{x \to 0} \frac{\log(1+x)}{x} = 1.$
  \item[(c)]
  $\lim_{x \to 0} (1+x)^{\frac{1}{x}} = e.$
  \item[(d)]
  $\lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n} = e^x.$ \\
\end{enumerate}

\emph{Proof of (a).}
\begin{align*}
  \lim_{x \to 0} \frac{b^x - 1}{x}
  &= \lim_{x \to 0} \frac{\exp(x \log b) - 1}{x} \\
  &= \left. \frac{d}{dx} \exp(x \log b) \right\vert_{x=0} \\
  &= \left. \exp(x \log b) \cdot \log b \right\vert_{x=0} \\
  &= \log b.
\end{align*}
$\Box$ \\

\emph{Proof of (b).}
\begin{align*}
  \lim_{x \to 0} \frac{\log(1+x)}{x}
  &= \left. \frac{d}{dx} \log(1+x) \right\vert_{x=0} \\
  &= \left. \frac{1}{x+1} \right\vert_{x=0} \\
  &= 1.
\end{align*}
$\Box$ \\

\emph{Proof of (c).}
\begin{align*}
  \lim_{x \to 0} (1+x)^{\frac{1}{x}}
  &= \lim_{x \to 0} \exp\left( \frac{\log(1+x)}{x} \right) \\
  &= \exp\left( \lim_{x \to 0} \frac{\log(1+x)}{x} \right) \\
  &= \exp(1) \\
  &= e.
\end{align*}
$\Box$ \\

\emph{Proof of (d).}
\begin{align*}
  \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n}
  &= \lim_{n \to \infty} \left( \left( 1 + \frac{x}{n} \right)^{\frac{n}{x}} \right)^{x} \\
  &= \left( \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{\frac{n}{x}} \right)^{x} \\
  &= \left( \lim_{y \to 0} (1+y)^{\frac{1}{y}} \right)^{x} \\
  &= \exp(x).
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.5.}
\addcontentsline{toc}{subsection}{Exercise 8.5.}
\emph{Find the following limits}
\begin{enumerate}
  \item[(a)]
  $\lim_{x \to 0} \frac{e-(1+x)^{\frac{1}{x}}}{x}.$
  \item[(b)]
  $\lim_{n \to \infty} \frac{n}{\log n}\left[ n^{\frac{1}{n}} - 1 \right].$
  \item[(c)]
  $\lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}.$
  \item[(d)]
  $\lim_{x \to 0} \frac{x - \sin x}{\tan x - x}.$ \\
\end{enumerate}

\emph{Proof of (a).}
By L'Hospital's rule (Theorem 5.13),
\begin{align*}
  \lim_{x \to 0} \frac{e-(1+x)^{\frac{1}{x}}}{x}
  &= \lim_{x \to 0} \frac{-(1+x)^{\frac{1}{x}} \cdot
    \frac{ \frac{x}{x+1} - \log(x+1) }{x^2}}{1} \\
  &= \lim_{x \to 0} \left( -(1+x)^{\frac{1}{x}} \cdot
    \frac{ \frac{x}{x+1} - \log(x+1) }{x^2} \right) \\
  &= - \lim_{x \to 0}(1+x)^{\frac{1}{x}} \cdot
    \lim_{x \to 0} \frac{ \frac{x}{x+1} - \log(x+1) }{x^2} \\
  &= -e \cdot
    \lim_{x \to 0} \frac{ \frac{x}{x+1} - \log(x+1) }{x^2}
    &\text{(Exercise 8.4(c))} \\
  &= -e \cdot \lim_{x \to 0} \frac{ -\frac{x}{(x+1)^2} }{2x} \\
  &= e \cdot \lim_{x \to 0} \frac{1}{2(x+1)^2} \\
  &= e \cdot \frac{1}{2} \\
  &= \frac{e}{2}.
\end{align*}
Here
\begin{align*}
  \frac{d}{dx}\left( e-(1+x)^{\frac{1}{x}} \right)
  &= \frac{d}{dx}\left( e-\exp\left( \frac{\log(x+1)}{x} \right) \right) \\
  &= -\exp\left( \frac{1}{x} \log(x+1) \right) \cdot
    \frac{ \frac{1}{x+1} \cdot x - \log(x+1) \cdot 1 }{x^2} \\
  &= -(1+x)^{\frac{1}{x}} \cdot \frac{ \frac{x}{x+1} - \log(x+1) }{x^2},
\end{align*}
and
\begin{align*}
  \frac{d}{dx}\left( \frac{x}{x+1} - \log(x+1) \right)
  &= \frac{(x+1) - x}{(x+1)^2} - \frac{1}{x+1} \\
  &= -\frac{x}{(x+1)^2}.
\end{align*}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Let $x = \frac{\log n}{n}$.
Note that $\lim_{n \to \infty} \frac{\log n}{n} = 0$.

\item[(2)]
\begin{align*}
  \lim_{n \to \infty} \frac{n}{\log n}\left[ n^{\frac{1}{n}} - 1 \right]
  &= \lim_{n \to \infty} \frac{n}{\log n}\left[ \exp\left( \frac{\log n}{n} \right) - 1 \right] \\
  &= \lim_{x \to 0} \frac{\exp(x) - 1}{x}
    &\text{((1))} \\
  &= \left.\frac{d}{dx} \exp(x) \right\vert_{x=0} \\
  &= \left.\exp(x) \right\vert_{x=0} \\
  &= 1.
\end{align*}
\end{enumerate}
$\Box$ \\

\emph{Proof of (c) (L'Hospital's rule).}
By L'Hospital's rule (Theorem 5.13) three times,
\begin{align*}
  \lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}
  &= \lim_{x \to 0} \frac{\sec^2 x - 1}{1 - \cos x + x \sin x} \\
  &= \lim_{x \to 0} \frac{2 \sec x (\tan x \sec x)}{\sin x + \sin x + x \cos x} \\
  &= \lim_{x \to 0} \frac{2 \tan x \sec^2 x}{2 \sin x + x \cos x} \\
  &= \lim_{x \to 0}
    \frac{2 [\sec^2 x \sec^2 x + \tan x \cdot 2 \sec x (\tan x \sec x)]}
    {2 \cos x + \cos x - x \sin x} \\
  &= \lim_{x \to 0} \frac{2 \sec^4 x + 2 \sec^2 x \tan^2 x}{3 \cos x - x \sin x} \\
  &= \frac{2}{3}.
\end{align*}
$\Box$ \\

\emph{Proof of (c) (Taylor series).}
Since
\begin{align*}
  \cos x &= 1 - \frac{x^2}{2} + O(x^4) \\
  \tan x &= x + \frac{x^3}{3} + O(x^5),
\end{align*}
we have
\[
  \lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}
  = \lim_{x \to 0} \frac{\frac{x^3}{3} + O(x^5)}{\frac{x^3}{2} + O(x^5)}
  = \frac{2}{3}.
\]
$\Box$ \\

\emph{Proof of (d) (L'Hospital's rule).}
By L'Hospital's rule (Theorem 5.13) three times,
\begin{align*}
  \lim_{x \to 0} \frac{x - \sin x}{\tan x - x}
  &= \lim_{x \to 0} \frac{1 - \cos x}{\sec^2 x - 1} \\
  &= \lim_{x \to 0} \frac{\sin x}{2 \sec x (\tan x \sec x)} \\
  &= \lim_{x \to 0} \frac{\sin x}{2 \tan x \sec^2 x} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 \tan x \sec^2 x} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 [\sec^2 x \sec^2 x + \tan x \cdot 2 \sec x (\tan x \sec x)]} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 \sec^4 x + 2 \sec^2 x \tan^2 x} \\
  &= \frac{1}{2}.
\end{align*}
$\Box$ \\

\emph{Proof of (d) (Taylor series).}
Since
\begin{align*}
  \sin x &= x - \frac{x^3}{6} + O(x^5) \\
  \tan x &= x + \frac{x^3}{3} + O(x^5),
\end{align*}
we have
\[
  \lim_{x \to 0} \frac{x - \sin x}{\tan x - x}
  = \lim_{x \to 0} \frac{\frac{x^3}{6} + O(x^5)}{\frac{x^3}{2} + O(x^5)}
  = \frac{1}{2}.
\]
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.6.}
\addcontentsline{toc}{subsection}{Exercise 8.6.}
\emph{Suppose $f(x)f(y) = f(x + y)$ for all real $x$ and $y$.}
\begin{enumerate}
\item[(a)]
\emph{Assuming that $f$ is differentiable and not zero, prove that
$$f(x) = e^{cx}$$
where $c$ is a constant.}
\item[(b)]
\emph{Prove the same thing, assuming only that $f$ is continuous.} \\
\end{enumerate}

Part (b) implies part (a). We prove part (b) directly. \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Since $f(x)$ is not zero, there exists $x_0 \in \mathbb{R}$ such that $f(x_0) \neq 0$.
So $f(0)f(x_0) = f(x_0)$, or $f(0) = 1$ by cancelling $f(x_0) \neq 0$.

\item[(2)]
Next, $f(\frac{n}{m}) = f(\frac{1}{m})^n$ for $m \in \mathbb{Z}$, $n \in \mathbb{Z}^{+}$.
Since $f$ is continuous at $x = 0$, $f$ is positive in the neighborhood of $x = 0$.
That is, there exists $N \in \mathbb{Z}^{+}$ such that $f(\frac{1}{m}) > 0$
whenever $|m| \geq N$.
So, $f(\frac{n}{m}) = f(\frac{1}{m})^n > 0$.
(Since $f(\frac{n}{m}) = f(\frac{kn}{km})$ for any $k \in \mathbb{Z}^{+}$,
we can rescale $m$ to $km$ such that $|km| \geq N$.)
That is, $f$ is positive on $\mathbb{Q}$.
Since $\mathbb{Q}$ is dense in $\mathbb{R}$ and $f$ is continuous on $\mathbb{R}$,
$f$ is positive on $\mathbb{R}$.

\item[(3)]
Now let $c = \log f(1)$ (which is well-defined since $f > 0$).
We write $f(1)$ in the two ways.
Firstly, $f(1) = f(\frac{n}{n}) = f(\frac{1}{n})^n$ where $n \in \mathbb{Z}^{+}$.
Secondly, $f(1) = e^c = (e^{\frac{c}{n}})^n$.
Since the positive $n$-th root is unique (Theorem 1.21),
$f(\frac{1}{n}) = e^{\frac{c}{n}}$ for $n \in \mathbb{Z}^{+}$.
By $f(x)f(-x) = f(0) = 1$ or $f(-x) = \frac{1}{f(x)}$,
$f(-\frac{1}{n}) = \frac{1}{e^{\frac{c}{n}}} = e^{-\frac{c}{n}}$ for $n \in \mathbb{Z}^{+}$.
Therefore,
\[
  f\left( \frac{1}{m} \right) = e^{\frac{c}{m}} \text{ where } m \in \mathbb{Z}.
\]

\item[(4)]
By using
$f(\frac{n}{m}) = f(\frac{1}{m})^n$ for $m \in \mathbb{Z}$, $n \in \mathbb{Z}^{+}$ again,
$f(\frac{n}{m}) = e^{c \frac{n}{m}}$ where $m \in \mathbb{Z}, n \in \mathbb{Z}^{+}$, or
$$f(x) = e^{cx} \text{ where } x \in \mathbb{Q}.$$
Since $g(x) = f(x) - e^{cx}$ vanishes on a dense set of $\mathbb{Q}$
and $g$ is continuous on $\mathbb{R}$, $g$ vanishes on $\mathbb{R}$.
Therefore, $f(x) = e^{cx}$ for $x \in \mathbb{R}$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement.}
\emph{Proof of (a).}

\begin{enumerate}
\item[(1)]
Since $f(x)$ is not zero, there exists $x_0 \in \mathbb{R}$ such that $f(x_0) \neq 0$.
So $f(0)f(x_0) = f(x_0)$, or $f(0) = 1$ by cancelling $f(x_0) \neq 0$.
\item[(2)]
Since $f$ is differentiable, for any $x \in \mathbb{R}$,
\begin{align*}
f'(x)
=& \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h} \\
=& \lim_{h \rightarrow 0} \frac{f(x)f(h) - f(x)}{h} \\
=& f(x) \lim_{h \rightarrow 0} \frac{f(h) - 1}{h} \\
=& f(x) \lim_{h \rightarrow 0} \frac{f(h) - f(0)}{h} \\
=& f(x) f'(0).
\end{align*}
Let $c = f'(0)$ be a constant. Then $f'(x) = c f(x)$.
So $f(x) = e^{cx}$ for $x \in \mathbb{R}$.
(To see this, let $g(x) = \frac{f(x)}{e^{cx}}$ be well-defined on $\mathbb{R}$. $g(0) = 1$.
$g'(x) = 0$ since $f'(x) = c f(x)$. So $g(x)$ is a constant, or $g(x) = 1$ since $g(0) = 1$.
Therefore, $f(x) = e^{cx}$ on $\mathbb{R}$.)
\end{enumerate}
$\Box$ \\



\textbf{Supplement.} Cauchy's functional equation.
\begin{enumerate}
\item[(1)]
\emph{(Cauchy's functional equation.) Suppose $f(x) + f(y) = f(x + y)$ for all real $x$ and $y$.
Assuming that $f$ is continuous, prove that $f(x) = cx$ where $c$ is a constant}. \\

Notice that we cannot let $g(x) = \log f(x)$
and apply Cauchy's functional equation on $g(x)$
to prove Exercise 8.6 since $f(x)$ is not necessary positive and thus
$g(x) = \log f(x)$ might be meaningless.
However, this wrong approach gives you some useful ideas such as
you need to prove that $f(x)$ is positive first,
and $f(x)$ should be equal to $e^{cx}$ where $c = g(1) = \log f(1)$.

\item[(2)]
\emph{Suppose $f(xy) = f(x) + f(y)$ for all positive real $x$ and $y$.
Assuming that $f$ is continuous, prove that $f(x) = c \log x$ where $c$ is a constant}.

\item[(3)]
\emph{Suppose $f(xy) = f(x)f(y)$ for all positive real $x$ and $y$.
Assuming that $f$ is continuous and positive,
prove that $f(x) = x^c$ where $c$ is a constant}.

\item[(4)]
\emph{Suppose $f(x + y) = f(x) + f(y) + xy$ for all real $x$ and $y$.
Assuming that $f$ is continuous,
prove that $f(x) = \frac{1}{2}x^2 + cx$ where $c$ is a constant}.

\item[(5)]
\emph{(USA 2002.) Suppose $f(x^2 - y^2) = x f(x) - y f(y)$ for all real $x$ and $y$.
Assuming that $f$ is continuous,
prove that $f(x) = cx$ where $c$ is a constant}. \\
\end{enumerate}



\textbf{Supplement.}
\emph{Show that the only automorphism of $\mathbb{Q}$ is the identity.} \\

\emph{Proof.}
Given any $\sigma \in \text{Aut}(\mathbb{Q})$.
\begin{enumerate}
\item[(1)]
\emph{Show that $\sigma(1) = 1$.}
Since $1^2 = 1$, $\sigma(1)\sigma(1) = \sigma(1)$. $\sigma(1) = 0$ or $1$.
There are only two possible cases.
  \begin{enumerate}
  \item[(a)]
  Assume that $\sigma(1) = 0$. So
  $$\sigma(a) = \sigma(a \cdot 1) = \sigma(a)\cdot \sigma(1) = \sigma(a) \cdot 0 = 0$$
  for any $a \in \mathbb{Q}$.
  That is, $\sigma = 0 \in \text{Aut}(\mathbb{Q})$, which is absurd.
  \item[(b)]
  Therefore, $\sigma(1) = 1$.
  \end{enumerate}
\item[(2)]
\emph{Show that $\sigma(n) = n$ for all $n \in \mathbb{Z}^+$.}
Write $n = 1 + 1 + \cdots + 1$ ($n$ times $1$).
Applying the additivity of $\sigma$, we have
$$\sigma(n) = \sigma(1) + \sigma(1) + \cdots + \sigma(1) = 1 + 1 + \cdots + 1 = n.$$
(Might use induction on $n$ to eliminate $\cdots$ symbols.)
\item[(3)]
\emph{Show that $\sigma(n) = n$ for all $n \in \mathbb{Z}$.}
By the additivity of $\sigma$, $\sigma(-n) = -\sigma(n) = -n$ for $n \geq 0$.
The result is established.
\end{enumerate}
For any $a = \frac{n}{m} \in \mathbb{Q}$ ($m, n \in \mathbb{Z}$, $n \neq 0$),
applying the multiplication of $\sigma$ on $am = n$,
that is,
$\sigma(a) \sigma(m) = \sigma(n)$. By (3), we have $\sigma(a)m = n$,
or $$\sigma(a) = \frac{m}{n} = a$$
provided $n \neq 0$,
or $\sigma$ is the identity.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.7.}
\addcontentsline{toc}{subsection}{Exercise 8.7.}
\emph{If $0 < x < \frac{\pi}{2}$, prove that
\[
  \frac{2}{\pi} < \frac{\sin x}{x} < 1.
\]} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let
\begin{equation*}
  f(x) =
    \begin{cases}
      \frac{\sin x}{x} & \text{ if } x \neq 0 \\
      1                & \text{ if } x = 0
    \end{cases}
\end{equation*}
be a continuous function on $\left[ 0, \frac{\pi}{2} \right]$
(since $\lim_{x \to 0+} f(x) = 1$).
So
\[
  f'(x) = \frac{x \cos x - \sin x}{x^2} < 0
\]
on $\left( 0, \frac{\pi}{2} \right)$
since $\tan x > x$ on $\left( 0, \frac{\pi}{2} \right)$.

\item[(2)]
\emph{Show that $\frac{\sin x}{x} < 1$ on $\left( 0, \frac{\pi}{2} \right)$.}
Given any $x \in \left( 0, \frac{\pi}{2} \right)$, there exists $\xi_1 \in (0,x)$ such that
\[
  \frac{f(x) - f(0)}{x - 0} = f'(\xi_1) < 0
\]
by the mean value theorem (Theorem 5.10).
So $f(x) < f(0) = 1$,
or $\frac{\sin x}{x} < 1$.

\item[(3)]
\emph{Show that $\frac{\sin x}{x} > \frac{2}{\pi}$ on $\left( 0, \frac{\pi}{2} \right)$.}
Given any $x \in \left( 0, \frac{\pi}{2} \right)$, there exists $\xi_2 \in (0,x)$ such that
\[
  \frac{f(\frac{\pi}{2}) - f(x)}{\frac{\pi}{2} - x} = f'(\xi_2) < 0
\]
by the mean value theorem (Theorem 5.10).
So $f(x) > f(\frac{\pi}{2}) = \frac{2}{\pi}$,
or $\frac{\sin x}{x} > \frac{2}{\pi}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.8.}
\addcontentsline{toc}{subsection}{Exercise 8.8.}
\emph{For $n=0,1,2,\ldots$, and $x$ real, prove that
\[
  |\sin(nx)| \leq n |\sin x|.
\]
Note that this inequality may be false for other values of $n$.
For instance,
\[
  \abs{ \sin(\frac{1}{2} \pi) } > \frac{1}{2}|\sin \pi|.
\]} \\

\emph{Proof.}
Induction on $n$.
\begin{enumerate}
\item[(1)]
Note that
\[
  \sin(a+b) = \sin a \cos b + \cos a \sin b
\]
for any $a, b \in \mathbb{R}$.
\item[(2)]
$n = 0,1$ are clearly true.
\item[(3)]
Assume the induction hypothesis that for the single case $n = k$ holds,
meaning
\[
  |\sin(kx)| \leq k |\sin x|
\]
is true.
It follows that
\begin{align*}
  |\sin((k+1)x)|
  &= |\sin(kx) \cos x + \cos(kx) \sin x|
    &\text{((1))} \\
  &\leq |\sin(kx)| |\cos x| + |\cos(kx)| |\sin x|
    &\text{(Triangle inequality)} \\
  &\leq |\sin(kx)| + |\sin x|
    &\text{($|\cos(\cdot)| \leq 1$)} \\
  &\leq k |\sin x| + |\sin x|
    &\text{(Induction hypothesis)} \\
  &\leq (k+1)|\sin x|.
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% https://rivoal.perso.math.cnrs.fr/articles/cteuler.pdf
% - Lots of identities about the Euler-Mascheroni constant (without proof)
% https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant
% - Wikipedia.



\subsection*{Exercise 8.9 (Euler-Mascheroni constant).}
\addcontentsline{toc}{subsection}{Exercise 8.9 (Euler-Mascheroni constant).}
\begin{enumerate}
\item[(a)]
\emph{Put $s_N = 1 + \frac{1}{2} + \cdots + \frac{1}{N}$.
Prove that
\[
  \lim_{N \to \infty}(s_N - \log N)
\]
exists.
(The limit, often denoted by $\gamma$, is called Euler's constant.
Its numerical value is $0.5772\ldots$.
It is not known whether $\gamma$ is rational or not.)}
\item[(b)]
\emph{Roughly how large must $m$ be so that $N = 10^m$ satisfies $s_N > 100$?} \\
\end{enumerate}

\emph{Proof of (a) (Theorem 3.14).}
\begin{enumerate}
\item[(1)]
Note that
\begin{align*}
  &\frac{1}{1+\frac{1}{n}} \leq \frac{1}{x} \leq 1
    \text{ for } x \in \left[ 1,1+\frac{1}{n} \right] \\
  \Longrightarrow&
  \int_{1}^{1+\frac{1}{n}} \frac{dx}{1+\frac{1}{n}}
  \leq \int_{1}^{1+\frac{1}{n}} \frac{dx}{x}
  \leq \int_{1}^{1+\frac{1}{n}} dx
    &\text{(Theorem 6.12(b))} \\
  \Longrightarrow&
  \frac{1}{n+1}
  \leq \int_{1}^{1+\frac{1}{n}} \frac{dx}{x}
  \leq \frac{1}{n} \\
  \Longrightarrow&
  \frac{1}{n+1}
  \leq \log \left(1 + \frac{1}{n} \right)
  \leq \frac{1}{n}.
    &\text{(Equation (39) on page 180)}
\end{align*}

\item[(2)]
Define
\[
  \gamma_n = s_n - \log n.
\]
It suffices to show that $\{\gamma_n\}$ is monotonic and bounded (Theorem 3.14).

\item[(3)]
\emph{Show that $\{\gamma_n\}$ is decreasing.}
\begin{align*}
  \gamma_{n+1} - \gamma_n
  &= (s_{n+1} - \log(n+1)) - (s_n - \log n) \\
  &= (s_{n+1} - s_n) - (\log(n+1)-\log n) \\
  &= \frac{1}{n+1} - \log \left( \frac{n+1}{n} \right) \\
  &= \frac{1}{n+1} - \log \left(1 + \frac{1}{n} \right) \\
  &\leq 0.
    &\text{((1))}
\end{align*}
\emph{Note.} $\gamma_n \leq \cdots \leq \gamma_1 = 1$ for all $n=1,2,3,\ldots$.

\item[(4)]
\emph{Show that $\gamma_n \geq 0$ for all $n=1,2,3,\ldots$.}
Since
\begin{align*}
  \log n
  &= \sum_{k=1}^{n-1} (\log (k+1) - \log k) \\
  &= \sum_{k=1}^{n-1} \log \frac{k+1}{k} \\
  &= \sum_{k=1}^{n-1} \log \left( 1 + \frac{1}{k} \right) \\
  &\leq \sum_{k=1}^{n-1} \frac{1}{k}
    &\text{((1))} \\
  &= s_{n-1},
\end{align*}
we have
\[
  \gamma_n = s_n - \log n \geq s_n - s_{n-1} = \frac{1}{n} > 0.
\]
\end{enumerate}
By (3)(4), $\{\gamma_n\}$ converges to $\lim_{N \to \infty} (s_N - \log N) = \gamma$.
$\Box$ \\



\textbf{Supplement.}
  \emph{Show that if $f \geq 0$ on $[0,\infty)$ and $f$ is monotonically decreasing,
  and if $$c_n = \sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx,$$
  then $\lim_{n \to \infty} c_n$ exists.}
  (Exercise 10 of Section 5.2 in the textbook:
  \emph{R Creighton Buck, Advanced Calculus, 3rd edition}. See page 235.)
  If this exercise is true,
  we can get the existence of $\gamma$ by taking $f(x) = \frac{1}{x}$.
  \begin{enumerate}
    \item[(1)]
    Note that
    \[
       f(n+1)
      \leq \int_{n}^{n+1} f(x) dx
      \leq f(n).
    \]

    \item[(2)]
    \emph{Show that $\{c_n\}$ is decreasing.}
    \[
      c_{n+1} - c_n
      = f(n+1) - \int_{n}^{n+1} f(x)dx
      \leq 0.
    \]

    \item[(3)]
    \emph{Show that $c_n \geq 0$.}
    Since $f(k) \geq \int_{k}^{k+1} f(x) dx$,
    \begin{align*}
      \sum_{k=1}^{n} f(k)
      &\geq \sum_{k=1}^{n} \int_{k}^{k+1} f(x) dx \\
      &= \int_{1}^{n+1} f(x) dx \\
      &\geq \int_{1}^{n} f(x) dx.
        &(f \geq 0)
    \end{align*}
    So that $c_n = \sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx \geq 0$.

    \item[(4)]
    By (2)(3), $\{c_n\}$ converges (Theorem 3.14).
  \end{enumerate}
  $\Box$ \\



\emph{Proof of (a) (Limit comparison test).}
Inspired by this paper:
\emph{Philippe Flajolet and Ilan Vardi,
Zeta Function Expansions of Classical Constants}.
\begin{enumerate}
\item[(1)]
Rewrite
\[
  \gamma_n + \log n - \log(n+1)
  = \sum_{k=1}^{n} \left( \frac{1}{k} - \log \left(1+\frac{1}{k}\right) \right)
\]
(similar to the argument in (a)(4)(Theorem 3.14)).
Let
\[
  c_k = \frac{1}{k} - \log \left(1+\frac{1}{k}\right).
\]
\item[(2)]
\emph{Show that
\[
  \lim_{k \to \infty}
  \frac{c_k}{ \frac{1}{k^2} }
  = \frac{1}{2}.
\]}

In fact,
\begin{align*}
  &\lim_{k \to \infty}
  \frac{c_k}{ \frac{1}{k^2} } \\
  =&
  \lim_{x \to 0} \frac{x - \log(1+x)}{ x^2 }
    &\text{(Put $x = \frac{1}{k}$)} \\
  =&
  \lim_{x \to 0} \frac{1 - \frac{1}{1+x}}{ 2x }
    &\text{(L'Hospital's rule)} \\
  =&
  \lim_{x \to 0} \frac{1}{ 2(x+1) } \\
  =& \frac{1}{2}.
\end{align*}
\item[(3)]
By limit comparison test or comparison test,
$\sum c_k$
converges since $\sum \frac{1}{k^2}$ converges.
Also, $$\lim_{n \to \infty} \log n - \log(n+1) = 0.$$
Therefore, $\lim_{n \to \infty} \gamma_n$ exists.
\end{enumerate}
$\Box$ \\

\emph{Note.}
This proof is based on \textbf{limit comparison test} (Theorem 8.21) in this textbook:
\emph{Tom. M. Apostol, Mathematical Analysis, 2nd edition.}
It is easy to prove by the original comparison test. \\



\emph{Proof of (a) (Comparison test).}
\begin{enumerate}
\item[(1)]
Note that
\[
  0 \leq x - \log(x+1) \leq \frac{x^2}{2}
\]
for all $x \geq 0$.

\item[(2)]
Write
\[
  c_n = \frac{1}{n} - \log \left(1+\frac{1}{n}\right).
\]
as in the the proof of (a) (Limit comparison test).
By (1),
\[
  |c_n| \leq \frac{1}{2n^2}
\]
for all $n=1,2,\ldots$.
Hence, by the comparison test (Theorem 3.25(a),
$\sum c_n$ converges since $\sum \frac{1}{n^2}$ converges (to $\frac{\pi^2}{6}$).
Use the same argument in the proof of (a) (Limit comparison test), since
\[
  \gamma_n + \log n - \log(n+1) = \sum c_n
  \text{ and }
  \lim_{n \to \infty} \log n - \log(n+1) = 0,
\]
we have the existence of $\lim \gamma_n = \gamma$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (a) (Uniformly convergence of $\sum \frac{x}{n(x+n)}$).}
(One example to Exercise 7 of Section 6.2 in the textbook:
\emph{R Creighton Buck, Advanced Calculus, 3rd edition}.
See pages 270 to 271.)
\begin{enumerate}
\item[(1)]
Let
\[
  f_n(x) = \frac{x}{n(x+n)} = \frac{1}{n} - \frac{1}{x+n}
\]
defined on $E = [0,1]$.

\item[(2)]
Note that
\[
  |f_n(x)| \leq \frac{1}{n^2}
\]
for all $x \in [0,1]$. Since $\sum \frac{1}{n^2}$ converges,
$\sum f_n$ converges uniformly on $[0,1]$ (Theorem 7.10).

\item[(3)]
Corollary to Theorem 7.16 implies that
\begin{align*}
  \int_{0}^{1} \sum_{n=1}^{\infty} \frac{x}{n(x+n)} dx
  &= \sum_{n=1}^{\infty} \int_{0}^{1} \frac{x}{n(x+n)} dx \\
  &= \sum_{n=1}^{\infty} \int_{0}^{1} \left( \frac{1}{n} - \frac{1}{x+n} \right) dx \\
  &= \sum_{n=1}^{\infty} \left( \frac{1}{n} - \log\frac{n+1}{n} \right) \\
  &= \lim_{N \to \infty}
    \left( \sum_{n=1}^{N} \frac{1}{n} - \log(N+1) \right) \\
  &= \lim_{N \to \infty}
    \left( s_N - \log(N+1) \right)
\end{align*}
exists.
Since $\lim_{N \to \infty}(\log(N+1) - \log N) = 0$,
\begin{align*}
  \gamma
  &= \lim_{N \to \infty}( s_N - \log N ) \\
  &= \lim_{N \to \infty}( s_N - \log(N+1) ) + \lim_{N \to \infty}(\log(N+1) - \log N)
\end{align*}
exists.
\end{enumerate}
$\Box$ \\



\emph{Proof of (a) (Existence of $\int_{1}^{\infty} \frac{\{x\}}{x^2} dx$).}
\begin{enumerate}
\item[(1)]
Define $\{x\} = x - [x]$ where $[x]$ is the greatest integer $\leq x$
(Exercise 6.16).
\emph{Show that
\[
  \int_{1}^{\infty} \frac{\{x\}}{x^2} dx < \infty.
\]}

Use the similar argument in Exercise 6.16(b).
Since
$\frac{\{x\}}{x^2} \leq \frac{1}{x^2}$ on $[1,\infty)$
and $\int_{1}^{\infty} \frac{1}{x^2} dx = 1$ exists,
the result is established (Theorem 6.12(b)).

\item[(2)]
\emph{Show that
\[
  \int_{1}^{N} \frac{[x]}{x^2} dx = s_N - 1.
\]}

Use the similar argument in Exercise 6.16(a),
\begin{align*}
  \int_{1}^{N} \frac{[x]}{x^2} dx
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{[x]}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{k}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{k}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \frac{1}{k+1} \\
  &= \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{N} \\
  &= s_N - 1.
\end{align*}

\textbf{Supplement (Euler's summation formula).}
(Theorem 7.13 in the textbook:
\emph{Tom. M. Apostol, Mathematical Analysis, 2nd edition.})
\emph{If $f$ has a continuous derivative $f'$ on $[a,b]$, then we have
\[
  \sum_{a < n \leq b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\{x\}dx + f(a)\{a\} - f(b)\{b\},
\]
where $\sum_{a < n \leq b}$ means the sum from $n=[a]+1$ to $n=[b]$.
When $a$ and $b$ are integers, this becomes
\[
  \sum_{n=a}^{b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\left( \{x\}-\frac{1}{2} \right)dx
  + \frac{f(a)+f(b)}{2}.
\]}
By taking $f(x) = \frac{1}{x}$ we can get the same result. \\

\item[(3)]
\emph{Show that
\[
  \int_{1}^{N} \frac{\{x\}}{x^2} dx = \log N - s_N + 1 = 1 - \gamma_N.
\]}

In fact,
\begin{align*}
  \int_{1}^{N} \frac{\{x\}}{x^2} dx
  &= \int_{1}^{N} \frac{x-[x]}{x^2} dx \\
  &= \int_{1}^{N} \frac{1}{x} dx - \int_{1}^{N} \frac{[x]}{x^2} dx \\
  &= \log N - (s_N - 1) \\
  &= \log N - s_N + 1 \\
  &= 1 - \gamma_N.
\end{align*}

\item[(4)]
Since
\[
  \lim_{N \to \infty} \int_{1}^{N} \frac{\{x\}}{x^2} dx
  = \int_{1}^{\infty} \frac{\{x\}}{x^2} dx
\]
exists (by (1)),
$\gamma = \lim \gamma_N$ exists.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
By $s_n - \log n > 0$ in (a)(4)(Theorem 3.14),
it suffices to choose $N = 10^m$ such that
$s_N \geq \log(N+1) > 100$, or
\[
  m > \frac{\log(\exp(100) - 1)}{\log 10},
\]
or choose $m$ satisfying
\[
  m
  > \frac{100}{\log 10}
  > \frac{\log(\exp(100) - 1)}{\log 10},
\]
or $m = 44$.
$\Box$ \\

\emph{Note.}
The exact value of $N$ is
\[
  15092688622113788323693563264538101449859497 \approx 1.509 \times 10^{43}.
\] \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.10.}
\addcontentsline{toc}{subsection}{Exercise 8.10.}
\emph{Prove that $\sum \frac{1}{p}$ diverges; the sum extends over all primes.} \\

There are many proofs of this result. We provide some of them. \\

\emph{Proof (Due to hint).}
Given $N$.
\begin{enumerate}
\item[(1)]
\emph{Show that
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}.
\]}

By the unique factorization theorem on $n \leq N$,
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \prod_{p \leq N} \left( 1 + \frac{1}{p} + \frac{1}{p^2} + \cdots \right)
  = \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}.
\]

\item[(2)]
By (1) and the fact that $\sum \frac{1}{n}$ diverges,
there are infinitely many primes.

\item[(3)]
\emph{Show that
\[
  \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]}

By applying the inequality $(1 - x)^{-1} < e^{2x}$ where $x \in (0, \frac{1}{2}]$
on any prime $p$,
\[
  \left( 1 - \frac{1}{p} \right)^{-1} < \exp \left( \frac{2}{p} \right).
\]
Now multiplying the inequality over all primes $p \leq N$ and noticing that
$\exp(x) \cdot \exp(y) = \exp(x + y)$, we have
\[
  \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]

\item[(4)]
By (1)(3),
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]
Since $\sum_{n \leq N} \frac{1}{n}$ diverges, the result holds.
\end{enumerate}
$\Box$ \\



\emph{Proof (Due to Kenneth Ireland and Michael Rosen).}
The proof in Kenneth Ireland and Michael Rosen,
A Classical Introduction to Modern Number Theory, Second Edition (Theorem 3 in Chapter 2)
does not use the inequality $(1 - x)^{-1} < e^{2x}$ ($x \in (0, \frac{1}{2}]$) directly.
Instead, the authors take the logarithm on $(1 - p^{-1})^{-1}$ and estimate it.
(So the length of proof is longer than the proof due to hint.)
That is,
\begin{align*}
- \log(1 - p^{-1})
&= \sum_{n = 1}^{\infty} \frac{p^{-n}}{n} \\
&= \frac{1}{p} + \sum_{n = 2}^{\infty} \frac{p^{-n}}{n} \\
&< \frac{1}{p} + \sum_{n = 2}^{\infty} p^{-n} \\
&= \frac{1}{p} + \frac{p^{-2}}{1 - p^{-1}} \\
&< \frac{1}{p} + 2 \cdot \frac{1}{p^2}.
\end{align*}
Now we sum over all primes $p \leq N$,
$$\log \left( \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1} \right)
< \sum_{p \leq N} \frac{1}{p} + 2 \sum_{p \leq N} \frac{1}{p^2}.$$
So
$$\log \sum_{n \leq N} \frac{1}{n}
< \sum_{p \leq N} \frac{1}{p} + 2 \sum_{p \leq N} \frac{1}{p^2}.$$
Notice that $\sum \frac{1}{n}$ diverges and $\sum \frac{1}{p^2}$ converges
(since $\sum \frac{1}{n^2}$ converges).
Therefore, $\sum \frac{1}{p}$ diverges.
$\Box$ \\



\emph{Proof (Due to I. Niven).}
It is an exercise in Kenneth Ireland and Michael Rosen,
A Classical Introduction to Modern Number Theory, Second Edition. See Exercise 27 in Chapter 2.

\begin{enumerate}
\item[(1)]
\emph{Show that ${\sum}' \frac{1}{n}$, the sum being over square free integers, diverges.}
For any positive integers $n$, we can write $n = a^2 b$ where $a \in \mathbb{Z}^+$ and
$b$ is a square free integer.
Given $N$,
$$\sum_{n \leq N} \frac{1}{n}
\leq \left(\sum_{a = 1}^{\infty} \frac{1}{a^2} \right)
\left( {\sum_{b \leq N}}' \frac{1}{b} \right).$$
Notice that $\sum_{a = 1}^{\infty} \frac{1}{a^2}$ converges.
Since $\sum_{n \leq N} \frac{1}{n} \rightarrow \infty$ as $N \rightarrow \infty$,
$\sum'_{b \leq N}\frac{1}{b} \rightarrow \infty$ as $N \rightarrow \infty$.

\item[(2)]
\emph{Show that
\[
  \prod_{p \leq N} ( 1 + \frac{1}{p} ) \rightarrow \infty \text{ as } N \rightarrow \infty.
\]}

By the unique factorization theorem on $n \leq N$,
$$\prod_{p \leq N} \left( 1 + \frac{1}{p} \right)
\geq {\sum_{n \leq N}}' \frac{1}{n}.$$
Since ${\sum_{n \leq N}}' \frac{1}{n} \rightarrow \infty$ as $N \rightarrow \infty$ by (1),
the conclusion is established.

\item[(3)]
By applying the inequality $e^x > 1 + x$ on any prime $p$,
$$\exp\left(\frac{1}{p}\right) > 1 + \frac{1}{p}.$$
Now multiplying the inequality over all primes $p \leq N$ and noticing that
$\exp(x) \cdot \exp(y) = \exp(x + y)$, we have
$$\exp\left(\sum_{p \leq N} \frac{1}{p} \right)
> \prod_{p \leq N} \left( 1 + \frac{1}{p} \right).$$
By (2),
$\exp\left(\sum_{p \leq N} \frac{1}{p} \right) \rightarrow \infty$ as $N \rightarrow \infty$, or
$\sum_{p \leq N} \frac{1}{p} \rightarrow \infty$ as $N \rightarrow \infty$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.11.}
\addcontentsline{toc}{subsection}{Exercise 8.11.}
\emph{Suppose $f \in \mathscr{R}$ on $[0,A]$ for all $A < \infty$,
and $f(x) \to 1$ as $x \to +\infty$.
Prove that
\[
  \lim_{t \to 0} t \int_{0}^{\infty} e^{-tx}f(x)dx = 1
  \qquad (t > 0),
\]} \\

It is similar to Exercise 3.14(a). \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
The integral $\int_{0}^{\infty} e^{-tx}f(x)dx$ is well-defined.
(It suffices to show that $\int_{0}^{\infty} e^{-tx}f(x)dx$
converges absolutely in the sense of Exercise 6.8.
It is quite easy since $f(x) \to 1$ as $x \to +\infty$
and well-behavior of $\int_{A_0}^{\infty} e^{-tx}f(x)dx$ for any $A_0 > 0$.)

\item[(2)]
Note that
\[
  t \int_{0}^{\infty} e^{-tx} dx = 1
\]
for any $t > 0$.

\item[(3)]
Since $f(x) \to 1$ as $x \to +\infty$,
there is $A_0 > 0$ such that
\[
  |f(x) - 1| < \frac{\varepsilon}{64} \text{ whenever } x \geq A_0.
\]

\item[(4)]
Since $f \in \mathscr{R}$ on $[0,A_0]$,
$f$ is bounded on $[0,A_0]$,
or $|f| \leq M$ on $[0,A_0]$ for some $M$ (Theorem 6.7(c)).

\item[(5)]
As $t > 0$,
\begin{align*}
  &\abs{\left( t \int_{0}^{\infty} e^{-tx}f(x)dx \right) - 1} \\
  =& \abs{t \int_{0}^{\infty} e^{-tx}(f(x)-1)dx }
    &((2)) \\
  \leq& t \int_{0}^{\infty} e^{-tx}|f(x)-1|dx
    &\text{((1) with Theorem 6.13)} \\
  =& t \int_{0}^{A_0} e^{-tx}|f(x)-1|dx
    + t \int_{A_0}^{\infty} e^{-tx}|f(x)-1|dx \\
  \leq& t \int_{0}^{A_0}(M + 1) dx + t \int_{A_0}^{\infty} e^{-tx}|f(x)-1|dx
    &\text{((3) and $e^{-tx} \leq 1$)} \\
  \leq& t \int_{0}^{A_0}(M + 1) dx + t \int_{A_0}^{\infty} e^{-tx} \frac{\varepsilon}{64} dx
    &\text{((4))} \\
  =& t A_0 (M + 1) + \exp(-A_0t) \frac{\varepsilon}{64} \\
  \leq& t A_0 (M + 1) + \frac{\varepsilon}{64}.
    &\text{($e^{-tx} \leq 1$)}
\end{align*}
Since $t$ is arbitrary, take $t = \frac{\varepsilon}{89 A_0 (M+1)} > 0$ to get
\[
  \abs{\left( t \int_{0}^{\infty} e^{-tx}f(x)dx \right) - 1}
  < \frac{\varepsilon}{89} + \frac{\varepsilon}{64}
  < \varepsilon,
\]
or
\[
  \lim_{t \to 0^+} t \int_{0}^{\infty} e^{-tx}f(x)dx = 1.
\]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.12.}
\addcontentsline{toc}{subsection}{Exercise 8.12.}
\emph{Suppose $0 < \delta < \pi$,
\begin{equation*}
  f(x) =
    \begin{cases}
      1 & \text{ if } |x| \leq \delta, \\
      0 & \text{ if } \delta < |x| \leq \pi,
    \end{cases}
\end{equation*}
and $f(x + 2\pi) = f(x)$ for all $x$.}

\begin{enumerate}
\item[(a)]
\emph{Compute the Fourier coefficients of $f$.}

\item[(b)]
\emph{Compute that
$$\sum_{n = 1}^{\infty} \frac{\sin(n\delta)}{n} = \frac{\pi - \delta}{2}
\qquad
(0 < \delta < \pi).$$}

\item[(c)]
\emph{Deduce from Parseval's theorem that
$$\sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \delta} = \frac{\pi - \delta}{2}.$$}

\item[(d)]
\emph{Let $\delta \rightarrow 0$ and prove that
$$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
= \frac{\pi}{2}.$$}

\item[(e)]
\emph{Put $\delta = \frac{\pi}{2}$ in (c). What do you get?} \\
\end{enumerate}

It is a centered square pulse around $x = 0$ with shift $\delta$.
Besides, $f(x)$ is an even function. \\

\emph{Proof of (a).}
\begin{align*}
c_0
&= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx \\
&= \frac{1}{2 \pi} \int_{-\delta}^\delta dx \\
&= \frac{\delta}{\pi}.
\end{align*}
For $0 \neq n \in \mathbb{Z}$,
\begin{align*}
c_n
&= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) e^{-inx} dx \\
&= \frac{1}{2 \pi} \int_{-\delta}^\delta e^{-inx} dx \\
&= \frac{1}{2 \pi} \cdot \frac{2 \sin(n \delta)}{n} \\
&= \frac{\sin(n \delta)}{n \pi}.
\end{align*}
$\Box$ \\

\textbf{Supplement.} Find $a_n$ and $b_n$ of this textbook. \\
By (a), $a_0 = \frac{\delta}{\pi}$,
$a_n = \frac{2 \sin(n \delta)}{n \pi}$, $b_n = 0$ for $n \in \mathbb{Z}^+$.
Surely, we can compute $a_n$ and $b_n$ ($n > 0$) directly.
Since $f(x)$ is an even function, $b_n = 0$.
And
\begin{align*}
a_n
&= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(nx) dx \\
&= \frac{2}{\pi} \int_{0}^\delta \cos(nx) dx \\
&= \frac{2 \sin(n \delta)}{n \pi}.
\end{align*}

\emph{Proof of (b).}
Given $x = 0$, there are constants $\delta' = \delta > 0$ and $M = 1 < \infty$ such that
$$|f(0 + t) - f(0)| \leq M|t|$$ for all $t \in (-\delta', \delta')$.
By Theorem 8.14,
$$\sum_{-\infty}^{\infty} c_n = f(0).$$
Notice that $c_{-n} = c_n$ for $n \in \mathbb{Z}^+$, so
\begin{align*}
\frac{\delta}{\pi} + 2 \sum_{n = 1}^{\infty} \frac{\sin(n \delta)}{n \pi}
&= 1 \\
\sum_{n = 1}^{\infty} \frac{\sin(n \delta)}{n}
&= \frac{\pi - \delta}{2}.
\end{align*}
$\Box$ \\

We can also use the expression $a_n$ and $b_n$ to prove the same thing.
Besides, taking $\delta = 1$ yields
$$\sum_{n = 1}^{\infty} \frac{\sin n}{n} = \frac{\pi - 1}{2}.$$ \\

\emph{Proof of (c).}
Since $f(x)$ is a Riemann-integrable function with period $2 \pi$,
by Parseval's theorem
$$\frac{1}{2 \pi} \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{-\infty}^{\infty} |c_n|^2.$$
So
$$\frac{\delta}{\pi}
= \frac{\delta^2}{\pi^2} + 2 \sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \pi^2}, $$
or
$$\sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \delta}
= \frac{\pi - \delta}{2}.$$
$\Box$ \\

Notices that
$$\sum_{n = 1}^{\infty} \frac{(\sin n)^2}{n^2} = \frac{\pi - 1}{2}$$
as $\delta = 1$. \\

\emph{Proof of (d).}
Given $\varepsilon > 0$.
By Exercise 6.8,
$$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx$$ exists.
So there exists $b > 0$ such that
$$\left| \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
- \int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
\right| < \frac{\varepsilon}{4}$$

By Supplement in Chapter 6, there exists
$\delta > 0$ such that for any partition
$P_m = \{ 0, \frac{b}{m}, \frac{2b}{m}, \ldots, \frac{(m - 1)b}{m}, b\}$ of $[0, b]$
with $\Vert P \Vert = \frac{b}{m} < \delta$, or $m > \frac{b}{\delta}$,
we have
\begin{align*}
\left|
\sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{(n \frac{b}{m})^2} \cdot \frac{b}{m}
- \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
\right|
&< \frac{\varepsilon}{4}, \\
\left|
\sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
- \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
\right| &< \frac{\varepsilon}{4}.
\end{align*}
For simplicity we resize $\delta$ to $\delta < \pi$ to make $0 < \frac{b}{m} < \delta < \pi$.
Besides, since $\sum_{n = 1}^{\infty} \frac{1}{n^2}$ converges,
there exists $N > 0$ such that
$$\left|
\sum_{n = 1}^{\infty} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
- \sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
\right|
< \frac{\varepsilon}{4}$$
whenever $m \geq N$.
By (c),
$$\left|
\frac{\pi - \frac{b}{m}}{2}
- \sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
\right|
< \frac{\varepsilon}{4}$$
whenever $m \geq N$.
Last, it is easy to get
$$\left|
\frac{\pi}{2}
- \frac{\pi - \frac{b}{m}}{2}
\right|
< \frac{\varepsilon}{4}$$
whenever $m > \frac{2b}{\varepsilon}$.
Now we have
$$\left|
\frac{\pi}{2}
- \int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
\right|
< \varepsilon
$$
whenever $m > \max(\frac{b}{\delta}, N, \frac{2b}{\varepsilon})$.
Since $\varepsilon$ is arbitrary,
$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
= \frac{\pi}{2}$.
$\Box$ \\


\emph{Proof of (e).}
$$\sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} = \frac{\pi^2}{8}.$$
Write
\begin{align*}
\sum_{n = 1}^{\infty} \frac{1}{n^2}
&= \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} + \sum_{n = 1}^{\infty} \frac{1}{(2n)^2} \\
&= \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} + \frac{1}{4} \sum_{n = 1}^{\infty} \frac{1}{n^2},
\end{align*}
so
$$\sum_{n = 1}^{\infty} \frac{1}{n^2}
= \frac{4}{3} \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2}
= \frac{\pi^2}{6}.$$
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.13.}
\addcontentsline{toc}{subsection}{Exercise 8.13.}
\emph{Put $f(x) = x$ if $0 \leq x < 2 \pi$, and apply Parseval's theorem to conclude that
$$\sum_{n = 1}^{\infty} \frac{1}{n^2} = \frac{\pi}{6}.$$} \\

\emph{Proof.}
\[
  c_0
  = \frac{1}{2 \pi} \int_{0}^{2 \pi} x dx
  = \pi.
\]
For $n \neq 0$,
\begin{align*}
c_n
&= \frac{1}{2 \pi} \int_{0}^{2 \pi} x e^{-inx} dx \\
&= \frac{1}{2 \pi} \left(
\left[ - \frac{1}{i n} x e^{-inx} \right]_{x = 0}^{x = 2 \pi}
- \int_{0}^{2 \pi} - \frac{1}{i n} e^{-inx} dx \right) \\
&= \frac{i}{n}.
\end{align*}
Since $f(x)$ is a Riemann-integrable function with period $2 \pi$,
by Parseval's theorem (Theorem 8.16)
$$\frac{1}{2 \pi} \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{-\infty}^{\infty} |c_n|^2.$$
So
$$\frac{1}{2 \pi} \cdot \frac{(2 \pi)^3}{3}
= \pi^2 + 2 \sum_{n = 1}^{\infty} \frac{1}{n^2}, $$
or
$$\sum_{n = 1}^{\infty} \frac{1}{n^2}
= \frac{\pi^2}{6}.$$
$\Box$ \\



\textbf{Supplement.} \emph{
Put $f(x) = x^n$ if $n \in \mathbb{Z}^+$ and $0 \leq x < 2 \pi$.
Might get
\[
  \zeta(2n)
  = \sum_{k = 1}^{\infty} \frac{1}{k^{2n}}
  = \frac{(-1)^{n+1} B_{2n} (2\pi)^{2n}}{2(2n)!}.
\]} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.14.}
\addcontentsline{toc}{subsection}{Exercise 8.14.}
\emph{If $f(x)=(\pi-|x|)^2$ on $[-\pi,\pi]$, prove that
\[
  f(x) = \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{4}{n^2} \cos(nx)
\]
and deduce that
\[
  \sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6},
  \qquad
  \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}.
\]
(A recent article by E. L. Stark contains many references to series of the form
$\sum n^{-s}$, where $s$ is a positive integer.
See Math. Mag., vol. 47, 1974, pp. 197-202.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\begin{equation*}
  c_n = \frac{1}{2 \pi} \int_{-\pi}^{\pi} (\pi - |x|)^2 e^{-inx} dx =
    \begin{cases}
      \frac{\pi^2}{3} & (n = 0), \\
      \frac{2}{n^2} & (n \neq 0).
    \end{cases}
\end{equation*}
Hence
\[
  f(x)
  \sim
  \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{2}{n^2}(e^{inx} + e^{-inx})
  =
  \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{4}{n^2} \cos(nx).
\]

\item[(2)]
\emph{Show that $f(x) = \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{4}{n^2} \cos(nx)$.}
By Theorem 8.14,
it suffices to show that there are constants $\delta > 0$ and $M < \infty$ such that
\[
  |f(x+t) - f(x)| \leq M|t| \text{ whenever } |t| < \delta.
\]
By the periodicity of $f(x)$, we might take both $x$ and $x+t$ in $[-\pi,\pi]$
(which implies that $|t| \leq 2\pi$).
Hence,
\begin{align*}
  |f(x+t) - f(x)|
  &= |(\pi-|x+t|)^2 - (\pi-|x|)^2| \\
  &= \abs{ 2\pi-|x|-|x+t| } \cdot \abs{ |x|-|x+t| } \\
  &\leq 2\pi |t|.
\end{align*}

\item[(3)]
\emph{Show that $\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$.}
Take $x = 0 \in [-\pi,\pi]$ in
\[
  (\pi-|x|)^2 = \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{4}{n^2} \cos(nx)
\]
to get
\[
  \pi^2 = \frac{\pi^2}{3} + \sum_{n=1}^{\infty} \frac{4}{n^2},
\]
or
\[
  \sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}.
\]

\item[(4)]
\emph{Show that $\sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}$.}
Since $f(x)$ is a Riemann-integrable function with period $2 \pi$,
by Parseval's theorem (Theorem 8.16)
\[
  \frac{1}{2 \pi} \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{-\infty}^{\infty} |c_n|^2.
\]
So
\[
  \frac{1}{2\pi} \cdot \frac{2\pi^5}{5}
  = \left( \frac{\pi^2}{3} \right)^2
    + 2 \sum_{n=1}^{\infty} \left( \frac{2}{n^2} \right)^2,
\]
or
\[
  \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}.
\]
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Basel problem).}
A simple proof of the formula $\zeta(2) = \frac{\pi^2}{6}$.
\begin{enumerate}
  \item[(1)]
    By equating imaginary parts in DeMoivre's formula
    \begin{align*}
      \cos(n\theta) + i \sin(n\theta)
      &= (\cos \theta + i \sin \theta)^n \\
      &= \sin^n \theta (i + \cot \theta)^n \\
      &= \sin^n \theta \sum_{k=0}^{n} {n \choose k} (i)^k (\cot \theta)^{n-k}
    \end{align*}
    to obtain
    \[
      \sin(n\theta)
      = \sin^n \theta
        \left[
          {n \choose 1} \cot^{n-1} \theta
          - {n \choose 3} \cot^{n-3} \theta
          + {n \choose 5} \cot^{n-5} \theta
          \mp \cdots
        \right].
    \]
  \item[(2)]
    Take $n = 2m+1$,
    \begin{align*}
      &\sin((2m+1)\theta) \\
      =& \sin^{2m+1} \theta
        \left[
          {2m+1 \choose 1} \cot^{2m} \theta
          - {2m+1 \choose 3} \cot^{2m-2} \theta
          + {2m+1 \choose 5} \cot^{2m-4} \theta
          \mp \cdots
        \right] \\
      =& \sin^{2m+1} \theta
        \left[
          {2m+1 \choose 1} (\cot^{2}\theta)^m
          - {2m+1 \choose 3} (\cot^{2}\theta)^{m-1}
          + {2m+1 \choose 5} (\cot^{2}\theta)^{m-2}
          \mp \cdots
        \right] \\
      =& \sin^{2m+1} \theta P_m(\cot^{2}\theta)
    \end{align*}
    where $P_m$ is the polynomail of degree $m$ given by
    \[
      P_m(x) = {2m+1 \choose 1} x^m
          - {2m+1 \choose 3} x^{m-1}
          + {2m+1 \choose 5} x^{m-2}
          \mp \cdots.
    \]
    As $m$ distinct $\theta_k = \frac{k\pi}{2m+1}$ for $1 \leq k \leq m$,
    $0 < \theta_k < \frac{\pi}{2}$, $\sin((2m+1)\theta_k) = 0$
    and $\sin^{2m+1} \theta_k \neq 0$.
    Hence, $m$ distince points $x_k = \cot^2 \theta_k$
    (since $\cot^2 x$ is one-to-one on $(0,\frac{\pi}{2})$)
    are roots of $P_m(x)$ exactly.

  \item[(3)]
    By Vieta's formulas,
    the sum of the zeros of $P_m$ is
    \[
      \sum_{k=1}^m \cot^2 \frac{k\pi}{2m+1}
      = -\frac{-{2m+1 \choose 3}}{{2m+1 \choose 1}}
      = \frac{m(2m-1)}{3}.
    \]

  \item[(4)]
    Start with the inequality $\sin x < x < \tan x$ with $x \in (0, \frac{\pi}{2})$,
    take reciprocals, and square each member to obtain
    \[
      \cot^2 x < \frac{1}{x} < 1 + \cot^2 x.
    \]

  \item[(5)]
    Now put $x = \frac{k\pi}{2m+1}$, where $k$ and $m$ are integers, with $1 \leq k \leq m$,
    and sum on $k$ to obtain
    \[
      \sum_{k=1}^{m} \cot^2 \frac{k\pi}{2m+1}
      < \frac{(2m+1)^2}{\pi^2} \sum_{k=1}^{m} \frac{1}{k^2}
      < m + \sum_{k=1}^{m} \cot^2 \frac{k\pi}{2m+1}.
    \]

  \item[(6)]
    By (3),
    \[
      \frac{m(2m-1)\pi^2}{3(2m+1)^2}
      < \sum_{k=1}^{m} \frac{1}{k^2}
      < \frac{2m(m+1)\pi^2}{3(2m+1)^2}.
    \]
    Now let $m \to \infty$ to obtain $\zeta(2) = \frac{\pi^2}{6}$.
\end{enumerate}
$\Box$ \\

\emph{Note.}
In (3), we might use Newton's identities (also known as the Girard-Newton formula)
to calculate
\[
  \sum_{k=1}^m \cot^{2n} \frac{k\pi}{2m+1}
  = \frac{(-1)^{n+1}B_{2n}2^{4n-1}}{(2n)!} m^{2n} + O(m^{2m-1}).
\]
Similar argument to get
\[
  \zeta(2n)
  = \sum_{k = 1}^{\infty} \frac{1}{k^{2n}}
  = \frac{(-1)^{n+1} B_{2n} (2\pi)^{2n}}{2(2n)!}.
\] \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.15.}
\addcontentsline{toc}{subsection}{Exercise 8.15.}
\emph{With the Dirichlet kernel $D_n$ as defined by
\[
  D_n(x)
  = \sum_{k=-n}^{n} \exp(ikx)
  = \frac{\sin(n+\frac{1}{2})x}{\sin(\frac{x}{2})},
\]
put the \textbf{Fej\'er kernel}
\[
  K_N(x) = \frac{1}{N+1} \sum_{n=0}^{N} D_n(x).
\]
Prove that
\[
  K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}
\]
and that}
\begin{enumerate}
  \item[(a)]
  $K_N \geq 0$,

  \item[(b)]
  $\frac{1}{2\pi} \int_{-\pi}^{\pi} K_N(x) dx = 1$,

  \item[(c)]
  \emph{$K_N(x) \leq \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta}$
  if $0 < \delta \leq |x| \leq \pi$.}
\end{enumerate}

\emph{If $s_N = s_N(f;x)$ is the $N$th partial sum of the Fourier series of $f$,
consider the arithmetic means
\[
  \sigma_N = \frac{s_0+s_1+\cdots+s_N}{N+1}.
\]
Prove that
\[
  s_N(f;x) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)K_N(t)dt,
\]
and hence prove \textbf{Fej\'er's theorem:}}
\begin{quote}
  \emph{If $f$ is continuous, with period $2\pi$,
  then $\sigma_N(f;x) \to f(x)$ uniformly on $[-\pi,\pi]$.}
\end{quote}
\emph{(Hint: Use properties (a),(b),(c) to proceed as in Theorem 7.26.)} \\



\emph{Proof of $K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}$.}
Since
  \begin{align*}
    (1-\cos x) K_N(x)
    =& 2 \left( \sin \frac{x}{2} \right)^2 \frac{1}{N+1}
      \sum_{n=0}^{N} \frac{\sin(n+\frac{1}{2})x}{\sin(\frac{x}{2})} \\
    =& \frac{1}{N+1}
      \sum_{n=0}^{N} 2 \sin \frac{x}{2} \sin(n+\frac{1}{2})x \\
    =& \frac{1}{N+1}
      \sum_{n=0}^{N} (\cos(nx) - \cos(n+1)x) \\
    =& \frac{1 - \cos(N+1)x}{N+1},
  \end{align*}
\[
  K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}
\]
if $x \neq 2k\pi$ for $k \in \mathbb{Z}$.
$\Box$ \\

\emph{Proof of (a).}
It is clear since $\cos x \leq 1$ for all $x \in \mathbb{R}$.
Or we may write
\[
  K_N(x) = \frac{1}{N+1} \left( \frac{\sin \frac{(N+1)x}{2}}{\sin \frac{x}{2}} \right)^2 \geq 0.
\]
$\Box$ \\

\emph{Proof of (b).}
By the definition of $D_n(x)$,
  \begin{align*}
     \frac{1}{2\pi} \int_{-\pi}^{\pi} K_N(x) dx
     =& \frac{1}{2\pi} \int_{-\pi}^{\pi} \frac{1}{N+1} \sum_{n=0}^{N} D_n(x) dx \\
     =& \frac{1}{N+1} \sum_{n=0}^{N} \frac{1}{2\pi} \int_{-\pi}^{\pi} D_n(x) dx \\
     =& \frac{1}{N+1} \sum_{n=0}^{N} 1 \\
     =& 1.
  \end{align*}
$\Box$ \\

\emph{Proof of (c).}
Since $\cos x$ is bounded by $1$ and monotonically decreasing on $(0, \pi]$,
  \begin{align*}
    K_N(x)
    =& \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x} \\
    \leq& \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta}.
  \end{align*}
$\Box$ \\

\emph{Proof of $s_N(f;x) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)K_N(t)dt$.}
  \begin{align*}
    \sigma_N(f;x)
    &= \frac{1}{N+1} \sum_{n=0}^{N} s_N(f;x) \\
    &= \frac{1}{N+1} \sum_{n=0}^{N} \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)D_N(t)dt \\
    &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)
      \left( \frac{1}{N+1} \sum_{n=0}^{N} D_N(t) \right) dt \\
    &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t) K_N(t) dt.
  \end{align*}
$\Box$ \\

\emph{Proof of Fej\'er's theorem.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
  \begin{align*}
    \abs{ \sigma_N(f;x) - f(x) }
    =& \abs{ \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t) K_N(t) dt
      - \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) K_N(t) dt } \\
    =& \abs{ \frac{1}{2\pi} \int_{-\pi}^{\pi} (f(x-t)-f(x)) K_N(t) dt } \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{\pi} |f(x-t)-f(x)| K_N(t) dt.
  \end{align*}

\item[(2)]
Since $f$ is continuous on a compact set $[-\pi,\pi]$, $f$ is continuous uniformly.
For such $\varepsilon > 0$, there exists $\delta > 0$
such that
\[
  |f(y) - f(x)| < \frac{\varepsilon}{2}
\]
whenever $x,y \in [-\pi,\pi]$ and $|y-x| < \delta$.

\item[(3)]
Since $f$ is continuous on a compact set $[-\pi,\pi]$, $f$ is bounded
on $[-\pi,\pi]$, say $M = \sup|f(x)|$.

\item[(4)]
Therefore,
  \begin{align*}
    &\abs{ \sigma_N(f;x) - f(x) } \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{\pi} |f(x-t)-f(x)| K_N(t) dt \\
    =& \frac{1}{2\pi} \int_{-\pi}^{-\delta} |f(x-t)-f(x)| K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{-\delta}^{\delta} |f(x-t)-f(x)| K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi} |f(x-t)-f(x)| K_N(t) dt \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{-\delta}
        2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
      &+ \frac{1}{2\pi} \int_{-\delta}^{\delta} \frac{\varepsilon}{2} K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi}
        2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
    =& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi}
      + \frac{\varepsilon}{2} \cdot \frac{1}{2\pi} \int_{-\delta}^{\delta} K_N(t) dt \\
    \leq& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi} + \frac{\varepsilon}{2}.
  \end{align*}

\item[(5)]
Since $N$ is arbitrary,
we can take an integer
$N > \frac{4M(\pi-\delta)}{(1-\cos\delta)\pi\varepsilon} - 1$
so that
  \begin{align*}
    \abs{ \sigma_N(f;x) - f(x) }
    \leq& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi} + \frac{\varepsilon}{2} \\
    <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    =& \varepsilon.
  \end{align*}
Therefore, the conclusion holds.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.16.}
\addcontentsline{toc}{subsection}{Exercise 8.16.}
\emph{Prove a pointwise version of Fej\'er's theorem:
If $f \in \mathscr{R}$ and $f(x+)$, $f(x-)$ exist for some $x$, then
\[
  \lim_{N \to \infty} \sigma_N(f;x) = \frac{1}{2}[f(x+)+f(x-)].
\]
} \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Since $K_N(-t) = K_N(t)$, we have
\[
  \sigma_N(f;x)
  = \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt
    + \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_N(t)dt
\]
and
\[
  \frac{1}{2\pi} \int_{0}^{\pi} K_N(t)dt = \frac{1}{2}.
\]

\item[(2)]
Since $f \in \mathscr{R}$,
$f$ is bounded on $[-\pi,\pi]$, say $M = \sup|f(x)|$.

\item[(3)]
Therefore,
  \begin{align*}
    &\abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt - \frac{1}{2} f(x-) } \\
    =& \abs{ \frac{1}{2\pi} \int_{0}^{\pi} (f(x-t)-f(x-))K_N(t)dt } \\
    \leq& \frac{1}{2\pi} \int_{0}^{\pi} |f(x-t)-f(x-)|K_N(t)dt.
  \end{align*}
Since $f(x-)$ exists,
for fixed $\varepsilon > 0$, there exists $\delta > 0$
such that
\[
  |f(y) - f(x-)| < \frac{\varepsilon}{2}
\]
whenever $y \in (x-\delta,x) \cap [-\pi,\pi]$.
Hence,
  \begin{align*}
    &\abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt - \frac{1}{2} f(x-) } \\
    \leq& \frac{1}{2\pi} \int_{0}^{\pi} |f(x-t)-f(x-)|K_N(t)dt \\
    =& \frac{1}{2\pi} \int_{0}^{\delta} |f(x-t)-f(x-)|K_N(t)dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi} |f(x-t)-f(x-)|K_N(t)dt \\
    \leq& \frac{1}{2\pi} \int_{0}^{\delta} \frac{\varepsilon}{2} K_N(t)dt
      + \frac{1}{2\pi} \int_{\delta}^{\pi} 2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
    =& \frac{\varepsilon}{2} \cdot \frac{1}{2\pi} \int_{0}^{\delta} K_N(t)dt
      + \frac{2M(\pi-\delta)}{(N+1)(1-\cos \delta)\pi} \\
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(N+1)(1-\cos \delta)\pi}. \\
  \end{align*}

\item[(4)]
Since $N$ is arbitrary,
we can take an integer
$N_1 > \frac{8M(\pi-\delta)}{(1-\cos\delta)\pi\varepsilon} - 1$
such that
  \begin{align*}
    \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_{n}(t)dt - \frac{1}{2} f(x-) }
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(n+1)(1-\cos \delta)\pi} \\
    <& \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
    =& \frac{\varepsilon}{2}
  \end{align*}
whenever $n \geq N_1$.
Similarly,
we can take an integer $N_2$ such that
  \begin{align*}
    \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_{n}(t)dt - \frac{1}{2} f(x+) }
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(n+1)(1-\cos \delta)\pi} \\
    <& \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
    =& \frac{\varepsilon}{2}.
  \end{align*}
whenever $n \geq N_2$.

\item[(5)]
Hence,
  \begin{align*}
    &\abs{ \sigma_n(f;x) - \frac{1}{2}[f(x+)+f(x-)] } \\
    \leq& \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_{n}(t)dt - \frac{1}{2} f(x-) } \\
      &+ \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_{n}(t)dt - \frac{1}{2} f(x+) } \\
    <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    =& \varepsilon.
  \end{align*}
whenever $n \geq \max\{N_1,N_2\}$.
Hence, $\lim \sigma_n(f;x) = \frac{1}{2}[f(x+)+f(x-)]$.
\end{enumerate}
$\Box$ \\

\textbf{Supplement.} Poisson's equation.
(Theorem 1 of Section 2.2 in the textbook:
\emph{Lawrence C. Evans, Partial Differential Equations.})
\emph{Let the fundamental solution of Laplace's equation be
\begin{equation*}
  \Phi(x) =
    \begin{cases}
      -\frac{1}{2\pi} \log|x|                       & (n = 2) \\
      \frac{1}{n(n-2)\alpha(n)} \frac{1}{|x|^{n-2}} & (n \geq 3),
    \end{cases}
\end{equation*}
where $x \in \mathbb{R}^n$, $x \neq 0$.
Let
\[
  u(x) = \int_{\mathbb{R}^n} \Phi(x-y)f(y) dy.
\]
Then $-\Delta u = f$ in $\mathbb{R}^n$.}
Note that $\Phi(x)$ blows up at $0$.
To calculate $\Delta u(x)$,
we need to isolate this singularity inside a small ball, say $B(0;\varepsilon)$.
Therefore,
\[
  \Delta u(x)
  = \int_{B(0;\varepsilon)} \Phi(y) \Delta_x f(x-y)dy
    + \int_{\mathbb{R}^n - B(0;\varepsilon)} \Phi(y) \Delta_x f(x-y)dy,
\]
and we can continue estimating two integrals individually as the textbook did. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.17.}
\addcontentsline{toc}{subsection}{Exercise 8.17.}
\emph{Assume $f$ is bounded and monotonic on $[-\pi,\pi)$
with Fourier coefficients $c_n$,
as given by
\[
  c_n = \frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)e^{-inx} dx.
\]}
\begin{enumerate}
\item[(a)]
\emph{Use Exercise 6.17 to prove that $\{nc_n\}$ is a bounded sequence.}
\item[(b)]
\emph{Combine (a) with Exercise 8.16 and with Exercise 3.14(e),
to conclude that
\[
  \lim_{N \to \infty} s_N(f;x) = \frac{1}{2}[f(x+)+f(x-)]
\]
for every $x$.}
\item[(c)]
\emph{Assume only that $f \in \mathscr{R}$ on $[-\pi,\pi]$ and
that $f$ monotonic in some segment $(\alpha,\beta) \subseteq [-\pi,\pi]$.
Prove that the conclusion of (b) holds for every $x \in (\alpha,\beta)$.
(This is an application of the localization theorem.)} \\
\end{enumerate}

\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since $f$ is bounded and monotonic on $[-\pi,\pi)$,
  we might extend $f$ to $[-\pi,\pi]$ by letting $f(\pi) = f(\pi-) < 0$ additionally.
  Note that $f \in \mathscr{R}$ on $[-\pi,\pi]$ (Theorem 6.9)
  and $e^{-inx} \in \mathscr{R}$ on $[-\pi,\pi]$.
  Thus $c_n$ is well-defined (Theorem 6.13).

\item[(2)]
  Let $\alpha(x) = f(x)$, $g(x) = e^{-inx}$ and $G(x) = \frac{1}{-in}e^{-inx}$.
  Since $G'(x) = g(x)$, by Exercise 6.17 we have
  \begin{align*}
    nc_n
    &= \frac{n}{2\pi} \int_{-\pi}^{\pi} f(x)e^{-inx} dx \\
    &= \frac{n}{2\pi} \left[ \frac{1}{-in}e^{-inx}f(x) \right]_{x=-\pi}^{x=\pi}
      - \frac{n}{2\pi} \int_{-\pi}^{\pi} \frac{1}{-in}e^{-inx} dx \\
    &= -\frac{1}{2\pi i} \left[ e^{-in\pi}f(\pi) - e^{in\pi}f(-\pi) \right].
  \end{align*}
  So
  \begin{align*}
    |nc_n|
    &\leq \frac{1}{2\pi} (|f(\pi)| + |f(-\pi)|)
  \end{align*}
  is bounded for every integer $n$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Given any $x \in [-\pi,\pi)$.
  Since $f$ is bounded and monotonic on $[-\pi,\pi)$,
  $f(x+)$ and $f(x-)$ exist.
  By Exercise 8.16,
  \[
    \lim_{N \to \infty} \sigma_N(f;x) = \frac{1}{2}[f(x+)+f(x-)].
  \]

\item[(2)]
  By (a) and Exercise 3.14(e),
  \[
    \lim_{N \to \infty} s_N(f;x)
    = \lim_{N \to \infty} \sigma_N(f;x)
    = \frac{1}{2}[f(x+)+f(x-)].
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  It is nature to define a monotonic function $g(x)$ on $[-\pi,\pi]$
  by
  \begin{equation*}
    g(x) =
      \begin{cases}
        f(\alpha) & (x \leq \alpha), \\
        f(x)      & (\alpha < x < \beta), \\
        f(\beta)  & (x \geq \beta).
      \end{cases}
  \end{equation*}

  $g(x)$ is well-defined.
  ($f \in \mathscr{R}$
  $\Longrightarrow$ $f$: bounded
  $\Longrightarrow$ $f(\alpha), f(\beta) < \infty$.)

\item[(2)]
  Since $f$ is monotonic on $(\alpha,\beta)$,
  $g(x)$ is bounded and monotonic.
  Thus by (b),
  \[
    \lim_{N \to \infty} s_N(g;x) = \frac{1}{2}[g(x+)+g(x-)].
  \]
  for every $x \in [-\pi,\pi]$.

\item[(3)]
  $g(x) = f(x)$,
  $g(x-) = f(x-)$,
  and $g(x+) = f(x+)$ for $x \in (\alpha,\beta)$ by definition.
  Hence by (2) we have,
  \[
    \lim_{N \to \infty} s_N(f;x) = \frac{1}{2}[f(x+)+f(x-)].
  \]
  for every $x \in (\alpha,\beta)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.18.}
\addcontentsline{toc}{subsection}{Exercise 8.18.}
\emph{Define
\begin{align*}
  f(x) &= x^3 - \sin^2 x \tan x \\
  g(x) &= 2x^2 - \sin^2 x - x \tan x.
\end{align*}
Find out, for each of these two functions, whether it is positive or negative
for all $x \in (0, \frac{\pi}{2})$,
or whether it changes sign.
Prove your answer.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
\[
  (\tan^n x)' = n \tan^{n-1} x + n \tan^{n+1} x
\]
if $n$ is a positive integer.}
Induction on $n$.
The case $n=1$ is true since
\[
  (\tan x)' = \sec^2 x = 1 + \tan^2 x.
\]
Assume the induction hypothesis that the case $n=k$ holds.
It follows that
\begin{align*}
  (\tan^{k+1} x)'
  &= (\tan^{k} x \cdot \tan x)' \\
  &= (\tan^{k} x)' \cdot \tan x + \tan^{k} x \cdot (\tan x)' \\
  &= (k \tan^{k-1} x + k \tan^{k+1} x) \tan x
    + \tan^{k} x \cdot (1 + \tan^2 x) \\
  &= k \tan^{k} x + k \tan^{k+2} x + \tan^{k} x + \tan^{k+2} x \\
  &= (k+1)\tan^{k} x + (k+1)\tan^{k+2} x.
\end{align*}
Since both the base case and the inductive step have been proved as true,
by mathematical induction the result holds for every positive integer $n$.

\item[(2)]
\emph{Show that $f(x) < 0$ if $x \in (0, \frac{\pi}{2})$.}
  \begin{enumerate}
    \item[(a)]
      Write
      \begin{align*}
        f(x)
        &= x^3 + (\cos^2 x-1) \tan x \\
        &= x^3 + \cos^2 x \tan x - \tan x\\
        &= x^3 + \cos x \sin x - \tan x \\
        &= x^3 + \frac{1}{2} \sin(2x) - \tan x.
      \end{align*}
      Hence,
      \begin{align*}
        f^{(1)}(x) =& 3x^2 + \cos(2x) - (1 + \tan^2 x). \\
        f^{(2)}(x) =& 6x - 2\sin(2x) - (2 \tan x + 2\tan^3 x). \\
        f^{(3)}(x) =& 6 - 4\cos(2x) - (2 + 8 \tan^2 x + 6\tan^4 x) \\
        f^{(4)}(x) =& 8\sin(2x) - (16\tan x + 40 \tan^3 x + 24 \tan^5 x). \\
        f^{(5)}(x) =& 16\cos(2x) - (16 + 136\tan^2 x + 240 \tan^4 x + 120 \tan^6 x). \\
        f^{(6)}(x) =& -32\sin(2x) - (272\tan x + 1232\tan^3 x + 1680\tan^5 x - 720\tan^7 x).
      \end{align*}

    \item[(b)]
      Clearly, all $f^{(j)}(0) = 0$ $(j=0,1,2,3,4,5,6)$.

    \item[(c)]
      Since $f^{(6)}(x) < 0$ for $x \in (0, \frac{\pi}{2})$,
      $f^{(5)}(x)$ is strictly monotonically decreasing (denoted by $\searrow$).

    \item[(d)]
      Since $f^{(5)}(0) = 0$ and $f^{(5)} \searrow$,
      $f^{(5)}(x) < 0$ for $x \in (0, \frac{\pi}{2})$.
      Repeat this argument for $f^{(4)}$, $f^{(3)}$, $f^{(2)}$ and $f^{(1)}$
      to get $f(x) < 0$ for $x \in (0, \frac{\pi}{2})$.
  \end{enumerate}

\item[(3)]
\emph{Show that $g(x) < 0$ if $x \in (0, \frac{\pi}{2})$.}
  \begin{enumerate}
    \item[(a)]
      \begin{align*}
        g(x) =& 2x^2 - \sin^2 x - x \tan x. \\
        g^{(1)}(x) =& 3x - \sin(2x) - \tan x - x\tan^2 x. \\
        g^{(2)}(x) =& 2 - 2\cos(2x) - 2 \tan^2 x - x(2 \tan x + 2\tan^3 x). \\
        g^{(3)}(x) =& 4\sin(2x) - (6\tan x + 6\tan^3 x) \\
          &- x(2 + 8 \tan^2 x + 6\tan^4 x). \\
        g^{(4)}(x) =& 8\cos(2x) - (8 + 32 \tan^2 x + 24\tan^4 x) \\
          &- x(16\tan x + 40 \tan^3 x + 24 \tan^5 x). \\
        g^{(5)}(x) =& -16\sin(2x) - (80 \tan x + 200\tan^3 x + 120\tan^5 x) \\
          &- x(16 + 136\tan^2 x + 240 \tan^4 x + 120 \tan^6 x).
      \end{align*}

    \item[(b)]
      Clearly, all $g^{(j)}(0) = 0$ $(j=0,1,2,3,4,5)$.

    \item[(c)]
      Since $g^{(5)}(x) < 0$ for $x \in (0, \frac{\pi}{2})$,
      $g^{(4)}(x)$ is strictly monotonically decreasing (denoted by $\searrow$).

    \item[(d)]
      Since $g^{(4)}(0) = 0$ and $g^{(4)} \searrow$,
      $g^{(4)}(x) < 0$ for $x \in (0, \frac{\pi}{2})$.
      Repeat this argument for $g^{(3)}$, $g^{(2)}$ and $g^{(1)}$
      to get $g(x) < 0$ for $x \in (0, \frac{\pi}{2})$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.19.}
\addcontentsline{toc}{subsection}{Exercise 8.19.}
\emph{Suppose $f$ is a continuous function on $\mathbb{R}$,
$f(x+2\pi) = f(x)$, and $\frac{\alpha}{\pi}$ is irrational.
Prove that
\[
  \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
  = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt
\]
for every $x$.
(Hint: Do it first for $f(x) = \exp(ikx)$.)}\\

\emph{Proof (Hint).}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Do it first for $f(x) = \exp(ikx)$.
Note that
\begin{equation*}
  \frac{1}{2\pi} \int_{-\pi}^{\pi}\exp(ikx)dt =
    \begin{cases}
      1 & (k = 0), \\
      0 & (k \neq 0).
    \end{cases}
\end{equation*}
  \begin{enumerate}
  \item[(a)]
    $k = 0$ is nothing to do.
  \item[(b)]
    Suppose $k \neq 0$.
    \begin{align*}
      \frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
      &= \frac{1}{N} \sum_{n=1}^{N} \exp(ik(x+n\alpha)) \\
      &= \frac{1}{N} \sum_{n=1}^{N} \exp(ikx) \exp(ik\alpha n) \\
      &= \frac{1}{N} \exp(ikx) \cdot
        \frac{\exp(ik\alpha) - \exp(ik\alpha(N+1))}{1 - \exp(ik\alpha)} \\
      &= \exp(ik(x+\alpha)) \left[
        \frac{1}{N} \cdot \frac{1 - \exp(ik\alpha N)}{1 - \exp(ik\alpha)}
        \right] \\
      &= f(x+\alpha) \frac{1}{N} \frac{1 - \exp(ik\alpha N)}{1 - \exp(ik\alpha)} \to 0
    \end{align*}
    as $N \to \infty$ since $\exp(iy)$ is bounded ($y \in \mathbb{R}$).
    (Note that the denominator
    $1-\exp(ik\alpha) \neq 0$ since $k \neq 0$ and $\frac{\alpha}{\pi}$ is irrational.)
  \end{enumerate}
  By (a)(b),
  \[
    \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
    = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt.
  \]
  for $f(x) = \exp(ikx)$ and any $x \in \mathbb{R}$.

\item[(2)]
Therefore,
\[
  \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
  = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt
\]
is also true for trigonometric polynomials $f(x)$.

\item[(3)]
By Theorem 8.15, there is a trigonometric polynomial
\[
  P(x) = \sum_{n=-N_1}^{N_1} c_n \exp(inx)
\]
such that
\[
  |P(x) - f(x)| < \frac{\varepsilon}{89}.
\]
By (2), there is an integer $N_2$ such that
\[
  \abs{\frac{1}{N} \sum_{n=1}^{N} P(x+n\alpha)
  - \frac{1}{2\pi} \int_{-\pi}^{\pi} P(t) dt} < \frac{\varepsilon}{64}
\]
whenever $N \geq N_2$.
Therefore,
  \begin{align*}
    &\abs{\frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
      - \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt} \\
    \leq&
    \abs{\frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha) - \frac{1}{N} \sum_{n=1}^{N} P(x+n\alpha)} \\
      &+ \abs{\frac{1}{N} \sum_{n=1}^{N} P(x+n\alpha) - \frac{1}{2\pi} \int_{-\pi}^{\pi} P(t) dt} \\
      &+ \abs{\frac{1}{2\pi} \int_{-\pi}^{\pi} P(t) dt - \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt} \\
    \leq&
    \frac{1}{N} \sum_{n=1}^{N} \abs{ f(x+n\alpha) - P(x+n\alpha) } \\
      &+ \abs{\frac{1}{N} \sum_{n=1}^{N} P(x+n\alpha) - \frac{1}{2\pi} \int_{-\pi}^{\pi} P(t) dt} \\
      &+ \frac{1}{2\pi} \int_{-\pi}^{\pi} \abs{P(t) - f(t)} dt \\
    <&
    \frac{1}{N} \sum_{n=1}^{N} \frac{\varepsilon}{89}
      + \frac{\varepsilon}{64}
      + \frac{1}{2\pi} \int_{-\pi}^{\pi} \frac{\varepsilon}{89} dt \\
    =&
    \frac{\varepsilon}{89} + \frac{\varepsilon}{64} + \frac{\varepsilon}{89} \\
    <& \varepsilon
  \end{align*}
whenever $N \geq N_2$.
Hence
\[
  \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} f(x+n\alpha)
  = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) dt
\]
is also true for continuous function $f(x)$ (with period $2\pi$).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.20.}
\addcontentsline{toc}{subsection}{Exercise 8.20.}
\emph{The following simple computation yields a good approximation to
Stirling's formula.
For $m=1,2,3,\ldots$, define
\[
  f(x) = (m+1-x) \log m + (x-m) \log(m+1)
\]
if $m \leq x \leq m+1$, and define
\[
  g(x) = \frac{x}{m} - 1 + \log m
\]
if $m - \frac{1}{2} \leq x < m + \frac{1}{2}$.
Draw the graphs of $f$ and $g$.
Note that $f(x) \leq \log x \leq g(x)$ if $x \geq 1$ and that
\[
  \int_{1}^{n} f(x)dx = \log(n!) - \frac{1}{2} \log n > -\frac{1}{8} + \int_{1}^{n} g(x)dx.
\]
Integrate $\log x$ over $[1,n]$.
Conclude that
\[
  \frac{7}{8} < \log(n!) - \left(n+\frac{1}{2}\right) \log n + n < 1
\]
for $n=2,3,4,\ldots$.
(Note: $\log \sqrt{2\pi} \approx 0.918\ldots$.)
Thus
\[
  e^{\frac{7}{8}} < \frac{n!}{(\frac{n}{e})^n \sqrt{n}} < e.
\]} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Omit the  graphs of $f$ and $g$.
Note that the concavity of $\log(x)$ implies that $f(x) \leq \log(x)$.
Here the equality holds if and only if $x \in \mathbb{Z}^+$.
Besides, since
$g(x)$ is the tangent line at $(x, \log x)$ whenever $x \in \mathbb{Z}^+$,
$g(x) \geq \log(x)$ and
the equality holds if and only if $x \in \mathbb{Z}^+$.

\item[(2)]
\begin{align*}
  \int_{1}^{n} f(x)dx
  =& \sum_{m=1}^{n-1} \int_{m}^{m+1} f(x)dx \\
  =& \sum_{m=1}^{n-1} \int_{m}^{m+1} (m+1-x) \log m + (x-m) \log(m+1) dx \\
  =& \sum_{m=1}^{n-1} \int_{m}^{m+1}
    (\log(m+1)-\log m)x + (m+1)\log m-m\log(m+1) dx \\
  =& \sum_{m=1}^{n-1} (\log(m+1)-\log m)\left( \frac{(m+1)^2-m^2}{2} \right)
    + (m+1)\log m - m\log(m+1) \\
  =& \sum_{m=1}^{n-1} \log m + \frac{1}{2} \sum_{m=1}^{n-1}(\log(m+1)-\log m) \\
  =& \log((n-1)!) + \frac{1}{2} \log n \\
  =& \log(n!) - \frac{1}{2} \log n.
\end{align*}

\item[(3)]
Write
\[
  \int_{1}^{n} g(x)dx
  = \left( \sum_{m=1}^{n} \int_{m-\frac{1}{2}}^{m+\frac{1}{2}} g(x)dx \right)
    - \int_{\frac{1}{2}}^{1} g(x)dx
    - \int_{n}^{n+\frac{1}{2}} g(x)dx.
\]
  \begin{enumerate}
  \item[(a)]
    \begin{align*}
      \sum_{m=1}^{n} \int_{m-\frac{1}{2}}^{m+\frac{1}{2}} g(x) dx
      &=
      \sum_{m=1}^{n} \int_{m-\frac{1}{2}}^{m+\frac{1}{2}}
        \left( \frac{x}{m} - 1 + \log m \right) dx \\
      &=
      \sum_{m=1}^{n} \log m \\
      &= \log(n!).
    \end{align*}
  \item[(b)]
    \[
      \int_{\frac{1}{2}}^{1} g(x)dx
      = \int_{\frac{1}{2}}^{1} (x - 1 + \log 1) dx
      = -\frac{1}{8}.
    \]
  \item[(c)]
    \[
      \int_{n}^{n+\frac{1}{2}} g(x)dx
      = \int_{\frac{1}{2}}^{1} \left( \frac{x}{n} - 1 + \log n \right) dx
      = \frac{1}{2} \log n - \frac{1}{8n}.
    \]
  \end{enumerate}
By (a)(b)(c),
\[
  \int_{1}^{n} g(x)dx
  = \log(n!) - \frac{1}{2} \log n + \frac{1}{8}(1 - \frac{1}{n})
  < \log(n!) - \frac{1}{2} \log n + \frac{1}{8}.
\]

\item[(4)]
Since $f(x) \leq \log x \leq g(x)$
and the equality holds if and only if $x \in \mathbb{Z}^+$ (by (1)),
\[
  \int_{1}^{n} f(x)dx \leq \int_{1}^{n} \log x dx \leq \int_{1}^{n} g(x)dx
\]
for all $n = 1,2,3,\ldots$. The equality holds if and only if $n=1$.
Hence by (2)(3)
\[
  \log(n!) - \frac{1}{2} \log n
  \leq n \log n - n + 1
  \leq \log(n!) - \frac{1}{2} \log n + \frac{1}{8}.
\]
Arrange the inequality to get
\[
  \frac{7}{8} < \log(n!) - \left(n+\frac{1}{2}\right) \log n + n \leq 1
\]
for $n = 1,2,3,\ldots$.
Note that the equality holds if and only if $n = 1$.
Therefore
\[
  \frac{7}{8} < \log(n!) - \left(n+\frac{1}{2}\right) \log n + n < 1
\]
for $n = 2,3,\ldots$.

\item[(5)]
Exponentiate to get
\[
  \exp(\frac{7}{8}) < \exp[\log(n!) - \left(n+\frac{1}{2}\right) \log n + n] < \exp(1),
\]
or
\[
  e^\frac{7}{8} < \frac{\exp(\log(n!))\exp(n)}{\exp[\left(n+\frac{1}{2}\right) \log n]} < e,
\]
or $e^{\frac{7}{8}} < \frac{n!}{(\frac{n}{e})^n \sqrt{n}} < e$
(since $\exp(x)$ is a strictly increasing function of $x$).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% https://warwick.ac.uk/fac/sci/masdoc/people/masdoc_alumni/davidmccormick/ara-v0.1.pdf



\subsection*{Exercise 8.21 (Norm of Dirichlet kernel).}
\addcontentsline{toc}{subsection}{Exercise 8.21 (Norm of Dirichlet kernel).}
\emph{Let
\[
  L_n = \frac{1}{2\pi} \int_{-\pi}^{\pi} \abs{ D_n(t) } dt
  \qquad (n = 1,2,3,\ldots).
\]
Prove that there exists a constant $C > 0$ such that
\[
  L_n > C \log n
  \qquad (n = 1,2,3,\ldots),
\]
or, more precisely, that the sequence
\[
  \left\{ L_n - \frac{4}{\pi^2} \log n \right\}
\]
is bounded.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Write
\begin{align*}
  L_n
  &= \frac{1}{2\pi} \int_{-\pi}^{\pi} \abs{ D_n(t) } dt \\
  &= \frac{1}{\pi} \int_{0}^{\pi} \abs{ D_n(t) } dt
    &(D_n(-t) = D_n(t)) \\
  &= \frac{1}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin(n+\frac{1}{2})t }}{\sin(\frac{t}{2})} dt.
    &\text{($\sin(\frac{t}{2}) \geq 0$ on $[0,\pi]$)}
\end{align*}

\item[(2)]
So,
\begin{align*}
  L_n
  &= \frac{1}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin(n+\frac{1}{2})t }}{\sin(\frac{t}{2})} dt \\
  &= \frac{1}{\pi} \int_{0}^{\pi} \abs{ \sin(n+\frac{1}{2})t }
    \left( \frac{1}{\sin(\frac{t}{2})} - \frac{1}{\frac{t}{2}} + \frac{1}{\frac{t}{2}} \right) dt \\
  &= \underbrace{
      \frac{1}{\pi} \int_{0}^{\pi} \abs{ \sin(n+\frac{1}{2})t }
      \left( \frac{1}{\sin(\frac{t}{2})} - \frac{1}{\frac{t}{2}} \right) dt }_{:= I_n}
    + \underbrace{
      \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin(n+\frac{1}{2})t }}{t} dt }_{:= J_n}.
\end{align*}

\item[(3)]
\emph{Show that $I_n$ is uniformly bounded.}
Note that $f(x) = \frac{1}{\sin(x)} - \frac{1}{x}$ is bounded
(since $\lim_{x \to 0} f(x) = 0$ by using L'Hospital's rule twice).
Also, $\abs{ \sin(n+\frac{1}{2})t } \leq 1$ for any $n$.
Hence
\[
  0 \leq I_n < \sup(f(x)) = \frac{2}{\pi}.
\]

\item[(4)]
\emph{Show that $J_n - \frac{4}{\pi^2}\log n$ is uniformly bounded.}
Since
\begin{align*}
  J_n
  &= \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin(n+\frac{1}{2})t }}{t} dt \\
  &= \frac{2}{\pi} \int_{0}^{\left( n+\frac{1}{2} \right)\pi}
      \frac{\abs{ \sin x }}{x} dx,
    &\text{(Let $x = \left( n+\frac{1}{2} \right) t$)}
\end{align*}
we have
\[
  \underbrace{ \frac{2}{\pi}
      \sum_{k=0}^{n-1} \int_{k\pi}^{(k+1)\pi}
      \frac{\abs{ \sin x }}{x} dx }_{:= J^{(1)}_n}
  \leq J_n
  \leq \underbrace{ \frac{2}{\pi}
      \sum_{k=0}^{n} \int_{k\pi}^{(k+1)\pi}
      \frac{\abs{ \sin x }}{x} dx }_{:= J^{(2)}_n}.
\]
So
\begin{align*}
  J^{(1)}_n
  &\geq
  \frac{2}{\pi} \sum_{k=0}^{n-1} \int_{k\pi}^{(k+1)\pi} \frac{\abs{ \sin x }}{(k+1)\pi} dx \\
  &=
  \frac{2}{\pi} \sum_{k=0}^{n-1} \frac{2}{(k+1)\pi}
    &(\int_{0}^{\pi}\abs{ \sin x } dx = 0) \\
  &\geq \frac{4}{\pi^2} \log n,
    &\text{(Exercise 8.9)}
\end{align*}
and
\begin{align*}
  J^{(2)}_n
  &=
  \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx
    + \frac{2}{\pi} \sum_{k=1}^{n} \int_{k\pi}^{(k+1)\pi} \frac{\abs{ \sin x }}{x} dx \\
  &\leq
  \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx
    + \frac{2}{\pi} \sum_{k=1}^{n} \int_{k\pi}^{(k+1)\pi} \frac{\abs{ \sin x }}{k\pi} dx \\
  &=
  \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx
    + \frac{2}{\pi} \sum_{k=1}^{n} \frac{2}{k\pi} \\
  &\leq
  \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx
    + \frac{4}{\pi^2} (\log n + 1) \\
  &=
  \frac{4}{\pi^2}\log n
    + \frac{4}{\pi^2}
    + \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx.
\end{align*}
Hence,
\[
  0
  \leq
  J_n - \frac{4}{\pi^2}\log n
  \leq
  \frac{4}{\pi^2} + \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx.
\]

\item[(5)]
By (3)(4),
\[
  0
  \leq
  L_n - \frac{4}{\pi^2}\log n
  \leq
  \frac{2}{\pi} + \frac{4}{\pi^2} + \frac{2}{\pi} \int_{0}^{\pi} \frac{\abs{ \sin x }}{x} dx.
\]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.22 (Newton's generalized binomial theorem).}
\addcontentsline{toc}{subsection}{Exercise 8.22 (Newton's generalized binomial theorem).}
\emph{If $\alpha$ is a real and $-1<x<1$, prove Newton's binomial theorem
\[
  (1+x)^{\alpha}
  = 1 + \sum_{n=1}^{\infty} \frac{\alpha(\alpha-1)\cdots(\alpha-n+1)}{n!} x^n.
\]
(Hint: Denote the right side by $f(x)$.
Prove that the series converges.
Prove that
\[
  (1+x)f'(x) = \alpha f(x)
\]
and solve this differential equation.)
Show also that
\[
  (1-x)^{-\alpha} = \sum_{n=0}^{\infty} \frac{\Gamma(n+\alpha)}{n!\Gamma(\alpha)} x^n
\]
if $-1<x<1$ and $\alpha > 0$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let
\[
  f(x) = \sum_{n=0}^{\infty} {\alpha \choose n} x^n
\]
where ${\alpha \choose n}$ is defined by
\[
  {\alpha \choose n} = \frac{\alpha(\alpha-1)\cdots(\alpha-n+1)}{n!}.
\]

\item[(2)]
\emph{Show that ${\alpha-1 \choose n} + {\alpha-1 \choose n-1} = {\alpha \choose n}$.}
\begin{align*}
  {\alpha-1 \choose n} + {\alpha-1 \choose n-1}
  &= \frac{(\alpha-1)\cdots(\alpha-n+1)(\alpha-n)}{n!}
    + \frac{(\alpha-1)\cdots(\alpha-n+1)}{(n-1)!} \\
  &= \frac{(\alpha-1)\cdots(\alpha-n+1)}{n!}[(\alpha-n) + n] \\
  &= \frac{\alpha(\alpha-1)\cdots(\alpha-n+1)}{n!} \\
  &= {\alpha \choose n}.
\end{align*}

\item[(3)]
\emph{Show that $f(x)$ converges.}
Write $c_n = {\alpha \choose n}$.
Since
\[
  \lim_{n \to \infty} \abs{ \frac{c_{n+1}}{c_{n}} }
  = \lim_{n \to \infty} \abs{ \frac{\alpha-n}{n+1} } = 1,
\]
we have
\[
  \lim_{n \to \infty} \sqrt[n]{|c_n|}
  = \lim_{n \to \infty} \abs{ \frac{c_{n+1}}{c_{n}} }
  = 1
\]
(Theorem 3.37) and thus the radius of convergence is $1$.
$f(x)$ converges if $|x| < 1$.

\item[(4)]
\emph{Show that $(1+x)f'(x) = \alpha f(x)$.}
By Theorem 8.1,
\begin{align*}
  f'(x)
  &= \sum_{n=0}^{\infty} {\alpha \choose n} n x^{n-1} \\
  &= \sum_{n=1}^{\infty} {\alpha \choose n} n x^{n-1} \\
  &= \sum_{n=1}^{\infty} \alpha {\alpha-1 \choose n-1} x^{n-1} \\
  &= \sum_{n=0}^{\infty} \alpha {\alpha-1 \choose n} x^{n}.
\end{align*}
Besides,
\[
  x f'(x)
  = \sum_{n=0}^{\infty} {\alpha \choose n} n x^{n}
  = \sum_{n=0}^{\infty} \alpha {\alpha-1 \choose n-1} x^{n}.
\]
Hence,
\begin{align*}
  (1+x)f'(x)
  &= \sum_{n=0}^{\infty} \alpha {\alpha-1 \choose n} x^{n}
    + \sum_{n=0}^{\infty} \alpha {\alpha-1 \choose n-1} x^{n} \\
  &= \alpha \sum_{n=0}^{\infty} \left[ {\alpha-1 \choose n}+{\alpha-1 \choose n-1} \right] x^{n} \\
  &= \alpha \sum_{n=0}^{\infty} {\alpha \choose n} x^{n}
    &((2)) \\
  &= \alpha f(x).
\end{align*}

\item[(5)]
\emph{Solve the differential equation $(1+x)f'(x) = \alpha f(x)$.}
Given any $1 > \varepsilon > 0$.
Use the notations in Exercise 5.27.
Let
\[
  \phi(x,y) = \frac{\alpha y}{1+x}
\]
defined on $[-1+\varepsilon,1-\varepsilon] \times \mathbb{R}$.
Let
\[
  g(x) = (1+x)^\alpha
\]
defined on $[-1+\varepsilon,1-\varepsilon]$.
Thus,
\[
  g'(x)
  = \alpha (1+x)^{\alpha-1}
  = \frac{\alpha (1+x)^\alpha}{1+x}
  = \frac{\alpha g(x)}{1+x}
  = \phi(x,g(x))
\]
and $g(0) = 1$.
(Clearly, $f'(x) = \phi(x,f(x))$ and $f(0)=1$.)
\emph{To show $f(x) = g(x)$, it suffices to show that
there is a constant $A$ such that
\[
  \abs{\phi(x,g(x)) - \phi(x,f(x))} \leq A \abs{g(x) - f(x)}
\]
whenever $(x,f(x)) \in \mathbb{R}$ and $(x,g(x)) \in \mathbb{R}$.}
In fact,
\begin{align*}
  \abs{\phi(x,g(x)) - \phi(x,f(x))}
  &= \abs{\frac{\alpha g(x)}{1+x} - \frac{\alpha f(x)}{1+x}} \\
  &= \frac{\alpha}{1+x} \abs{g(x)-f(x)} \\
  &\leq \frac{\alpha}{\varepsilon} \abs{g(x)-f(x)}.
\end{align*}
(Here $A = \frac{\alpha}{\varepsilon}$ is a constant.)
By Exercise 5.27,
$f(x) = g(x)$ on $[-1+\varepsilon,1-\varepsilon]$ for any $1 > \varepsilon > 0$.
So
$f(x) = g(x)$ on $(-1,1)$,
or
\[
  \sum_{n=0}^{\infty} {\alpha \choose n} x^n = (1+x)^\alpha
\]
if $x \in (-1,1)$.

\item[(6)]
\emph{Show that
\[
  (1-x)^{-\alpha} = \sum_{n=0}^{\infty} \frac{\Gamma(n+\alpha)}{n!\Gamma(\alpha)} x^n
\]
if $-1<x<1$ and $\alpha > 0$.}
In fact,
\begin{align*}
  (1-x)^{-\alpha}
  &= \sum_{n=0}^{\infty} {-\alpha \choose n} (-x)^n \\
  &= \sum_{n=0}^{\infty} \frac{(-\alpha)(-\alpha-1)\cdots(-\alpha-n+1)}{n!} (-1)^n x^n \\
  &= \sum_{n=0}^{\infty} \frac{(\alpha)(\alpha+1)\cdots(\alpha+n-1)}{n!} x^n \\
  &= \sum_{n=0}^{\infty} \frac{\Gamma(n+\alpha)}{n!\Gamma(\alpha)} x^n.
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.23.}
\addcontentsline{toc}{subsection}{Exercise 8.23.}
\emph{Let $\gamma$ be a continuously differentiable \textbf{closed} curve in the complex plane,
with parameter interval $[a,b]$,
and assume that $\gamma(t) \neq 0$ for every $t \in [a,b]$.
Define the \textbf{index} of $\gamma$ to be
\[
  \mathrm{Ind}(\gamma)
  = \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma'(t)}{\gamma(t)} dt.
\]
Prove that $\mathrm{Ind}(\gamma)$ is always an integer.
(Hint: There exists $\varphi$ on $[a,b]$ with $\varphi' = \frac{\gamma'}{\gamma}$,
$\varphi(a) = 0$.
Hence $\gamma \exp(-\varphi)$ is constant.
Since $\gamma(a) = \gamma(b)$ it follows that $\exp(\varphi(b)) = \exp(\varphi(b)) = 1$.
Note that $\varphi(b) = 2\pi i \mathrm{Ind}(\gamma)$.)}
\emph{Compute $\mathrm{Ind}(\gamma)$ when $\gamma(t) = \exp(int)$, $a=0$, $b=2\pi$.}
\emph{Explain why $\mathrm{Ind}(\gamma)$ is often called the
\textbf{winding number} of $\gamma$ around $0$.} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
\emph{Show that $\mathrm{Ind}(\gamma)$ is always an integer.}
Define
\[
  \varphi(x) = \int_{a}^{x} \frac{\gamma'(t)}{\gamma(t)} dt
\]
if $x \in [a,b]$.
  \begin{enumerate}
  \item[(a)]
    \emph{Show that $\varphi(x)$ is well-defined.}
    Since $\gamma$ is continuously differentiable with $\gamma(t) \neq 0$ on $[a,b]$,
    $\frac{\gamma'(t)}{\gamma(t)}$ is continuous on $[a,b]$.
    Hence $\varphi(x)$ is well-defined.

  \item[(b)]
    \emph{Show that $\varphi' = \frac{\gamma'}{\gamma}$ and $\varphi(a) = 0$.}
    By Theorem 6.20,
    $\varphi(x)$ is continuous.
    Furthermore, $\varphi(x)$ is differentiable on $[a,b]$ and
    $\varphi'(x) = \frac{\gamma'(x)}{\gamma(x)}$.
    By the definition of $\varphi$, $\varphi(a) = 0$.

  \item[(c)]
    \emph{Show that $\gamma \exp(-\varphi)$ is constant.}
    Write $f(x) = \gamma(x) \exp(-\varphi(x))$.
    \begin{align*}
      f'(x)
      &= \gamma'(x) \exp(-\varphi(x)) + \gamma(x)(-\varphi'(x)) \exp(-\varphi(x)) \\
      &= (\gamma'(x) - \gamma(x)\varphi'(x)) \exp(-\varphi(x)) \\
      &= 0.
    \end{align*}
    Hence $f = \gamma \exp(-\varphi)$ is constant (Theorem 5.11(b)).

  \item[(d)]
    \emph{Show that $\mathrm{Ind}(\gamma) \in \mathbb{Z}$.}
    By (c),
    \begin{align*}
      &\gamma(b) \exp(-\varphi(b)) = \gamma(a) \exp(-\varphi(a)) \\
      \Longrightarrow&
      \exp(-\varphi(b)) = \exp(-\varphi(a))
        &\text{($\gamma$ is closed)} \\
      \Longrightarrow&
      \exp(\varphi(b)) = \exp(\varphi(a)) \\
      \Longrightarrow&
      \exp(2\pi i \mathrm{Ind}(\gamma)) = \exp(0) = 1
        &\text{((b))} \\
      \Longrightarrow&
      2\pi i \mathrm{Ind}(\gamma) = 2\pi i n \text{ for some } n \in \mathbb{Z}
        &\text{(Theorem 8.7)} \\
      \Longrightarrow&
      \mathrm{Ind}(\gamma) = n \text{ for some } n \in \mathbb{Z}.
    \end{align*}
  \end{enumerate}

\item[(2)]
\emph{Compute $\mathrm{Ind}(\gamma)$ when $\gamma(t) = \exp(int)$, $a=0$, $b=2\pi$.}
\begin{align*}
  \mathrm{Ind}(\gamma)
  &= \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma'(t)}{\gamma(t)} dt \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{in\exp(int)}{\exp(int)} dt \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} in dt \\
  &= n.
\end{align*}

\item[(3)]
\emph{Explain why $\mathrm{Ind}(\gamma)$ is often called the
\textbf{winding number} of $\gamma$ around $0$.}
As (2) suggested,
$\mathrm{Ind}(\gamma)$ is an integer
representing the total number of times that curve travels counterclockwise around $0$.
That's why we might say $\mathrm{Ind}(\gamma)$ is the winding number.
\end{enumerate}
$\Box$ \\



\textbf{Supplement.} The fundamental group of the circle.
\begin{enumerate}
\item[(1)]
  $\pi_1(S^1) \cong \mathbb{Z}$.
  That is, $\pi_1(S^1)$ is an infinite cyclic group generated by
  the homotopy class of the loop
  $\omega(s) = (\cos(2\pi s), \sin(2\pi s))$ based at $(1,0)$.

\item[(2)]
  Applications
  (\emph{Allen Hatcher, Algebraic Topology}
  \href{https://pi.math.cornell.edu/~hatcher/AT/AT.pdf}{[Link]}):
  \begin{enumerate}
  \item[(a)]
    Fundamental theorem of algebra.
  \item[(b)]
    The 2-dimensional case of Brouwer's fixed-point theorem.
  \item[(c)]
    The 2-dimensional case of Borsuk-Ulam theorem. \\\\
  \end{enumerate}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.24.}
\addcontentsline{toc}{subsection}{Exercise 8.24.}
\emph{Let $\gamma$ be as in Exercise 8.23, and assume in addition that
the range of $\gamma$ does not intersect the negative real axis.
Prove that $\mathrm{Ind}(\gamma)$ = 0.
(Hint: For $0 \leq c < \infty$, $\mathrm{Ind}(\gamma+c)$ is a continuous
integer-valued function of $c$.
Also, $\mathrm{Ind}(\gamma) \to 0$ as $c \to \infty$.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Let $f(t,c) = \gamma(t) + c$ defined on $E = [a,b] \times [0,\infty)$.

\item[(2)]
\emph{Define $E_K = [a,b] \times [0,K]$ for some $K \geq 0$.
Show that
\[
  0 < m_K \leq |f(E_K)| \leq M_K
\]
for some $m_K, M_K$.
Especially, take $K = 0$ to get
\[
  0 < |f(t,0)| = |\gamma(t)| \leq M_0
\]
and thus
\[
  |f(t,c)| = |\gamma(t)+c| \geq c - |\gamma(t)| > 2M_0 - M_0 = M_0
\]
whenever $c > 2M_0$.}
  \begin{enumerate}
  \item[(a)]
    Since $f$ is a continuous mapping of compact metric space $E_K$
    into $\mathbb{R}^2$, $f(E_K)$ is bounded (Theorem 4.15).
    Hence
    $0 \leq m_K \leq |f(E_K)| \leq M_K$
    for some $m_K$ and $M_K$.

  \item[(b)]
  Note that $f(t,c) \neq 0$
  since the range of $\gamma$ does not intersect the negative real axis.
  Hence, $m_K \neq 0$ (Theorem 4.16).
  \end{enumerate}

\item[(3)]
\emph{Show that
\[
  m = \inf_{(t,c) \in E} |f(t,c)| > 0.
\]}
  \begin{enumerate}
  \item[(a)]
  By (2), there exists $M_0 > 0$ such that
  \[
    |f(t,c)| \geq M_0 \text{ on } E - E_{2M_0}.
  \]

  \item[(b)]
  For such $K = 2M_0$, we apply (2) again to get that
  there exists $m_K = m_{2M_0} > 0$ such that
  \[
    |f(t,c)| \geq m_{2M_0} \text{ on } E_{2M_0}.
  \]

  \item[(c)]
  By (a)(b),
  \[
    |f(t,c)| > \min\{M_0,m_{2M_0}\} > 0 \text{ on } E.
  \]
  \end{enumerate}

\item[(4)]
  \emph{Show that $\mathrm{Ind}(\gamma+c)$ is uniformly continuous.}
  Since $[a,b]$ is compact and $\gamma'$ is continuous,
  $|\gamma'| \leq M$ for some $M > 0$.
  Hence for any $c_1, c_2 \in [0,\infty)$, we have
  \begin{align*}
    &\abs{ \mathrm{Ind}(\gamma+c_1) - \mathrm{Ind}(\gamma+c_2) } \\
    =& \abs{ \frac{1}{2\pi i}\int_{a}^{b} \frac{(\gamma(t)+c_1)'}{\gamma(t)+c_1} dt
      - \frac{1}{2\pi i}\int_{a}^{b} \frac{(\gamma(t)+c_2)'}{\gamma(t)+c_2} dt } \\
    =& \abs{ \frac{1}{2\pi i}\int_{a}^{b}
        \frac{\gamma'(t)(c_2-c_1)}{(\gamma(t)+c_1)(\gamma(t)+c_2)} dt } \\
    \leq& \frac{1}{2\pi} \int_{a}^{b}
        \frac{|\gamma'(t)|}{|(\gamma(t)+c_1)(\gamma(t)+c_2)|} |c_1-c_2|dt \\
    \leq& \frac{(b-a)M}{2\pi m^2} |c_1-c_2|.
  \end{align*}
  Hence, $\mathrm{Ind}(\gamma+c)$ is uniformly continuous on $[0,\infty)$.

\item[(5)]
  \emph{Show that $\mathrm{Ind}(\gamma) = 0$.}

  Since $[0,\infty)$ is connected, by (4) with Theorem 4.22
  the image of $\mathrm{Ind}(\gamma+c)$ is connected.
  Since the image of $\mathrm{Ind}(\gamma+c)$ is contained in $\mathbb{Z}$ (by Exercise 8.23),
  $\mathrm{Ind}(\gamma+c)$ is constant.
  \emph{It suffices to show that $\mathrm{Ind}(\gamma+c_0) = 0$ for some $c_0 \in [0,\infty)$.}
  In fact,
  \begin{align*}
    \abs{ \mathrm{Ind}(\gamma+c) }
    =& \abs{ \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma'(t)}{\gamma(t)+c} dt } \\
    \leq& \frac{1}{2\pi} \int_{a}^{b} \frac{|\gamma'(t)|}{|\gamma(t)+c|} dt \\
    \leq& \frac{1}{2\pi} \int_{a}^{b} \frac{|\gamma'(t)|}{|\gamma(t)|+c} dt \\
    \leq& \frac{(b-a)M}{2\pi(m+c)}.
  \end{align*}
  Let $c \to \infty$ to get $\abs{ \mathrm{Ind}(\gamma+c) } \to 0$.
  Hence, $\mathrm{Ind}(\gamma+c_0) = 0$ for some $c_0 \in [0,\infty)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.25.}
\addcontentsline{toc}{subsection}{Exercise 8.25.}
\emph{Suppose $\gamma_1$ and $\gamma_2$ are curves as in Exercise 8.23,
and
\[
  \abs{ \gamma_1(t) - \gamma_2(t) } < \abs{ \gamma_1(t) }
  \qquad (a \leq t \leq b).
\]
Prove that $\mathrm{Ind}(\gamma_1) = \mathrm{Ind}(\gamma_2)$.
(Hint: Put $\gamma = \frac{\gamma_2}{\gamma_1}$.
Then $|1-\gamma| < 1$, hence $\mathrm{Ind}(\gamma) = 0$, by Exercise 8.24.
Also,
\[
  \frac{\gamma'}{\gamma} = \frac{\gamma_2'}{\gamma_2} - \frac{\gamma_1'}{\gamma_1}.)
\]} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
  Put $\gamma = \frac{\gamma_2}{\gamma_1}$.
  It is well-defined since $\gamma_1(t) \neq 0$.
  Then
  \[
    \frac{\gamma'}{\gamma}
    = \frac{\frac{\gamma_1'\gamma_2-\gamma_1\gamma_2'}{\gamma_1^2}}{\frac{\gamma_2}{\gamma_1}}
    = \frac{\gamma_1'\gamma_2-\gamma_1\gamma_2'}{\gamma_1\gamma_2}
    = \frac{\gamma_2'}{\gamma_2} - \frac{\gamma_1'}{\gamma_1}.
  \]

\item[(2)]
Since
\[
  |1-\gamma|
  = \abs{ 1 - \frac{\gamma_2}{\gamma_1} }
  = \abs{ \frac{\gamma_1 - \gamma_2}{\gamma_1} }
  = \frac{|\gamma_1 - \gamma_2|}{|\gamma_1|}
  < 1,
\]
the range of $\gamma$ does not intersect the negative real axis.
(If not, there were some $t_0 \in [a,b]$ such that $\gamma(t_0) \leq 0$.
Thus $|1-\gamma(t_0)| \geq 1$, contrary to the assumption.)
By Exercise 8.24, $\mathrm{Ind}(\gamma) = 0$.

\item[(3)]
Since
\begin{align*}
  \mathrm{Ind}(\gamma)
  &= \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma'(t)}{\gamma(t)} dt \\
  &= \frac{1}{2\pi i}\int_{a}^{b}
      \left( \frac{\gamma_2'(t)}{\gamma_2(t)} - \frac{\gamma_1'(t)}{\gamma_1(t)} \right) dt
    &((1)) \\
  &= \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma_2'(t)}{\gamma_2(t)} dt
    - \frac{1}{2\pi i}\int_{a}^{b} \frac{\gamma_1'(t)}{\gamma_1(t)} dt \\
  &= \mathrm{Ind}(\gamma_2) - \mathrm{Ind}(\gamma_1),
\end{align*}
$\mathrm{Ind}(\gamma) = 0$ implies
$\mathrm{Ind}(\gamma_1) = \mathrm{Ind}(\gamma_2)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.26.}
\addcontentsline{toc}{subsection}{Exercise 8.26.}
\emph{Let $\gamma$ be a closed curve in the complex plane (not necessarily differentiable)
with parameter interval $[0,2\pi]$, such that $\gamma(t) \neq 0$ for every $t \in [0,2\pi]$.
Choose $\delta > 0$ so that $|\gamma(t)| > \delta$ for all $t \in [0,2\pi]$.
If $P_1$ and $P_2$ are trigonometric polynomials such that
$|P_j(t)-\gamma(t)| < \frac{\delta}{4}$ ($j = 1, 2$)
for all $t \in [0,2\pi]$ (their existence is assured by Theorem 8.15), prove that
\[
  \mathrm{Ind}(P_1) = \mathrm{Ind}(P_2)
\]
by applying Exercise 8.25.
Define this common value to be $\mathrm{Ind}(\gamma)$.
Prove that the statements of Exercises 8.24 and 8.25 hold without any
differentiability assumption.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that $\mathrm{Ind}(P_1) = \mathrm{Ind}(P_2)$.
Hence $\mathrm{Ind}(\gamma)$ is well-defined.}
Since
\begin{align*}
  |P_1(t) - P_2(t)|
  &\leq |P_1(t) - \gamma(t)| + |\gamma(t) - P_2(t)|
  \leq \frac{\delta}{4} + \frac{\delta}{4}
  = \frac{\delta}{2}, \\
  |P_1(t)|
  &\geq |\gamma(t)| - |\gamma(t) - P_1(t)|
  \geq \delta - \frac{\delta}{4}
  = \frac{3\delta}{4},
\end{align*}
we have
\[
  |P_1(t) - P_2(t)| \leq \frac{\delta}{2} < \frac{3\delta}{4} < |P_1(t)|.
\]
By Exercise 8.25, $\mathrm{Ind}(P_1) = \mathrm{Ind}(P_2)$.

\item[(2)]
\emph{Show that the new definition of $\mathrm{Ind}(\gamma)$
is the same as Exercise 8.23 if $\gamma$ is differentiable.}
Since
\[
  |P_j(t)-\gamma(t)|
  < \frac{\delta}{4}
  < \delta
  < |\gamma(t)|,
\]
by Exercise 8.25 we have $\mathrm{Ind}(\gamma) = \mathrm{Ind}(P_j)$.

\item[(3)]
\emph{Show that the statements of Exercises 8.24 holds
without any differentiability assumption.}
  \begin{enumerate}
  \item[(a)]
    Note that in the argument of Exercise 8.24,
    we never use the differentiability of $\gamma$ to get
    \[
      m = \inf_{(t,c) \in E} |\gamma(t)+c| > 0.
    \]
    where $E = [0,2\pi] \times [0,\infty)$.

  \item[(b)]
  Choose $\delta > 0$ so that $|\gamma(t)| > \delta$.
  By Theorem 8.15, there is a trigonometric polynomial $P(t)$ such that
  \[
    |P(t) - \gamma(t)| < \frac{\min\{m, \delta\}}{4} < \frac{m}{4}.
  \]
  Hence, $\mathrm{Ind}(\gamma) = \mathrm{Ind}(P)$.

  \item[(c)]
  \emph{Show that $P(t)$ does not intersect the negative real axis.}
  Given any $c \in [0,\infty)$, we have
  \[
    |P(t)+c|
    \geq |\gamma(t)+c| - |\gamma(t)-P(t)|
    \geq m - \frac{m}{4}
    = \frac{3m}{4}
    > 0.
  \]

  \item[(d)]
  Apply Exercise 8.24 for $P(t)$ to get $\mathrm{Ind}(P) = 0$.
  Therefore,
  \[
    \mathrm{Ind}(\gamma) = \mathrm{Ind}(P) = 0
  \]
  without any differentiability assumption about $\gamma$.
  \end{enumerate}

\item[(4)]
\emph{Show that the statements of Exercises 8.25 holds
without any differentiability assumption.}
  \begin{enumerate}
  \item[(a)]
  Define $f(t) = |\gamma_1(t)| - |\gamma_1(t) - \gamma_2(t)| > 0$.
  Since $f(t)$ is continuous on a compact set $[0,2\pi]$,
  by Theorem 4.16 there exists $m > 0$ such that $f(t) \geq m$ for all $t \in [0,2\pi]$.
  Hence
  \[
    |\gamma_1(t) - \gamma_2(t)| \leq |\gamma_1(t)| - m
  \]
  where $m > 0$ for all $t \in [0,2\pi]$.

  \item[(b)]
  Choose $\delta_j$ ($j=1,2$) so that $|\gamma_j(t)| > \delta$
  for all $t \in [0,2\pi]$.
  By Theorem 8.15, there are trigonometric polynomials $P_j(t)$ such that
  \[
    |P_j(t) - \gamma_j(t)| < \frac{\min\{m, \delta_j\}}{4} < \frac{m}{4}
  \]
  for all $t \in [0,2\pi]$.
  Hence, $\mathrm{Ind}(\gamma_j) = \mathrm{Ind}(P_j)$.

  \item[(c)]
  \emph{Show that $|P_1(t)-P_2(t)| < |P_1(t)|$ for all $t \in [0,2\pi]$.}
  Since
  \begin{align*}
    |P_1(t)-P_2(t)|
    \leq& |P_1(t)-\gamma_1(t)| + |\gamma_1(t)-\gamma_2(t)| + |\gamma_2(t)-P_2(t)| \\
    <& \frac{m}{4} + |\gamma_1(t)| - m + \frac{m}{4} \\
    =& |\gamma_1(t)| - \frac{m}{2}, \\
    |P_1(t)|
    \geq& |\gamma_1(t)| - |\gamma_1(t)-P_1(t)| \\
    >& |\gamma_1(t)| - \frac{m}{4},
  \end{align*}
  we have
  \[
    |P_1(t)-P_2(t)| < |\gamma_1(t)| - \frac{m}{2} < |\gamma_1(t)| - \frac{m}{4} < |P_1(t)|.
  \]

  \item[(d)]
  Apply Exercise 8.25 for $P_1(t)$ and $P_2(t)$ to get
  $\mathrm{Ind}(P_1) = \mathrm{Ind}(P_2)$.
  Therefore,
  \[
    \mathrm{Ind}(\gamma_1) = \mathrm{Ind}(P_1) = \mathrm{Ind}(P_2) = \mathrm{Ind}(\gamma_2)
  \]
  without any differentiability assumption about $\gamma_j$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.27.}
\addcontentsline{toc}{subsection}{Exercise 8.27.}
\emph{Let $f$ be a continuous complex function defined in the complex plane.
Suppose there is a positive integer $n$ and a complex number $c \neq 0$ such that
\[
  \lim_{|z| \to \infty} z^{-n} f(z) = c.
\]
Prove that $f(z) = 0$ for at least one complex number $z$.
Note that this is a generalization of Theorem 8.8.
(Hint: Assume $f(z) \neq 0$ for all $z$,
define
\[
  \gamma_r(t) = f(re^{it})
\]
for $0 \leq r < \infty$, $0 \leq t \leq 2\pi$,
and prove the following statements about the curves $\gamma_r(t)$:}
\begin{enumerate}
  \item[(a)]
  \emph{$\mathrm{Ind}(\gamma_0) = 0$.}

  \item[(b)]
  \emph{$\mathrm{Ind}(\gamma_r) = n$ for all sufficiently larger $r$.}

  \item[(c)]
  \emph{$\mathrm{Ind}(\gamma_r)$ is a continuous function of $r$, on $[0,\infty)$.}
\end{enumerate}
\emph{[In (b) and (c), use the last part of Exercise 8.26.]}
\emph{Show that (a), (b), and (c) are contradictory, since $n > 0$).} \\

\emph{Proof of (a).}
\begin{align*}
  \mathrm{Ind}(\gamma_0)
  &= \mathrm{Ind}(f(0))
    &\text{(Well-defined)} \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{f(0)'}{f(0)} dt
    &\text{($f(0)$ is differentiable)} \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} 0 dt
    &\text{($f(0)$ is constant)} \\
  &= 0.
\end{align*}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
  \item[(1)]
  Take $z = re^{it}$ in $\lim_{|z| \to \infty} z^{-n} f(z) = c$
  to get
  \[
    c
    = \lim_{r \to \infty} (re^{it})^{-n} f(re^{it})
    = \lim_{r \to \infty} \frac{\gamma_r(t)}{r^n e^{int}}.
  \]

  Thus, for $|c| > 0$, there exists $M > 0$
  such that
  \[
    \abs{ \frac{\gamma_r(t)}{r^n e^{int}} - c } < |c|
    \text{ whenever } r \geq M.
  \]
  Hence, for some sufficiently large $r \geq M$ we have
  \begin{align*}
    &\abs{ \frac{\gamma_r(t)}{r^n e^{int}} - c } < |c| \\
    \Longrightarrow&
    \abs{ \frac{\gamma_r(t)}{r^n e^{int}} - c }|r^n e^{int}| < |c||r^n e^{int}|
      &(|r^n e^{int}| \neq 0) \\
    \Longrightarrow&
    | \gamma_r(t) - cr^n e^{int} | < |cr^n e^{int}|.
  \end{align*}

  \item[(2)]
  Let $\gamma(t) = cr^n e^{int}$ defined for $0 \leq r < \infty$, $0 \leq t \leq 2\pi$.
  $\gamma$ is a differentiable closed curve in the complex plane.
  By the last part of Exercise 8.26,
  \begin{align*}
    \mathrm{Ind}(\gamma_r)
    &= \mathrm{Ind}(\gamma) \\
    &= \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{\gamma'}{\gamma} dt \\
    &= \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{in cr^n e^{int}}{cr^n e^{int}} dt \\
    &= \frac{1}{2\pi i}\int_{0}^{2\pi} in dt \\
    &= n.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
  \item[(1)]
  \emph{Show that $m = \inf_{t \in [0, 2\pi]} |\gamma_r(t)| > 0$
  for any given $r \in [0,\infty)$.}
  Since
  $t \mapsto re^{it} \mapsto f(re^{it}) = \gamma_r(t) \mapsto |\gamma_r(t)|$
  is a composition of three continuous functions,
  $t \mapsto |\gamma_r(t)|$ is a continuous real function (Theorem 4.7).
  Moreover, since $[0,2\pi]$ is a compact subset of $\mathbb{R}$,
  and $|\gamma_r(t)| \neq 0$,
  \[
    m = \inf_{t \in [0, 2\pi]} |\gamma_r(t)| > 0
  \]
  (Theorem 4.16).

  \item[(2)]
  \emph{Show that there exists $\delta > 0$ such that
  $|\gamma_r(t) - \gamma_s(t)| < |\gamma_r(t)|$
  whenever $|r-s| < \delta$ (with $r, s \in [0,\infty)$).}
  Similar to (1), $r \mapsto \gamma_r(t)$ is also continuous.
  So given any $r \in [0,\infty)$,
  for $\varepsilon = m > 0$ there exists $\delta > 0$
  such that
  \[
    |\gamma_r(t) - \gamma_s(t)| < m
  \]
  whenever $|r-s| < \delta$ (with $s \in [0,\infty)$).
  By (1),
  \[
    |\gamma_r(t) - \gamma_s(t)| < m \leq |\gamma_r(t)|.
  \]
  whenever $|r-s| < \delta$ (with $s \in [0,\infty)$).

  \item[(3)]
  \emph{Show that $\mathrm{Ind}(\gamma_r)$ is continuous on $[0,\infty)$.}
  Given any $r \in [0,\infty)$.
  By the last part of Exercise 8.26, there exists $\delta > 0$ such that
  \[
    \mathrm{Ind}(\gamma_r) = \mathrm{Ind}(\gamma_s)
  \]
  whenever $|r-s| < \delta$ (with $r, s \in [0,\infty)$).
  Hence $\mathrm{Ind}(\gamma_r)$ is continuous.
  Since $r \in [0,\infty)$ is arbitrary,
  $\mathrm{Ind}(\gamma_r)$ is continuous on $[0,\infty)$
\end{enumerate}
$\Box$ \\



\emph{Proof of contradiction among (a)(b)(c).}
Since $[0,\infty)$ is connected, by (c) with Theorem 4.22
the image of $\mathrm{Ind}(\gamma_r)$ is connected.
Since the image of $\mathrm{Ind}(\gamma_r)$ is contained in $\mathbb{Z}$,
$\mathrm{Ind}(\gamma_r)$ is constant, contrary to (a)(b) since $n > 0$ or $n \neq 0$.
Therefore, $f(z) = 0$ for some $z \in \mathbb{C}$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.28.}
\addcontentsline{toc}{subsection}{Exercise 8.28.}
\emph{Let $D^2$ be the closed unit disc in the complex plane.
(Thus $z \in D^2$ if and only if $|z| \leq 1$.)
Let $g$ be a continuous mapping of $D^2$ into the unit circle $S^1$.
(Thus $|g(z)| = 1$ for every $z \in S^1$.)
Prove that $g(z)=-z$ for at least one $z \in S^1$.
(Hint:
For $0 \leq r \leq 1$, $0 \leq t \leq 2\pi$,
put
\[
  \gamma_r(t) = g(re^{it}),
\]
and put $\psi(t) = e^{-it} \gamma_1(t)$.
If $g(z) \neq -z$ for every $z \in S^1$, then $\psi(t) \neq -1$ for every $t \in [0,2\pi]$.
Hence $\mathrm{Ind}(\psi) = 0$, by Exercise 8.24 and 8.26.
It follows that $\mathrm{Ind}(\gamma_1) = 1$.
But $\mathrm{Ind}(\gamma_0) = 0$.
Derive a contradiction, as in Exercise 8.27.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
  \item[(1)]
  For $0 \leq r \leq 1$, $0 \leq t \leq 2\pi$,
  put
  \[
    \gamma_r(t) = g(re^{it}),
  \]
  and put $\psi(t) = e^{-it} \gamma_1(t) = e^{-it} g(e^{it})$.
  Clearly, $\psi$ is a continuous closed curve in $\mathbb{C}$
  and the image of $\psi$ is contained in $S^1$.
  Especially $\psi(t) \neq 0$ for all $t \in [0,2\pi]$.

  \item[(2)]
  \emph{Show that $\mathrm{Ind}(\psi) = 0$.}
  (Reductio ad absurdum)
  Assume $g(z) \neq -z$ for all $z \in S^1$.
  Thus $\psi(t) \neq -1$ for every $t \in [0,2\pi]$.
  So that the range of $\psi$ does not intersect the negative real axis.
  By the last part of Exercise 8.26,
  \[
    \mathrm{Ind}(\psi) = 0.
  \]

  \item[(3)]
  \emph{Show that $\mathrm{Ind}(\gamma_0) = 0$.}
  Similar to Exercise 8.27(a).
  \begin{align*}
  \mathrm{Ind}(\gamma_0)
  &= \mathrm{Ind}(g(0))
    &\text{(Well-defined)} \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{g(0)'}{g(0)} dt
    &\text{($g(0)$ is differentiable)} \\
  &= \frac{1}{2\pi i}\int_{0}^{2\pi} 0 dt
    &\text{($g(0)$ is constant)} \\
  &= 0.
  \end{align*}

  \item[(4)]
  \emph{Show that $\mathrm{Ind}(\gamma_1) = 1$.}
  \begin{enumerate}
    \item[(a)]
    By the first part of Exercise 8.26, we choose $\delta > 0$
    so that $|\psi(t)| > \delta$ for all $t \in [0,2\pi]$.
    There exists a trigonometric polynomial $P(t)$
    such that
    \[
      |P(t) - \psi(t)| < \frac{\delta}{4}
      \qquad
      (t \in [0,2\pi]).
    \]
    Here
    \[
      \mathrm{Ind}(P) = \mathrm{Ind}(\psi) = 0.
    \]

    \item[(b)]
    Write $Q(t) = e^{it}P(t)$.
    So that
    \begin{align*}
      |Q(t) - \gamma_1(t)|
      &= |e^{it}P(t) - e^{it}\psi(t)| \\
      &= |e^{it}||P(t) - \psi(t)| \\
      &< \frac{\delta}{4}.
    \end{align*}
    Since $Q(t)$ is differentiable,
    as the argument in Exercise 8.26, we have
    \[
      \mathrm{Ind}(Q) = \mathrm{Ind}(\gamma_1).
    \]

    \item[(c)]
    As the hint in Exercise 8.25, we have
    \[
      \frac{Q'}{Q} = \frac{(e^{it})'}{e^{it}} + \frac{P'}{P}
    \]
    and thus
    \[
      \mathrm{Ind}(Q) = 1 + \mathrm{Ind}(P) = 1.
    \]
    (the second part of Exercise 8.23), or $\mathrm{Ind}(\gamma_1) = 1$.
  \end{enumerate}

  \item[(5)]
  \emph{Show that $\mathrm{Ind}(\gamma_r)$ is a continuous function of $r$, on $[0,1]$.}
  Similar to Exercise 8.27(c).
  \begin{enumerate}
    \item[(a)]
    Since $r \mapsto re^{it} \mapsto g(re^{it}) = \gamma_r(t)$
    is a composition of two continuous functions,
    $r \mapsto \gamma_r(t)$ is a continuous function (Theorem 4.7).
    So given any $r \in [0,1]$,
    for $\varepsilon = \frac{64}{89} > 0$,
    there exists $\delta' > 0$ such that
    \[
      |\gamma_r(t) - \gamma_s(t)| < \frac{64}{89}
    \]
    whenever $|r-s| < \delta'$ (with $s \in [0,1]$).
    By $|\gamma_r(t)| = |g(re^{it})| = 1$,
    \[
      |\gamma_r(t) - \gamma_s(t)| < \frac{64}{89} < 1 = |\gamma_r(t)|
    \]
    whenever $|r-s| < \delta'$ (with $s \in [0,1]$).

    \item[(b)]
    By the last part of Exercise 8.26, there exists $\delta' > 0$ such that
    \[
      \mathrm{Ind}(\gamma_r) = \mathrm{Ind}(\gamma_s)
    \]
    whenever $|r-s| < \delta'$ (with $s \in [0,1]$).
    Hence $\mathrm{Ind}(\gamma_r)$ is continuous.
  \end{enumerate}

  \item[(6)]
  Since $[0,1]$ is connected, by (c) with Theorem 4.22
  the image of $\mathrm{Ind}(\gamma_r)$ is connected.
  Since the image of $\mathrm{Ind}(\gamma_r)$ is contained in $\mathbb{Z}$,
  $\mathrm{Ind}(\gamma_r)$ is constant, contrary to (3)(4) since $0 \neq 1$.
  Therefore, $g(z) = -z$ for some $z \in S^1$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.29 (Brouwer's fixed-point theorem).}
\addcontentsline{toc}{subsection}{Exercise 8.29 (Brouwer's fixed-point theorem).}
\emph{Prove that every continuous mapping $f$ of $D^2$ into $D^2$
has a fixed point in $D^2$.
(This is the 2-dimensional case of Brouwer's fixed-point theorem.)
(Hint: Assume $f(z) \neq z$ for every $z \in D^2$.
Associate to each $z \in D^2$ the point $g(z) \in S^1$
which lies on the ray that starts at $f(z)$ and passes through $z$.
Then $g$ maps $D^2$ into $S^1$,
$g(z) = z$ if $z \in S^1$, and $g$ is continuous,
because
\[
  g(z) = z - s(z)[f(z)-z],
\]
where $s(z)$ is the unique nonnegative root of a certain quadratic equation whose
coefficients are continuous functions of $f$ and $z$.
Apply Exercise 8.28.)} \\

\emph{Proof.}
\begin{enumerate}
  \item[(1)]
  The ray that starts at $f(z)$ and passes through $z$
  is given by $f(z) + s(z - f(z)) = sz + (1-s)f(z)$ where $s = s(z) \geq 0$.
  $g(z)$ satisfies
  \begin{align*}
    |g(z)| = 1
    &\Longleftrightarrow
    g(z) \overline{g(z)} = 1 \\
    &\Longleftrightarrow
    (sz + (1-s)f(z))(s\overline{z} + (1-s)\overline{f(z)}) = 1 \\
    &\Longleftrightarrow
    s^2 |z|^2 + (1-s)^2 |f(z)|^2 + s(1-s) 2 \Re(z\overline{f(z)}) = 1 \\
    &\Longleftrightarrow
    |z-f(z)|^2 s^2 + 2(\Re(z\overline{f(z)}) - |f(z)|^2)s + (|f(z)|^2 - 1) = 0.
  \end{align*}
  Since $s = s(z) \geq 0$,
  $s$ is the unique nonnegative root of a certain quadratic equation whose
  coefficients are continuous functions of $f$ and $z$.
  Therefore, $g(z) = f(z) + s(z)(z - f(z))$ is continuous.

  \item[(2)]
  Note that $|g(z)| = 1$ by construction,
  by Exercise 8.28 we have
  \[
    g(z_0) = -z_0 \text{ for some } z_0 \in S_1.
  \]
  However, for such $z_0$ we have $g(z_0) = z_0$ by construction.
  So $z_0 = 0$, contrary to $z_0 \in S_1$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement.} Brouwer's fixed-point theorem.
\begin{enumerate}
  \item[(1)]
    In the $\mathbb{R}^1$, see Exercise 4.14.

  \item[(2)]
    In the $\mathbb{R}^2$, see Exercise 8.29 itself.

  \item[(3)]
    In the $\mathbb{R}^n$,
    every continuous function from a closed ball of a Euclidean space $\mathbb{R}^n$
    into itself has a fixed point (without proof).

  \item[(4)]
    In a Banach space, Schauder fixed-point theorem. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.30.}
\addcontentsline{toc}{subsection}{Exercise 8.30.}
\emph{Use Stirling's formula to prove that
\[
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)} = 1
\]
for every real constant $c$.} \\

\emph{Proof.}
By Stirling's formula,
\begin{align*}
  \lim_{x \to \infty}
  \frac{\Gamma(x+c)}{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}
  &= 1 \\
  \lim_{x \to \infty}
  \frac{\Gamma(x)}{\left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}}
  &= 1,
\end{align*}
we have
\begin{align*}
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)}
  =&
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)} \\
  &\times
  \lim_{x \to \infty}
  \frac{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}{\Gamma(x+c)} \\
  &\times
  \lim_{x \to \infty}
  \frac{\Gamma(x)}{\left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}} \\
  =&
  \lim_{x \to \infty} \frac{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}
    {x^c \left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}} \\
  =&
  \lim_{x \to \infty}
  \frac{\left( \frac{x+c-1}{e} \right)^{c}}{x^c}
  \frac{\left( \frac{x+c-1}{e} \right)^{x-1}}{\left( \frac{x-1}{e} \right)^{x-1}}
  \sqrt{\frac{x+c-1}{x-1}} \\
  =& \frac{1}{e^c} \cdot e^c \cdot 1 \\
  =& 1
\end{align*}
since
\begin{enumerate}
\item[(1)]
\[
  \lim_{x \to \infty}
    \frac{\left( \frac{x+c-1}{e} \right)^{c}}{x^c}
  = \frac{1}{e^c} \lim_{x \to \infty} \left( \frac{x+c-1}{x} \right)^{c}
  = \frac{1}{e^c}.
\]

\item[(2)]
\[
  \lim_{x \to \infty}
    \frac{\left( \frac{x+c-1}{e} \right)^{x-1}}{\left( \frac{x-1}{e} \right)^{x-1}}
  = \lim_{x \to \infty} \left( \frac{x+c-1}{x-1} \right)^{x-1}
  = \lim_{x \to \infty} \left( 1+\frac{c}{x-1} \right)^{x-1}
  = e^c.
\]

\item[(3)]
and
\[
  \lim_{x \to \infty} \sqrt{\frac{x+c-1}{x-1}}
  = \lim_{x \to \infty} \sqrt{1+\frac{c}{x-1}} = 1.
\]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 8.31.}
\addcontentsline{toc}{subsection}{Exercise 8.31.}
\emph{In the proof of Theorem 7.26 it was shown that
\[
  \int_{-1}^{1} (1-x^2)^n dx \geq \frac{4}{3\sqrt{n}}
\]
for $n=1,2,3,\ldots$.
Use Theorem 8.20 and Exercise 8.30 to show the more precise result
\[
\lim_{n \to \infty} \sqrt{n} \int_{-1}^{1} (1-x^2)^n dx = \sqrt{\pi}.
\]} \\

\emph{Proof.}
\begin{align*}
  &\lim_{n \to \infty} \sqrt{n} \int_{-1}^{1} (1-x^2)^n dx \\
  =& \lim_{n \to \infty} \sqrt{n} \int_{0}^{1} u^{-\frac{1}{2}} (1-u)^n dx
    &(u = x^2) \\
  =& \lim_{n \to \infty} \sqrt{n}
    \frac{\Gamma\left( \frac{1}{2} \right) \Gamma(n+1)}{\Gamma\left( n+\frac{3}{2} \right)}
    &\text{(Theorem 8.20)} \\
  =& \Gamma\left( \frac{1}{2} \right) \lim_{n \to \infty}
    \frac{n^{\frac{1}{2}} \Gamma(n+1)}{\Gamma\left( n+\frac{3}{2} \right)} \\
  =& \Gamma\left( \frac{1}{2} \right)
    &\text{(Exercise 8.30)} \\
  =& \sqrt{\pi}.
    &\text{(Some consequences 8.21)}
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 9: Functions of Several Variables \\}
\addcontentsline{toc}{section}{Chapter 9: Functions of Several Variables}



\subsection*{Exercise 9.1.}
\addcontentsline{toc}{subsection}{Exercise 9.1.}
\emph{If $S$ is a nonempty subset of a vector space $X$,
prove (as asserted in Section 9.1) that the span of $S$ is a vector space.} \\

Denote the span of $S$ by $\mathrm{span}(S)$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Since $S \neq \varnothing$, there is $\mathbf{z} \in S$.
  So $1\mathbf{z} = \mathbf{z} \in \mathrm{span}(S) \neq \varnothing$.
  (In fact, $\mathrm{span}(S) \supseteq S$.)

\item[(2)]
  If $\mathbf{x}, \mathbf{y} \in \mathrm{span}(S)$,
  then there exist elements
  $\mathbf{x}_1, \ldots, \mathbf{x}_m$, $\mathbf{y}_1, \ldots, \mathbf{y}_n \in S$
  and scalars $a_1, \ldots, a_m$, $b_1, \ldots, b_n$ such that
  \begin{align*}
    \mathbf{x} &= a_1 \mathbf{x}_1 + \cdots + a_m \mathbf{x}_m, \\
    \mathbf{y} &= b_1 \mathbf{y}_1 + \cdots + b_n \mathbf{y}_n.
  \end{align*}
  Then
  \[
    \mathbf{x}+\mathbf{y}
    = a_1 \mathbf{x}_1 + \cdots + a_m \mathbf{x}_m
      + b_1 \mathbf{y}_1 + \cdots + b_n \mathbf{y}_n
  \]
  is a linear combination of the elements of $S$.
  For any scalar $c$,
  \[
    c\mathbf{x} = (ca_1) \mathbf{x}_1 + \cdots + (ca_m) \mathbf{x}_m
  \]
  is again linear combination of the elements of $S$.

\item[(3)]
  By (1)(2), $\mathrm{span}(S)$ is a vector space.
\end{enumerate}
$\Box$ \\

\emph{Note.}
Any subspace of $X$ that contains $S$ must also contain $\mathrm{span}(S)$. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.2.}
\addcontentsline{toc}{subsection}{Exercise 9.2.}
\emph{Prove (as asserted in Section 9.6) that $BA$ is linear
if $A$ and $B$ are linear transformations.
Prove also that $A^{-1}$ is linear and invertible if $A$ is invertible.} \\

\emph{Proof.}
Use the notation in Definitions 9.6.
\begin{enumerate}
\item[(1)]
  \emph{Show that $BA$ is linear if $A$ and $B$ are linear transformations.}
  Let $X, Y, Z$ be vector spaces, $A \in L(X,Y)$ and $B \in L(Y,Z)$.
  \begin{enumerate}
  \item[(a)]
    Given any $\mathbf{x}_1, \mathbf{x}_2 \in X$.
    \begin{align*}
      (BA)(\mathbf{x}_1+\mathbf{x}_2)
      &= B(A(\mathbf{x}_1+\mathbf{x}_2)) \\
      &= B(A\mathbf{x}_1+A\mathbf{x}_2)
        & (\text{$A$ is a linear transformation}) \\
      &= B(A\mathbf{x}_1) + B(A\mathbf{x}_2)
        & (\text{$B$ is a linear transformation}) \\
      &= (BA)\mathbf{x}_1 + (BA)\mathbf{x}_2.
    \end{align*}

  \item[(b)]
    For any $\mathbf{x} \in X$ and scalar $c$,
    \begin{align*}
    (BA)(c\mathbf{x})
    &= B(A(c\mathbf{x})) \\
    &= B(cA\mathbf{x})
      & (\text{$A$ is a linear transformation}) \\
    &= cB(A\mathbf{x})
      & (\text{$B$ is a linear transformation}) \\
    &= c(BA)\mathbf{x}.
    \end{align*}
  \end{enumerate}
  By (a)(b), $BA \in L(X,Z)$.

\item[(2)]
  \emph{Show that $A^{-1}$ is linear if $A$ is invertible.}
  \begin{enumerate}
  \item[(a)]
    Given any $\mathbf{y}_1, \mathbf{y}_2 \in X$.
    Since $A$ is surjective,
    there exist $\mathbf{x}_1, \mathbf{x}_2 \in X$ such that
    \begin{align*}
      \mathbf{y}_1 &= A\mathbf{x}_1 \\
      \mathbf{y}_2 &= A\mathbf{x}_2.
    \end{align*}
    So
    \begin{align*}
      A^{-1}\mathbf{y}_1 &= A^{-1}(A\mathbf{x}_1) = \mathbf{x}_1 \\
      A^{-1}\mathbf{y}_2 &= A^{-1}(A\mathbf{x}_2) = \mathbf{x}_2
    \end{align*}
    (by Definitions 9.4).
    Hence
    \begin{align*}
      A^{-1}(\mathbf{y}_1+\mathbf{y}_2)
      &= A^{-1}(A\mathbf{x}_1+A\mathbf{x}_2) \\
      &= A^{-1}(A(\mathbf{x}_1+\mathbf{x}_2))
        & (\text{$A$ is a linear transformation}) \\
      &= \mathbf{x}_1+\mathbf{x}_2
        & (\text{Definitions 9.4}) \\
      &= A^{-1}\mathbf{y}_1+A^{-1}\mathbf{y}_2.
    \end{align*}

  \item[(b)]
    For any $\mathbf{y} \in X$ and scalar $c$,
    there is a corresponding $\mathbf{x} \in X$ such that $\mathbf{y} = A\mathbf{x}$
    since $A$ is surjective. So $A^{-1}\mathbf{y} = \mathbf{x}$ by Definition 9.4.
    Hence
    \begin{align*}
      A^{-1}(c\mathbf{y})
      &= A^{-1}(cA\mathbf{x}) \\
      &= A^{-1}(A(c\mathbf{x}))
        & (\text{$A$ is a linear transformation}) \\
      &= c\mathbf{x}
        & (\text{Definitions 9.4}) \\
      &= cA^{-1}\mathbf{y}.
    \end{align*}
  \end{enumerate}
  By (a)(b), $A^{-1} \in L(X)$.

\item[(3)]
  \emph{Show that $A^{-1}$ is invertible if $A$ is invertible.}
  It suffices to show that $A^{-1}$ is injective and surjective.
  \begin{enumerate}
  \item[(a)]
    \emph{Show that $A^{-1}$ is injective.}
    Given any $\mathbf{y}_1, \mathbf{y}_2 \in X$.
    Since $A$ is surjective,
    there exist $\mathbf{x}_1, \mathbf{x}_2 \in X$ such that
    \begin{align*}
      \mathbf{y}_1 &= A\mathbf{x}_1 \\
      \mathbf{y}_2 &= A\mathbf{x}_2.
    \end{align*}
    Suppose $A^{-1}\mathbf{y}_1 = A^{-1}\mathbf{y}_2$.
    So $A^{-1}(A\mathbf{x}_1) = A^{-1}(A\mathbf{x}_2)$,
    or $\mathbf{x}_1 = \mathbf{x}_2$,
    or $\mathbf{y}_1 = A\mathbf{x}_1 = A\mathbf{x}_2 = \mathbf{y}_2$.

  \item[(b)]
    \emph{Show that $A^{-1}$ is surjective.}
    For any $\mathbf{x} \in X$, there exists $A\mathbf{x} \in X$ such that
    $A^{-1}(A\mathbf{x}) = \mathbf{x}$ by Definitions 9.4.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.3.}
\addcontentsline{toc}{subsection}{Exercise 9.3.}
\emph{Assume $A \in L(X,Y)$ and $A\mathbf{x} = \mathbf{0}$ only when $\mathbf{x} = \mathbf{0}$.
Prove that $A$ is then $1$-$1$.} \\

\emph{Proof.}
Suppose $A\mathbf{x} = A\mathbf{y}$.
Since $A$ is a linear transformation,
$A(\mathbf{x}-\mathbf{y}) = A\mathbf{x} - A\mathbf{y} = \mathbf{0}$.
By assumption, $\mathbf{x}-\mathbf{y} = \mathbf{0}$
or $\mathbf{x} = \mathbf{y}$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.4.}
\addcontentsline{toc}{subsection}{Exercise 9.4.}
\emph{Prove (as asserted in Section 9.30) that null spaces and ranges of
linear transformations are vector spaces.} \\

\emph{Proof.}
Use the notation in Definitions 9.30.
Suppose $X$, $Y$ are vector spaces, and $A \in L(X,Y)$, as in Definition 9.6.
\begin{enumerate}
\item[(1)]
  \emph{Show that $\mathscr{N}(A)$ is a vector space in $X$.}
  \begin{enumerate}
  \item[(a)]
    Note that $\mathbf{0} \in X$.
    Since $A\mathbf{0} = \mathbf{0}$, $\mathbf{0} \in \mathscr{N}(A) \neq \varnothing$.

  \item[(b)]
    Suppose $\mathbf{x}_1, \mathbf{x}_2 \in \mathscr{N}(A)$.
    Then
    \begin{align*}
      A(\mathbf{x}_1+\mathbf{x}_2)
      &= A\mathbf{x}_1+A\mathbf{x}_2
        & (\text{$A$ is a linear transformation}) \\
      &= \mathbf{0}+\mathbf{0}
        & (\mathbf{x}_1, \mathbf{x}_2 \in \mathscr{N}(A)) \\
      &= \mathbf{0}.
    \end{align*}
    So $\mathbf{x}_1+\mathbf{x}_2 \in \mathscr{N}(A)$.

  \item[(c)]
    Suppose $\mathbf{x} \in \mathscr{N}(A)$ and $c$ is a scalar.
    Then
    \begin{align*}
      A(c\mathbf{x})
      &= cA\mathbf{x}
        & (\text{$A$ is a linear transformation}) \\
      &= c\mathbf{0}
        & (\mathbf{x} \in \mathscr{N}(A)) \\
      &= \mathbf{0}.
    \end{align*}
    So $c\mathbf{x} \in \mathscr{N}(A)$.
  \end{enumerate}
  By (a)(b)(c), $\mathscr{N}(A)$ is a vector space.

\item[(2)]
  \emph{Show that $\mathscr{R}(A)$ is a vector space in $Y$.}
  \begin{enumerate}
  \item[(a)]
    Note that $\mathbf{0} \in X$.
    So $A\mathbf{0} = \mathbf{0} \in \mathscr{R}(A) \neq \varnothing$.

  \item[(b)]
    Suppose $\mathbf{y}_1, \mathbf{y}_2 \in \mathscr{R}(A)$.
    Then there exist $\mathbf{x}_1, \mathbf{x}_2 \in X$
    such that $A\mathbf{x}_1 = \mathbf{y}_1$
    and $A\mathbf{x}_2 = \mathbf{y}_2$.
    Hence
    \begin{align*}
      \mathbf{y}_1+\mathbf{y}_2
      &= A\mathbf{x}_1+A\mathbf{x}_2 \\
      &= A(\mathbf{x}_1+\mathbf{x}_2)
        & (\text{$A$ is a linear transformation}).
    \end{align*}
    So $\mathbf{y}_1+\mathbf{y}_2 \in \mathscr{R}(A)$.

  \item[(c)]
    Suppose $\mathbf{y} \in \mathscr{R}(A)$ and $c$ is a scalar.
    Then there exists $\mathbf{x} \in X$ such that $A\mathbf{x} = \mathbf{y}$.
    Hence
    \begin{align*}
      c\mathbf{y}
      &= cA\mathbf{x} \\
      &= A(c\mathbf{x})
        & (\text{$A$ is a linear transformation}).
    \end{align*}
    So $c\mathbf{y} \in \mathscr{R}(A)$.
  \end{enumerate}
  By (a)(b)(c), $\mathscr{R}(A)$ is a vector space.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.5.}
\addcontentsline{toc}{subsection}{Exercise 9.5.}
\emph{Prove that to every $A \in L(\mathbb{R}^n, \mathbb{R}^1)$
corresponds a unique $\mathbf{y} \in \mathbb{R}^n$ such that
$A\mathbf{x} = \mathbf{x} \cdot \mathbf{y}$.
Prove also that $\norm{A} = |\mathbf{y}|$.
(Hint: Under certain conditions, equality holds in the Schwarz inequality.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Recall that $\{ \mathbf{e}_1, \ldots, \mathbf{e}_n \}$
  is the standard basis of $\mathbb{R}^n$ (Definitions 9.1).
  Given any $\mathbf{x} \in \mathbb{R}^n$,
  write $\mathbf{x} = (x_1, \ldots, x_n)$ as $\mathbf{x} = \sum x_j \mathbf{e}_j$.

\item[(2)]
  \emph{Show that $\mathbf{y}$ exists.}
  Since $A$ is a linear transformation,
  \begin{align*}
    A\mathbf{x}
    &= A\left(\sum x_j \mathbf{e}_j\right) \\
    &= \sum x_j A\mathbf{e}_j \\
    &= (x_1, \ldots, x_n) \cdot (A\mathbf{e}_1, \ldots, A\mathbf{e}_n) \\
    &= \mathbf{x} \cdot \sum (A\mathbf{e}_j) \mathbf{e}_j.
  \end{align*}
  Define $\mathbf{y} = \sum (A\mathbf{e}_j) \mathbf{e}_j \in \mathbb{R}^n$
  so that $A\mathbf{x} = \mathbf{x} \cdot \mathbf{y}$.

\item[(3)]
  \emph{Show that $\mathbf{y}$ is unique.}
  Suppose there exists some $\mathbf{z} \in \mathbb{R}^n$
  such that $A\mathbf{x} = \mathbf{x} \cdot \mathbf{z}$.
  So
  \begin{align*}
    0
    &= A\mathbf{x} -  A\mathbf{x} \\
    &= \mathbf{x} \cdot \mathbf{y}-\mathbf{x} \cdot \mathbf{z} \\
    &= \mathbf{x} \cdot (\mathbf{y}-\mathbf{z})
  \end{align*}
  for any $\mathbf{x} \in \mathbb{R}^n$.
  In particular, take $\mathbf{x} = \mathbf{y}-\mathbf{z} \in \mathbb{R}^n$
  to get
  \[
    0
    = (\mathbf{y}-\mathbf{z}) \cdot (\mathbf{y}-\mathbf{z})
    = |\mathbf{y}-\mathbf{z}|^2
  \]
  or $\mathbf{y}-\mathbf{z} = \mathbf{0}$ or $\mathbf{y}=\mathbf{z}$.

\item[(4)]
  \emph{Show that $\norm{A} = |\mathbf{y}|$.}
  By the Schwarz inequality (Theorem 1.37(d)),
  \[
    |A\mathbf{x}| = |\mathbf{x} \cdot \mathbf{y}| \leq |\mathbf{x}||\mathbf{y}|
    \leq |\mathbf{y}|
  \]
  as $|\mathbf{x}| \leq 1$.
  Take the sup over all $|\mathbf{x}| \leq 1$ to get
  \[
    \norm{A} \leq |\mathbf{y}|.
  \]
  If $\mathbf{y} = \mathbf{0}$, then $\norm{A} = |\mathbf{y}| = 0$.
  If $\mathbf{y} \neq \mathbf{0}$,
  then the equality holds when
  $\mathbf{x} = \frac{\mathbf{y}}{|\mathbf{y}|} \in \mathbb{R}^n$.
  (Here $|\mathbf{x}| = 1$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.6.}
\addcontentsline{toc}{subsection}{Exercise 9.6.}
\emph{If $f(0,0) = 0$ and
\[
  f(x,y) = \frac{xy}{x^2+y^2}
  \qquad \text{if }
  (x,y) \neq (0,0),
\]
prove that
$(D_1 f)(x,y)$ and $(D_2 f)(x,y)$ exist at every point of $\mathbb{R}^2$,
although $f$ is not continuous at $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that}
  \begin{equation*}
  (D_1 f)(x,y) =
    \begin{cases}
      0                              & \text{ if $(x,y)=(0,0)$}, \\
      \frac{y(y^2-x^2)}{(x^2+y^2)^2} & \text{ if $(x,y)\neq(0,0)$}.
    \end{cases}
  \end{equation*}
  Write
  \begin{align*}
    (D_1 f)(x,y)
    &= \lim_{t \to 0} \frac{f((x,y) + t(1,0)) - f(x,y)}{t} \\
    &= \lim_{t \to 0} \frac{f(x+t,y) - f(x,y)}{t}.
  \end{align*}
  If $(x,y) = (0,0)$,
  \[
    (D_1 f)(0,0)
    = \lim_{t \to 0} \frac{f(t,0) - f(0,0)}{t}
    = \lim_{t \to 0} \frac{0 - 0}{t}
    = 0.
  \]
  If $(x,y) \neq (0,0)$,
  \begin{align*}
    (D_1 f)(x,y)
    &= \lim_{t \to 0} \frac{f(x+t,y) - f(x,y)}{t} \\
    &= \lim_{t \to 0} \frac{\frac{(x+t)y}{(x+t)^2+y^2} - \frac{xy}{x^2+y^2}}{t} \\
    &= \lim_{t \to 0} \frac{y(y^2-x^2)-txy}{((x+t)^2+y^2)(x^2+y^2)} \\
    &= \frac{y(y^2-x^2)}{(x^2+y^2)^2}.
  \end{align*}

\item[(2)]
  \emph{Show that}
  \begin{equation*}
  (D_2 f)(x,y) =
    \begin{cases}
      0                              & \text{ if $(x,y)=(0,0)$}, \\
      \frac{x(x^2-y^2)}{(x^2+y^2)^2} & \text{ if $(x,y)\neq(0,0)$}.
    \end{cases}
  \end{equation*}
  Similar to (1).

\item[(3)]
  \emph{Show that $f$ is not continuous at $(0,0)$.}
  Note that
  \[
    \lim_{n \to \infty} f\left(\frac{1}{n},\frac{1}{n}\right)
    = \lim_{n \to \infty} \frac{\frac{1}{n} \cdot \frac{1}{n}}{\frac{1}{n^2}+\frac{1}{n^2}}
    = \lim_{n \to \infty} \frac{1}{2}
    = \frac{1}{2}
  \]
  and
  \[
    \lim_{n \to \infty} f\left(\frac{1}{n},0\right)
    = \lim_{n \to \infty} \frac{0}{\frac{1}{n^2} + 0}
    = \lim_{n \to \infty} 0
    = 0.
  \]
  Hence the limit $\lim_{(x,y) \to (0,0)} f(x,y)$ does not exist.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.7.}
\addcontentsline{toc}{subsection}{Exercise 9.7.}
\emph{Suppose that $f$ is a real-valued function
defined in an open set $E \subseteq \mathbb{R}^n$,
and that the partial derivatives $D_1 f, \ldots, D_n f$ are bounded in $E$.
Prove that $f$ is continuous in $E$.
(Hint: Proceed as in the proof of Theorem 9.21.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Since $D_j f$ is bounded in $E$, there is a real number $M_j$ such that
  $|D_j f| \leq M_j$ in $E$.
  Take $M = \max_{1 \leq j \leq n} M_j$ so that $|D_j f| \leq M$ in $E$ for all $1 \leq j \leq n$.

\item[(2)]
  Fix $\mathbf{x} \in E$ and $\varepsilon > 0$.
  Since $E$ is open,
  there is an open neighborhood
  \[
    B(\mathbf{x};r)
    = \{ \mathbf{x}+\mathbf{h} \in E : |\mathbf{h}| < r \}
    \subseteq E
  \]
  with
  \[
    0 < r < \frac{\varepsilon}{n(M+1)}.
  \]

\item[(3)]
  Write $\mathbf{h} = \sum h_j \mathbf{e}_j$, $|\mathbf{h}| < r$,
  put $\mathbf{v}_0 = \mathbf{0}$,
  and $\mathbf{v}_k = h_1 \mathbf{e}_1 + \cdots + h_k \mathbf{e}_k$
  for $1 \leq k \leq n$.
  Then
  \[
    f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x})
    = \sum_{j=1}^{n}
      [f(\mathbf{x}+\mathbf{v}_j) - f(\mathbf{x}+\mathbf{v}_{j-1})].
  \]
  Since $|\mathbf{v}_k| < r$ for $1 \leq k \leq n$ and
  since $B(\mathbf{x};r)$ is convex,
  the open interval with end points
  $\mathbf{x}+\mathbf{v}_{j-1}$ and $\mathbf{x}+\mathbf{v}_{j}$ lie in $B(\mathbf{x};r)$.
  Since $\mathbf{v}_{j} = \mathbf{v}_{j-1} - h_j\mathbf{e}_j$,
  the mean value theorem (Theorem 5.10) show that
  \[
    f(\mathbf{x}+\mathbf{v}_j) - f(\mathbf{x}+\mathbf{v}_{j-1})
    = h_j (D_j f)(\mathbf{x}+\mathbf{v}_{j-1} + \theta_j h_j \mathbf{e}_j)
  \]
  for some $\theta_j \in (0,1)$.

\item[(4)]
  Note that $\abs{h_j} \leq \abs{\mathbf{h}} < r < \frac{\varepsilon}{n(M+1)}$.
  Hence
  \begin{align*}
    \abs{f(\mathbf{x}+\mathbf{h}) - f(\mathbf{x})}
    &\leq \sum_{j=1}^{n}
      \abs{f(\mathbf{x}+\mathbf{v}_j) - f(\mathbf{x}+\mathbf{v}_{j-1})} \\
    &= \sum_{j=1}^{n}
      \abs{h_j} \abs{(D_j f)(\mathbf{x}+\mathbf{v}_{j-1} + \theta_j h_j \mathbf{e}_j)} \\
    &\leq \sum_{j=1}^{n}
      \frac{\varepsilon}{n(M+1)} \cdot M \\
    &< \varepsilon
  \end{align*}
  as $\abs{\mathbf{h}} < r < \frac{\varepsilon}{n(M+1)}$.
  Hence $f$ is continuous at all $\mathbf{x} \in E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.8.}
\addcontentsline{toc}{subsection}{Exercise 9.8.}
\emph{Suppose that $f$ is a differentiable real function in an open set
$E \subseteq \mathbb{R}^n$,
and that $f$ has a local maximum at a point $\mathbf{x} \in E$.
Prove that $f'(\mathbf{x}) = 0$.} \\

\emph{Proof (Theorem 5.8).}
\begin{enumerate}
\item[(1)]
Apply Theorem 5.8 to each $D_j f$ for $1 \leq j \leq n$.
Since $f$ has a local maximum at a point $\mathbf{x} \in E$,
there is an open neighborhood $B(\mathbf{x};r)$ of $\mathbf{x}$ in $E$
such that
\[
  f(\mathbf{y}) \leq f(\mathbf{x})
\]
for all $\mathbf{y} \in B(\mathbf{x};r)$.
Therefore,
\[
  f(\mathbf{x} + t\mathbf{e}_j) \leq f(\mathbf{x})
\]
for all $|t| < r$ and $1 \leq j \leq n$,
or $t \mapsto f(\mathbf{x} + t\mathbf{e}_j)$ has a local maximum at a point
$t = 0 \in (-r,r)$.


\item[(2)]
Since $f$ is a differentiable in $E$, each partial derivatives $D_j f$ exist (Theorem 9.21).
Hence Theorem 5.8 implies that $(D_j f)(\mathbf{x}) = 0$ for all $1 \leq j \leq n$.
So
\[
  f'(\mathbf{x})
  = [ (D_1 f)(\mathbf{x}) \cdots (D_k f)(\mathbf{x}) ]
  = [ 0 \cdots 0 ]
  = 0
\]
(as the zero matrix).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.9.}
\addcontentsline{toc}{subsection}{Exercise 9.9.}
\emph{If $\mathbf{f}$ is a differentiable mapping
of a connected open set $E \subseteq \mathbb{R}^n$,
and if $\mathbf{f}'(\mathbf{x}) = 0$ for every $\mathbf{x} \in E$,
prove that $\mathbf{f}$ is a constant in $E$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\mathbf{f}$ is \textbf{locally constant}.}
  Given any $\mathbf{x} \in E$.
  Since $E$ is open, there exists an open neighborhood $B(\mathbf{x};r)$ of $\mathbf{x}$
  such that $B(\mathbf{x};r) \subseteq E$ and $r > 0$.
  Corollary to Theorem 9.19 implies that
  $\mathbf{f}$ is a constant on $B(\mathbf{x};r)$, that is,
  $\mathbf{f}$ is locally constant.

\item[(2)]
  \emph{Show that $\mathbf{f}$ is constant
  if $\mathbf{f}$ is locally constant in a connected set $E \subseteq \mathbb{R}^n$.}
  Might assume that $E \neq \varnothing$. (Otherwise there is nothing to do.)
  Take some $\mathbf{x}_0 \in E$.
  \begin{enumerate}
  \item[(a)]
    Let
    \[
      U = \{ \mathbf{y} \in E : \mathbf{f}(\mathbf{y}) = \mathbf{f}(\mathbf{x}_0) \}.
    \]

  \item[(b)]
    $U$ is open since $\mathbf{f}$ is locally constant (by (1)).
    (Take any $\mathbf{y} \in U$.
    Since $\mathbf{f}$ is locally constant,
    there is an open neighborhood $B(\mathbf{y}) \subseteq E$
    of $\mathbf{y}$ such that $f(\mathbf{z}) = f(\mathbf{y}) = f(\mathbf{x}_0)$
    whenever $\mathbf{z} \in B(\mathbf{y})$.
    So that $B(\mathbf{y}) \subseteq U$, or $U$ is open.)

  \item[(c)]
    Besides, since $\mathbf{f}$ is continuous (Remarks 9.13(c)),
    the set $U$ is closed.
    (The proof is the same as Proof (Definition 2.18(d)) in Exercise 4.3.)

  \item[(d)]
    So $U$ is open and closed.
    Write $E = U \cup (E - U)$.
    Here $U$ and $E - U$ are both open and closed.
    Hence $U \cap \overline{E-U} = U \cap (E-U) = \varnothing$
    and $\overline{U} \cap (E-U) = U \cap (E-U) = \varnothing$.
    Note that $\mathbf{x}_0 \in U \neq \varnothing$.
    By the connectedness of $E$, $E-U = \varnothing$, or $E = U$,
    or $\mathbf{f}$ is constant on $E$.
  \end{enumerate}

  \emph{Note.}
  \emph{The only subsets of a connected set $E$ which are both open and closed
  are $E$ and $\varnothing$.}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.10.}
\addcontentsline{toc}{subsection}{Exercise 9.10.}
\emph{If $f$ is a real function defined in a convex open set $E \subseteq \mathbb{R}^n$,
such that $(D_1 f)(\mathbf{x})$ = 0 for every $\mathbf{x} \in E$,
prove that $f(\mathbf{x})$ depends only on $x_2, \ldots, x_n$.
Show that the convexity of $E$ can be replaced by a weaker condition,
but that some condition is required.
For example, if $n=2$ and $E$ is shaped like a horseshoe,
the statement may be false.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  It suffices to show that
  \[
    f(a, x_2, \ldots, x_n) = f(b, x_2, \ldots, x_n)
  \]
  whenever $\mathbf{x} = (a, x_2, \ldots, x_n) \in E$
  and $\mathbf{y} = (b, x_2, \ldots, x_n) \in E$
  if $(D_1 f)(\mathbf{x}) = 0$ in the convex open set $E$.

\item[(2)]
  Might assume that $a < b$.
  Since $g: t \mapsto f(t, x_2, \ldots, x_n)$ is a real continuous function
  on $[a,b]$ (by the openness of $E$) and differentiable in $(a,b)$
  (by the existence of $D_1 f$),
  \[
    g(b) - g(a) = (b - a)g'(\xi)
  \]
  for some $\xi \in (a,b)$.
  Note that
  \[
    g'(\xi) = (D_1 f)(\xi, x_2, \ldots, x_n) = 0
  \]
  by assumption. $g(b) = g(a)$ or $f(a, x_2, \ldots, x_n) = f(b, x_2, \ldots, x_n)$.

\item[(3)]
  (2) shows that the convexity of $E$ can be replaced by a weaker condition
  that $E \subseteq \mathbb{R}^n $ is convex in the first coordinate, say
  $E$ is open and
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda a + (1-\lambda) b, x_2, \ldots, x_n) \in E
  \]
  whenever
  $\mathbf{x} = (a, x_2, \ldots, x_n) \in E$,
  $\mathbf{y} = (b, x_2, \ldots, x_n) \in E$, and $0 < \lambda < 1$.

\item[(4)]
  \emph{Show that the convexity of $E$ or some weaker condition is required.}
  Define $f(x,y) = \mathrm{sgn}(x)$ on $E = \{ (x,y) \in \mathbb{R}^2 : x \neq 0 \}$.
  $E$ is open and $(D_1 f)(x,y) = 0$ in $E$.
  Note that $f(1989, 0) = 1$ and $f(-64, 0) = -1$,
  and thus $f(x,y)$ does not depend only on $y = 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.11.}
\addcontentsline{toc}{subsection}{Exercise 9.11.}
\emph{If $f$ and $g$ are differentiable real functions in $\mathbb{R}^n$,
prove that
\[
  \nabla(fg) = f \nabla g + g \nabla f
\]
and that
\[
  \nabla\left(\frac{1}{f}\right) = -\frac{1}{f^2} \nabla f
\]
whenever $f \neq 0$.} \\

\emph{Proof.}
Recall Example 9.18:
\[
  (\nabla(f))(\mathbf{x}) = \sum_{i=1}^{n}(D_i f)(\mathbf{x})\mathbf{e}_i.
\]

\begin{enumerate}
\item[(1)]
  \emph{Show that $\nabla(fg) = f \nabla g + g \nabla f$.}
  For any $\mathbf{x} \in \mathbb{R}^n$,
  \begin{align*}
    (\nabla(fg))(\mathbf{x})
    &= \sum_{i=1}^{n}(D_i(fg))(\mathbf{x})\mathbf{e}_i \\
    &= \sum_{i=1}^{n}(g (D_i f) + f (D_i g))(\mathbf{x})\mathbf{e}_i
      &(\text{Theorem 5.3(b)}) \\
    &= \sum_{i=1}^{n} \left[ g(\mathbf{x}) (D_i f)(\mathbf{x})
      + f(\mathbf{x}) (D_i g)(\mathbf{x}) \right]\mathbf{e}_i \\
    &= g(\mathbf{x}) \sum_{i=1}^{n} (D_i f)(\mathbf{x})\mathbf{e}_i
      + f(\mathbf{x}) \sum_{i=1}^{n} (D_i g)(\mathbf{x})\mathbf{e}_i \\
    &= g(\mathbf{x}) (\nabla f)(\mathbf{x})
      + f(\mathbf{x}) (\nabla g)(\mathbf{x}) \\
    &= (f \nabla g + g \nabla f)(\mathbf{x}).
  \end{align*}

\item[(2)]
  \emph{Show that $$\nabla\left(\frac{1}{f}\right) = -\frac{1}{f^2} \nabla f$$
  whenever $f \neq 0$.}
  Note that $\nabla(1) = 0$
  since
  \[
    \nabla(1)(\mathbf{x})
    = \sum (D_i 1)(\mathbf{x})\mathbf{e}_i
    = \sum (0)(\mathbf{x})\mathbf{e}_i
    = \sum 0 \mathbf{e}_i
    = 0.
  \]
  Hence as $f \neq 0$, we have
  \begin{align*}
    0
    &= \nabla(1) \\
    &= \nabla\left(f \frac{1}{f}\right)
      & (f \neq 0) \\
    &= f \nabla\left(\frac{1}{f}\right) + \frac{1}{f} \nabla f
      & ((1)),
  \end{align*}
  or $\nabla\left(\frac{1}{f}\right) = -\frac{1}{f^2} \nabla f$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.12.}
\addcontentsline{toc}{subsection}{Exercise 9.12.}
\emph{Fix two real numbers $a$ and $b$, $0<a<b$.
Define a mapping $\mathbf{f}=(f_1,f_2,f_3)$ of $\mathbb{R}^2$ into $\mathbb{R}^3$ by
\begin{align*}
  f_1(s,t) &= (b + a\cos s)\cos t \\
  f_2(s,t) &= (b + a\cos s)\sin t \\
  f_3(s,t) &= a \sin s.
\end{align*}
Describe the range $K$ if $\mathbf{f}$.
(It is a certain compact subset of $\mathbb{R}^3$.)}
\begin{enumerate}
\item[(a)]
  \emph{Show that there are exactly $4$ points $\mathbf{p} \in K$ such that
  \[
    (\nabla f_1)(\mathbf{f}^{-1}(\mathbf{p})) = \mathbf{0}.
  \]
  Find these points.}

\item[(b)]
  \emph{Determine the set of all $\mathbf{q} \in K$ such that
  \[
    (\nabla f_3)(\mathbf{f}^{-1}(\mathbf{q})) = \mathbf{0}.
  \]}
\item[(c)]
  \emph{Show that
  one of the point $\mathbf{p}$ found in part (a) corresponds to a local maximum of $f_1$,
  one corresponds to a local minimum,
  and that the other two are neither (they are so-called ``saddle points'').
  Which of the points $\mathbf{q}$ found in part (b) corresponds to maxima or minima?}

\item[(d)]
  \emph{Let $\lambda$ be an irrational real number, and define
  $\mathbf{g}(t) = \mathbf{f}(t,\lambda t)$.
  Prove that $\mathbf{g}$ is a one-to-one mapping of $\mathbb{R}^1$
  onto a dense subset of $K$.
  Prove that
  \[
    \abs{ \mathbf{g}'(t) }^2= a^2 + \lambda^2(b + a \cos t)^2.
  \]}
\end{enumerate}



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  $K$ is a torus,
  where
  \begin{enumerate}
  \item[(a)]
    $s, t$ are angles which make a full circle
    (so that their values start and end at the same point).
  \item[(b)]
    $b$ is the distance from the center of the tube to the center of the torus.
  \item[(c)]
    $a$ is the radius of the tube.
  \end{enumerate}

\item[(2)]
  \emph{Show that $K$ is compact.}
  Since $\sin$ and $\cos$ are periodic (with period $2\pi$),
  $K = \mathbf{f}\left([0,2\pi]^2\right)$ is compact
  by the compactness of $[0,2\pi]^2$ and the continuity of $\mathbf{f}$ (Theorem 4.14).

\end{enumerate}
$\Box$ \\



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    (\nabla f_1)(\mathbf{x})
    &= (D_1 f_1)(\mathbf{x})\mathbf{e}_1 + (D_2 f_1)(\mathbf{x})\mathbf{e}_2 \\
    &= ((D_1 f_1)(s,t), (D_2 f_1)(s,t)) \\
    &= (-a \sin s \cos t, -(b + a \cos t) \sin t)
  \end{align*}
  So
  $(\nabla f_1)(\mathbf{x}) = \mathbf{0}$
  if and only if
  \begin{align*}
    0 &= -a \sin s \cos t, \\
    0 &= -(b + a \cos t) \sin t.
  \end{align*}

\item[(2)]
  Note that $b + a \cos t > 0$ for any $b > a > 0$ and $t \in \mathbb{R}^1$.
  Hence $(\nabla f_1)(\mathbf{x}) = \mathbf{0}$ if and only if $\sin t = \sin s = 0$.
  Therefore,
  $\mathbf{p} = (\pm (b \pm a),0,0)$,
  or there are exactly $4$ points
  $\mathbf{p} = (b + a, 0, 0)$,
  $(b - a, 0, 0)$,
  $(-b - a, 0, 0)$, or
  $(-b + a, 0, 0) \in K$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    (\nabla f_3)(\mathbf{x})
    &= (D_1 f_3)(\mathbf{x})\mathbf{e}_1 + (D_2 f_3)(\mathbf{x})\mathbf{e}_2 \\
    &= ((D_1 f_3)(s,t), (D_2 f_3)(s,t)) \\
    &= (a \cos s, 0)
  \end{align*}
  So
  $(\nabla f_1)(\mathbf{x}) = \mathbf{0}$
  if and only if $\cos s = 0$ (since $a > 0$).

\item[(2)]
  Therefore,
  $\mathbf{q} = (b\cos t, b\sin t, \pm a)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Since $-1 \leq \cos s \leq 1$ and $-1 \leq \cos t \leq 1$,
  $-b-a \leq f_1(s,t) \leq b+a$.

  \begin{enumerate}
  \item[(a)]
    $(b+a, 0, 0)$ corresponds to a local maximum of $f_1$.

  \item[(b)]
    $(-b-a, 0, 0)$ corresponds to a local minimum of $f_1$.

  \item[(c)]
    $(b - a, 0, 0)$ and $(-b + a, 0, 0)$ are saddle points
    by considering any open neighborhood of $(s,t)$
    at which $\cos s = \pm 1$ and $\cos t = \mp 1$.
  \end{enumerate}

\item[(2)]
  Since $-1 \leq \sin s \leq 1$,
  $-a \leq f_3(s,t) \leq a$.
  \begin{enumerate}
  \item[(a)]
    $(b\cos t, b\sin t, a)$ corresponds to a local maximum of $f_3$.

  \item[(b)]
    $(b\cos t, b\sin t, -a)$ corresponds to a local minimum of $f_3$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \[
    \mathbf{g}(t)
    = \mathbf{f}(t,\lambda t)
    = ( (b+a\cos t)\cos(\lambda t), (b+a\cos t)\sin(\lambda t), a\sin t ).
  \]

\item[(2)]
  \emph{Show that $\mathbf{g}$ is a one-to-one mapping of $\mathbb{R}^1$.}
  It suffices to show that $\mathbf{g}(t) = \mathbf{g}(s)$ implies $t = s$.

  \begin{enumerate}
  \item[(a)]
    By $\mathbf{g}(t) = \mathbf{g}(s)$,
    \begin{align*}
      (b+a\cos t)\cos(\lambda t) &= (b+a\cos s)\cos(\lambda s) \tag{I}, \\
      (b+a\cos t)\sin(\lambda t) &= (b+a\cos s)\sin(\lambda s) \tag{II}, \\
      a\sin t &= a\sin s \tag{III}.
    \end{align*}
    (I) and (II) imply that $\cos t = \cos s$ (since $b > a > 0$).
    (III) implies that $\sin t = \sin s$.
    Hence
    \[
      t = s+2n\pi
    \]
    for some integer $n$.

  \item[(b)]
    Again, (I) and (II) imply that
    \[
      \cos(\lambda t) = \cos(\lambda s)
      \qquad \text{ and } \qquad
      \sin(\lambda t) = \sin(\lambda s).
    \]
    Hence
    \[
      \lambda t = \lambda s + 2m\pi
    \]
    for some integer $m$.
    By assumption that $t = s+2n\pi$, we have $m = n \lambda$.
    Since $\lambda$ is irrational, $m = n = 0$.
    Therefore $t = s$ holds.
  \end{enumerate}

\item[(3)]
  \emph{Show that $\mathbf{g}(\mathbb{R}^1)$ is dense in $K$.}
  Note that $\mathbf{f}\left([0,2\pi]^2\right) = K$.
  Use the notations $\{x\}$ in Exercise 4.16.
  It suffices to show that the set
  \[
    \left\{ \left(
      2\pi\left\{\frac{t}{2\pi}\right\},
      2\pi\left\{\frac{\lambda t}{2\pi}\right\}
    \right) : t \in \mathbb{R}^1 \right\}
  \]
  is dense in $[0,2\pi]^2$ (Exercise 4.4),
  or to show that
  \[
    \left\{ \left(\{ t \}, \{ \lambda t \} \right) : t \in \mathbb{R}^1 \right\}
  \]
  is dense in $[0,1]^2$, which is the conclusion of Exercise 4.25(b).

\item[(4)]
  \emph{Show that $\abs{ \mathbf{g}'(t) }^2= a^2 + \lambda^2(b + a \cos t)^2$.}
  By
  \begin{align*}
    \mathbf{g}'(t)
    = \left( \right.
        &-a\sin t \cos(\lambda t)-\lambda(b+a\cos t)\sin(\lambda t), \\
        &-a\sin t \sin(\lambda t)+\lambda(b+a\cos t)\cos(\lambda t), \\
        &a\cos t
      \left. \right),
  \end{align*}
  \begin{align*}
    \abs{ \mathbf{g}'(t) }^2
    =& \mathbf{g}'(t) \cdot \mathbf{g}'(t) \\
    =& (-a\sin t \cos(\lambda t)-\lambda(b+a\cos t)\sin(\lambda t))^2 \\
      &+ (-a\sin t \sin(\lambda t)+\lambda(b+a\cos t)\cos(\lambda t))^2 + (a\cos t)^2 \\
    =& \underbrace{a^2 \sin^2 t \cos^2(\lambda t) + a^2 \cos^2 t}_{= a^2} \\
      &+ \underbrace{\lambda^2(b+a\cos t)^2\sin^2(\lambda t)
        + \lambda^2(b+a\cos t)^2\cos^2(\lambda t)}_{= \lambda^2(b+a\cos t)^2} \\
      &+ 2a\lambda \sin t\cos(\lambda t)\lambda(b+a\cos t)\sin(\lambda t) \\
      &- 2a\lambda \sin t\sin(\lambda t)\lambda(b+a\cos t)\cos(\lambda t) \\
    =& a^2 + \lambda^2(b+a\cos t)^2.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.13.}
\addcontentsline{toc}{subsection}{Exercise 9.13.}
\emph{Suppose $\mathbf{f}$ is a differentiable mapping of $\mathbb{R}^1$ into
$\mathbb{R}^3$ such that $|\mathbf{f}(t)| = 1$ for every $t$.
Prove that $\mathbf{f}'(t) \cdot \mathbf{f}(t) = 0$.
Interpret this result geometrically.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write $\mathbf{f} = (f_1, f_2, f_3)$ as a vector-valued function.
  By Remarks 5.16, $\mathbf{f}$ is differentiable if and only if each $f_1, f_2, f_3$
  is differentiable. So $\mathbf{f}' = (f_1', f_2', f_3)'$.
  Hence
  \begin{align*}
    &\text{$|\mathbf{f}(t)| = 1$ for every $t$} \\
    \Longleftrightarrow&
    \mathbf{f}(t) \cdot \mathbf{f}(t) = 1 \\
    \Longleftrightarrow&
    f_1(t)^2 + f_2(t)^2 + f_3(t)^2 = 1 \\
    \Longrightarrow&
    2 f_1(t) f_1'(t) + 2 f_2(t) f_2'(t) + 2 f_3(t) f_3'(t) = 0 \\
    \Longleftrightarrow&
    f_1(t) f_1'(t) + f_2(t) f_2'(t) + f_3(t) f_3'(t) = 0 \\
    \Longleftrightarrow&
    (f_1(t), f_2(t), f_3(t)) \cdot (f_1'(t), f_2'(t), f_3'(t)) = 0 \\
    \Longleftrightarrow&
    \mathbf{f}(t) \cdot \mathbf{f}'(t) = \mathbf{f}'(t) \cdot \mathbf{f}(t) = 0.
  \end{align*}

\item[(2)]
  The vector $\mathbf{f}'(t)$ is called the
  \emph{\textbf{tangent vector}} (or \emph{\textbf{velocity vector}})
  of $\mathbf{f}$ at $t$.
  Geometrically,
  given any mapping $\mathbf{f}$ lying on the sphere $S^2$,
  its tangent vector at $t$ is lying on the tangent plane of $S^2$ at $t$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.14.}
\addcontentsline{toc}{subsection}{Exercise 9.14.}
\emph{Define $f(0,0) = 0$ and}
\[
  f(x,y) = \frac{x^3}{x^2 + y^2}
  \qquad \text{if }
  (x,y) \neq (0,0).
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that $D_1 f$ and $D_2 f$ are bounded functions in $\mathbb{R}^2$.
  (Hence $f$ is continuous.)}

\item[(b)]
  \emph{Let $\mathbf{u}$ be any unit vector in $\mathbb{R}^2$.
  Show that the directional derivative
  $(D_{\mathbf{u}}f)(0,0)$ exists, and that its absolute value is at most $1$.}

\item[(c)]
  \emph{Let $\gamma$ be a differentiable mapping of $\mathbb{R}^1$ into $\mathbb{R}^2$
  (in other words, $\gamma$ is a differentiable curve in $\mathbb{R}^2$),
  with $\gamma(t) = (0,0)$ and $\gamma'(t) \neq (0,0)$ for any $t \in \mathbb{R}^1$.
  Put $g(t) = f(\gamma(t))$ and prove that
  $g$ is differentiable for every $t \in \mathbb{R}^1$.
  If $\gamma \in \mathscr{C}'$, prove that $g \in \mathscr{C}'$.}

\item[(d)]
  \emph{In spite of this, prove that $f$ is not differentiable at $(0,0)$.} \\

\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that}
    \begin{equation*}
    (D_1 f)(x,y) =
      \begin{cases}
        1
          & \text{ if $(x,y)=(0,0)$}, \\
        \frac{x^2(x^2 +3y^2)}{(x^2+y^2)^2}
          & \text{ if $(x,y)\neq(0,0)$}.
      \end{cases}
    \end{equation*}
    If $(x,y) = (0,0)$,
    \[
      (D_1 f)(0,0)
      = \lim_{t \to 0} \frac{f(t,0) - f(0,0)}{t}
      = \lim_{t \to 0} \frac{t - 0}{t}
      = 1.
    \]
    If $(x,y) \neq (0,0)$,
    \begin{align*}
      (D_1 f)(x,y)
      &= \lim_{t \to 0} \frac{f(x+t,y) - f(x,y)}{t} \\
      &= \lim_{t \to 0} \frac{\frac{(x+t)^3}{(x+t)^2+y^2} - \frac{x^3}{x^2+y^2}}{t} \\
      &= \lim_{t \to 0}
        \frac{x^2(x^2+3y^2) + tx(2x^2+3y^2) + t^2(x^2+y^2)}{((x+t)^2+y^2)(x^2+y^2)} \\
      &= \frac{x^2(x^2 +3y^2)}{(x^2+y^2)^2}.
    \end{align*}
    (Or differentiate directly.)

\item[(2)]
  \emph{Show that $(D_1 f)(x,y)$ is bounded.}
  It suffices to show that $(D_1 f)(x,y)$ is bounded if $(x,y) \neq (0,0)$.
  Write $x = r\cos\theta$ and $y = r\sin\theta$ in the polar coordinates.
  (Here $r > 0$.)
  Hence
  \[
    (D_1 f)(x,y)
    = \frac{x^2(x^2 +3y^2)}{(x^2+y^2)^2}
    = \cos^2\theta (\cos^2\theta + 3\sin^2\theta)
  \]
  is bounded by $1\cdot(1+3) = 4$.

\item[(3)]
  \emph{Show that}
    \begin{equation*}
    (D_2 f)(x,y) =
      \begin{cases}
        0
          & \text{ if $(x,y)=(0,0)$}, \\
        \frac{-2x^3y}{(x^2+y^2)^2}
          & \text{ if $(x,y)\neq(0,0)$}.
      \end{cases}
    \end{equation*}
    If $(x,y) = (0,0)$,
    \[
      (D_2 f)(0,0)
      = \lim_{t \to 0} \frac{f(0,t) - f(0,0)}{t}
      = \lim_{t \to 0} \frac{0 - 0}{t}
      = 0.
    \]
    If $(x,y) \neq (0,0)$,
    \begin{align*}
      (D_2 f)(x,y)
      &= \lim_{t \to 0} \frac{f(x,y+t) - f(x,y)}{t} \\
      &= \lim_{t \to 0} \frac{\frac{x^3}{x^2+(y+t)^2} - \frac{x^3}{x^2+y^2}}{t} \\
      &= \lim_{t \to 0}
        \frac{-2x^3y - tx^3}{(x^2+(y+t)^2)(x^2+y^2)} \\
      &= \frac{-2x^3y}{(x^2+y^2)^2}.
    \end{align*}
    (Or differentiate directly.)

\item[(4)]
  \emph{Show that $(D_2 f)(x,y)$ is bounded.}
  Similar to (2).

\item[(5)]
  \emph{Show that $f$ is continuous.}
  Apply Exercise 9.7 to (2)(4).
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Write $\mathbf{u} = (u_1, u_2)$.
  The formula
  \[
    (D_{\mathbf{u}}f)(0,0) = (D_1 f)(0,0)u_1 + (D_2 f)(0,0)u_2 = u_1
  \]
  might be false since we don't know if $f$ is differentiable or not.
  Actually, we will show that $(D_{\mathbf{u}}f)(0,0) = u_1^3 \neq u_1$.

\item[(2)]
  \begin{align*}
    (D_{\mathbf{u}}f)(0,0)
    &= \lim_{t \to 0} \frac{f(tu_1,tu_2) - f(0,0)}{t} \\
    &= \lim_{t \to 0} \frac{\frac{t^3u_1^3}{t^2u_1^2 + t^2u_2^2} - 0}{t} \\
    &= \lim_{t \to 0} u_1^3
      & (|\mathbf{u}| = 1) \\
    &= u_1^3.
  \end{align*}
  Also $\abs{(D_{\mathbf{u}}f)(0,0)} = \abs{u_1}^3 \leq 1$
  since $|\mathbf{u}| = 1$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Given any $t \in \mathbb{R}^1$.
  \[
    g'(t)
    = \lim_{x \to t} \frac{g(x)-g(t)}{x-t}
    = \lim_{x \to t} \frac{f(\gamma(x))-f(\gamma(t))}{x-t}.
  \]
  Write $\gamma(t) = (\gamma_1(t),\gamma_2(t))$.

\item[(2)]
  Suppose that $\gamma(t) \neq (0,0)$.
  Since $\gamma$ is differentiable, $\gamma$ is continuous.
  So there exists an open neighborhood $B(t) \subseteq \mathbb{R}^1$ of $t$
  such that $\gamma(x) \neq (0,0)$ whenever $x \in B(t)$.
  Hence
  \begin{align*}
    g'(t)
    &= \lim_{x \to t} \frac{\frac{\gamma_1(x)^3}{\gamma_1(x)^2+\gamma_2(x)^2}
      -\frac{\gamma_1(t)^3}{\gamma_1(t)^2+\gamma_2(t)^2}}{x-t} \\
    &= \frac{d}{dt}\left(\frac{\gamma_1(t)^3}{\gamma_1(t)^2+\gamma_2(t)^2}\right) \\
    &= \frac{3\gamma_1(t)^2\gamma_1'(t)}{\gamma_1(t)^2+\gamma_2(t)^2}
      - \frac{\gamma_1(t)^3(2\gamma_1(t)\gamma_1'(t)+2\gamma_2(t)\gamma_2'(t))}
        {(\gamma_1(t)^2+\gamma_2(t)^2)^2}.
  \end{align*}
  exists
  since $\gamma_1$ and $\gamma_2$ are differentiable.

\item[(3)]
  Suppose that $\gamma(t) = (0,0)$ and thus $\gamma'(t) \neq (0,0)$.
  So
  \[
    g'(t) = \lim_{x \to t} \frac{f(\gamma(x))}{x-t}
  \]
  Note that $\gamma(x) \neq (0,0)$ in some open neighborhood of $t$
  since
  \[
    \lim_{\substack{x \to t \\ \gamma(x) = (0,0)}} \frac{\gamma(x) - \gamma(t)}{x-t}
    = (0,0),
  \]
  contrary to the assumption that $\gamma'(t) \neq (0,0)$.
  Note that $\gamma_1(t) = \gamma_2(t) = 0$.
  So
  \begin{align*}
    g'(t)
    &= \lim_{x \to t} \frac{f(\gamma(x))}{x-t} \\
    &= \lim_{x \to t}
      \frac{\gamma_1(x)^3}{\gamma_1(x)^2+\gamma_2(x)^2}
      \cdot \frac{1}{x-t} \\
    &= \lim_{x \to t}
      \frac{(\gamma_1(x)-\gamma_1(t))^3}
        {(\gamma_1(x)-\gamma_1(t))^2+(\gamma_2(x)-\gamma_2(t))^2}
      \cdot \frac{1}{x-t} \\
    &= \lim_{x \to t}
      \frac{\left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^3}
        {\left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^2
        +\left(\frac{\gamma_2(x)-\gamma_2(t)}{x-t}\right)^2} \\
    &= \frac{\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}
  \end{align*}
  since $\gamma'(t) \neq (0,0)$.

\item[(4)]
By (2)(3), $g'(t)$ exists and
\begin{equation*}
  g'(t) =
    \begin{cases}
      \frac{3\gamma_1(t)^2\gamma_1'(t)}{\gamma_1(t)^2+\gamma_2(t)^2}
        - \frac{\gamma_1(t)^3(2\gamma_1(t)\gamma_1'(t)+2\gamma_2(t)\gamma_2'(t))}
          {(\gamma_1(t)^2+\gamma_2(t)^2)^2}
        & \text{ if $\gamma(t) \neq (0,0)$}, \\
      \frac{\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}
        & \text{ if $\gamma(t) = (0,0)$}.
    \end{cases}
\end{equation*}

\item[(5)]
  Now suppose $\gamma \in \mathscr{C}'$.
  To show $g' \in \mathscr{C}'$, it suffices to show that
  \[
    \lim_{x \to t} g'(x) = g'(t)
  \]
  if $\gamma(t) = (0,0)$ since $g'(t)$ is always continuous if $\gamma(t) \neq (0,0)$.
  Here all $\gamma_1, \gamma_2, \gamma_1', \gamma_2'$ are continuous
  and $\gamma_1(t)^2+\gamma_2(t)^2 \neq 0$ by assumption.
  So
  \begin{align*}
    &\lim_{x \to t}
      \frac{3\gamma_1(x)^2\gamma_1'(x)}{\gamma_1(x)^2+\gamma_2(x)^2} \\
    =& \lim_{x \to t}
      \frac{3\left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^2\gamma_1'(x)}
      {\left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^2
        +\left(\frac{\gamma_2(x)-\gamma_2(t)}{x-t}\right)^2} \\
    =& \frac{3\gamma_1'(t)^2 \cdot \gamma_1'(t)}{\gamma_1'(t)^2+\gamma_2'(t)^2} \\
    =& \frac{3\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}
  \end{align*}
  and similarly
  \begin{align*}
    &\lim_{x \to t}
      \frac{\gamma_1(t)^3(2\gamma_1(t)\gamma_1'(t)+2\gamma_2(t)\gamma_2'(t))}
        {(\gamma_1(t)^2+\gamma_2(t)^2)^2} \\
    =& \lim_{x \to t}
      \frac{\left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^3
        \left( 2\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\gamma_1'(t)
        +2\frac{\gamma_2(x)-\gamma_2(t)}{x-t}\gamma_2'(t) \right)}
      {\left( \left(\frac{\gamma_1(x)-\gamma_1(t)}{x-t}\right)^2
        +\left(\frac{\gamma_2(x)-\gamma_2(t)}{x-t}\right)^2 \right)^2} \\
    =& \frac{\gamma_1'(t)^3 \cdot (2\gamma_1'(t)\gamma_1'(t) + 2\gamma_2'(t)\gamma_2'(t))}
      {(\gamma_1'(t)^2+\gamma_2'(t)^2)^2} \\
    =& \frac{2\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}.
  \end{align*}
  Hence
  \[
    \lim_{x \to t} g'(x)
    = \frac{3\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}
      - \frac{2\gamma_1'(t)^3}{\gamma_1'(t)^2+\gamma_2'(t)^2}
    = g'(t).
  \]

\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
  (Reductio ad absurdum)
  If $f$ were differentiable,
  then
  \[
    (D_{\mathbf{u}}f)(0,0) = (D_1 f)(0,0)u_1 + (D_2 f)(0,0)u_2 = u_1
  \]
  (Formula (40) in Chapter 9), contrary to (b)
  if we take $\mathbf{u} = \left(\frac{1}{64}, \frac{\sqrt{4095}}{64} \right)$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.15.}
\addcontentsline{toc}{subsection}{Exercise 9.15.}
\emph{Define $f(0,0) = 0$, and put
\[
  f(x,y) = x^2+y^2-2x^2y-\frac{4x^6y^2}{(x^4+y^2)^2}
\]
if $(x,y) \neq (0,0)$.}
\begin{enumerate}
\item[(a)]
  \emph{Prove, for all $(x,y) \in \mathbb{R}^2$, that
  \[
    4x^4y^2 \leq (x^4+y^2)^2.
  \]
  Conclude that $f$ is continuous.}

\item[(b)]
  \emph{For $0 \leq \theta \leq 2\pi$, $-\infty < t < \infty$, define
  \[
    g_{\theta}(t) = f(t\cos\theta, t\sin\theta).
  \]
  Show that $g_{\theta}(0) = 0$, $g_{\theta}'(0) = 0$, $g_{\theta}''(0) = 2$.
  Each $g_{\theta}$ has therefore a strict local minimum at $t=0$.
  In other words, the restriction of $f$ to each line through $(0,0)$
  has a strict local minimum at $(0,0)$.}

\item[(c)]
  \emph{Show that $(0,0)$ is nevertheless not a local minimum for $f$,
  since $f(x,x^2)=-x^4$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since $t^2 \geq 0$ for all $t \in \mathbb{R}^1$,
  \[
    (x^4+y^2)^2 - 4x^4y^2 = (x^4-y^2)^2 \geq 0.
  \]
  Hence $4x^4y^2 \leq (x^4+y^2)^2$.
\item[(2)]
  $f(x,y)$ is continuous at $(x,y) \neq (0,0)$.
  Besides,
  \begin{align*}
    \abs{f(x,y)}
    &= \abs{x^2+y^2-2x^2y-\frac{4x^6y^2}{(x^4+y^2)^2}} \\
    &\leq \abs{x^2}+\abs{y^2}+\abs{2x^2y}+\abs{x^2}\abs{\frac{4x^4y^2}{(x^4+y^2)^2}} \\
    &\leq \abs{x^2}+\abs{y^2}+\abs{2x^2y}+\abs{x^2}.
  \end{align*}
  Hence
  $\abs{x^2}+\abs{y^2}+\abs{2x^2y}+\abs{x^2} \to 0$ as $(x,y) \to (0,0)$,
  or
  \[
    \lim_{(x,y) \to (0,0)} \abs{f(x,y)} = 0 = f(0,0),
  \]
  or $\lim_{(x,y) \to (0,0)} f(x,y) = f(0,0)$,
  or $f(x,y)$ is continuous at $(0,0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \begin{equation*}
    g_{\theta}(t) =
      \begin{cases}
        t^2 - 2t^3 \cos^2\theta\sin\theta
          - \frac{4t^4\cos^6\theta\sin^2\theta}{(t^2\cos^4\theta+\sin^2\theta)^2}
          & \text{ if $t \neq 0$}, \\
        0
          & \text{ if $t = 0$}.
      \end{cases}
  \end{equation*}
  (Note that $\frac{4t^4\cos^6\theta\sin^2\theta}{(t^2\cos^4\theta+\sin^2\theta)^2}$
  is undefined as $t = 0$ and $\sin\theta = 0$.)

\item[(2)]
  $g_{\theta}(0) = 0$ by definition.

\item[(3)]
  \emph{Show that $g_{\theta}'(0) = 0$ for any $\theta \in [0,2\pi]$.}
  If $\sin\theta \neq 0$ ($\theta \neq 0, \pi, 2\pi$), then
  \begin{align*}
    g_{\theta}'(0)
    &= \lim_{t \to 0}
      \frac{t^2 - 2t^3 \cos^2\theta\sin\theta
        - \frac{4t^4\cos^6\theta\sin^2\theta}{(t^2\cos^4\theta+\sin^2\theta)^2} - 0}{t} \\
    &= \lim_{t \to 0}
      \left( t - 2t^2 \cos^2\theta\sin\theta
        - \frac{4t^3\cos^6\theta\sin^2\theta}{(t^2\cos^4\theta+\sin^2\theta)^2} \right) \\
    &= 0.
  \end{align*}
  If $\sin\theta = 0$, then
  \[
    g_{\theta}'(0)
    = \lim_{t \to 0} \frac{t^2 - 0}{t}
    = \lim_{t \to 0} t
    = 0.
  \]

\item[(4)]
  Combine (3) and a direct calculation for the case $t \neq 0$, we have
  \begin{equation*}
    g_{\theta}'(t) =
      \begin{cases}
        2t - 6t^2\cos^2\theta\sin\theta
          - \frac{16t^3\cos^6\theta\sin^4\theta}{(t^2\cos^4\theta+\sin^2\theta)^3}
          & \text{ if $t \neq 0$}, \\
        0
          & \text{ if $t = 0$}.
      \end{cases}
  \end{equation*}

\item[(5)]
  \emph{Show that $g_{\theta}''(0) = 2$ for any $\theta \in [0,2\pi]$.}
  If $\sin\theta \neq 0$ ($\theta \neq 0, \pi, 2\pi$), then
  \begin{align*}
    g_{\theta}''(0)
    &= \lim_{t \to 0}
      \frac{2t - 6t^2\cos^2\theta\sin\theta
        - \frac{16t^3\cos^6\theta\sin^4\theta}{(t^2\cos^4\theta+\sin^2\theta)^3} - 0}{t} \\
    &= \lim_{t \to 0}
      \left( t - 6t\cos^2\theta\sin\theta
        - \frac{16t^2\cos^6\theta\sin^4\theta}{(t^2\cos^4\theta+\sin^2\theta)^3} \right) \\
    &= 2.
  \end{align*}
  If $\sin\theta = 0$, then
  \[
    g_{\theta}''(0)
    = \lim_{t \to 0} \frac{2t - 0}{t}
    = \lim_{t \to 0} 2
    = 2.
  \]

\item[(6)]
  Since $g_{\theta}''(0) > 0$ and $g_{\theta}'(0) = 0$,
  $g_{\theta}$ has a strict local minimum at $t=0$.
  As $\theta$ is fixed, $f$ is restricted to some line through $(0,0)$.
  Hence, such restriction of $f$ has a strict local minimum at $t=0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
  Since $f(x,x^2) = -x^4 \leq 0 = f(0,0)$ in any open neighborhood of $(0,0)$,
  $f(0,0) = 0$ cannot be a local minimum for $f$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.16.}
\addcontentsline{toc}{subsection}{Exercise 9.16.}
\emph{Show that the continuity of $\mathbf{f}'$ at the point $\mathbf{a}$
is needed in the inverse function theorem, even in the case $n=1$: If
\[
  f(t) = t + 2t^2 \sin \frac{1}{t}
\]
for $t \neq 0$, and $f(0) = 0$, then $f'(0) = 1$, $f'$ is bouneded in $(-1,1)$,
but $f$ is not one-to-one in any neighborhood of $0$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that}
  \begin{equation*}
    f'(t) =
      \begin{cases}
        1 + 4t\sin\frac{1}{t} - 2\cos\frac{1}{t}
          & \text{ if $t \neq 0$}, \\
        1
          & \text{ if $t = 0$}.
      \end{cases}
  \end{equation*}
  It suffices to show that $f'(0) = 1$.
  In fact,
  \[
    f'(0)
    = \lim_{t \to 0} \frac{t + 2t^2 \sin \frac{1}{t} - 0}{t - 0}
    = \lim_{t \to 0} \left( 1 + 2t \sin \frac{1}{t} \right)
    = 1
  \]
  (since $\sin \frac{1}{t}$ is bounded and $2t \to 0$ as $t \to 0$).
\end{enumerate}

\emph{Note.} $f'(t)$ is not continuous at $t = 0$.

\begin{enumerate}
\item[(2)]
  \emph{Show that $f'$ is bouneded in $(-1,1)$.}
  \[
    \abs{f'(t)}
    \leq 1 + 4\abs{t}\abs{\sin\frac{1}{t}} + 2\abs{\cos\frac{1}{t}}
    \leq 1 + 4 + 2
    = 7
  \]
  if $t \neq 0$.
  Hence $f'$ is bounded by $7$ in $(-1,1)$.

\item[(3)]
  \emph{Show that $f$ is not one-to-one in any neighborhood of $0$.}
  Take
  \[
    x_n = \frac{1}{2n\pi}
    \qquad
    \text{ and }
    \qquad
    y_n = \frac{1}{2n\pi+\pi}
  \]
  for $n = 1,2,3,\ldots$.
  So that
  \[
    f'(x_n) = -1 < 0
    \qquad
    \text{ and }
    \qquad
    f'(y_n) = 3 > 0.
  \]
  Since $f'(t)$ is continuous if $t \neq 0$,
  there exists $\xi_n \in (y_n, x_n)$ such that $f'(\xi_n) = 0$ (Theorem 4.23).
  Then Theorem 5.11 implies that $f$ has a local maximum at $\xi_n$,
  that is, $f$ is not one-to-one in the interval $[y_n, x_n]$
  (by applying Theorem 4.23 again).
  Since $x_n \to 0$ and $y_n \to 0$ as $n \to \infty$,
  $f$ is not one-to-one in any neighborhood of $0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.17.}
\addcontentsline{toc}{subsection}{Exercise 9.17.}
\emph{Let $\mathbf{f} = (f_1,f_2)$ be the mapping of $\mathbb{R}^2$ into $\mathbb{R}^2$
given by}
\[
  f_1(x,y) = e^x \cos y,
  \qquad
  f_2(x,y) = e^x \sin y.
\]
\begin{enumerate}
\item[(a)]
  \emph{What is the range of $\mathbf{f}$?}

\item[(b)]
  \emph{Show that the Jacobian of $\mathbf{f}$ is not zero at any point of $\mathbb{R}^2$.
  Thus every point of $\mathbb{R}^2$ has a neighborhood in which $\mathbf{f}$ is one-to-one.
  Nevertheless, $\mathbf{f}$ is not one-to-one on $\mathbb{R}^2$.}

\item[(c)]
  \emph{Put $\mathbf{a} = \left(0, \frac{\pi}{3}\right)$,
  $\mathbf{b} = \mathbf{f}(\mathbf{a})$, let $\mathbf{g}$ be the continuous inverse of $\mathbf{f}$,
  defined in a neighborhood of $\mathbf{b}$ such that $\mathbf{g}(\mathbf{b}) = \mathbf{a}$.
  Find an explicit formula for $\mathbf{g}$,
  compute $\mathbf{f}'(\mathbf{a})$ and $\mathbf{g}'(\mathbf{b})$,
  and verify the formula}
  \[
    \mathbf{g}'(\mathbf{y})
    = \{ \mathbf{f}'(\mathbf{g}(\mathbf{y})) \}^{-1}.
  \]

\item[(d)]
  \emph{What are the images under $\mathbf{f}$ of lines parallel to the coordinate axes?} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  The range of $\textbf{f}$ is $\mathbb{R}^2 - \{(0,0)\}$.

\item[(2)]
  If $(a,b) \neq (0,0)$,
  then
  $\mathbf{f}: \left(\log \sqrt{a^2+b^2}, \mathrm{atan2}(b,a)\right) \mapsto (a,b)$
  where
  \begin{equation*}
    \mathrm{atan2}(b,a) =
      \begin{cases}
        \arctan\left(\frac{b}{a}\right) & \text{ if $a > 0$}, \\
        \arctan\left(\frac{b}{a}\right) + \pi & \text{ if $a < 0$ and $b \geq 0$}, \\
        \arctan\left(\frac{b}{a}\right) - \pi & \text{ if $a < 0$ and $b < 0$}, \\
        \frac{\pi}{2} & \text{ if $a = 0$ and $b > 0$}, \\
        -\frac{\pi}{2} & \text{ if $a = 0$ and $b < 0$}.
      \end{cases}
  \end{equation*}
  (Or apply Theorem 8.7(d).)

\item[(3)]
  If $(a,b) = (0,0)$,
  then for any $(x,y) \in \mathbb{R}^2$
  we have
  $f_1(x,y)^2 + f_2(x,y)^2 = e^{2x} \neq 0$.
  So that there is no $(x,y)$ such that $\mathbf{f}: (x,y) \mapsto (0,0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \[
    [\mathbf{f}'(x,y)]
    =
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y &  e^x \cos y
    \end{bmatrix}.
  \]
  So $\mathbf{f}'$ is continuous and
  \[
    J_{\mathbf{f}}(x,y)
    = \det \mathbf{f}'(x,y)
    = e^{2x}
    \neq 0.
  \]

\item[(2)]
  Since $J_{\mathbf{f}}(x,y) \neq 0$, $\mathbf{f}'(x,y)$ is invertible (Theorem 9.36).
  So the inverse function theorem (Theorem 9.24) implies that
  there exists an open neighborhood $B(x,y)$ of $(x,y)$ such that
  $\mathbf{f}$ is injective on $B(x,y)$.

\item[(3)]
  Note that
  \[
    \mathbf{f}(0,0) = \mathbf{f}(0,2\pi) = (1,0).
  \]
  So that $\mathbf{f}$ is not injective on the whole $\mathbb{R}^2$.
  (Injectivity of $\mathbf{f}$ is a local property.)
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  If $\mathbf{a} = \left(0, \frac{\pi}{3}\right)$, then
  $\mathbf{b} = \mathbf{f}(\mathbf{a}) = \left( \frac{1}{2}, \frac{\sqrt{3}}{2} \right)$.

\item[(2)]
  Similar to (2) in the proof of (a),
  define $\mathbf{g}: U \to \mathbb{R}^2$ by
  \[
    \mathbf{g}(x,y) = \left(\log \sqrt{x^2+y^2}, \arctan\left(\frac{y}{x}\right) \right).
  \]
  where $U$ is some open neighborhood of the point $\mathbf{b} \in \mathbb{R}^2$ described in (b).
  So $\mathbf{g}$ is a continuous inverse of $\mathbf{f}$.

\item[(3)]
  Since
  \[
    [\mathbf{f}'(x,y)]
    =
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y &  e^x \cos y
    \end{bmatrix},
  \]
  \[
    [\mathbf{f}'(\mathbf{a})]
    =
    \begin{bmatrix}
      \mathbf{f}'\left(0, \frac{\pi}{3}\right)
    \end{bmatrix}
    =
    \begin{bmatrix}
      e^0 \cos \frac{\pi}{3} & -e^0 \sin \frac{\pi}{3} \\
      e^0 \sin \frac{\pi}{3} &  e^0 \cos \frac{\pi}{3}
    \end{bmatrix}
    =
    \begin{bmatrix}
      \frac{1}{2} & -\frac{\sqrt{3}}{2} \\
      \frac{\sqrt{3}}{2} & \frac{1}{2}
    \end{bmatrix}.
  \]

\item[(4)]
  Since
  \[
    [\mathbf{g}'(x,y)]
    =
    \begin{bmatrix}
      \frac{x}{x^2+y^2} & \frac{y}{x^2+y^2} \\
      \frac{-y}{x^2+y^2} & \frac{x}{x^2+y^2}
    \end{bmatrix},
  \]
  \[
    [\mathbf{g}'(\mathbf{b})]
    =
    \begin{bmatrix}
      \mathbf{g}'\left( \frac{1}{2}, \frac{\sqrt{3}}{2} \right)
    \end{bmatrix} \\
    =
    \begin{bmatrix}
      \frac{1}{2} & \frac{\sqrt{3}}{2} \\
      -\frac{\sqrt{3}}{2} & \frac{1}{2}
    \end{bmatrix}.
  \]
  Here we can see $[\mathbf{f}'(\mathbf{a})][\mathbf{g}'(\mathbf{b})]
  = [\mathbf{g}'(\mathbf{b})][\mathbf{f}'(\mathbf{a})] = 1$.

\item[(5)]
  \begin{align*}
    [\mathbf{g}'(\mathbf{y})]
    &= [\mathbf{g}'(\mathbf{f}(\mathbf{x}))] \\
    &= [\mathbf{g}'(e^x \cos y, e^x \sin y)] \\
    &=
    \begin{bmatrix}
      \frac{e^x \cos y}{e^{2x}} & \frac{e^x \sin y}{e^{2x}} \\
      \frac{-e^x \sin y}{e^{2x}} & \frac{e^x \cos y}{e^{2x}}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      e^{-x} \cos y & e^{-x} \sin y \\
      -e^{-x} \sin y & e^{-x} \cos y
    \end{bmatrix},
  \end{align*}
  and
  \[
    [\mathbf{f}'(\mathbf{g}(\mathbf{y}))]
    = [\mathbf{f}'(\mathbf{x})]
    =
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y &  e^x \cos y
    \end{bmatrix}.
  \]
  Note that
  \[
    \begin{bmatrix}
      e^{-x} \cos y & e^{-x} \sin y \\
      -e^{-x} \sin y & e^{-x} \cos y
    \end{bmatrix}
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y &  e^x \cos y
    \end{bmatrix}
    = 1.
  \]
  Therefore
  \[
    \mathbf{g}'(\mathbf{y})
    = \{ \mathbf{f}'(\mathbf{g}(\mathbf{y})) \}^{-1}
  \]
  on $\mathbf{g}(U)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  The case $L_r = \{ (x,y) \in \mathbb{R}^2 : x = r \}$ parallel to $y$-axis
  where $r \in \mathbb{R}^1$ is constant.
  The image under $\mathbf{f}$ is
  \begin{align*}
    \mathbf{f}(L_r)
    &= \{ (e^r \cos y, e^r \sin y) \in \mathbb{R}^2 : y \in \mathbb{R}^1 \} \\
    &= \{ (s, t) \in \mathbb{R}^2 : s^2 + t^2 = (e^r)^2 \},
  \end{align*}
  a circle which is centered at the origin $(0,0) \in \mathbb{R}^2$ with radius $e^{r} > 0$.

\item[(2)]
  The case $L_{\theta} = \{ (x,y) \in \mathbb{R}^2 : y = \theta \}$ parallel to $x$-axis
  where $\theta \in \mathbb{R}^1$ is constant.
  The image under $\mathbf{f}$ is
  \begin{align*}
    \mathbf{f}(L_{\theta})
    &= \{ (e^x \cos \theta, e^x \sin \theta) \in \mathbb{R}^2 : x \in \mathbb{R}^1 \} \\
    &= \{ (y \cos \theta, y \sin \theta) \in \mathbb{R}^2 : y > 0 \},
  \end{align*}
  which is a ray from the origin $(0,0)$ (not included) to the infinity
  passing through a point $(\cos\theta, \sin\theta)$ in the unit circle.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.18.}
\addcontentsline{toc}{subsection}{Exercise 9.18.}
\emph{Answer analogous questions for the mapping defined by}
\[
  u = x^2-y^2, \qquad v = 2xy.
\]

\emph{Outline.}
Let $\mathbf{f}(x,y) = (u, v) = (x^2-y^2,2xy)$.

\begin{enumerate}
\item[(a)]
  \emph{What is the range of $\mathbf{f}$?}

\item[(b)]
  \emph{Show that the Jacobian of $\mathbf{f}$ is not zero
  at any point of $\mathbb{R}^2 - \{(0,0)\}$.
  Thus every point of $\mathbb{R}^2 - \{(0,0)\}$ has a neighborhood
  in which $\mathbf{f}$ is one-to-one.
  Nevertheless, $\mathbf{f}$ is not one-to-one on $\mathbb{R}^2 - \{(0,0)\}$.}

\item[(c)]
  \emph{Put $\mathbf{a} = \left(1, 1\right)$,
  $\mathbf{b} = \mathbf{f}(\mathbf{a})$, let $\mathbf{g}$ be the continuous inverse of $\mathbf{f}$,
  defined in a neighborhood of $\mathbf{b}$ such that $\mathbf{g}(\mathbf{b}) = \mathbf{a}$.
  Find an explicit formula for $\mathbf{g}$,
  compute $\mathbf{f}'(\mathbf{a})$ and $\mathbf{g}'(\mathbf{b})$,
  and verify the formula}
  \[
    \mathbf{g}'(\mathbf{y})
    = \{ \mathbf{f}'(\mathbf{g}(\mathbf{y})) \}^{-1}.
  \]

\item[(d)]
  \emph{What are the images under $\mathbf{f}$ of lines parallel to the coordinate axes?} \\
\end{enumerate}



\emph{Proof of (a).}
  \emph{Show that the range of $\textbf{f}$ is $\mathbb{R}^2$.}
  Clearly, $f(0,0) = (0,0)$.
  If $(a,b) \neq (0,0)$,
  then
  \[
    \mathbf{f}: \left(
      \sqrt{\frac{\sqrt{a^2+b^2}+a}{2}},
      \mathrm{sgn}(b) \sqrt{\frac{\sqrt{a^2+b^2}-a}{2}},
    \right) \mapsto (a,b).
  \]
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \[
    [\mathbf{f}'(x,y)]
    =
    \begin{bmatrix}
      2x & -2y \\
      2y &  2x
    \end{bmatrix}.
  \]
  So $\mathbf{f}'$ is continuous and
  \[
    J_{\mathbf{f}}(x,y)
    = \det \mathbf{f}'(x,y)
    = 4(x^2+y^2)
    \neq 0
  \]
  if $(x,y) \neq (0,0)$.

\item[(2)]
  Since $J_{\mathbf{f}}(x,y) \neq 0$ if $(x,y) \neq (0,0)$,
  $\mathbf{f}'(x,y)$ is invertible if $(x,y) \neq (0,0)$ (Theorem 9.36).
  So the inverse function theorem (Theorem 9.24) implies that
  there exists an open neighborhood $B(x,y)$ of $(x,y) \neq (0,0)$ such that
  $\mathbf{f}$ is injective on $B(x,y)$.

\item[(3)]
  Note that
  \[
    \mathbf{f}(1,0) = \mathbf{f}(-1,0) = (1,0).
  \]
  So that $\mathbf{f}$ is not injective on the whole $\mathbb{R}^2 - \{(0,0)\}$.
  (Injectivity of $\mathbf{f}$ is a local property.)
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  If $\mathbf{a} = \left(1, 1\right)$, then
  $\mathbf{b} = \mathbf{f}(\mathbf{a}) = \left(0,2\right)$.

\item[(2)]
  Similar to (2) in the proof of (a),
  define $\mathbf{g}: U \to \mathbb{R}^2$ by
  \[
    \mathbf{g}(x,y)
    = \left(
      \sqrt{\frac{\sqrt{x^2+y^2}+x}{2}},
      \sqrt{\frac{\sqrt{x^2+y^2}-x}{2}},
    \right),
  \]
  where $U$ is some open neighborhood of the point
  $\mathbf{b} \in \mathbb{R}^2 - \{(0,0)\}$ described in (b).
  So $\mathbf{g}$ is a continuous inverse of $\mathbf{f}$.

\item[(3)]
  Since
  \[
    [\mathbf{f}'(x,y)]
    =
    \begin{bmatrix}
      2x & -2y \\
      2y &  2x
    \end{bmatrix},
  \]
  \[
    [\mathbf{f}'(\mathbf{a})]
    =
    \begin{bmatrix}
      \mathbf{f}'(1, 1)
    \end{bmatrix}
    =
    \begin{bmatrix}
      2 & -2 \\
      2 &  2
    \end{bmatrix}.
  \]

\item[(4)]
  Since
  \[
    [\mathbf{g}'(x,y)]
    =
    \frac{1}{2\sqrt{x^2+y^2}}
    \begin{bmatrix}
      \sqrt{\frac{\sqrt{x^2+y^2}+x}{2}}
        & \sqrt{\frac{\sqrt{x^2+y^2}-x}{2}} \\
      -\sqrt{\frac{\sqrt{x^2+y^2}-x}{2}}
        & \sqrt{\frac{\sqrt{x^2+y^2}+x}{2}}
    \end{bmatrix},
  \]
  \[
    [\mathbf{g}'(\mathbf{b})]
    =
    \begin{bmatrix}
      \mathbf{g}'(0,2)
    \end{bmatrix} \\
    =
    \begin{bmatrix}
       \frac{1}{4} & \frac{1}{4} \\
      -\frac{1}{4} & \frac{1}{4}
    \end{bmatrix}.
  \]
  Here we can see $[\mathbf{f}'(\mathbf{a})][\mathbf{g}'(\mathbf{b})]
  = [\mathbf{g}'(\mathbf{b})][\mathbf{f}'(\mathbf{a})] = 1$.

\item[(5)]
  \begin{align*}
    [\mathbf{g}'(\mathbf{y})]
    &= [\mathbf{g}'(\mathbf{f}(\mathbf{x}))] \\
    &= [\mathbf{g}'(x^2-y^2, 2xy)] \\
    &=
    \begin{bmatrix}
       \frac{x}{2(x^2+y^2)} & \frac{y}{2(x^2+y^2)} \\
      -\frac{y}{2(x^2+y^2)} & \frac{x}{2(x^2+y^2)}
    \end{bmatrix},
  \end{align*}
  and
  \[
    [\mathbf{f}'(\mathbf{g}(\mathbf{y}))]
    = [\mathbf{f}'(\mathbf{x})]
    =
    \begin{bmatrix}
      2x & -2y \\
      2y &  2x
    \end{bmatrix}.
  \]
  Note that
  \[
    \begin{bmatrix}
       \frac{x}{2(x^2+y^2)} & \frac{y}{2(x^2+y^2)} \\
      -\frac{y}{2(x^2+y^2)} & \frac{x}{2(x^2+y^2)}
    \end{bmatrix}
    \begin{bmatrix}
      2x & -2y \\
      2y &  2x
    \end{bmatrix}
    = 1.
  \]
  Therefore
  \[
    \mathbf{g}'(\mathbf{y})
    = \{ \mathbf{f}'(\mathbf{g}(\mathbf{y})) \}^{-1}
  \]
  on $\mathbf{g}(U)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  The case $L_{\alpha} = \{ (x,y) \in \mathbb{R}^2 : x = \alpha \}$ parallel to $y$-axis
  where $\alpha \in \mathbb{R}^1$ is constant.
  If $\alpha = 0$, then
  \[
    \mathbf{f}(L_{0})
    = \{ (-y^2, 0) \in \mathbb{R}^2 : y \in \mathbb{R}^1 \}
    = \{ (-t, 0) \in \mathbb{R}^2 : t \in \mathbb{R}^1, t \geq 0 \}
  \]
  is a ray from the origin $(0,0)$ (included) to the infinity $(-\infty,0)$.
  If $\alpha \neq 0$, then
  \begin{align*}
    \mathbf{f}(L_{\alpha})
    &= \{ (\alpha^2 - y^2, 2\alpha y) \in \mathbb{R}^2 : y \in \mathbb{R}^1 \} \\
    &= \left\{ (s, t) \in \mathbb{R}^2 : s = \alpha^2 - \frac{t^2}{4\alpha^2} \right\},
  \end{align*}
  which is a parabola.

\item[(2)]
  The case $L_{\beta} = \{ (x,y) \in \mathbb{R}^2 : y = \beta \}$ parallel to $x$-axis
  where $\beta \in \mathbb{R}^1$ is constant.
  If $\beta = 0$, then
  \[
    \mathbf{f}(L_{0})
    = \{ (x^2, 0) \in \mathbb{R}^2 : x \in \mathbb{R}^1 \}
    = \{ (t, 0) \in \mathbb{R}^2 : t \in \mathbb{R}^1, t \geq 0 \}
  \]
  is a ray from the origin $(0,0)$ (included) to the infinity $(\infty,0)$.
  If $\beta \neq 0$, then
  \begin{align*}
    \mathbf{f}(L_{\beta})
    &= \{ (x^2 - \beta^2, 2\beta x) \in \mathbb{R}^2 : x \in \mathbb{R}^1 \} \\
    &= \left\{ (s, t) \in \mathbb{R}^2 : s = \frac{t^2}{4\beta^2} - \beta^2 \right\},
  \end{align*}
  which is a parabola.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.19.}
\addcontentsline{toc}{subsection}{Exercise 9.19.}
\emph{Show that the system of equations
\begin{align*}
  3x+y-z+u^2 &= 0 \\
  x-y+2z+u &= 0 \\
  2x+2y-3z+2u &= 0
\end{align*}
can solved
for $x,y,u$ in terms of $z$;
for $x,z,u$ in terms of $y$;
for $y,z,u$ in terms of $x$;
but not for $x,y,z$ in terms of $u.$} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Denote
  \begin{align*}
    3x+y-z+u^2 &= 0 \tag{I} \\
    x-y+2z+u &= 0 \tag{II} \\
    2x+2y-3z+2u &= 0 \tag{III}
  \end{align*}
  So (I) - 3(II) implies that
  \[
    4y + u(u-3) = 7z \tag{IV},
  \]
  and (III) - 2(II) implies that
  \[
    4y = 7z \tag{V}.
  \]
  By (IV)(V), we have $u(u-3) = 0$.
  Hence $u = 0$ or $u = 3$ in any case.

\item[(2)]
  \emph{Show that (I)(II)(III) can be solve for $x,y,u$ in terms of $z$.}
  (V) implies that $y = \frac{7z}{4}$.
  Hence
  \[
    (x,y,u)
    =
    \left(-\frac{z}{4}, \frac{7z}{4}, 0 \right),
    \left(-\frac{z}{4}-3, \frac{7z}{4}, 3 \right).
  \]

\item[(3)]
  \emph{Show that (I)(II)(III) can be solve for $x,z,u$ in terms of $y$.}
  \[
    (x,z,u)
    =
    \left(-\frac{y}{7}, \frac{4y}{7}, 0 \right),
    \left(-\frac{y}{7}-3, \frac{4y}{7}, 3 \right).
  \]

\item[(4)]
  \emph{Show that (I)(II)(III) can be solve for $y,z,u$ in terms of $x$.}
  \[
    (y,z,u)
    =
    \left(-7x, -4x, 0 \right),
    \left(-7x-21, -4x-12, 3 \right).
  \]

\item[(5)]
  \emph{Show that (I)(II)(III) can not be solve for $x,y,z$ in terms of $u$.}
  Actually,
  \[
    (x,y,z)
    = \left(-t-u, 7t, 4t \right)
  \]
  for all $t \in \mathbb{R}^1$.
\end{enumerate}
$\Box$ \\



\emph{Proof (The implicit function theorem).}
\begin{enumerate}
\item[(1)]
  Define $\mathbf{f}$ be a $\mathscr{C}'$-mapping
  of $\mathbb{R}^{3+1}$ into $\mathbb{R}^3$ by
  \[
    \mathbf{f}(x,y,z,u)
    = (3x+y-z+u^2,x-y+2z+u,2x+2y-3z+2u).
  \]
  Note that $\mathbf{f}(0,0,0,0) = \mathbf{0}$
  and $\mathbf{f}(-3,0,0,3) = \mathbf{0}$.

\item[(2)]
  Since
  \[
    [\mathbf{f}'(x,y,z,u)]
    = \begin{bmatrix}
      3 &  1 & -1 & 2u \\
      1 & -1 &  2 &  1 \\
      2 &  2 & -3 &  2
    \end{bmatrix},
  \]
  $\mathbf{f}'$ is continuous,
  \[
    [\mathbf{f}'(0,0,0,0)]
    = \begin{bmatrix}
      3 &  1 & -1 &  0 \\
      1 & -1 &  2 &  1 \\
      2 &  2 & -3 &  2
    \end{bmatrix},
  \]
  and
  \[
    [\mathbf{f}'(-3,0,0,3)]
    = \begin{bmatrix}
      3 &  1 & -1 &  6 \\
      1 & -1 &  2 &  1 \\
      2 &  2 & -3 &  2
    \end{bmatrix}.
  \]

\item[(3)]
  The submatrix
  \[
    [\mathbf{f}'(0,0,0,0)]_x
    = \begin{bmatrix}
       1 & -1 &  0 \\
      -1 &  2 &  1 \\
       2 & -3 &  2
    \end{bmatrix}
  \]
  is invertiable since its determinant is $3 \neq 0$.
  By the implicit function theorem (Theorem 9.28),
  the system can be solved for $y,z,u$ in terms of $x$.
  Similar arguments to
  $[\mathbf{f}'(0,0,0,0)]_y$,
  $[\mathbf{f}'(0,0,0,0)]_z$,
  $[\mathbf{f}'(-3,0,0,3)]_x$,
  $[\mathbf{f}'(-3,0,0,3)]_y$, and
  $[\mathbf{f}'(-3,0,0,3)]_z$.

\item[(4)]
  Note that $[\mathbf{f}'(0,0,0,0)]_u$ and $[\mathbf{f}'(-3,0,0,3)]_u$
  are not invertible, we cannot apply the implicit function theorem (Theorem 9.28).
  We need to show by brute-force in this case.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.20.}
\addcontentsline{toc}{subsection}{Exercise 9.20.}
\emph{Take $n=m=1$ in the implicit function theorem,
and interpret the theorem (as well as its proof) graphically.} \\



\textbf{Implicit function theorem (for $n=m=1$).}
  Let $f(x,y)$ be a $\mathscr{C}'$-mapping of
  an open set $E \subseteq \mathbb{R}^2$ into $\mathbb{R}$,
  such that $f(a,b) = 0$ for some point $(a,b) \in E$.
  Assume that
  \[
    D_1 f(a,b) \neq 0.
  \]
  Then there exist open sets $U \subseteq E$ and $W \subseteq \mathbb{R}^1$,
  with $(a,b) \in U$ and $b \in W$, having the following property:
  \begin{quote}
    To every $y \in W$ corresponds a unique $x$ such that
    \[
      (x,y) \in U
      \qquad \text{ and } \qquad
      f(x,y) = 0.
    \]
    If this $x$ is defined to be $g(y)$,
    then $g$ is a $\mathscr{C}'$-mapping of $W$ into $\mathbb{R}^1$,
    $g(b) = a$,
    \[
      f(g(y),y) = 0
      \qquad
      (y \in W),
    \]
    and
    \[
      g'(b) = - \frac{D_2 f(a,b)}{D_1 f(a,b)}.
    \]
  \end{quote}


\emph{Proof.}
\begin{enumerate}
\item[(1)]
In the notations of Exercise 4.6,
define the graph of $f$ by the set
\[
  S = \{ (x,y) \in E : f(x,y) = 0 \}.
\]

\item[(2)]
Consider the graph $S$.
As $D_1 f(a,b) \neq 0$ and $f(x,y) \in \mathscr{C}'$,
there are an open neighborhood $U \subseteq E$ of $(a,b)$
and an open neighborhood $W$ of $b$
such that $x \mapsto f(x,y)$ is strictly monotonic whenever $y \in W$.
``Graphically'' by the monotony of $f(x,y)$, for any fixed $y$
there is a unique $x$ such that $f(x,y) = 0$.

\item[(3)]
``Graphically''
the tangent line passing through $(a,b)$ is
\[
  D_1 f(a,b)(x - a) + D_2 f(a,b)(y - b) = 0.
\]
Thus $g'(b) = -\frac{D_2 f(a,b)}{D_1 f(a,b)}$ if $D_1 f(a,b) \neq 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.21.}
\addcontentsline{toc}{subsection}{Exercise 9.21.}
\emph{Define $f$ in $\mathbb{R}^2$ by}
\[
  f(x,y) = 2x^3-3x^2+2y^3+3y^2.
\]
\begin{enumerate}
\item[(a)]
  \emph{Find the four points in $\mathbb{R}^2$ at which the gradient of $f$ is zero.
  Show that $f$ has exactly one local maximum and one local minimum in $\mathbb{R}^1$.}

\item[(b)]
  \emph{Let $S$ be the set of all $(x,y) \in \mathbb{R}^2$ at which $f(x,y) = 0$.
  Find those points of $S$ that have no neighborhoods in which the equation
  $f(x,y) = 0$ can be solved for $y$ in terms of $x$ (or for $x$ in terms of $y$).
  Describe $S$ as precisely as you can.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \[
    (\nabla f)(x,y)
    = ((D_1 f)(x,y), (D_2 f)(x,y))
    = (6x(x-1), 6y(y+1)).
  \]
  So $(\nabla f)(x,y) = 0$ if and only if
  $(x,y) = (0,0)$, $(0,-1)$, $(1,0)$, $(1,-1)$.

\item[(2)]
  $x \mapsto 2x^3 - 3x^2$
  have one local maximum at $x = 0$ and one local minimum at $x = 1$.
  $y \mapsto 2y^3 + 3y^2$
  have one local maximum at $y = -1$ and one local minimum at $y = 0$.

\item[(3)]
  Hence $f: (x,y) \mapsto to (2x^3 - 3x^2) + (2y^3 + 3y^2)$
  have one local maximum at $(x,y) = (0,-1)$ and
  one local minimum at $(x,y) = (1,0)$.
  Other two points $(0,0)$ and $(1,-1)$ are saddle points.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By definition,
  \begin{align*}
    S
    &= \{ f(x,y) = 0 \} \\
    &= \{ (x+y)(2x^2-2xy-3x+2y^2+3y) = 0 \} \\
    &= \{ x+y = 0 \} \cup \{ 2x^2-2xy-3x+2y^2+3y = 0 \},
  \end{align*}
  which is a union of a line $L = \{ x+y = 0 \}$
  and an ellipse $E = \{ 2x^2-2xy-3x+2y^2+3y = 0 \}$.
  The intersection of $L \cap E$ is $\{ (0,0), (1,-1) \}$,
  and it suggested that $f(x,y) = 0$ cannot be solved for $y$ in terms of $x$
  (or for $x$ in terms of $y$) on $L \cap E = \{ (0,0), (1,-1) \}$.

\item[(2)]
  By (1) in the proof of (a) and the implicit function theorem (Theorem 9.28),
  $f(x,y) = 0$ can be solved for $y$ in terms of $x$ (or for $x$ in terms of $y$)
  whenever $(D_2 f)(x,y) \neq 0$ (or $(D_1 f)(x,y) \neq 0$).

\item[(3)]
  \emph{Show that $f(x,y) = 0$ cannot be solved for $y$ in terms of $x$
  if $(D_2 f)(x,y) = 0$.}
  $(D_2 f)(x,y) = 0$ if and only if
  \[
    (x,y) \in T = \left\{
      (0,0),
      \left(\frac{3}{2},0\right),
      (1,-1),
      \left(-\frac{1}{2}, -1\right)
    \right\}.
  \]
  Solve $y$ to get
  \begin{align*}
    y &= -x \\
    y &= \frac{1}{4}\left( 2x-3 + \sqrt{-3(2x+1)(2x-3)} \right) \\
    y &= \frac{1}{4}\left( 2x-3 - \sqrt{-3(2x+1)(2x-3)} \right)
  \end{align*}
  In any case, $y$ can not be uniquely determined by $x$ for any $(x,y) \in T$.
  (``Graphically'' we can see the set $S$ to get the conclusion.
  Explicitly, we can take the limit to each expression
  (as $(s,t) \to (x,y) \in T$), and observe that not all limits are equal.)

\item[(4)]
  \emph{Show that $f(x,y) = 0$ cannot be solved for $x$ in terms of $y$
  if $(D_1 f)(x,y) = 0$.}
  $(D_1 f)(x,y) = 0$ if and only if
  \[
    (x,y) \in T = \left\{
      (0,0),
      \left(0,-\frac{3}{2}\right),
      (1,-1),
      \left(1,\frac{1}{2}\right)
    \right\}.
  \]
  Similar to (3),
  $x$ can not be uniquely determined by $y$ for any $(x,y) \in T$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Second-derivative test for extrema).}
\begin{enumerate}
\item[(1)]
  \emph{(Theorem 13.11 in Tom M. Apostol, Mathematical Analysis, 2nd edition).
  Let $f$ be a real-valued function with
  continuous second-order partial derivatives at a stationary point $\mathbf{a} \in \mathbb{R}^2$.
  Let
  \[
    A = (D_{11}f)(\mathbf{a}),
    \qquad
    B = (D_{12}f)(\mathbf{a}),
    \qquad
    C = (D_{22}f)(\mathbf{a}),
  \]
  and let
  \[
    \Delta
    = \det\begin{bmatrix}
      A & B \\
      B & C
    \end{bmatrix}
    = AC - B^2.
  \]
  Then we have:}
  \begin{enumerate}
  \item[(a)]
    \emph{If $\Delta > 0$ and $A > 0$, $f$ has a local minimum at $\mathbf{a}$.}

  \item[(b)]
    \emph{If $\Delta > 0$ and $A < 0$, $f$ has a local maximum at $\mathbf{a}$.}

  \item[(c)]
    \emph{If $\Delta < 0$, $f$ has a saddle point at $\mathbf{a}$.}
  \end{enumerate}

\item[(2)]
  We can give another proof of (a) by the second-derivative test for extrema. \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.22.}
\addcontentsline{toc}{subsection}{Exercise 9.22.}
\emph{Given a similar discussion for}
\[
  f(x,y) = 2x^3+6xy^2-3x^2+3y^2.
\]



\emph{Outline.}
\begin{enumerate}
\item[(a)]
  \emph{Find the two points in $\mathbb{R}^2$ at which the gradient of $f$ is zero.
  Show that $f$ has one saddle point and one local minimum in $\mathbb{R}^1$.}

\item[(b)]
  \emph{Let $S$ be the set of all $(x,y) \in \mathbb{R}^2$ at which $f(x,y) = 0$.
  Find those points of $S$ that have no neighborhoods in which the equation
  $f(x,y) = 0$ can be solved for $y$ in terms of $x$ (or for $x$ in terms of $y$).
  Describe $S$ as precisely as you can.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \[
    (\nabla f)(x,y)
    = ((D_1 f)(x,y), (D_2 f)(x,y))
    = (6(x^2+y^2-x), 6y(2x+1)).
  \]
  So $(\nabla f)(x,y) = 0$ if and only if
  $(x,y) = (0,0)$ or $(1,0)$.

\item[(2)]
  \emph{Show that $f$ has one saddle point at $(x,y) = (0,0)$.}
  Since $f(x,x) = 8x^3$,
  $f(x,x) \leq 0 = f(0,0)$ if $x < 0$
  and $f(x,x) \geq 0 = f(0,0)$ if $x > 0$.
  Hence $(x,y)$ is not a local maximum or a local minimum for $f$.

\item[(3)]
  \emph{Show that $f$ has one local minimum at $(x,y) = (1,0)$.}
  Write
  \[
    f(x,y) = 2x^3-3x^2 + (6x+3)y^2.
  \]
  Note that $2x^3-3x^2 \geq -1$ and $(6x+3)y^2 \geq 0$
  in some open neighborhood $B\left((1,0);\frac{1}{64}\right)$ of $(1,0)$.
  Therefore $f$ has one local minimum at $(x,y) = (1,0)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  $S$ is a folium of Descartes
  with a double point at the origin and asymptote $x + \frac{1}{2} = 0$.

  whenever $(D_2 f)(x,y) \neq 0$ (or $(D_1 f)(x,y) \neq 0$).

\item[(3)]
  \emph{Show that $f(x,y) = 0$ cannot be solved for $y$ in terms of $x$
  if $(D_2 f)(x,y) = 0$.}
  $(D_2 f)(x,y) = 0$ if and only if
  \[
    (x,y) \in T = \left\{ (0,0), \left(\frac{3}{2},0\right) \right\}.
  \]
  Solve $y$ to get
  \begin{align*}
    y &= \sqrt{\frac{-x^2(2x-3)}{3(2x+1)}} \\
    y &= -\sqrt{\frac{-x^2(2x-3)}{3(2x+1)}}
  \end{align*}
  In any case, $y$ can not be uniquely determined by $x$ for any $(x,y) \in T$.
  (``Graphically'' we can see the set $S$ to get the conclusion.
  Explicitly, we can take the limit to each expression
  (as $(s,t) \to (x,y) \in T$), and observe that two limits are different.)

\item[(4)]
  \emph{Show that $f(x,y) = 0$ cannot be solved for $x$ in terms of $y$
  if $(D_1 f)(x,y) = 0$.}
  $(D_1 f)(x,y) = 0$ if and only if
  \[
    (x,y) \in T = \left\{ (0,0), \pm \sqrt{ -\frac{3}{4} + \sqrt{\frac{3}{4}}} \right\}.
  \]
  Similar to (3),
  $x$ can not be uniquely determined by $y$ for any $(x,y) \in T$.
  That is,
  \begin{align*}
    x =& g(y) \\
      =& \frac{1-4y^2}{2} \left\{ 2\sqrt{16y^6+24y^4-3y^2}-12y^2+1 \right\}^{-\frac{1}{3}} \\
      &+ \left\{ 2\sqrt{16y^6+24y^4-3y^2}-12y^2+1 \right\}^{\frac{1}{3}} + 1.
  \end{align*}
  So as $y \neq 0$, $x = g(y) = g(-y)$. The expression $x = g(y)$ is not unique.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.23.}
\addcontentsline{toc}{subsection}{Exercise 9.23.}
\emph{Define $f$ in $\mathbb{R}^3$ by
\[
  f(x,y_1,y_2) = x^2 y_1 + e^x + y_2.
\]
Show that $f(0,1,-1) = 0$, $(D_1 f)(0, 1, -1) \neq 0$, and that
there exists therefore a differentiable function $g$ in some neighborhood of
$(1,-1)$ in $\mathbb{R}^2$, such that $g(1,-1) = 0$ and
\[
  f(g(y_1,y_2),y_1,y_2) = 0.
\]
Find $(D_1 g)(1,-1)$ and $(D_2 g)(1,-1)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that $f(0,1,-1) = 0$.
  Since
  \begin{align*}
    \left[ \nabla f((x,y_1,y_2) \right]_{(x,y_1,y_2)=(0,1,-1)}
    &= \left[ (2xy_1 + e^x, x^2, 1) \right]_{(x,y_1,y_2)=(0,1,-1)} \\
    &= (1,0,1),
  \end{align*}
  $A_x = (1)$ and $A_y = (0,1)$.
  By the implicit function theorem (Theorem 9.28),
  there exists a $\mathscr{C}'$ function
  in some open neighborhood of $(1,-1)$ such that $g(1,-1) = 0$
  and $f(g(y_1,y_2),y_1,y_2) = 0$.

\item[(2)]
  Besides,
  $g'(1,-1) = -(A_x)^{-1} A_y = (0,-1)$
  implies that $(D_1 g)(1,-1) = 0$ and $(D_2 g)(1,-1) = -1$.

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.24.}
\addcontentsline{toc}{subsection}{Exercise 9.24.}
\emph{For $(x,y) \neq (0,0)$, define $\mathbf{f} = (f_1,f_2)$ by
\[
  f_1(x,y) = \frac{x^2-y^2}{x^2+y^2},
  \qquad
  f_2(x,y) = \frac{xy}{x^2+y^2}.
\]
Compute the rank of $\mathbf{f}'(x,y)$, and find the range of $\mathbf{f}$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \[
    [\mathbf{f}'(x,y)]
    =
    \begin{bmatrix}
      \frac{4xy^2}{(x^2+y^2)^2} & \frac{4x^2y}{(x^2+y^2)^2} \\
      \frac{-y(x^2-y^2)}{(x^2+y^2)^2} & \frac{x(x^2-y^2)}{(x^2+y^2)^2}
    \end{bmatrix}.
  \]

\item[(2)]
  \emph{Show that $\rank([\mathbf{f}'(x,y)]) \neq 2$.}
  It is equivalent to show that $\det[\mathbf{f}'(x,y)] = 0$.
  Actually,
  \[
    \det[\mathbf{f}'(x,y)]
    = \frac{4xy^2}{(x^2+y^2)^2} \cdot \frac{x(x^2-y^2)}{(x^2+y^2)^2}
      - \frac{4x^2y}{(x^2+y^2)^2} \cdot \frac{-y(x^2-y^2)}{(x^2+y^2)^2}
    = 0.
  \]

\item[(3)]
  \emph{Show that $\rank([\mathbf{f}'(x,y)]) \neq 0$.}
  \begin{align*}
    [\mathbf{f}'(x,y)]
    \begin{bmatrix}
      1 \\
      0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      \frac{4xy^2}{(x^2+y^2)^2} & \frac{4x^2y}{(x^2+y^2)^2} \\
      \frac{-y(x^2-y^2)}{(x^2+y^2)^2} & \frac{x(x^2-y^2)}{(x^2+y^2)^2}
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      0
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \frac{4xy^2}{(x^2+y^2)^2} \\
      \frac{-y(x^2-y^2)}{(x^2+y^2)^2}
    \end{bmatrix} \\
    &\neq
    \begin{bmatrix}
      0 \\
      0
    \end{bmatrix}
  \end{align*}
  for all $(x,y) \in \mathbb{R}^2 - \{(0,0)\}$.

\item[(4)]
  Since the rank of $\mathbf{f}'$ is the dimension of the subspace
  $\mathscr{R}(\mathbf{f}')$ in $\mathbb{R}^2$,
  $\rank([\mathbf{f}'(x,y)]) = 0, 1, 2$.
  By (2)(3), $\rank([\mathbf{f}'(x,y)]) = 1$.

\item[(5)]
  \emph{Show that the range of $\mathbf{f}$ is an ellipse}
  \[
    E = \{ (s,t) \in \mathbb{R}^2 : s^2 + 4t^2 = 1 \}.
  \]
  \begin{enumerate}
  \item[(a)]
    Clearly, $(f_1(x,y), f_2(x,y)) \in E$.

  \item[(b)]
    Conversely, for any $(s,t) \in E$ write
    \[
      s = \cos\theta
      \qquad \text{ and } \qquad
      t = \frac{1}{2} \sin\theta
    \]
    for some unique $\theta \in [0,2\pi)$ (Theorem 8.7(d)).
    By the tangent half-angle formula,
    \[
      s = \cos\theta = \frac{1-\tan^2\frac{\theta}{2}}{1+\tan^2\frac{\theta}{2}}
      \qquad \text{ and } \qquad
      t = \frac{1}{2} \sin\theta = \frac{\tan\frac{\theta}{2}}{1+\tan^2\frac{\theta}{2}}.
    \]
    Thus, there exists a point
    $\left(1, \tan\frac{\theta}{2}\right) \in \mathbb{R}^2$ such that
    \[
      f: \left(1, \tan\frac{\theta}{2}\right)
      \mapsto (s, t) \in E.
    \]

  \item[(c)]
    Or we can do a linear projection from a given point $P = (1,0)$,
    say for any $\lambda \in \mathbb{R}^1$ we define a line through $P$
    with slope $-\lambda$ meeting $E$ in a further point
    \[
      Q_{\lambda}
      = \left(\frac{\lambda^2-1}{\lambda^2+1},
        \frac{\lambda}{\lambda^2+1}\right).
    \]
    Might define $Q_{\infty} = P$.
    Graphically and informally,
    \[
      \{ Q_{\lambda} : \lambda \in \mathbb{P}^1(\mathbb{R}) = \mathbb{R} \cup \{\infty\} \} = E.
    \]
    Therefore, $f(1,0) = P$ and $f(\lambda, 1) \in E-\{P\}$.

  \end{enumerate}
  By (a)(b), the range of $\mathbf{f}$ is exactly the same as an ellipse $E$.

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.25.}
\addcontentsline{toc}{subsection}{Exercise 9.25.}
\emph{Suppose $A \in L(\mathbb{R}^n, \mathbb{R}^m)$, let $r$ be the rank of $A$.}
\begin{enumerate}
\item[(a)]
  \emph{Define $S$ as the proof of Theorem 9.32.
  Show that $SA$ is a projection in $\mathbb{R}^n$
  whose null space is $\mathscr{N}(A)$ and whose range is $\mathscr{R}(S)$.
  (Hint: By (68), $SASA = SA$.)}

\item[(b)]
  \emph{Use (a) to show that}
  \[
    \dim \mathscr{N}(A) + \dim \mathscr{R}(A) = n.
  \]
\end{enumerate}



\emph{Proof of (a).}
Might assume $r > 0$.
\begin{enumerate}
\item[(1)]
  Since $\dim \mathscr{R}(A) = r$ (Definition 9.30),
  $\mathscr{R}(A)$ has a basis $\{ \mathbf{y}_1,\ldots,\mathbf{y}_r \}$.
  Choose $\mathbf{z}_i \in \mathbb{R}^n$ so that
  $A\mathbf{z}_i = \mathbf{y}_i$ ($1 \leq i \leq r$),
  and define a linear mapping $S$ of $\mathscr{R}(A)$ into $\mathbb{R}^n$ by setting
  \[
    S(c_1 \mathbf{y}_1 + \cdots + c_r \mathbf{y}_r)
    = c_1 \mathbf{z}_1 + \cdots + c_r \mathbf{z}_r
  \]
  for all scalars $c_1, \ldots, c_r$.

\item[(2)]
  \emph{Show that $SA$ is a projection.}
  Given any $\mathbf{x} \in \mathbb{R}^n$.
  Since $A\mathbf{x} \in \mathscr{R}(A)$, there exist scalars $c_1, \ldots, c_r$
  such that
  \[
    A\mathbf{x} = c_1 \mathbf{y}_1 + \cdots + c_r \mathbf{y}_r.
  \]
  Note that $AS \mathbf{y}_i = A \mathbf{z}_i = \mathbf{y}_i$ for $1 \leq i \leq r$.
  Hence
  \begin{align*}
    SASA\mathbf{x}
    &= SAS(c_1 \mathbf{y}_1 + \cdots + c_r \mathbf{y}_r) \\
    &= SA(c_1 \mathbf{z}_1 + \cdots + c_r \mathbf{z}_r) \\
    &= S(c_1 \mathbf{y}_1 + \cdots + c_r \mathbf{y}_r) \\
    &= SA\mathbf{x},
  \end{align*}

\item[(3)]
  \emph{Show that $\mathscr{N}(SA) = \mathscr{N}(A)$.}
  It is clear that $\mathscr{N}(SA) \supseteq \mathscr{N}(A)$.
  Conversely, given any $\mathbf{x} \in \mathscr{N}(SA)$.
  Write $\mathbf{0} = SA\mathbf{x} = S(A\mathbf{x})$.
  Since $S$ is injective, $A\mathbf{x} = 0$, or $\mathbf{x} \in \mathscr{N}(A)$.

\item[(4)]
  \emph{Show that $\mathscr{R}(SA) = \mathscr{R}(S)$.}
  It is clear that $\mathscr{R}(SA) \subseteq \mathscr{R}(S)$.
  Conversely, given any $\mathbf{z} \in \mathscr{R}(S)$.
  There exists $\mathbf{y} \in \mathscr{R}(A)$ such that $\mathbf{z} = S\mathbf{y}$.
  Since $\mathbf{y} \in \mathscr{R}(A)$,
  there exists $\mathbf{x} \in \mathbb{R}^n$ such that $\mathbf{y} = A\mathbf{x}$.
  So $\mathbf{z} = S\mathbf{y} = SA\mathbf{x}$, or $\mathbf{z} \in \mathscr{R}(SA)$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By Projections 9.31(a),
  \[
    \dim \mathscr{N}(P) + \dim \mathscr{R}(P) = n
  \]
  for any projection $P$.

\item[(2)]
  Since $SA$ is a projection,
  \[
    \dim \mathscr{N}(SA) + \dim \mathscr{R}(SA) = n.
  \]
  Since $\mathscr{N}(SA) = \mathscr{N}(A)$ and $\mathscr{R}(SA) = \mathscr{R}(S)$,
  it suffices to show that $\dim \mathscr{R}(S) = \dim \mathscr{R}(A)$.
  Since $S$ is injective, $\mathscr{R}(A) \cong S(\mathscr{R}(A)) = \mathscr{S}(A)$.
  Thus $\dim \mathscr{R}(S) = \dim \mathscr{R}(A)$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.26.}
\addcontentsline{toc}{subsection}{Exercise 9.26.}
\emph{Show that the existence (and even the continuity) of $D_{12}f$
does not imply the existence of $D_1 f$.
For example, let $f(x,y) = g(x)$, where $g$ is nowhere differentiable.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Consider the function $g$ defined on $\mathbb{R}^1$ by
  \begin{equation*}
    g(x) =
      \begin{cases}
        0           & (\text{$x$ irrational}), \\
        \frac{1}{n} & (x = \frac{m}{n}).
      \end{cases}
  \end{equation*}
  $g(x)$ is nowhere differentiable by (1) in the note of Exercise 4.18.
  Define $f(x,y) = g(x)$ on $\mathbb{R}^2$. \\

\item[(2)]
  $(D_1 f)(x,y) = g'(x)$ does not exist on $\mathbb{R}^2$.
  However, $(D_{12} f)(x,y) = (D_1 0)(x,y) = 0$ is continuous on $\mathbb{R}^2$.
\end{enumerate}
$\Box$ \\

\emph{Note.} Some nowhere differentiable functions.
  \begin{enumerate}
  \item[(1)]
    Exercise 4.18.

  \item[(2)]
    Theorem 7.18.

  \item[(3)]
    (Weierstrass functions.)
    \[
      f(x) = \sum_{n=0}^{\infty} a^n \cos(b^n \pi x)
    \]
    where $0 < a < 1$, $b$ is a positive odd integer, and $ab > 1 + \frac{3}{2}\pi$.

  \item[(4)]
    \[
      f(x) = \sum_{n=1}^{\infty} \frac{1}{n^2}\sin(n^2 \pi x).
    \]
  \end{enumerate}
  (And so on.) \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.27.}
\addcontentsline{toc}{subsection}{Exercise 9.27.}
\emph{Put $f(0,0) = 0$, and
\[
  f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2}
\]
if $(x,y) \neq (0,0)$. Prove that}
\begin{enumerate}
\item[(a)]
  \emph{$f$, $D_1 f$, $D_2 f$ are continuous in $\mathbb{R}^2$.}

\item[(b)]
  \emph{$D_{12}f$ and $D_{21}f$ exist at every point of $\mathbb{R}^2$,
  and are continuous except at $(0,0)$.}

\item[(c)]
  \emph{$(D_{12}f)(0,0) = 1$, and $(D_{21}f)(0,0) = -1$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $f$ is continuous in $\mathbb{R}^2$.}
  \begin{enumerate}
  \item[(a)]
    Clearly, $f(x,y)$ is continuous if $(x,y) \neq (0,0)$.
    So it suffices to show that
    \[
      \lim_{(x,y) \to (0,0)} f(x,y) = f(0,0) = 0.
    \]

  \item[(b)]
    Write $x = r\cos\theta$ and $y = r\sin\theta$ in the polar coordinates.
    (Here $r > 0$.)
    Hence
    \begin{align*}
      \lim_{(x,y) \to (0,0)} f(x,y)
      &= \lim_{(x,y) \to (0,0)} \frac{xy(x^2-y^2)}{x^2+y^2} \\
      &= \lim_{r \to 0} r^2 \cos\theta\sin\theta(\cos^2\theta-\sin^2\theta) \\
      &= 0
    \end{align*}
    since $\cos\theta\sin\theta(\cos^2\theta-\sin^2\theta)$ is bounded by $2$.
  \end{enumerate}

\item[(2)]
  \emph{Show that $D_1 f$ is continuous in $\mathbb{R}^2$.}
  \begin{enumerate}
  \item[(a)]
    $(x,y) \neq (0,0)$ implies that
    \[
      (D_1 f)(x,y) = \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2}.
    \]
    Besides,
    \begin{align*}
      (D_1 f)(0,0)
      &= \lim_{x \to 0} \frac{f(x,0) - f(0,0)}{x - 0} \\
      &= \lim_{x \to 0} \frac{0}{x} \\
      &= 0.
    \end{align*}
    In summary,
    \begin{equation*}
      (D_{1} f)(x,y) =
      \begin{cases}
        0
          & \text{if $(x,y) = (0,0)$} \\
        \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2}
          & \text{if $(x,y) \neq (0,0)$}.
      \end{cases}
    \end{equation*}

  \item[(b)]
    Clearly, $(D_1 f)(x,y)$ is continuous if $(x,y) \neq (0,0)$.
    So it suffices to show that
    \[
      \lim_{(x,y) \to (0,0)} (D_1 f)(x,y) = (D_1 f)(0,0) = 0.
    \]

  \item[(c)]
    Similar to (1)(b).
    Write $x = r\cos\theta$ and $y = r\sin\theta$ in the polar coordinates.
    (Here $r > 0$.)
    Hence
    \begin{align*}
      \lim_{(x,y) \to (0,0)} (D_1 f)(x,y)
      &= \lim_{(x,y) \to (0,0)} \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2} \\
      &= \lim_{r \to 0} r (\cos^4\theta\sin\theta+4\cos^2\theta\sin^3\theta-\sin^5\theta) \\
      &= 0
    \end{align*}
    since $\cos^4\theta\sin\theta+4\cos^2\theta\sin^3\theta-\sin^5\theta$ is bounded by $6$.
  \end{enumerate}

\item[(3)]
  Similar to (2).
  \emph{Show that $D_2 f$ is continuous in $\mathbb{R}^2$.}
  \begin{enumerate}
  \item[(a)]
    $(x,y) \neq (0,0)$ implies that
    \[
      (D_2 f)(x,y) = \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}.
    \]
    Besides,
    \begin{align*}
      (D_2 f)(0,0)
      &= \lim_{y \to 0} \frac{f(0,y) - f(0,0)}{y - 0} \\
      &= \lim_{y \to 0} \frac{0}{y} \\
      &= 0.
    \end{align*}
    In summary,
    \begin{equation*}
      (D_{2} f)(x,y) =
      \begin{cases}
        0
          & \text{if $(x,y) = (0,0)$} \\
        \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}
          & \text{if $(x,y) \neq (0,0)$}.
      \end{cases}
    \end{equation*}

  \item[(b)]
    Clearly, $(D_2 f)(x,y)$ is continuous if $(x,y) \neq (0,0)$.
    So it suffices to show that
    \[
      \lim_{(x,y) \to (0,0)} (D_2 f)(x,y) = (D_2 f)(0,0) = 0.
    \]

  \item[(c)]
    Similar to (1)(b).
    Write $x = r\cos\theta$ and $y = r\sin\theta$ in the polar coordinates.
    (Here $r > 0$.)
    Hence
    \begin{align*}
      \lim_{(x,y) \to (0,0)} (D_2 f)(x,y)
      &= \lim_{(x,y) \to (0,0)} \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2} \\
      &= \lim_{r \to 0} r(\cos^5\theta-4\cos^3\theta\sin^2\theta-\cos\theta\sin^4\theta) \\
      &= 0
    \end{align*}
    since $\cos^5\theta-4\cos^3\theta\sin^2\theta-\cos\theta\sin^4\theta$ is bounded by $6$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $D_{12}f$ exists at every point of $\mathbb{R}^2$.}
  \begin{enumerate}
  \item[(a)]
    $(x,y) \neq (0,0)$ implies that
    \[
      (D_{12} f)(x,y)
      = (D_{1}D_{2} f)(x,y)
      = \frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}.
    \]

  \item[(b)]
    Besides,
    \begin{align*}
      (D_{12} f)(0,0)
      &= \lim_{x \to 0} \frac{(D_2 f)(x,0) - (D_2 f)(0,0)}{x - 0} \\
      &= \lim_{x \to 0} \frac{x}{x} \\
      &= 1.
    \end{align*}
  \end{enumerate}
  In summary,
  \begin{equation*}
    (D_{12} f)(x,y) =
    \begin{cases}
      1
        & \text{if $(x,y) = (0,0)$} \\
      \frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}
        & \text{if $(x,y) \neq (0,0)$}.
    \end{cases}
  \end{equation*}

\item[(2)]
  \emph{Show that $D_{12}f$ is continuous except at $(0,0)$.}
  \begin{enumerate}
  \item[(a)]
    Clearly, $(D_{12} f)(x,y)$ is continuous if $(x,y) \neq (0,0)$.
    So it suffices to show that
    \[
      \lim_{(x,y) \to (0,0)} (D_{12} f)(x,y)
    \]
    does not exist.

  \item[(b)]
    Take
    \[
      \mathbf{p}_n = \left( \frac{1}{n}, 0 \right)
      \qquad
      \text{ and }
      \qquad
      \mathbf{q}_n = \left( 0, \frac{1}{n} \right)
    \]
    for $n = 1,2,3,\ldots$.
    So $\lim \mathbf{p}_n = \lim \mathbf{q}_n = \mathbf{0}$,
    \[
      \lim (D_{12} f)(\mathbf{p}_n) = 1
      \qquad
      \text{ and }
      \qquad
      \lim (D_{12} f)(\mathbf{q}_n) = -1.
    \]
    Hence $\lim_{(x,y) \to (0,0)} (D_{12} f)(x,y)$ does not exist.
  \end{enumerate}

\item[(3)]
  \emph{Show that $D_{21}f$ exists at every point of $\mathbb{R}^2$.}
  Similar to (1).
  \begin{enumerate}
  \item[(a)]
    $(x,y) \neq (0,0)$ implies that
    \[
      (D_{21} f)(x,y)
      = (D_{2}D_{1} f)(x,y)
      = \frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3},
    \]
    which is the same as $(D_{12} f)(x,y)$.

  \item[(b)]
    Besides,
    \begin{align*}
      (D_{21} f)(0,0)
      &= \lim_{y \to 0} \frac{(D_1 f)(0,y) - (D_1 f)(0,0)}{y - 0} \\
      &= \lim_{y \to 0} \frac{-y}{y} \\
      &= -1.
    \end{align*}
  \end{enumerate}
  In summary,
  \begin{equation*}
    (D_{21} f)(x,y) =
    \begin{cases}
      -1
        & \text{if $(x,y) = (0,0)$} \\
      \frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}
        & \text{if $(x,y) \neq (0,0)$}.
    \end{cases}
  \end{equation*}

\item[(4)]
  \emph{Show that $D_{21}f$ is continuous except at $(0,0)$.}
  Exactly the same as (2) since $(D_{21} f)(x,y) = (D_{12} f)(x,y)$ if $(x,y) \neq (0,0)$.

\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
  See (2)(4) in the proof of (b).
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.28.}
\addcontentsline{toc}{subsection}{Exercise 9.28.}
\emph{For $t \geq 0$, put
\begin{equation*}
  \varphi(x,t) =
    \begin{cases}
      x
        & (0 \leq x \leq \sqrt{t}), \\
      -x + 2\sqrt{t}
        & (\sqrt{t} \leq x \leq 2\sqrt{t}), \\
      0
        & (\text{otherwise}).
    \end{cases}
\end{equation*}
and put $\varphi(x,t) = -\varphi(x,|t|)$ if $t < 0$.
Show that $\varphi$ is continuous on $\mathbb{R}^2$, and
\[
  (D_2 \varphi)(x,0) = 0
\]
for all $x$.
Define
\[
  f(t) = \int_{-1}^{1} \varphi(x,t)dx.
\]
Show that $f(t) = t$ if $|t| < \frac{1}{4}$.
Hence}
\[
  f'(0) \neq \int_{-1}^{1} (D_2 \varphi)(x,0) dx.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\varphi$ is continuous on $\mathbb{R}^2$.}
  \begin{enumerate}
  \item[(a)]
    Define $g(x) = \max\{ 1-|1-x|,0 \}$ on $\mathbb{R}^1$.
    Write
    \begin{equation*}
      \varphi(x,t) =
        \begin{cases}
          \mathrm{sgn}(t) |t|^{\frac{1}{2}} g\left(x|t|^{-\frac{1}{2}}\right)
            & (t \neq 0), \\
          0
            & (t = 0).
        \end{cases}
    \end{equation*}
    Note that $|\varphi(x,t)| \leq \sqrt{t}$ for all $(x,t) \in \mathbb{R}^2$.

  \item[(b)]
    So $\varphi(x,t)$ is continuous on $\{ (x,t) \in \mathbb{R}^2 : t \neq 0 \}$.
  \item[(c)]
    For any $(y,0) \in \{ (x,t) \in \mathbb{R}^2 : t = 0 \}$,
    it suffices to show that $\varphi(x,t)$ is continuous at $(y,0)$.
    Given any $\varepsilon > 0$.
    There is an open neighborhood
    \[
      B\left( (y,0);\frac{\varepsilon^2}{64} \right)
    \]
    of $(y,0)$
    such that
    \begin{align*}
      |\varphi(x,t) - \varphi(y,0)|
      &= |\varphi(x,t) - 0| \\
      &\leq \sqrt{t} \\
      &\leq \sqrt{\frac{\varepsilon^2}{64}} \\
      &< \varepsilon
    \end{align*}
    whenever $(x,t) \in B\left( (y,0);\frac{\varepsilon^2}{64} \right)$.
    Hence $\varphi(x,t)$ is continuous on $\{ (x,t) \in \mathbb{R}^2 : t = 0 \}$.
  \end{enumerate}
  By (b)(c), the result is true.

\item[(2)]
  \emph{Show that $(D_2 \varphi)(x,0) = 0$ for all $x \in \mathbb{R}^1$.}
  \begin{enumerate}
  \item[(a)]
    Fix $x \in \mathbb{R}^1$.
    \emph{It suffices to show that
    \[
      (D_2 \varphi)(x,0)
      = \lim_{t \to 0}\frac{\varphi(x,t) - \varphi(x,0)}{t - 0}
      = \lim_{t \to 0}\frac{\varphi(x,t)}{t}
      = 0
    \]
    for all $x \in \mathbb{R}^1$.}

  \item[(b)]
    Note that
    \[
      \varphi(x,t)
      = \mathrm{sgn}(t) |t|^{\frac{1}{2}} g\left(x|t|^{-\frac{1}{2}}\right)
    \]
    if $t \neq 0$ (by (1)(a)).
    If $x \leq 0$, then $g\left(x|t|^{-\frac{1}{2}}\right) = 0$ is automatically true.
    If $x > 0$, then as $\frac{x^2}{4} > |t| > 0$
    we have $g\left(x|t|^{-\frac{1}{2}}\right) = 0$ again.
    In any case, $\varphi(x,t) = 0$ if $t$ is small enough.
  \end{enumerate}
  Therefore, $(D_2 \varphi)(x,0) = 0$.

\item[(3)]
  \emph{Show that $f(t) = \int_{-1}^{1} \varphi(x,t)dx = t$ if $|t| < \frac{1}{4}$.}
  As $0 \leq t < \frac{1}{4}$,
  \begin{align*}
    f(t)
    &= \int_{-1}^{1} \varphi(x,t)dx \\
    &= \int_{-1}^{0} \varphi(x,t)dx
      + \int_{0}^{\sqrt{t}} \varphi(x,t)dx
      + \int_{\sqrt{t}}^{2\sqrt{t}} \varphi(x,t)dx
      + \int_{2\sqrt{t}}^{1} \varphi(x,t)dx \\
    &= 0
      + \int_{0}^{\sqrt{t}} x dx
      + \int_{\sqrt{t}}^{2\sqrt{t}} (-x+2\sqrt{t})dx
      + 0 \\
    &= \left[ \frac{x^2}{2} \right]_{x=0}^{x=\sqrt{t}}
      + \left[ -\frac{x^2}{2} + 2\sqrt{t}x \right]_{x=\sqrt{t}}^{x=2\sqrt{t}} \\
    &= t.
  \end{align*}
  As $-\frac{1}{4} < t \leq 0$,
  \[
    f(t)
    = \int_{-1}^{1} \varphi(x,t)dx
    = -\int_{-1}^{1} \varphi(x,-t)dx
    = -(-t)
    = t.
  \]
  Hence $f(t) = t$ if $-\frac{1}{4} < t < \frac{1}{4}$.

\item[(4)]
  \emph{Show that $f'(0) \neq \int_{-1}^{1} (D_2 \varphi)(x,0) dx$.}
  By (3),
  \[
    f'(0)
    = \lim_{t \to 0} \frac{f(t) - f(0)}{t - 0}
    = \lim_{t \to 0} \frac{t - 0}{t - 0}
    = 1.
  \]
  By (2),
  \[
    \int_{-1}^{1} (D_2 \varphi)(x,0) dx
    = \int_{-1}^{1} 0 dx
    = 0.
  \]
  Hence $f'(0) \neq \int_{-1}^{1} (D_2 \varphi)(x,0) dx$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.29 (Symmetry of second derivatives).}
\addcontentsline{toc}{subsection}{Exercise 9.29 (Symmetry of second derivatives).}
\emph{Let $E$ be an open set in $\mathbb{R}^n$.
The classes $\mathscr{C}'(E)$ and $\mathscr{C}''(E)$ are defined in the text.
By induction, $\mathscr{C}^{(k)}(E)$ can be defined as follows,
for all positive integer $k$:
To say that $f \in \mathscr{C}^{(k)}(E)$
means that the partial derivatives $D_1 f, \ldots D_n f$ belongs to $\mathscr{C}^{(k-1)}(E)$.
Assume $f \in \mathscr{C}^{(k)}(E)$, and show (by repeated application of Theorem 9.41)
that the $k$th-order derivative
\[
  D_{i_1 i_2 \cdots i_k} f = D_{i_1} D_{i_2} \cdots D_{i_k} f
\]
is unchanged if the subscripts $i_1,\ldots,i_k$ are permuted.
For instance, if $n \geq 3$, then
\[
  D_{1213} f = D_{3112} f
\]
for every $f \in \mathscr{C}^{(4)}(E)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that the $k$th-order derivative is unchanged if
  any two adjacent subscripts $i_{h}$ and $i_{h+1}$ are exchanged.}
  Since $D_{i_{h+2}} \cdots D_{i_{k}} f \in \mathscr{C}^{(k-h-1)}(E)
  \subseteq \mathscr{C}^{2}(E)$,
  \[
    D_{i_{h+1} i_{h} i_{h+2} \cdots i_k} f
    = D_{i_{h} i_{h+1} i_{h+2} \cdots i_k} f.
  \]
  Hence
  \[
    D_{i_1 \cdots i_{h-1} i_{h+1} i_{h} i_{h+2} \cdots i_k} f
    = D_{i_1 \cdots i_{h-1} i_{h} i_{h+1} i_{h+2} \cdots i_k} f
    = D_{i_1 \cdots i_k} f.
  \]

\item[(2)]
  \emph{Show that every permutation can be written as a product of adjacent transpositions.}
  It is well known that
  every permutation can be written as a product of transpositions.
  Notice that
  \[
    (i \:\: j) = (i \:\: i+1)(i+1 \:\: i+2) \cdots
    (j-1 \:\: j)(j-2 \:\: j-1) \cdots (i \:\: i+1)
  \]
\end{enumerate}
By (1)(2), the result is established.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.30.}
\addcontentsline{toc}{subsection}{Exercise 9.30.}
\emph{Let $f \in \mathscr{C}^{(m)}(E)$, where $E$ is an open subset of $\mathbb{R}^n$.
Fix $\mathbf{a} \in E$,
and suppose $\mathbf{x} \in \mathbb{R}^n$ is so close to $\mathbf{0}$ that the points
\[
 \mathbf{p}(t) = \mathbf{a} + t \mathbf{x}
\]
lie in $E$ whenever $0 \leq t \leq 1$.
Define
\[
  h(t) = f(\mathbf{p}(t))
\]
for all $t \in \mathbb{R}^1$ for which $\mathbf{p}(t) \in E$.}
\begin{enumerate}
\item[(a)]
  \emph{For $1 \leq k \leq m$, show (by repeated application of the chain rule) that
  \[
    h^{(k)}(t) = \sum (D_{i_1 \cdots i_k}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_k}.
  \]
  The sum extends over all ordered $k$-tuples
  $(i_1,\ldots,i_k)$
  in which each $i_j$ is one of the integers $1,\ldots,n$.}

\item[(b)]
  \emph{By Taylor's theorem (Theorem 5.15),
  \[
    h(1) = \sum_{k=0}^{m-1} \frac{h^{(k)}(0)}{k!} + \frac{h^{(m)}(t)}{m!}
  \]
  for some $t \in (0,1)$.
  Use this to prove Taylor's theorem in $n$ variables by show that the formula
  \[
    f(\mathbf{a} + \mathbf{x})
    = \sum_{k=0}^{m-1} \frac{1}{k!}(D_{i_1 \cdots i_k}f)(\mathbf{a})x_{i_1} \cdots x_{i_k}
     + r(\mathbf{x})
  \]
  represents $f(\mathbf{a} + \mathbf{x})$ as the sum of its so-called
  ``Taylor polynomial of degree $m-1$,''
  plus a remainder that satisfies
  \[
    \lim_{\mathbf{x} \to \mathbf{0}} \frac{r(\mathbf{x})}{|\mathbf{x}|^{m-1}} = 0.
  \]
  Each of the inner sums extends over all ordered $k$-tuples
  $(i_1,\ldots,i_k)$, as in part (a);
  as usual, the zero-order derivative of $f$ is simply $f$,
  so that the constant term of the Taylor polynomial of $f$ at $\mathbf{a}$ is
  $f(\mathbf{a})$.}

\item[(c)]
  \emph{Exercise 9.29 shows that repetition occurs in the Taylor polynomial
  as written in part (b).
  For instance, $D_{113}$ occurs three times, as $D_{113}, D_{131}, D_{311}$.
  The sum of the corresponding three terms can be written in the form
  \[
    3 (D_1^2 D_3 f)(\mathbf{a})x_1^2 x_3.
  \]
  Prove (by calculating how often each derivative occurs) that the Taylor polynomial
  in be can be written in the form
  \[
    \sum \frac{(D_1^{s_1} \cdots D_n^{s_n}f)(\mathbf{a})}{s_1! \cdots s_n!}
      x_1^{s_1} \cdots x_n^{s_n}.
  \]
  Here the summation extends over all ordered $n$-tuples $(s_1,\ldots,s_n)$
  such that each $s_i$ is a nonnegative integer, and $s_1 + \cdots + s_n \leq m-1$.} \\
\end{enumerate}



\emph{Proof of (a).}
Induction on $k$.
\begin{enumerate}
\item[(1)]
  The base case $k = 1$.
  Note that
  \[
    f'(\mathbf{p}(t))
    =
    \begin{bmatrix}
      (D_1 f)(\mathbf{p}(t)) & \cdots & (D_n f)(\mathbf{p}(t))
    \end{bmatrix}
  \]
  and
  \[
    \mathbf{p}'(t)
    = \mathbf{x}
    =
    \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n
    \end{bmatrix}.
  \]
  Hence by the chain rule (Theorem 9.15),
  \begin{align*}
    h'(t)
    &= f'(\mathbf{p}(t)) \mathbf{p}'(t) \\
    &= \begin{bmatrix}
      (D_1 f)(\mathbf{p}(t)) & \cdots & (D_n f)(\mathbf{p}(t))
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n
    \end{bmatrix} \\
    &= \sum_{i=1}^{n} (D_i f)(\mathbf{p}(t))x_i.
  \end{align*}

\item[(2)]
  The inductive step.
  \emph{Show that for any $s \geq 1$,
  if $h^{(s)}(t) = \sum (D_{i_1 \cdots i_s}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_s}$ holds,
  then $h^{(s+1)}(t)
  = \sum (D_{i_1 \cdots i_{s+1}}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_{s+1}}$ also holds.}
  \begin{align*}
    h^{(s+1)}(t)
    &= \frac{d}{dt} h^{(s)}(t) \\
    &= \frac{d}{dt} \sum
      (D_{i_1 \cdots i_s}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_s} \\
    &= \sum
      \frac{d}{dt} (D_{i_1 \cdots i_s}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_s} \\
    &= \sum
      \left( \sum D_{i_{s+1}} (D_{i_1 \cdots i_s}f)(\mathbf{p}(t))x_{i_{s+1}} \right)
      x_{i_1} \cdots x_{i_s}
      &(\text{The chain rule}) \\
    &= \sum
      (D_{i_{s+1} i_1 \cdots i_s}f)(\mathbf{p}(t)) x_{i_{s+1}} x_{i_1} \cdots x_{i_{s}} \\
    &= \sum (D_{i_1 \cdots i_{s+1}}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_{s+1}}
      &(\text{Rearrange index}).
  \end{align*}
  Here
  \begin{align*}
    \frac{d}{dt} (D_{i_1 \cdots i_s}f)(\mathbf{p}(t))
    &= \begin{bmatrix}
      (D_1 D_{i_1 \cdots i_s}f)(\mathbf{p}(t)) & \cdots &
        (D_n D_{i_1 \cdots i_s}f)(\mathbf{p}(t))
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n
    \end{bmatrix} \\
    &= \sum_{i_{s+1}=1}^{n} D_{i_{s+1}} (D_{i_1 \cdots i_s}f)(\mathbf{p}(t))x_{i_{s+1}}.
  \end{align*}

\item[(3)]
  Since both the base case ((1)) and the inductive step ((2)) have been proved as true,
  by mathematical induction the conclusion holds for every positive integer $k$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    f(\mathbf{a}+\mathbf{x})
    =& h(1) \\
    =& \sum_{k=0}^{m-1} \frac{h^{(k)}(0)}{k!} + \frac{h^{(m)}(t)}{m!}
      &(\text{Theorem 5.15}) \\
    =& \sum_{k=0}^{m-1}
       \frac{1}{k!}
         \sum (D_{i_1 \cdots i_k}f)(\mathbf{p}(0)) x_{i_1} \cdots x_{i_k} \\
        &+ \sum \frac{1}{m!}(D_{i_1 \cdots i_m}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_m}
      &(\text{(a)}) \\
    =& \sum_{k=0}^{m-1}
       \frac{1}{k!}
         \sum (D_{i_1 \cdots i_k}f)(\mathbf{a}) x_{i_1} \cdots x_{i_k}
       + r(\mathbf{x})
  \end{align*}
  where
  \begin{align*}
    r(\mathbf{x})
    &= \frac{1}{m!} \sum (D_{i_1 \cdots i_m}f)(\mathbf{p}(t)) x_{i_1} \cdots x_{i_m} \\
    &= \frac{1}{m!} \sum (D_{i_1 \cdots i_m}f)(\mathbf{a}+t\mathbf{x}) x_{i_1} \cdots x_{i_m}
  \end{align*}
  for some $t \in (0,1)$.

\item[(2)]
  Since $f \in \mathscr{C}^{(m)}(E)$,
  $f$ is continuous on a compact subset
  \[
    K = \{ \mathbf{y} : |\mathbf{a} - \mathbf{y}| \leq |\mathbf{x}| \}
  \]
  of $E$ (by the construction of $\mathbf{x}$).
  Note that all $\mathbf{p}(t) = \mathbf{a} + t \mathbf{x} \in K$ for all $0 \leq t \leq 1$.
  Hence
  $(D_{i_1 \cdots i_m}f)(\mathbf{a}+t\mathbf{x})$ is bounded by some $M \in \mathbb{R}^1$
  (Theorem 4.15).
  Hence
  \begin{align*}
    \abs{ r(\mathbf{x}) }
    &= \abs{ \frac{h^{(m)}(t)}{m!} } \\
    &= \abs{ \frac{1}{m!}
      \sum (D_{i_1 \cdots i_m}f)(\mathbf{a}+t\mathbf{x}) x_{i_1} \cdots x_{i_m} } \\
    &\leq \frac{1}{m!} \sum \abs{ (D_{i_1 \cdots i_m}f)(\mathbf{a}+t\mathbf{x}) }
      \abs{ x_{i_1} } \cdots \abs{ x_{i_m} } \\
    &\leq \frac{1}{m!} \sum M |\mathbf{x}|^m \\
    &= \frac{1}{m!} \cdot m! M |\mathbf{x}|^m \\
    &= M |\mathbf{x}|^m .
  \end{align*}
  So
  \[
    0 \leq \abs{ \frac{r(\mathbf{x})}{|\mathbf{x}|^{m-1}} }
    \leq |\mathbf{x}|.
  \]
  Therefore,
  \[
    \lim_{\mathbf{x} \to \mathbf{0}} \frac{r(\mathbf{x})}{|\mathbf{x}|^{m-1}} = 0.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  As $s_1 + \cdots + s_n = k$,
  the number of terms of the form
  \[
    (D_1^{s_1} \cdots D_n^{s_n}f)(\mathbf{a}) x_1^{s_1} \cdots x_n^{s_n}
  \]
  is
  \[
    {k \choose s_1 \:\: \cdots \:\: s_n}
    = \frac{k!}{s_1! \cdots s_n!}.
  \]

\item[(2)]
  Hence we can write
  \begin{align*}
    f(\mathbf{a} + \mathbf{x})
    &= \sum_{k=0}^{m-1} \frac{1}{k!}(D_{i_1 \cdots i_k}f)(\mathbf{a})x_{i_1} \cdots x_{i_k}
      + r(\mathbf{x}) \\
    &= \sum_{s_1+\cdots+s_n \leq m-1}
      \frac{1}{k!} \frac{k!}{s_1! \cdots s_n!}
      (D_1^{s_1} \cdots D_n^{s_n}f)(\mathbf{a}) x_1^{s_1} x_n^{s_n} + r(\mathbf{x}) \\
    &= \sum_{s_1+\cdots+s_n \leq m-1}
      \frac{(D_1^{s_1} \cdots D_n^{s_n}f)(\mathbf{a})}{s_1! \cdots s_n!}
        x_1^{s_1} \cdots x_n^{s_n} + r(\mathbf{x}).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 9.31.}
\addcontentsline{toc}{subsection}{Exercise 9.31.}
\emph{Suppose $f \in \mathscr{C}^{(3)}$ in some neighborhood of a point
$\mathbf{a} \in \mathbb{R}^2$,
the gradient of $f$ is $\mathbf{0}$ at $\mathbf{a}$,
but not all second-order derivatives of $f$ are $0$ at $\mathbf{a}$.
Show how one can then determine from the Taylor polynomial of $f$ at $\mathbf{a}$
(of degree $2$) whether $f$ has a local maximum,
or a local minimum, or neither, at the point $\mathbf{a}$.
Extend this to $\mathbb{R}^n$ in place of $\mathbb{R}^2$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Since the gradient of $f$ is $\mathbf{0}$ at $\mathbf{a}$,
  \[
    (D_1 f)(\mathbf{a}) = (D_2 f)(\mathbf{a}) = 0.
  \]
  So that the Taylor polynomial of $f$ at $\mathbf{a}$ is
  \begin{align*}
    f(\mathbf{a}+\mathbf{x}) - f(\mathbf{a})
    =& (D_1 f)(\mathbf{a})x_1 + (D_2 f)(\mathbf{a})x_2 \\
      &+ \frac{1}{2}\left[
        (D_1^2 f)(\mathbf{a})x_1^2
        + 2(D_1 D_2 f)(\mathbf{a})x_1 x_2
        + (D_2^2 f)(\mathbf{a})x_2^2\right] \\
      &+ r(\mathbf{x}) \\
    =& \frac{1}{2}\left[
        (D_1^2 f)(\mathbf{a})x_1^2
        + 2(D_1 D_2 f)(\mathbf{a})x_1 x_2
        + (D_2^2 f)(\mathbf{a})x_2^2\right] \\
      &+ r(\mathbf{x}) \\
    =& \frac{1}{2}
        \begin{bmatrix}
          x_1 & x_2
        \end{bmatrix}
        \begin{bmatrix}
          (D_{11}f)(\mathbf{a}) & (D_{12}f)(\mathbf{a}) \\
          (D_{21}f)(\mathbf{a}) & (D_{22}f)(\mathbf{a})
        \end{bmatrix}
        \begin{bmatrix}
          x_1 \\
          x_2
        \end{bmatrix}
      + r(\mathbf{x}).
  \end{align*}
  Here $\mathbf{x} \in \mathbb{R}^2$ is so close to $\mathbf{0}$,
  and the remainder satisfies
  \[
    \lim_{\mathbf{x} \to \mathbf{0}} \frac{r(\mathbf{x})}{|\mathbf{x}|^{2}} = 0.
  \]

\item[(2)]
  Define the \textbf{Hessian matrix} of $f$ of $\mathbf{a}$ be
  \[
    H(\mathbf{a}) = \begin{bmatrix}
      (D_{11}f)(\mathbf{a}) & (D_{12}f)(\mathbf{a}) \\
      (D_{21}f)(\mathbf{a}) & (D_{22}f)(\mathbf{a})
    \end{bmatrix}.
  \]
  Let $H(\mathbf{a})_k$ be the submatrix of $H(\mathbf{a})$ obtained by
  taking the upper left-hand corner $k \times k$ submatrix of $H(\mathbf{a})$.
  Furthermore, let $\Delta_k = \det H(\mathbf{a})_k$, the $k$th principal
  minor of $H(\mathbf{a})$.
  \begin{enumerate}
  \item[(a)]
    $f$ has a local minimum if $H(\mathbf{a})$ is positive definite.
    Since $H(\mathbf{a})$ is positive definite if and only if
    $\Delta_k > 0$,
    $f$ has a local minimum if $\Delta_k > 0$ ($k = 1, 2$).

  \item[(b)]
    $f$ has a local maximum if $H(\mathbf{a})$ is negative definite.
    Since $H(\mathbf{a})$ is negative definite if and only if
    $(-1)^k \Delta_k > 0$,
    $f$ has a local maximum if $(-1)^k \Delta_k > 0$ ($k = 1, 2$).

  \item[(c)]
    $f$ has no local minimum or local maximum at the point $\mathbf{a}$
    if $H(\mathbf{a})$ is indefinite.
  \end{enumerate}
  (See Supplement (Second-derivative test for extrema) in Exercise 9.21.)

\item[(3)]
  Now we extend this to $\mathbb{R}^n$ in place of $\mathbb{R}^2$.
  Similar to (1)-(5),
  Define the \textbf{Hessian matrix} of $f$ of $\mathbf{a}$ be
  \[
    H(\mathbf{a}) = \begin{bmatrix}
      (D_{11}f)(\mathbf{a}) & \cdots & (D_{1n}f)(\mathbf{a}) \\
      \vdots & \ddots & \vdots \\
      (D_{n1}f)(\mathbf{a}) & \cdots & (D_{nn}f)(\mathbf{a})
    \end{bmatrix}.
  \]
  So
  \begin{enumerate}
  \item[(a)]
    $f$ has a local minimum if $\Delta_k > 0$ ($k = 1, \cdots, n$).

  \item[(b)]
    $f$ has a local maximum if $(-1)^k \Delta_k > 0$ ($k = 1, \cdots, n$).

  \item[(c)]
    $f$ has no local minimum or local maximum at the point $\mathbf{a}$
    if $H(\mathbf{a})$ is indefinite.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 10: Integration of Differential Forms \\}
\addcontentsline{toc}{section}{Chapter 10: Integration of Differential Forms}



\subsection*{Exercise 10.1.}
\addcontentsline{toc}{subsection}{Exercise 10.1.}
\emph{Let $H$ be a compact convex set in $\mathbb{R}^k$, with nonempty interior.
Let $f \in \mathscr{C}(H)$, put $f(\mathbf{x}) = 0$ in the complement of $H$,
and define $\int_{H} f$ as in Definition 10.3.
Prove that $\int_{H} f$ is independent of the order in which the $k$ integrations are carried out.
(Hint: Approximate $f$ by functions that are continuous on $\mathbb{R}^k$
and whose supports are in $H$,
as was done in Example 10.4.)} \\



\emph{Proof (Example 10.4).}
\begin{enumerate}
\item[(1)]
  Since $H$ is compact, $H$ is closed and bounded (Theorem 2.41).
  So there is a $k$-cell
  \[
    I^k = (a_1,b_1) \times \cdots \times (a_k,b_k) \subseteq \mathbb{R}^k
  \]
  such that $I^k \supseteq H$.
  Extend $f$ to a function on $I^k$ be setting $f(\mathbf{x}) = 0$ off $H$, and define
  \[
    \int_{H} f = \int_{I^k} f.
  \]
  Since $f$ may be discontinuous on $I^k$, the existence of the integral $\int_{I^k} f$
  needs proof.

\item[(2)]
  Define a function $\rho(\mathbf{x}): \mathbb{R}^k \to \mathbb{R}^1$ by
  \begin{equation*}
    \rho(\mathbf{x}) =
      \begin{cases}
        \rho_{\mathrm{bdd}(H)}(\mathbf{x})
        = \inf_{\mathbf{z} \in \mathrm{bdd}(H)} \abs{ \mathbf{x} - \mathbf{z} }
          & \text{ if $\mathbf{x} \in H$}, \\
        0
          & \text{ if $\mathbf{x} \not\in H$}.
      \end{cases}
  \end{equation*}
  \begin{enumerate}
  \item[(a)]
    \emph{Show that $\rho(\mathbf{x})$ is well-defined, that is,
    $\mathrm{bdd}(H) \neq \varnothing$.}
    (Reductio ad absurdum)
    If $\mathrm{bdd}(H) = \varnothing$, then for every $\mathbf{x} \in H$,
    there is an open neighborhood contained in $H$, that is, $H$ is open.
    So $H$ is closed and open, or $H = \varnothing$ or $H = \mathbb{R}^k$,
    contrary to $\mathrm{int}(H) \neq \varnothing$ (assumption) and the boundedness of $H$.

  \item[(b)]
    By the similar argument in Exercise 4.20(b),
    $\rho(\mathbf{x})$ is uniformly continuous on $\mathbb{R}^k$.
  \end{enumerate}

\item[(3)]
  Also, define a function $\varphi_{\delta}(t): \mathbb{R}^1 \to \mathbb{R}^1$
  for some $\delta \in (0,1)$ by
  \begin{equation*}
    \varphi_{\delta}(t) =
      \begin{cases}
        1
          & \text{ if $t \geq \delta$}, \\
        \frac{t}{\delta}
          & \text{ if $\delta \geq t \geq 0$}, \\
        0
          & \text{ if $t = 0$}.
      \end{cases}
  \end{equation*}

\item[(4)]
  Finally define
  \[
    F_{\delta}(\mathbf{x}) = \varphi_{\delta}(\rho(\mathbf{x}))f(\mathbf{x})
    \qquad
    (\mathbf{x} \in I^k).
  \]
  We prove that $F_{\delta}$ is continuous on $I^k$.
  \begin{enumerate}
  \item[(a)]
    By construction, $F_{\delta}$ is continuous on $\mathrm{int}(H) \bigcup \mathrm{int}(I^k-H)$.
    So it suffices to show that $F_{\delta}$ is continuous on
    \[
      I^k - \left(\mathrm{int}(H) \bigcup \mathrm{int}(I^k-H)\right) = \mathrm{bdd}(H).
    \]

  \item[(b)]
    Given $\mathbf{x}_0 \in \mathrm{bdd}(H)$.
    Since $\varphi_{\delta}$ and $\rho$ are continuous,
    $\varphi_{\delta} \circ \rho$ is continuous (Theorem 4.7).
    Besides, since $f$ is continuous on a compact set $H$ and $f = 0$ off $H$,
    $f$ is bounded on $I^k$ (Theorem 4.14).
    Note that $\varphi_{\delta}(\rho(\mathbf{x}_0)) = 0$.
    Therefore,
    \[
      \lim_{\mathbf{x} \to \mathbf{x}_0} F_{\delta}(\mathbf{x})
      = \lim_{\mathbf{x} \to \mathbf{x}_0} \varphi_{\delta}(\rho(\mathbf{x}))f(\mathbf{x})
      = 0
      = F_{\delta}(\mathbf{x}_0).
    \]
  \end{enumerate}

\item[(5)]
  Summary:
  \begin{enumerate}
  \item[(a)]
    $f$ is bounded on $I^k$.

  \item[(b)]
    $F_{\delta} = f$ off $H$ for all $\delta$.

  \item[(c)]
    As $\delta \to 0$,
    $F_{\delta}$ converges to $f$ on $\mathrm{int}(H)$ and
    converges uniformly on every compact subset $K \subseteq \mathrm{int}(H)$.
  \end{enumerate}

\item[(6)]
  Given an integer $1 \leq m \leq k$.
  Write $\mathbf{x} = (\mathbf{x}',x_m,\mathbf{x}'')$ where
  $\mathbf{x}' = (x_1,\ldots,x_{m-1})$ and $\mathbf{x}'' = (x_{m+1},\ldots,x_k)$
  in the sense of Theorem 10.38.
  (When $m = 1$, $\mathbf{x}'$ is absent; when $m = k$, $\mathbf{x}''$ is absent.)
  Let $H_{m}$ be the projection of $H$ on $\mathbb{R}^{m}$ in the sense of Theorem 10.38,
  say
  \[
    H_{m} = \{ (\mathbf{x}',x_m) : (\mathbf{x}',x_m,\mathbf{x}'') \in H
    \text{ for some } \mathbf{x}'' \}.
  \]
  Moreover, we apply the same definition for the projection $I^{m}$ of $I^{k}$
  In particular, $I^{m} = (a_1,b_1) \times \cdots \times (a_m,b_m) \subseteq \mathbb{R}^m$.
  Note that $H_{m}$ is convex and the interior of a convex set is convex.
  \begin{enumerate}
  \item[(a)]
    Take $\mathbf{a}, \mathbf{b} \in \mathrm{int}(H_{m})$ and $\lambda \in (0,1)$.
    Consider $\mathbf{c} = \lambda \mathbf{a} + (1-\lambda) \mathbf{b}$.
    Since $H_{m}$ is convex, $\mathbf{c} \in H_m$.

  \item[(b)]
    Also, there are open neighborhoods
    $B(\mathbf{a};r_{\mathbf{a}}), B(\mathbf{b};r_{\mathbf{b}}) \subseteq H_{m}$.
    Therefore, $B(\mathbf{c};r_{\mathbf{c}})$
    with $r_{\mathbf{c}} = \lambda r_{\mathbf{a}} + (1-\lambda) r_{\mathbf{b}})$
    is an open neighborhood contained in $H_m$.

  \item[(c)]
    In fact, for any $\mathbf{p}_c \in B(\mathbf{c};r_{\mathbf{c}})$,
    there exists
    \begin{align*}
      \mathbf{p}_a
      &= \mathbf{a} + \frac{r_{\mathbf{a}}}{r_{\mathbf{c}}}( \mathbf{p}_c - \mathbf{c} )
        \in B(\mathbf{a};r_{\mathbf{a}}) \subseteq H_m, \\
      \mathbf{p}_b
      &= \mathbf{b} + \frac{r_{\mathbf{b}}}{r_{\mathbf{c}}}( \mathbf{p}_c - \mathbf{c} )
        \in B(\mathbf{b};r_{\mathbf{b}}) \subseteq H_m
    \end{align*}
    such that $\mathbf{p}_c = \lambda \mathbf{p}_a + (1-\lambda) \mathbf{p}_b \in H_m$
    by the convexity of $H_m$.
  \end{enumerate}

\item[(7)]
  \emph{Show that (5) also holds for $I^{k-1}$, $H_{k-1}$, $f_{k-1}$ and $F_{k-1}$, that is,}
  \begin{enumerate}
  \item[(a)]
    \emph{$f_{k-1}$ is bounded on $I^{k-1}$.}

  \item[(b)]
    \emph{$F_{\delta,k-1} = f_{k-1}$ off $H_{k-1}$ for all $\delta$.}

  \item[(c)]
    As $\delta \to 0$,
    $F_{\delta,k-1}$ converges to $f_{k-1}$ on $\mathrm{int}(H_{k-1})$ and
    converges uniformly on every compact subset $K \subseteq \mathrm{int}(H_{k-1})$.
  \end{enumerate}
  (a)(b) are trivial.
  For (c), it suffices to show that for every
  \[
    \mathbf{y} = (x_1, \ldots, x_{k-1}) \in \mathrm{int}(H_{k-1}),
  \]
  there is an open neighborhood $B(\mathbf{y})$ satisfying
  $\overline{B(\mathbf{y})} \subseteq \mathrm{int}(H_{k-1})$
  such that $F_{\delta,k-1}$ converges to $f_{k-1}$ uniformly on $B(\mathbf{y})$.

\item[(8)]
  Now we prove (7) as follows:
  \begin{enumerate}
  \item[(a)]
    Define
    \[
      S_k = \{ x_k : (\mathbf{y}, x_k) \in H \} \subseteq \mathbb{R}^1.
    \]
    $S_k$ is an interval since $H$ is convex.
    Besides, $S_k$ is compact since $H$ is compact.
    Therefore, $S_k$ is a closed interval of finite length.

  \item[(b)]
    Define $\mathbf{x} = (\mathbf{y}, x_k)$ and
    \begin{align*}
      T_k
      &=
      \{ x_k : F_{\delta}(\mathbf{x}) \neq f(\mathbf{x}) \} \\
      &=
      \{ x_k : \varphi_{\delta}(\rho(\mathbf{x})) \neq 1
        \text{ and }
        f(\mathbf{x}) \neq 0 \} \\
      &=
      \{ x_k : 0 \leq \rho(\mathbf{x}) < \delta
        \text{ and }
        \mathbf{x} \in H \} \\
      &=
      \{ x_k : 0 \leq \rho_{\mathrm{bdd}(H)}(\mathbf{x}) < \delta
        \text{ and }
        \mathbf{x} \in H \}
    \end{align*}
    as a subset of $S_k$.
    If $x_k \in T_k$ is an interior point of $S_k$,
    then clearly $\mathbf{x}$ is an interior point of $H$,
    or $\rho_{\mathrm{bdd}(H)}(\mathbf{x}) > 0$
    since $\rho_{\mathrm{bdd}(H)}(\mathbf{x}) = 0$ if and only if
    $\mathbf{x} \in \mathrm{bdd}(H)$ (Exercise 4.20(a)).

  \item[(c)]
    Write $S_k = [\alpha_k,\beta_k]$ and
    consider smaller subsets
    \begin{align*}
      S_{k,\varepsilon}
      &=
      [\alpha_k+\varepsilon,\beta_k-\varepsilon]
        \subseteq
        \mathrm{int}(S_k), \\
      T_{k,\varepsilon}
      &=
      \{ x_k \in S_{k,\varepsilon} : 0 < \rho_{\mathrm{bdd}(H)}(\mathbf{x}) < \delta
        \text{ and }
        \mathbf{x} \in H \}
    \end{align*}
    for some $\varepsilon > 0$.
    Define an uniformly continuous $\mu(x_k): S_{k,\varepsilon} \to \mathbb{R}^1$ by
    \[
      \mu(x_k)
      = \rho_{\mathrm{bdd}(H)}(\mathbf{x}).
    \]
    Note that $\mu(x_k) > 0$ for all $x_k \in S_{k,\varepsilon}$
    since $S_{k,\varepsilon} \cap \mathrm{bdd}(H) = \varnothing$
    by construction.

  \item[(d)]
    \emph{Show that $\mu(x_k)$ is concave on $S_{k,\varepsilon}$.}
    Given any $a, b \in S_{k,\varepsilon}$ and $\lambda \in (0,1)$.
    $\mu(a) = \rho_{\mathrm{bdd}(H)}(\mathbf{a}) > 0$ and
    $\mu(b) = \rho_{\mathrm{bdd}(H)}(\mathbf{b}) > 0$.
    ($\mathbf{a} = (\mathbf{y},a)$ and $\mathbf{b} = (\mathbf{y},b)$.)
    Since $\mathrm{bdd}(H)$ is compact,
    there exist corresponding $\mathbf{p}_a, \mathbf{p}_b \in \mathrm{bdd}(H)$
    such that
    \begin{align*}
      \mu(a) &= \abs{\mathbf{a} - \mathbf{p}_a} > 0, \\
      \mu(b) &= \abs{\mathbf{b} - \mathbf{p}_b} > 0.
    \end{align*}
    That is,
    \begin{align*}
      B(\mathbf{a}; \mu(a)) &\subseteq H, \\
      B(\mathbf{b}; \mu(b)) &\subseteq H.
    \end{align*}
    By (6)(b), we have
    \[
      B(\lambda\mathbf{a} + (1-\lambda)\mathbf{b}; \lambda\mu(a) + (1-\lambda)\mu(b))
      \subseteq H.
    \]
    So
    \begin{align*}
      \mu(\lambda a + (1-\lambda) b)
      &= \rho_{\mathrm{bdd}(H)}(\lambda\mathbf{a} + (1-\lambda)\mathbf{b}) \\
      &\geq \lambda\mu(a) + (1-\lambda)\mu(b)
    \end{align*}
    by the definition of $\mathrm{bdd}(H)$.
    Therefore, $\mu(x_k)$ is concave on $S_{k,\varepsilon}$.

  \item[(e)]
    By (d),
    \[
      \mu(x_k)
      \geq
      \underbrace{
        \min\{ \mu(\alpha_k+\varepsilon), \mu(\beta_k-\varepsilon) \}}_{
          \text{say } c_{\varepsilon} > 0}
    \]
    for all $x_k \in S_{k,\varepsilon}$.
    Thus in (b) we have
    \begin{align*}
      \delta
      &>
      \rho_{\mathrm{bdd}(H)}(\mathbf{x}) \\
      &=
      \mu(x_k) \\
      &\geq
      \underbrace{\frac{c_{\varepsilon}}{\beta_k-\alpha_k+1}}_{
        \text{say } C_{\varepsilon} > 0}
      \cdot \min\{ x_k - (\alpha_k+\varepsilon), (\beta_k-\varepsilon) - x_k \},
    \end{align*}
    or
    \[
      \min\{ x_k - (\alpha_k+\varepsilon), (\beta_k-\varepsilon) - x_k \}
      < C_{\varepsilon}^{-1} \delta.
    \]

  \item[(f)]
    By (b)(e), the length of $T_k$ is
    \[
      |T_k| \leq 2(\varepsilon + C_{\varepsilon}^{-1} \delta).
    \]
    Thus we have
    \[
      |F_{\delta,k-1}(\mathbf{y}) - f_{k-1}(\mathbf{y})|
      \leq 2(\varepsilon + C_{\varepsilon}^{-1} \delta) \norm{f}
    \]
    for any fixed $\mathbf{y} \in \mathrm{int}(H_{k-1})$.
    Hence, as $\varepsilon \to 0$ and then $\delta \to 0$,
    we have
    $F_{\delta,k-1}$ converges to $f_{k-1}$ pointwise on $\mathrm{int}(H_{k-1})$,
    and
    uniformly on any open neighborhood $B(\mathbf{y})$ of $\mathbf{y}$
    satisfying $\overline{B(\mathbf{y})} \subseteq \mathrm{int}(H_{k-1})$.
  \end{enumerate}

\item[(9)]
  Continue this process described in (7) from $k-1$ to $1$.
  Note that $H_1$ is a closed interval of finite length (by (8)(a)).
  So (5) and Theorem 6.12(d) implies that
  \[
    \int_{a_1}^{b_1} F_{\delta,1}(x_1) dx_1 \to \int_{a_1}^{b_1} f(x_1) dx_1
  \]
  uniformly.
  Since $F_{\delta} \in \mathscr{C}(I^k)$, $\int F_{\delta}$ is unaffected by
  any change in this order.
  Hence it is also true of $\int_{I^k} f = \int_{H} f$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.2.}
\addcontentsline{toc}{subsection}{Exercise 10.2.}
\emph{For $i=1,2,3,\ldots$, let $\varphi_i \in \mathscr{C}(\mathbb{R}^1)$ have support
in $(2^{-i},2^{1-i})$, such that $\int \varphi_i = 1$.
Put
\[
  f(x,y) = \sum_{i=1}^{\infty}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
\]
Then $f$ has compact support in $\mathbb{R}^2$,
$f$ is continuous except at $(0,0)$,
and
\[
  \int dy \int f(x,y) dx = 0
  \qquad
  \text{ but }
  \qquad
  \int dx \int f(x,y) dy = 1.
\]
Observe that $f$ is unbounded in every neighborhood of $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  If $f, g: \mathbb{R}^{n} \to \mathbb{R}^{m}$ are two functions,
  then
  \begin{enumerate}
  \item[(a)]
    $\mathrm{supp}(fg) \subseteq \mathrm{supp}(f) \cap \mathrm{supp}(g)$.
  \item[(b)]
    $\mathrm{supp}(f+g) \subseteq \mathrm{supp}(f) \cup \mathrm{supp}(g)$.
  \end{enumerate}

\item[(2)]
  Note that $f(x,y)$ is well-defined on $\mathbb{R}^2$
  since only finitely many terms are nonzero for each fixed point $(x,y) \in \mathbb{R}^2$ (by (1)).
  Besides,
  \begin{align*}
    &\mathrm{supp}([ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)) \\
    \subseteq \: &
    \{ (x,y) :
      x \in \mathrm{supp}(\varphi_i) \cup \mathrm{supp}(\varphi_{i+1}),
      y \in \mathrm{supp}(\varphi_i)
    \} \\
    \subseteq \: &
    \{ (x,y) :
      x \in (2^{-i},2^{-i+1}) \cup (2^{-i-1},2^{-i}),
      y \in (2^{-i},2^{-i+1})
    \} \\
    \subseteq \: &
    \{ (x,y) :
      x \in (0,1),
      y \in (0,1)
    \}
  \end{align*}
  for all $i=1,2,3,\ldots$.
  So $\mathrm{supp}(f) \subseteq (0,1)^2$, or $\mathrm{supp}(f)$ is bounded.
  As $\mathrm{supp}(f)$ is closed (by definition),
  $\mathrm{supp}(f)$ is compact (Theorem 2.41).

\item[(3)]
  \emph{Show that $f(x,y)$ is not continuous at $(0,0)$.}
  \begin{enumerate}
  \item[(a)]
    Note that $f(0,0) = 0$ since $(0,0) \not\in \mathrm{supp}(f) \subseteq (0,1)^2$.
    It suffices to show that there exists a sequence
    $\{ (t_n,t_n) \}$ in $\mathbb{R}^2$ such that
    $(t_n,t_n) \neq (0,0)$, $\lim_{n \to \infty}(t_n,t_n) = (0,0)$
    but $\lim_{n \to \infty} f(t_n,t_n)$ does not converge to $0$
    (Theorem 4.2).

  \item[(b)]
    For any $n = 1,2,3,\ldots$,
    \begin{align*}
      1
      = \int \varphi_n
      = \int_{2^{-n}}^{2^{-n+1}} \varphi(t)dt
      \leq 2^{-n} \sup_{t \in [2^{-n},2^{-n+1}]} \varphi(t),
    \end{align*}
    or $\sup_{t \in [2^{-n},2^{-n+1}]} \varphi(t) \geq 2^n$.
    By the continunity of $\varphi_n$, there exists $t_n \in [2^{-n},2^{-n+1}]$
    such that $\varphi_n(t_n) \geq 2^n$ (Theorem 4.16).

  \item[(c)]
    We construct $\{ (t_n,t_n) \}$ in $\mathbb{R}^2$ by (b) for all $n=1,2,3,\ldots$.
    Clearly, $(t_n,t_n) \neq (0,0)$ and $\lim_{n \to \infty}(t_n,t_n) = (0,0)$.
    However,
    \[
      f(t_n,t_n)
      = [\varphi_{n}(t_n) - \varphi_{n+1}(t_n)]\varphi_{n}(t_n)
      = \varphi_{n}(t_n)^2
      \geq 2^{2n}
    \]
    does not converge to $0$ as $n \to \infty$.
  \end{enumerate}

\item[(4)]
  \emph{Show that $f(x,y)$ is continuous at $\mathbf{x}_0 = (x_0,y_0) \neq (0,0)$.}
  Consider an open neighborhood
  $B(\mathbf{x}_0;r)$ of $\mathbf{x}_0$ with $r=\frac{\norm{\mathbf{x}_0}}{64} > 0$.
  Hence,
  \[
    f(x,y)|_{B(\mathbf{x}_0;r)}
    = \sum_{i=1}^{N}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
  \]
  is the sum of finitely many terms
  where $N = \log_2 \frac{89}{\norm{\mathbf{x}_0}} \geq 1$
  (since $[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) = 0$
  on ${B(\mathbf{x}_0;r)}$ whenever $i \geq N$).
  Therefore,
  $f(x,y)|_{B(\mathbf{x}_0;r)}$ is continuous
  by the continunity of $\varphi_i$.

\item[(5)]
  \emph{Show that $\int dy \int f(x,y) dx = 0$.}
  For any fixed $y$,
  there is a positive integer $N(y)$ such that
  $\varphi_{N(y)+1}(y) = \varphi_{N(y)+2}(y) = \ldots = 0$ and
  \[
    f(x,y) = \sum_{i=1}^{N(y)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y).
  \]
  So
  \begin{align*}
    \int f(x,y) dx
    &= \int \sum_{i=1}^{N(y)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) dx \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \int [ \varphi_i(x)-\varphi_{i+1}(x) ] dx \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \left( \int \varphi_i(x) dx - \int \varphi_{i+1}(x) dx \right) \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \left( 1 - 1 \right) \\
    &= 0,
  \end{align*}
  and thus
  \[
    \int dy \int f(x,y) dx = \int 0 dy = 0.
  \]

\item[(6)]
  \emph{Show that $\int dx \int f(x,y) dy = 0$.}
  For any fixed $x$,
  there is a positive integer $N(x)$ such that
  $\varphi_{N(x)+1}(x) = \varphi_{N(x)+2}(x) = \ldots = 0$ and
  \[
    f(x,y) = \sum_{i=1}^{N(x)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y).
  \]
  So
  \begin{align*}
    \int f(x,y) dy
    &= \int \sum_{i=1}^{N(x)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) dy \\
    &= \sum_{i=1}^{N(x)} [ \varphi_i(x)-\varphi_{i+1}(x) ] \int \varphi_i(y) dy \\
    &= \sum_{i=1}^{N(x)} [ \varphi_i(x)-\varphi_{i+1}(x) ] \\
    &= \varphi_1(x), \\
  \end{align*}
  and thus
  \[
    \int dx \int f(x,y) dy = \int \varphi_1(x) dx = 1.
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.3.}
\addcontentsline{toc}{subsection}{Exercise 10.3.}
\begin{enumerate}
\item[(a)]
  \emph{If $\mathbf{F}$ is as in Theorem 10.7,
  put $\mathbf{A} = \mathbf{F}'(\mathbf{0})$,
  $\mathbf{F}_1(\mathbf{x}) = \mathbf{A}^{-1} \mathbf{F}(\mathbf{x})$.
  Then $\mathbf{F}_1(\mathbf{0}) = \mathbf{I}$.
  Show that
  \[
    \mathbf{F}_1(\mathbf{x})
    = \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x})
  \]
  in some neighborhood of $\mathbf{0}$,
  for certain primitive mappings $\mathbf{G}_{1}, \ldots, \mathbf{G}_{n}$.
  This gives another version of Theorem 10.7:}
  \[
    \mathbf{F}(\mathbf{x})
    =
    \mathbf{F}'(\mathbf{0})
    \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x}).
  \]

\item[(b)]
  \emph{Prove that the mapping $(x,y) \mapsto (y,x)$ of $\mathbb{R}^2$ onto $\mathbb{R}^2$
  is not the composition of any two primitive mappings,
  in any neighborhood of the origin.
  (This shows that the flips $B_i$ cannot be omitted from the statement of Theorem 10.7.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Suppose $\mathbf{F}$ is a $\mathscr{C}'$-mapping of an open set $E \subseteq \mathbb{R}^n$
  into $\mathbb{R}^n$, $\mathbf{0} \in E$, $\mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'(\mathbf{0})$ is invertible.

\item[(2)]
  Similar to the proof of Theorem 10.7.
  Put $\mathbf{F}_1 = \mathbf{F}$.

\item[(3)]
  As $m = 1$, there is an open neighborhood $V_1 \subseteq E$ of $\mathbf{0}$
  such that
  $\mathbf{F}_1(\mathbf{0}) = (\mathbf{F}'(\mathbf{0}))^{-1} \mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  $\mathbf{F}'_1(\mathbf{0}) = \mathbf{I}$ is invertible, and
  \[
    \mathbf{F}_1(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_1$.
  Hence
  \[
    \mathbf{F}'_1(\mathbf{0})\mathbf{e}_1
    = \sum_{i=1}^{n} (D_1 \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_1 \alpha_1)(\mathbf{0}) = 1 \neq 0$, and we might pick $B_1 = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_1(\mathbf{x})
    = \mathbf{x} + [\alpha_1(\mathbf{x}) - x_1] \mathbf{e}_1
    \qquad
    (\mathbf{x} \in V_1).
  \]
  Then $\mathbf{G}_1 \in \mathscr{C}'(V_1)$, $\mathbf{G}_1$ is primitive,
  and $\mathbf{G}'_1(\mathbf{0}) = \mathbf{I}$ is invertible.

\item[(4)]
  Now we make the induction hypothesis for $1 \leq m \leq n-1$.

\item[(5)]
  Since $\mathbf{G}'_{m}(\mathbf{0}) = \mathbf{I}$ is invertible,
  the inverse function theorem shows that there is an open set $U_{m}$,
  with $\mathbf{0} \in U_{m} \subseteq V_{m}$, such that
  $\mathbf{G}_m$ is an injective mapping of $U_{m}$ onto a neighborhood $V_{m+1}$ of $\mathbf{0}$,
  in which $\mathbf{G}_m^{-1} \in \mathscr{C}'(V_{m+1})$.
  Define $\mathbf{F}_{m+1}$ by
  \[
    \mathbf{F}_{m+1}(\mathbf{y}) = \mathbf{F}_{m} \circ \mathbf{G}_m^{-1}(\mathbf{y})
    \qquad
    (\mathbf{y} \in V_{m+1}).
  \]
  Then $\mathbf{F}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{F}_m(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible by the chain rule
  and the inverse function theorem.
  So
  \[
    \mathbf{F}_{m+1}(\mathbf{x})
    = P_m \mathbf{x}
      + \sum_{i=m+1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_{m+1}$.
  Hence
  \[
    \mathbf{F}'_{m+1}(\mathbf{0})\mathbf{e}_{m+1}
    = \sum_{i=m+1}^{n} (D_{m+1} \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_{m+1} \alpha_{m+1})(\mathbf{0}) = 1 \neq 0$,
  and we might pick $B_{m+1} = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_{m+1}(\mathbf{x})
    = \mathbf{x} + [\alpha_{m+1}(\mathbf{x}) - x_{m+1}] \mathbf{e}_{m+1}
    \qquad
    (\mathbf{x} \in V_{m+1}).
  \]
  Then $\mathbf{G}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{G}_{m+1}$ is primitive,
  and $\mathbf{G}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible.
  Our induction hypothesis holds therefore with $m+1$ in place of $m$.

\item[(6)]
  Note that
  \[
    \mathbf{F}_{m}(\mathbf{x}) = \mathbf{F}_{m+1}(\mathbf{G}_{m}(\mathbf{x}))
    \qquad
    (\mathbf{x} \in U_{m}).
  \]
  If we apply this with $m = 1, \ldots, n-1$, we successively obtain
  \[
    \mathbf{F}_1
    = \mathbf{F}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1
  \]
  in some open neighborhood of $\mathbf{0}$.
  Note that $\mathbf{F}_{n}$ is primitive since
  \[
    \mathbf{F}_{n}(\mathbf{x})
    = P_{n-1} \mathbf{x} + \alpha_n(\mathbf{x}) \mathbf{e}_n.
  \]
  This completes the proof.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  For $(x,y) \in \mathbb{R}^2$, define
  \[
    \mathbf{F}(x,y) = (y,x).
  \]

\item[(2)]
  (Reductio ad absurdum)
  If $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$ for some primitive mappings
  $\mathbf{G}_{i}$ ($i = 1,2$) in some neighborhood $V_{i}$ of the origin,
  $\mathbf{G}_{i}(\mathbf{0}) = \mathbf{0}$ and $\mathbf{G}_{i}'$ is invertible,
  then we may assume that
  \[
    \mathbf{G}_1(x,y) = (x, g_1(x,y))
    \qquad
    \text{and}
    \qquad
    \mathbf{G}_2(x,y) = (g_2(x,y),y).
  \]
  Here the case $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  is similar to the above case.
  Besides, $\mathbf{G}_1(x,y) = (x,g_1(x,y))$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  implies that
  \[
    \mathbf{G}_2 \circ \mathbf{G}_1(x,y) = (x,g_2(x,g_1(x,y)))
    \neq (y,x) = \mathbf{F}(x,y).
  \]
  Same reason for
  $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (g_2(x,y),y)$.

\item[(3)]
  Note that
  \[
    \mathbf{F}'(\mathbf{0})
    =
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
  \]
  Since
  \[
    \mathbf{F}'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{G}_1(\mathbf{0})) \mathbf{G}_1'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{0}) \mathbf{G}_1'(\mathbf{0}),
  \]
  we have
  \begin{align*}
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      D_1 g_2(0,0) & D_2 g_2(0,0) \\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      * & * \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix}.
  \end{align*}
  So $D_1 g_1(0,0) = 1$ and $D_2 g_1(0,0) = 0$, and thus
  $\mathbf{G}_1'(\mathbf{0})
  =
  \begin{bmatrix}
    1 & 0 \\
    1 & 0
  \end{bmatrix}$
  is not invertible, which is absurd.


\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.4.}
\addcontentsline{toc}{subsection}{Exercise 10.4.}
\emph{For $(x,y) \in \mathbb{R}^2$, define
\[
  \mathbf{F}(x,y) = (e^x \cos y - 1, e^x \sin y)
\]
Prove that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$, where
\begin{align*}
  \mathbf{G}_1(x,y) &= (e^x \cos y - 1, y) \\
  \mathbf{G}_2(u,v) &= (u, (1+u) \tan v)
\end{align*}
are primitive in some neighborhood of $(0,0)$.
Compute the Jacobians of $\mathbf{G}_1$, $\mathbf{G}_2$, $\mathbf{F}$ at $(0,0)$.
Define
\[
  \mathbf{H}_2(x,y) = (x, e^x \sin y)
\]
and find
\[
  \mathbf{H}_1(u,v) = (h(u,v),v)
\]
so that $\mathbf{F} = \mathbf{H}_1 \circ \mathbf{H}_2$
is in some neighborhood of $(0,0)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Definition 10.5,
  \begin{align*}
    \mathbf{G}_1(x,y) &= (e^x \cos y - 1) \mathbf{e}_1 + y \mathbf{e}_2, \\
    \mathbf{G}_2(u,v) &= u \mathbf{e}_1 + ((1+u) \tan v) \mathbf{e}_2
  \end{align*}
  are primitive in some neighborhood of $(0,0)$.

\item[(2)]
  \emph{Show that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$.}
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{G}_2 \circ \mathbf{G}_1)(x,y)
    &= \mathbf{G}_2(\mathbf{G}_1(x,y)) \\
    &= \mathbf{G}_2(e^x \cos y - 1, y) \\
    &= (e^x \cos y - 1, (1+(e^x \cos y - 1)) \tan y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\item[(3)]
  Since
  \begin{align*}
    J_{\mathbf{G}_1}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      0 & 1
    \end{bmatrix}
    = e^x \cos y \\
    J_{\mathbf{G}_2}(x,y)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      \tan y & (1+x)\sec^2 y
    \end{bmatrix}
    = (1+x)\sec^2 y \\
    J_{\mathbf{F}}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y & e^x \cos y
    \end{bmatrix}
    = e^{2x},
  \end{align*}
  \begin{align*}
    J_{\mathbf{G}_1}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{G}_2}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{F}}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1.
  \end{align*}

\item[(4)]
  Define $h(u,v) = \sqrt{e^{2u} - v^{2}} - 1$ on
  \[
    B\left((0,0);\frac{1}{64}\right) \subseteq \mathbb{R}^2.
  \]
  $h(u,v)$ is well-defined since $e^{2u}-v^2 > 0$
  for all $(u,v) \in B\left((0,0);\frac{1}{64}\right)$.

\item[(5)]
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{H}_1 \circ \mathbf{H}_2)(x,y)
    &= \mathbf{H}_1(\mathbf{H}_2(x,y)) \\
    &= \mathbf{H}_1(x, e^x \sin y) \\
    &= (\sqrt{e^{2x} - (e^x \sin y)^2} - 1, e^x \sin y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.5.}
\addcontentsline{toc}{subsection}{Exercise 10.5.}
\emph{Formulate and prove an analogue of Theorem 10.8,
in which $K$ is a compact subset of an arbitrary metric space.
(Replace the functions $\varphi_i$ that occur in the proof of Theorem 10.8
by functions of the type constructed in Exercise 4.22.)} \\

\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  \emph{(Partitions of unity.)
  Suppose $K$ is a compact subset of a metric space $X$,
  and $\{V_{\alpha}\}$ is an open cover of $K$.
  Then there exist functions $\psi_1, \ldots, \psi_s \in \mathscr{C}(X)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_i \leq 1$ for $1 \leq i \leq s$.}

  \item[(b)]
    \emph{each $\psi_i$ has its support in some $V_{\alpha}$, and}

  \item[(c)]
    \emph{$\psi_1(x) + \cdots + \psi_s(x) = 1$ for every $x \in K$.}
  \end{enumerate}

\item[(2)]
  It is trivial that some $V_{\alpha} = X$
  by taking $s = 1$ and $\psi_1(x) = 1 \in \mathscr{C}(X)$.
  Now we assume that all $V_{\alpha} \subsetneq X$.

\item[(3)]
  Associate with each $x \in K$ an index $\alpha(x)$ so that $x \in V_{\alpha(x)}$.
  Then there are open balls $B(x)$ and $W(x)$, centered at $x$,
  with
  \[
    x
    \in B(x)
    \subseteq \overline{B(x)}
    \subseteq W(x)
    \subseteq \overline{W(x)}
    \subseteq V_{\alpha(x)}
  \]
  (Since $V_{\alpha(x)}$ is open, there exists $r > 0$
  such that $B(x;r) \subseteq V_{\alpha(x)}$.
  Take $B(x) = B\left(x;\frac{r}{89}\right)$
  and $W(x) = B\left(x;\frac{r}{64}\right)$.)

\item[(4)]
  Since $K$ is compact, there are finitely many points
  $x_1, \ldots, x_s \in K$ such that
  \[
    K \subseteq B(x_1) \cup \cdots \cup B(x_s).
  \]
  Note that
  \begin{enumerate}
  \item[(a)]
    $\overline{B(x_i)}$ is a nonempty closed set since $x_i \in B(x_i) \subseteq \overline{B(x_i)}$.

  \item[(b)]
    $X - W(x_i) \supseteq X - V_{\alpha(x_i)}$ is a nonempty closed set by the assumption in (2).

  \item[(c)]
    $\overline{B(x_i)} \cap (X - W(x_i)) \subseteq W(x_i) \cap (X - W(x_i)) = \varnothing$.

  \end{enumerate}
  By Exercise 4.22, there is a function
  \[
    \varphi_i(x)
    = \frac{\rho_{\overline{B(x_i)}}(x)}{\rho_{\overline{B(x_i)}}(x) + \rho_{X-W(x_i)}(x)}
    \in \mathscr{C}(X)
  \]
  such that $\varphi_i(x) = 1$ on $\overline{B(x_i)}$,
  $\varphi_i(x) = 0$ outside $W(x_i)$, and $0 \leq \varphi_i(x) \leq 1$ on $X$
  for $1 \leq i \leq s$.

\item[(5)]
  Define $\psi_{1} = \varphi_{1} \in \mathscr{C}(X)$ and
  \[
    \psi_{i+1} = (1-\varphi_{1}) \cdots (1-\varphi_{i})\varphi_{i+1} \in \mathscr{C}(X)
  \]
  for $1 \leq i \leq s-1$.
  Properties (a) and (b) in (1) are clear.
  Also,
  \[
    \psi_1(x) + \cdots + \psi_s(x) = 1 - (1-\varphi_1(x)) \cdots (1-\varphi_s(x))
  \]
  by the construction of $\psi_i$.
  If $x \in K$, then $x \in B(x_i)$ for some $i$, hence $\varphi_i(x)=1$,
  and the product $(1-\varphi_1(x)) \cdots (1-\varphi_s(x)) = 0$.
  This proves property (c) in (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.6.}
\addcontentsline{toc}{subsection}{Exercise 10.6.}
\emph{Strengthen the conclusion of Theorem 10.8 by showing that
the functions $\psi_i$ can be made differentiable, and even infinitely differentiable.
(Use Exercise 8.1 in the construction of the auxiliary functions $\psi_i$.)} \\



\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  It is trivial that some $V_{\alpha} = \mathbb{R}^n$
  by taking $s = 1$ and $\psi_1(\mathbf{x}) = 1 \in \mathscr{C}^{\infty}(\mathbb{R}^n)$.
  Now we assume that all $V_{\alpha} \subsetneq \mathbb{R}^n$.

\item[(2)]
  Associate with each $\mathbf{x} \in K$ an index $\alpha(x)$ so that $\mathbf{x} \in V_{\alpha(x)}$.
  Then there are open $n$-cells $B(\mathbf{x})$ and $W(\mathbf{x})$ (Definition 10.1),
  centered at $\mathbf{x}$,
  with
  \[
    \mathbf{x}
    \in B(\mathbf{x})
    \subseteq \overline{B(\mathbf{x})}
    \subseteq W(\mathbf{x})
    \subseteq \overline{W(\mathbf{x})}
    \subseteq V_{\alpha(\mathbf{x})}
  \]
  (Since $V_{\alpha(\mathbf{x})}$ is open, there exists $r > 0$
  such that $B(\mathbf{x};r) \subseteq V_{\alpha(\mathbf{x})}$.
  Take
  \[
    B(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{89\sqrt{n}}\right),
    \qquad
    W(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{64\sqrt{n}}\right)
  \]
  where $I(\mathbf{p};r)$ is the open $n$-cell centered at $\mathbf{p} = (p_1,\ldots,p_n)$
  defined by
  \[
    I(\mathbf{p};r)
    = (p_1-r,p_1+r) \times \cdots \times (p_n-r,p_n+r) \subseteq \mathbb{R}^n.)
  \]

\item[(3)]
  Define
  \begin{equation*}
    f(y) =
    \begin{cases}
      e^{-\frac{1}{y^2}} & (y > 0), \\
      0 & (y \leq 0).
    \end{cases}
  \end{equation*}
  $f(y) \in \mathscr{C}^{\infty}(\mathbb{R}^1)$
  by applying the similar argument in Exercise 8.1.

\item[(4)]
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in K$
  and construct $B(\mathbf{x})$ and $W(\mathbf{x})$ as in (2).
  Define
  \[
    g_{x_j}(y_j)
    = \frac{f(y_j)}{f(y_j)+f\left(\frac{r}{64\sqrt{n}}-\frac{r}{89\sqrt{n}}-y_j\right)}
  \]
  for $1 \leq j \leq n$.
  $g_{x_j}$ is well-defined and $g_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    g_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq 0, \\
      \text{strictly increasing}
        & \text{if } 0 \leq y_j \leq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } y_j \geq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Next, define
  \[
    h_{x_j}(y_j)
    = g_{x_j}\left(y_j-x_j+\frac{r}{64\sqrt{n}}\right)
      g_{x_j}\left(x_j+\frac{r}{64\sqrt{n}}-y_j\right)
  \]
  for $1 \leq j \leq n$.
  $h_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    h_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq x_j - \frac{r}{64\sqrt{n}}, \\
      \text{strictly increasing}
        & \text{if } x_j - \frac{r}{64\sqrt{n}} \leq y_j \leq x_j - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } x_j - \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{89\sqrt{n}}, \\
      \text{strictly decreasing}
        & \text{if } x_j + \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{64\sqrt{n}}, \\
      0
        & \text{if } y_j \geq x_j + \frac{r}{64\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Finally we define $\mathbf{h}_{\mathbf{x}}: \mathbb{R}^n \to \mathbb{R}^1$ by
  \[
    \mathbf{h}_{\mathbf{x}}(\mathbf{y})
    = \prod_{j=1}^{n} h_{x_j}(y_j)
  \]
  where $\mathbf{y} = (y_1,\ldots,y_n) \in \mathbb{R}^n$.
  Hence, $\mathbf{h}_{\mathbf{x}} \in \mathscr{C}^{\infty}(\mathbb{R}^n)$ (Theorem 9.21).
  Also, $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 1$ on $\overline{B(\mathbf{x})}$,
  $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 0$ outside $W(\mathbf{x})$,
  and $0 \leq \mathbf{h}_{\mathbf{x}}(\mathbf{y}) \leq 1$.

\item[(5)]
  Since $K$ is compact, there are finitely many points
  $\mathbf{x}_1, \ldots, \mathbf{x}_s \in K$ such that
  \[
    K \subseteq B(\mathbf{x}_1) \cup \cdots \cup B(\mathbf{x}_s).
  \]
  Take
  \[
    \varphi_i(\mathbf{x})
    = \mathbf{h}_{\mathbf{x}_i}(\mathbf{x})
    \in \mathscr{C}^{\infty}(\mathbb{R}^n)
  \]
  for $1 \leq i \leq s$.

\item[(6)]
  The rest are the same as the proof of Theorem 10.8 or Exercise 10.5.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.7.}
\addcontentsline{toc}{subsection}{Exercise 10.7.}
\begin{enumerate}
\item[(a)]
  \emph{Show that the simplex $Q^k$ is the smallest convex subset of $\mathbb{R}^k$
  such that contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}

\item[(b)]
  \emph{Show that affine mappings take convex sets to convex sets.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $Q^k$ contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}
  Recall
  \[
    Q^k = \{ (x_1,\ldots,x_k) \in \mathbb{R}^k :
      x_1 + \cdots + x_k \leq 1 \text{ and }
      x_1, \ldots, x_k \geq 0 \}
  \]
  (Example 10.14).
  Hence $\mathbf{0} = (0,\ldots,0) \in Q^k$ and
  \[
    \mathbf{e}_i = (0,\ldots,\underbrace{1}_{\text{$i$th coordinate}},\ldots,0) \in Q^k.
  \]

\item[(2)]
  \emph{Show that $Q^k$ is a convex subset of $\mathbb{R}^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$,
  $\mathbf{y} = (y_1,\ldots,y_k) \in Q^k$ and $0 < \lambda < 1$.
  Hence
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda x_1 + (1-\lambda)y_1, \ldots, \lambda x_k + (1-\lambda)y_k) \in Q^k
  \]
  since each $\lambda x_i + (1-\lambda)y_i \geq 0$
  and
  \[
    \sum_{i=1}^{k} (\lambda x_i + (1-\lambda)y_i)
    = \lambda \sum_{i=1}^{k} x_i + (1-\lambda) \sum_{i=1}^{k} y_i
    \leq \lambda + (1-\lambda)
    = 1.
  \]

\item[(3)]
  \emph{Given any convex set $E \subseteq \mathbb{R}^k$ containing
  $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.
  Show that $E \supseteq Q^k$.}
  \begin{enumerate}
  \item[(a)]
    Induction on $k$.
    Base case: $k = 1$. Given any $\mathbf{x} = (x_1) \in Q^1$.
    We have $0 \leq x_1 \leq 1$ by the definition of $Q^1$.
    So that $\mathbf{x} = x_1 \mathbf{e}_1 + (1-x_1) \mathbf{0} \in E$
    since $\mathbf{0}, \mathbf{e}_1 \in E$ and $E$ is convex.

  \item[(b)]
    Inductive step: suppose the statement holds for $k = n$.
    Given any $\mathbf{x} = (x_1,\ldots,x_n,x_{n+1}) \in Q^{n+1}$.
    If $x_{n+1} = 1$, then $x_1 = \cdots = x_n = 0$ by the definition of $Q^{n+1}$.
    So $\mathbf{x} = \mathbf{e}_{n+1} \in E$ by the assumption of $E$.
    If $0 \leq x_{n+1} < 1$, then $x_1 + \cdots + x_n \leq 1 - x_{n+1}$ or
    \[
      \frac{x_1}{1-x_{n+1}} + \cdots + \frac{x_{n}}{1-x_{n+1}} \leq 1.
    \]
    So the point
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}} \right)
      \in Q^n,
    \]
    or
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}}, 0 \right),
      \text{ say } \widehat{\mathbf{x}}, \in E
    \]
    by the induction hypothesis.
    Note that $\mathbf{e}_{n+1} \in E$.
    Hence
    \[
      \mathbf{x}
      = x_{n+1} \mathbf{e}_{n+1} + (1-x_{n+1})\widehat{\mathbf{x}}
      \in E
    \]
    by the convexity of $E$.

  \item[(c)]
    Conclusion: Since both the base case and the inductive step have been proved as true,
    by mathematical induction the statement holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{f}$ be an affine mapping that carries a vector space $X$ into a vector space $Y$
  such that
  \[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{0}) + A\mathbf{x}
  \]
  for some $A \in L(X,Y)$.

\item[(2)]
  Given any convex subset $C$ of $X$.
  To show that $\mathbf{f}(C)$ is convex, it suffices to show that
  \[
    \lambda \mathbf{y}_1 + (1-\lambda) \mathbf{y}_2 \in \mathbf{f}(C)
  \]
  for any $\mathbf{y}_1, \mathbf{y}_2 \in \mathbf{f}(C)$ and $0 < \lambda < 1$.
  Write $\mathbf{y}_1 = \mathbf{f}(\mathbf{x}_1)$,
  $\mathbf{y}_2 = \mathbf{f}(\mathbf{x}_2)$ for some $\mathbf{x}_1, \mathbf{x}_2 \in C$.
  Note that $\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2 \in C$ by the convexity of $C$.
  Hence
  \begin{align*}
    &\mathbf{f}(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + A(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + \lambda A\mathbf{x}_1 + (1-\lambda) A\mathbf{x}_2
      &(A \in L(X,Y)) \\
    =& \lambda(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_1)
      + (1-\lambda)(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_2) \\
    =& \lambda \mathbf{f}(\mathbf{x}_1) + (1-\lambda)\mathbf{f}(\mathbf{x}_2) \\
    =& \lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 \in \mathbf{f}(C).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.8.}
\addcontentsline{toc}{subsection}{Exercise 10.8.}
\emph{Let $H$ be the parallelogram in $\mathbb{R}^2$ whose vertices are
$(1,1)$, $(3,2)$, $(4,5)$, $(2,4)$.
Find the affine map $T$ which sends
$(0,0)$ to $(1,1)$, $(1,0)$ to $(3,2)$, $(1,1)$ to $(4,5)$, $(0,1)$ to $(2,4)$.
Show that $J_{T} = 5$.
Use $T$ to convert the integral
\[
  \alpha = \int_{H} e^{x-y} dx \: dy
\]
to an integral over $I^2$ and thus compute $\alpha$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Affine simplexes 10.26,
  \[
    T(\mathbf{x}) = T(\mathbf{0}) + A\mathbf{x},
  \]
  where $A \in L(\mathbb{R}^2, \mathbb{R}^2)$, say
  $A = \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}$.
  Note that $T:
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} \mapsto
  \begin{bmatrix}
    1 \\
    1
  \end{bmatrix}$.
  Thus
  \[
    T:
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix} \mapsto
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
    +
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix}
    =
    \begin{bmatrix}
      1+ax+by \\
      1+cx+dy
    \end{bmatrix}.
  \]

\item[(2)]
  By $T: (1,0) \mapsto (3,2)$ and $T: (0,1) \mapsto (2,4)$,
  we can solve $A$ as
  \[
    A = \begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}.
  \]
  It is easy to verify such
  \[
    T:
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    \mapsto
    \underbrace{\begin{bmatrix}
      1 \\
      1
    \end{bmatrix}}_{T(\mathbf{0})}
    +
    \underbrace{\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    =
    \begin{bmatrix}
      1+2x+y \\
      1+x+3y
    \end{bmatrix}
  \]
  satisfying our requirement.

\item[(3)]
  \[
    J_T
    =
    \det\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}
    = 5.
  \]

\item[(4)]
  By Example 10.4 and Theorem 10.9, we have
  \begin{align*}
    \int_{H} e^{x-y} dx \: dy
    &= \int_{I^2} e^{(1+2u+v)-(1+u+3v)} \abs{J_T} du \: dv \\
    &= 5 \int_{I^2} e^{u-2v} du \: dv \\
    &= 5 \left\{ \int_{0}^{1} e^u du \right\}\left\{ \int_{0}^{1} e^{-2v} dv \right\}
      &(\text{Theorem 10.2}) \\
    &= \frac{5}{2}(e-1)(1-e^{-2}).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.9.}
\addcontentsline{toc}{subsection}{Exercise 10.9.}
\emph{Define $(x,y) = T(r,\theta)$ one the rectangle
\[
  0 \leq r \leq a,
  \qquad
  0 \leq \theta \leq 2\pi
\]
by the equations
\[
  x = r \cos\theta,
  \qquad
  y = r \sin\theta.
\]
Show that $T$ maps this rectangle onto the closed disc $D$ with center at $(0,0)$ and radius $a$,
that $T$ is one-to-one in the interior of the rectangle,
and that $J_T(r,\theta) = r$.
If $f \in \mathscr{C}(D)$, prove the formula for integration in polar coordinates:}
\[
  \int_{D} f(x,y)dx \: dy
  = \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta.
\] \\

\emph{(Hint: Let $D_0$ be the interior of $D$,
minus the interval from $(0,0)$ to $(a,0)$.
As it stands, Theorem 10.9 applies to continuous functions $f$
whose support lies in $D_0$.
To remove this restriction,
proceed as in Example 10.4.)} \\

\emph{Proof.}
Define $E = [0,a] \times [0,2\pi]$.
\begin{enumerate}
\item[(1)]
  \emph{Show that $T$ maps $E$ onto $D$.}
  Given any $(x,y) \in D$.
  \begin{enumerate}
  \item[(a)]
    It is equivalent to solve $(r,\theta)$ from
    \begin{align*}
      x &= r\cos\theta \\
      y &= r\sin\theta
    \end{align*}
    in terms of $(x,y)$.
    Let $L$ be the closed interval from $(0,0)$ to $(a,0)$,
    say $L = \{ (r,0) \in \mathbb{R}^2 : 0 \leq r \leq a \} \subseteq D$.

  \item[(b)]
    If $(x,y) \in L$, say $(x,y) = (r,0)$ for some $r \in [0,a]$,
    then there exists $(r,0)$ or $(r,2\pi)$
    such that $T(r,0) = T(r,2\pi) = (r,0)$.
    (Therefore, $T$ is not one-to-one on $L$.)

  \item[(c)]
    If $(x,y) \in D - L$, then there is $r = (x^2+y^2)^{\frac{1}{2}}$ in $[0,a]$
    (since $(x,y) \in D = \{ (x,y) \in \mathbb{R}^2 : (x^2+y^2)^{\frac{1}{2}} \leq a \}$).
    Define
    \begin{equation*}
      \theta =
        \begin{cases}
          \arccos\left(\frac{x}{r}\right) & \text{ if $y \geq 0$}, \\
          2\pi - \arccos\left(\frac{x}{r}\right) & \text{ if $y < 0$}.
        \end{cases}
    \end{equation*}
    It is well-defined since $r \neq 0$.
    Besides, $\theta \in [0,2\pi]$ and $T(r,\theta) = (x,y)$.
  \end{enumerate}

\item[(2)]
  \emph{Show that $T$ is one-to-one in the interior of the rectangle $E$.}
  Suppose $(r_1,\theta_1), (r_2,\theta_2) \in \mathrm{int}(E) = (0,a) \times (0,2\pi)$
  and $T(r_1,\theta_1) = T(r_2,\theta_2) = (x,y) \in D$.
  Then
  \begin{align*}
    x &= r_1 \cos\theta_1 = r_2 \cos\theta_2, \\
    y &= r_1 \sin\theta_1 = r_2 \sin\theta_2.
  \end{align*}
  Note that $r_1^2 = r_2^2 = x^2+y^2$ and $r_1, r_2 > 0$,
  we have $r_1 = r_2$.
  Solve $\cos\theta_1 = \cos\theta_2$ and $\sin\theta_1 = \sin\theta_2$
  to get $\theta_1 = \theta_2 + 2m\pi$ for all $m \in \mathbb{Z}$.
  Here $m$ must be zero since $\theta_1, \theta_2 \in (0,2\pi)$.
  Therefore, $(r_1,\theta_1) = (r_2,\theta_2)$.

\item[(3)]
  $T(\mathrm{int}(E)) = D_0 = \mathrm{int}(D) - L$ (by (1)(2)).

\item[(4)]
  \emph{Show that $J_T(r,\theta) = r$.}
  \[
    J_T(r,\theta)
    =
    \det
    \begin{bmatrix}
      \cos\theta & -r\sin\theta \\
      \sin\theta & r\cos\theta
    \end{bmatrix}
    = r.
  \]

\item[(5)]
  \emph{If $f \in \mathscr{C}(D)$, show that}
  \[
    \int_{D} f(x,y)dx \: dy
  \]
  is well-defined.
  Similar to Example 10.4.
  \begin{enumerate}
  \item[(a)]
    Extend $f$ to a function on $I^2 = [-a,a]^2$
    by setting $f(x,y) = 0$ off $D$, and define
    \[
      \int_{D} f = \int_{I^2} f.
    \]
    Since $f$ may be discontinuous on $I^2$,
    the existence of the integral $\int_{I^2} f$.
    We also wish to show that this integral is independent of
    the order in which the $2$ integrations are carried out.

  \item[(b)]
    To do this, suppose $0 < \delta < 1$, put
    \begin{equation*}
      \varphi_{\delta}(t) =
      \begin{cases}
        1 & \text{ if $t \leq 1 - \delta$}, \\
        \frac{1-t}{\delta} & \text{ if $1 - \delta \leq t \leq 1$}, \\
        0 & \text{ if $t \geq 1$},
      \end{cases}
    \end{equation*}
    and define
    \[
      F_{\delta}(x,y) = \varphi_{\delta}\left(\frac{\sqrt{x^2+y^2}}{a}\right)f(x,y).
    \]
    Then $F_{\delta} \in \mathscr{C}(I^2)$ (or $\mathscr{C}(\mathbb{R}^2)$).

  \item[(c)]
    For each $x \in [-a,a]$, the set of all $y$ such that $F_{\delta}(x,y) \neq f(x,y)$
    is contained a union of two segment whose length does not exceed
    \[
      a\sqrt{1^2 - (1-\delta)^2}
      = a\sqrt{2\delta - \delta^2}
      < a\sqrt{2\delta}.
    \]
    Since $0 \leq \varphi_{\delta} \leq 1$, it follows that
    \[
      \abs{
        \int_{-a}^{a} F_{\delta}(x,y) dy - \int_{-a}^{a} f(x,y) dy
      }
      \leq 2a \sqrt{2\delta}\norm{f}
    \]
    where
    $\norm{f} = \max_{(x,y) \in I^2}|f(x,y)|$.
    So the sequence of continuous function
    \[
      \left\{ \int_{-a}^{a} F_{\delta}(x,y) dy \right\}
      \to
      \int_{-a}^{a} f(x,y) dy
      := g(x)
    \]
    uniformly as $\delta \to 0$.
    ($\delta = \frac{1}{n}$ for example.)
    So $g(x) \in \mathscr{C}([-a,a])$,
    and the further integrations present no problem, that is,
    $\int_{I^2} f$ is existed.

  \item[(d)]
    Moreover,
    \[
      \abs{ \int_{I^2} F_{\delta}(x,y) dxdy - \int_{I^2} f(x,y) dxdy }
      \leq 4a^2 \sqrt{2\delta} \norm{f}
    \]
    It is true, regardless of the order in which the $2$ single integrations
    are carried out.
    Since $F_{\delta} \in \mathscr{C}(I^2)$,
    $\int F_{\delta}$ is unaffected by any change in this order.
    Hence the inequality shows that the same is true of $\int f$.
  \end{enumerate}

\item[(6)]
  \emph{Show that}
  \[
    \int_{D} f(x,y)dx \: dy
    = \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta.
  \]
  \begin{enumerate}
  \item[(a)]
    Note that $T$ is a one-to-one $\mathscr{C}'$-mapping of an open set $D_0$
    such that $J_T(r,\theta) = r \neq 0$ for all $(r,\theta) \in \mathrm{int}(E)$.
    To apply Theorem 10.9, we will leverage Example 10.4 again.

  \item[(b)]
    Given any $\min\left\{ \frac{a}{89}, \frac{\pi}{64} \right \} > \delta > 0$.
    Define
    \[
      E_{\delta}
      =
      [\delta,a-\delta] \times [\delta,2\pi-\delta] \subseteq \textrm{int}(E)
    \]
    and $D_{\delta} = T(E_{\delta})$.
    Similar to Example 10.4,
    let $\varphi_{\delta}$ be a continuous function on $\mathbb{R}^2$
    such that $\varphi_{\delta}(x,y) = 1$ on $D_{\delta}$
    and $\varphi_{\delta}(x,y) = 0$ off $D_{\frac{\delta}{2}}$.
    Consider $f_{\delta}(x,y) = \varphi_{\delta}(x,y)f(x,y)$ on $\mathbb{R}^2$.
    By construction, $f_{\delta}$ is a continuous function on $\mathbb{R}^2$
    whose support is compact and lies in $D_{\frac{\delta}{2}} \subseteq D_0$.
    Hence, by Theorem 10.9
    \begin{align*}
      \int_{\mathbb{R}^2} f_{\delta}(x,y) dx dy
      &= \int_{D_{\frac{\delta}{2}}} f_{\delta}(x,y) dx dy \\
      &= \int_{E_{\frac{\delta}{2}}} f_{\delta}(T(r,\theta)) \abs{J_T(r,\theta)} dr d\theta \\
      &= \int_{\frac{\delta}{2}}^{a-\frac{\delta}{2}}
        \int_{\frac{\delta}{2}}^{2\pi-\frac{\delta}{2}}
        f(T(r,\theta)) r dr d\theta.
    \end{align*}

  \item[(c)]
    Since $f \in \mathscr{C}(D)$ and Exercise 6.7,
    \[
      \lim_{\delta \to 0} \int_{\frac{\delta}{2}}^{a-\frac{\delta}{2}}
        \int_{\frac{\delta}{2}}^{2\pi-\frac{\delta}{2}}
        f(T(r,\theta)) r dr d\theta
      = \int_{0}^{a}
        \int_{0}^{2\pi}
        f(T(r,\theta)) r dr d\theta.
    \]
    Therefore, it suffices to show that
    \[
      \lim_{\delta \to 0} \int_{\mathbb{R}^2} f_{\delta} = \int_{\mathbb{R}^2} f.
    \]

  \item[(d)]
    Note that $\int_{\mathbb{R}^2} f = \int_{D} f$ (by (5))
    and $\int_{\mathbb{R}^2} f_{\delta} = \int_{D_{\frac{\delta}{2}}} f_{\delta}
    = \int_{D} f_{\delta}$ (by (b)).
    So
    \[
      \abs{ \int_{\mathbb{R}^2} f - \int_{\mathbb{R}^2} f_{\delta} }
      = \abs{ \int_{D} f - \int_{D} f_{\delta} }.
    \]
    To estimate the difference between $\int_{D} f$ and $\int_{D} f_{\delta}$,
    we notice that $f$ and $f_{\delta}$ are coincide on $D_{\delta}$ by construction.
    Given any $(x,y) \in D - D_{\delta}$.
    Fix $y$, the set of all $x$ such that $f(x,y) \neq f_{\delta}(x,y)$
    is contained a union of two segment whose length does not exceed
    $\sqrt{a^2 - (a-\delta)^2 \cos^2\delta}$.
    Similarly as in (5), we have
    \[
      \abs{ \int_{D} f - \int_{D} f_{\delta} }
      \leq 4a \sqrt{a^2 - (a-\delta)^2 \cos^2\delta} \norm{f}.
    \]
    Hence, $\lim \int_{D} f_{\delta} = \int_{D} f$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.10.}
\addcontentsline{toc}{subsection}{Exercise 10.10.}
\emph{Let $a \to \infty$ in Exercise 10.9 and prove that
\[
  \int_{\mathbb{R}^2} f(x,y) dx dy
  = \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta,
\]
for continuous functions $f$ that decrease sufficiently rapidly
as $|x|+|y| \to \infty$.
(Find a more precise formulation.)
Apply this to
\[
  f(x,y) = \exp(-x^2-y^2)
\]
to derive formula}
\[
  \int_{-\infty}^{\infty} e^{-s^2} ds = \sqrt{\pi}.
\]



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define
  \begin{align*}
    D_a &= \{ (x,y) \in \mathbb{R}^2 : x^2+y^2 \leq a^2 \}, \\
    I_a &= \{ (x,y) \in \mathbb{R}^2 : |x| \leq a, |y| \leq a \},
  \end{align*}
  for any $a > 0$.
  Similar to Definition 10.1,
  define
  \[
    \int_{\mathbb{R}^2} f(x,y) dx dy
    = \lim_{a \to \infty} \int_{I_a} f(x,y) dx dy.
  \]
  Besides, since $f \in \mathscr{C}(\mathbb{R}^2)$, we have
  \begin{align*}
    \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta
    &= \lim_{a \to \infty} \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta \\
    &= \lim_{a \to \infty} \int_{D_a} f(x,y) dx dy
  \end{align*}
  by the definition in Exercise 6.8 and Exercise 10.9.
  So it suffices to show that
  both $\lim_{a \to \infty} \int_{I_a} f(x,y) dx dy$
  and $\lim_{a \to \infty} \int_{D_a} f(x,y) dx dy$ exist and they are equal
  if $f$ decreases sufficiently rapidly as $|x|+|y| \to \infty$.

\item[(2)]
  Say that $f$ decreases sufficiently rapidly as $|x|+|y| \to \infty$
  if there is a real number $M > 0$ such that
  \[
    |f(x,y)| \leq \alpha (x^2+y^2)^{-1-\beta}
  \]
  for some $\alpha > 0$ and $\beta > 0$
  whenever $|x|+|y| \geq M$.
  Similar to Theorem 3.29, we can give other weaker hypotheses:
  \[
    |f(x,y)|
    \leq
    \alpha (x^2+y^2)^{-1} (\log(x^2+y^2))^{-1-\beta}
  \]
  or
  \[
    |f(x,y)|
    \leq
    \alpha (x^2+y^2)^{-1} (\log(x^2+y^2))^{-1} (\log\log(x^2+y^2))^{-1-\beta}
  \]
  and so on.

\item[(3)]
  Write
  \begin{align*}
    & \: \int_{0}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta \\
    =& \:
      \int_{0}^{M} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta
      + \int_{M}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta.
  \end{align*}
  Note that
  \[
    \int_{0}^{M} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta < +\infty
  \]
  since $f \in \mathscr{C}(\mathbb{R}^2)$.
  Besides,
  \begin{align*}
    \int_{M}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta
    \leq& \:
    \int_{M}^{a} \int_{0}^{2\pi} \alpha (x^2+y^2)^{-1-\beta} r dr d\theta \\
    =& \:
    \int_{M}^{a} \int_{0}^{2\pi} \alpha r^{-1-2\beta} dr d\theta \\
    =& \:
    \pi \alpha \beta^{-1} (M^{-2\beta} - a^{-2\beta})
  \end{align*}
  is finite as $a \to \infty$.
  Hence, $\int_{0}^{\infty} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta$
  converges absolutely,
  or $\lim_{a \to \infty} \int_{D_a} f(x,y) dx dy$ exists.

\item[(4)]
  Note that
  \[
    \int_{D_a} |f(x,y)| dx dy
    \leq \int_{I_a} |f(x,y)| dx dy
    \leq \int_{D_{\sqrt{2}a}} |f(x,y)| dx dy.
  \]
  Let $a \to \infty$,
  we have
  \[
    \lim_{a \to \infty} \int_{I_a} |f(x,y)| dx dy
    = \lim_{a \to \infty} \int_{D_a} |f(x,y)| dx dy
  \]
  also exists.

\item[(5)]
  \emph{Show that $\int_{-\infty}^{\infty} e^{-s^2} ds = \sqrt{\pi}$.}
  Take $f(x,y) = \exp(-x^2-y^2) \in \mathscr{C}(\mathbb{R}^2)$.
  So
  \[
    \lim_{x^2+y^2 \to \infty} |f(x,y)|(x^2+y^2)^2
    = \lim_{x^2+y^2 \to \infty} e^{-(x^2+y^2)} (x^2+y^2)^2
    = 0
  \]
  by Theorem 8.6(f).
  Note that
  \begin{align*}
    \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta
    &= \int_{0}^{\infty} \int_{0}^{2\pi} e^{-r^2} r dr d\theta \\
    &= 2 \pi \int_{0}^{\infty} e^{-r^2} r dr \\
    &= \pi \left[-e^{-r^2}\right]_{r=0}^{r=\infty} \\
    &= \pi
  \end{align*}
  and
  \begin{align*}
    \int_{\mathbb{R}^2} f(x,y) dx dy
    &=
    \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
      e^{-x^2-y^2} dx dy \\
    &=
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}
    \left\{ \int_{-\infty}^{\infty} e^{-y^2} dy \right\} \\
    &=
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}^2.
  \end{align*}
  Hence,
  \begin{align*}
    & \:
    \int_{\mathbb{R}^2} f(x,y) dx dy
    = \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta \\
    \Longrightarrow& \:
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}^2 = \pi \\
    \Longrightarrow& \:
    \int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}.
  \end{align*}
  (Here $\int_{-\infty}^{\infty} e^{-x^2} dx$ is always nonnegative.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.11.}
\addcontentsline{toc}{subsection}{Exercise 10.11.}
\emph{Define $(u,v)=T(s,t)$ on the strip
\[
  0 < s < \infty,
  \qquad
  0 < t < 1
\]
by setting $u=s-st$, $v=st$.
Show that $T$ is a $1$-$1$ mapping of the strip onto the positive quadrant $Q$ in $\mathbb{R}^2$.
Show that $J_T(s,t) = s$.
For $x > 0$, $y > 0$, integrate
\[
  u^{x-1} e^{-u} v^{y-1} e^{-v}
\]
over $Q$, use Theorem 10.9 to convert the integral to one over the strip,
and derive
\[
  \int_{0}^{1} t^{x-1}(1-t)^{y-1} dt = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\]
in this way.
(For this application,
Theorem 10.9 has to be extended so as to cover certain improper integrals.
Provide this extension.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \begin{align*}
    S &= \{ (s,t) \in \mathbb{R}^2 : 0 < s < \infty, 0 < t < 1 \} \\
    Q &= \{ (u,v) \in \mathbb{R}^2 : u > 0, v > 0 \}.
  \end{align*}

\item[(2)]
  \emph{Show that $T$ maps $S$ onto $Q$.}
  Given any $(u,v) \in Q$.
  It is equivalent to solve $(s,t)$ from
  \begin{align*}
    u &= s-st \\
    v &= st
  \end{align*}
  in terms of $(u,v)$.
  It is trivial since $s = (s-st)+st = u+v \in (0,\infty)$
  and $t = \frac{v}{s} = \frac{v}{u+v} \in (0,1)$.
  Hence, $T: (u+v,\frac{v}{u+v}) \mapsto (u,v)$ is onto.

\item[(3)]
  \emph{Show that $T$ is one-to-one.}
  Suppose $(s_1,t_1), (s_2,t_2) \in S$
  and $T(s_1,t_1) = T(s_2,t_2) = (u,v) \in Q$.
  Then
  \begin{align*}
    u &= s_1 - s_1 t_1 = s_2 - s_2 t_2, \\
    v &= s_1 t_1 = s_2 t_2.
  \end{align*}
  Note that $s_1 = s_2 = u + v > 0$.
  As $u+v \neq 0$, $t_1 = \frac{v}{s_1} = \frac{v}{s_2} = t_2$.
  Therefore, $(s_1,t_1) = (s_2,t_2)$.

\item[(4)]
  \emph{Show that $J_T(s,t) = s$.}
  \[
    J_T(s,t)
    =
    \det
    \begin{bmatrix}
      1-t & -s \\
      t & s
    \end{bmatrix}
    = s.
  \]

\item[(5)]
  Let $f(u,v) = u^{x-1} e^{-u} v^{y-1} e^{-v}$ be defined on $Q$.
  Note that
  \begin{align*}
    \int_{Q} f(u,v) du dv
    &= \int_{0}^{\infty} \int_{0}^{\infty} u^{x-1} e^{-u} v^{y-1} e^{-v} du dv \\
    &= \left\{ \int_{0}^{\infty} u^{x-1} e^{-u} du \right\}
      \left\{ \int_{0}^{\infty} v^{y-1} e^{-v} dv \right\} \\
    &= \Gamma(x) \Gamma(y)
  \end{align*}
  and
  \begin{align*}
    \int_{S} f(T(s,t))s ds dt
    &= \int_{0}^{\infty} \int_{0}^{1} s^{x+y-1} e^{-s} t^{y-1}(1-t)^{x-1} ds dt \\
    &= \left\{ \int_{0}^{\infty} s^{x+y-1} e^{-s} ds \right\}
      \left\{ \int_{0}^{1} t^{y-1}(1-t)^{x-1} dt \right\} \\
    &= \Gamma(x+y)
      \left\{ \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt \right\}.
  \end{align*}
  Now we assume that $\int_{Q} f(u,v) du dv = \int_{S} f(T(s,t))s ds dt$ holds, then
  \[
    \Gamma(x) \Gamma(y)
    =
    \Gamma(x+y)
    \left\{ \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt \right\}
  \]
  or
  \[
    \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}.
  \]

\item[(6)]
  \emph{Given $f \in \mathscr{C}(Q)$.
  Suppose both $\int_{Q} f(u,v) du dv$ and $\int_{S} f(T(s,t))s ds dt$ converge
  as improper integrals in the sense of Exercises 6.7 and 6.8.
  Show that}
  \[
    \int_{Q} f(u,v) du dv = \int_{S} f(T(s,t))s ds dt
  \]
  \begin{enumerate}
  \item[(a)]
    Given $a > 64$ and $1 > \delta > 0$.
    Define
    \begin{align*}
      Q_{a,\delta} &= [\delta, a-\delta]^2 \subseteq Q \\
      S_{a,\delta} &= [2\delta, 2a-2\delta] \times
        \left[ \frac{\delta}{2a-\delta}, 1-\frac{\delta}{2a-\delta} \right] \subseteq S.
    \end{align*}
    So $T(S_{a,\delta}) = Q_{a,\delta}$.

  \item[(b)]
    Similar to (6) in the proof of Exercise 10.9,
    there is a continuous function $\varphi_{a,\delta}$ on $\mathbb{R}^2$
    such that $\varphi_{a,\delta}(x,y) = 1$ on $Q_{a,\delta}$
    and $\varphi_{a,\delta}(x,y) = 0$ off $Q_{a,\frac{\delta}{2}}$.
    Consider $f_{a,\delta}(x,y) = \varphi_{a,\delta}(x,y)f(x,y)$ on $\mathbb{R}^2$.
    By construction, $f_{a,\delta}$ is a continuous function on $\mathbb{R}^2$
    whose support is compact and lies in $Q_{a,\frac{\delta}{2}} \subseteq Q$.
    Hence, by Theorem 10.9
    \begin{align*}
      \int_{\mathbb{R}^2} f_{a,\delta}(u,v) du dv
      &= \int_{Q_{a,\frac{\delta}{2}}} f_{a,\delta}(u,v) du dv \\
      &= \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) \abs{J_T(s,t)} ds dt \\
      &= \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) s ds dt \\
      &= \int_{\mathbb{R}^2} f_{a,\delta}(T(s,t)) s ds dt.
    \end{align*}

  \item[(c)]
    Given any $\varepsilon > 0$.
    Since $\int_{Q} f(u,v) du dv$ converges,
    there exists a real number $a > 0$ such that
    \[
      \abs{ \int_{[0,a]^2} f(u,v) du dv - \int_{Q} f(u,v) du dv }
      \leq \frac{\varepsilon}{89}.
    \]
    Write
    \begin{align*}
      \abs{ \int_{\mathbb{R}^2} f_{a,\delta} \: du dv - \int_{\mathbb{R}^2} f \: du dv }
      =& \: \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{Q} f \: du dv } \\
      \leq& \:
        \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv } \\
        &+ \abs{ \int_{[0,a]^2} f \: du dv - \int_{Q} f \: du dv }.
    \end{align*}
    So
    \[
      \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv }
      \leq
      \int_{[0,a]^2} \abs{f_{a,\delta} - f} \: du dv.
    \]
    Notice that $f$ and $f_{a,\delta}$ are coincide on $Q_{a,\delta}$ by construction.
    A simple calculation shows that
    \[
      \int_{[0,a]^2} \abs{f_{a,\delta} - f} \: du dv
      \leq
      2a \delta \norm{f}.
    \]
    Choose $\delta
    = \min \left\{ \frac{1}{2}, \frac{\varepsilon}{64 \cdot 2a(\norm{f} + 1)} \right\} > 0$
    so that
    \[
      \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv }
      \leq
      \frac{\varepsilon}{64}.
    \]
    Hence,
    \[
      \abs{ \int_{\mathbb{R}^2} f_{a,\delta} \: du dv - \int_{\mathbb{R}^2} f \: du dv }
      \leq \frac{\varepsilon}{89} + \frac{\varepsilon}{64}
      < \varepsilon.
    \]
    Therefore,
    \[
      \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta} \: du dv
      = \int_{\mathbb{R}^2} f \: du dv.
    \]

  \item[(d)]
    Similar to (c),
    \[
      \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) s \: ds dt
      = \int_{\mathbb{R}^2} f(T(s,t)) s \: ds dt.
    \]
    Hence
    \begin{align*}
      \int_{Q} f(u,v) \: du dv
      &= \int_{\mathbb{R}^2} f(u,v) \: du dv \\
      &= \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta}(u,v) \: du dv \\
      &= \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta}(T(s,t)) s \: ds dt \\
      &= \int_{\mathbb{R}^2} f(T(s,t)) s \: ds dt \\
      &= \int_{S} f(T(s,t)) s \: ds dt.
    \end{align*}
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.12.}
\addcontentsline{toc}{subsection}{Exercise 10.12.}
\emph{Let $I^k$ be the set of all $\mathbf{u} = (u_1,\ldots,u_k) \in \mathbb{R}^k$
with $0 \leq u_i \leq 1$ for all $i$;
let $Q^k$ be the set of all $\mathbf{x} = (x_1,\ldots,x_k) \in \mathbb{R}^k$
with $x_i \geq 0$, $\sum x_i \leq 1$.
($I^k$ is the unit cube; $Q^k$ is the standard simplex in $\mathbb{R}^k$.)
Define $\mathbf{x} = T(\mathbf{u})$ by
\begin{align*}
  x_1 &= u_1 \\
  x_2 &= (1-u_1)u_2 \\
  &\cdots \\
  x_k &= (1-u_1) \cdots (1-u_{k-1})u_k.
\end{align*}
Show that
\[
  \sum_{i=1}^{k} x_i = 1 - \prod_{i=1}^{k} (1-u_i).
\]
Show that $T$ maps $I^k$ onto $Q^k$, that $T$ is $1$-$1$ in the interior of $I^k$,
and that its inverse $S$ is defined in the interior of $Q^k$ by $u_1 = x_1$ and
\[
  u_i = \frac{x_i}{1-x_1-\cdots-x_{i-1}}
\]
for $i = 2, \ldots, k$.
Show that
\[
  J_T(\mathbf{u}) = (1-u_1)^{k-1}(1-u_2)^{k-2} \cdots (1-u_{k-1}),
\]
and}
\[
  J_S(\mathbf{x}) = [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
\] \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that
  \[
    \sum_{i=1}^{m} x_i = 1 - \prod_{i=1}^{m} (1-u_i)
  \]
  for all $1 \leq m \leq k$.}
  Induction on $m$.
  Base case: $x_1 = 1 - (1-u_1)$.
  Inductive step: Suppose the case $m = h$ is true.
  Consider the the case $m = h+1$:
  \begin{align*}
    \sum_{i=1}^{h+1} x_i
    &= \left( \sum_{i=1}^{h} x_i \right) + x_{h+1} \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + x_{h+1}
      &(\text{Induction hypothesis}) \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + u_{h+1}\prod_{i=1}^{h} (1-u_i)
      &(\text{Definition of $x_{h+1}$}) \\
    &= 1 - (1-u_{h+1})\prod_{i=1}^{h} (1-u_i) \\
    &= 1 - \prod_{i=1}^{h+1} (1-u_i).
  \end{align*}
  Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement is established.

\item[(2)]
  \emph{Show that $T$ maps $I^k$ onto $Q^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$.
  It is equivalent to solve $\mathbf{u} = (u_1, \ldots, u_k)$ from
  \begin{align*}
    x_1 &= u_1 \\
    x_2 &= (1-u_1)u_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k
  \end{align*}
  in terms of $\mathbf{x} = (x_1, \ldots, x_k)$.
  It is clear that $u_1 = x_1$
  and
  \begin{equation*}
    u_i =
      \begin{cases}
        x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
          & \text{ if $x_1+\cdots+x_{i-1} \neq 1$}, \\
        0
          & \text{ if $x_1+\cdots+x_{i-1} = 1$}.
      \end{cases}
  \end{equation*}
  for $i = 2, \ldots, k$.
  (If $x_1+\cdots+x_{i-1} \neq 1$, by (1) we have
  \[
    \prod_{j=1}^{i-1}(1-u_j)
    = 1 - \sum_{j=1}^{i-1} x_i
    \neq 0
  \]
  and thus
  \[
    u_i
    = x_i \left\{ \prod_{j=1}^{i-1}(1-u_j) \right\}^{-1}
    = x_i(1 - x_1 - \cdots - x_{i-1})^{-1}.
  \]
  If $x_1+\cdots+x_{i-1} = 1$,
  then $x_{i} = \cdots = x_{k} = 0$.
  We may take $u_{i} = 0$ to set the expression $x_{i} = (1-u_1)\cdots(1-u_{i-1})u_i$ to zero.)
  Note that the solution $\mathbf{u} \in I^k$ is well-defined by construction,
  or $T(I^k) = Q^k$.

\item[(3)]
  \emph{Show that $T$ is $1$-$1$ in the interior of $I^k$.}
  Suppose $T(\mathbf{u}) = T(\mathbf{v}) = \mathbf{x}$ with
  $\mathbf{u}, \mathbf{v} \in \mathrm{int}(I^k)$.
  Then we consider the following equation:
  \begin{align*}
    x_1 &= u_1 = v_1 \\
    x_2 &= (1-u_1)u_2 = (1-v_1)v_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k = (1-v_1) \cdots (1-v_{k-1})v_k.
  \end{align*}
  By (1),
  \[
    \mathbf{x} \in \mathrm{int}(Q^k)
    = \left\{ (x_1,\ldots,x_k) \in \mathbb{R}^k : x_i > 0, \sum x_i < 1 \right\}.
  \]
  Hence,
  \begin{align*}
    u_1 = v_1 &= x_1 \\
    u_2 = v_1 &= x_2(1-x_1)^{-1} \\
    &\cdots \\
    u_k = v_k &= x_k(1-x_1-\cdots-x_{k-1})^{-1}.
  \end{align*}
  Here all $(1-x_1)^{-1}, \ldots, (1-x_1-\cdots-x_{i})^{-1}$
  are well-defined since $\mathbf{x} \in \mathrm{int}(Q^k)$.
  Therefore, $T$ is injective on $\mathrm{int}(I^k)$.

\item[(4)]
  By (2)(3), $T$ maps $\mathrm{int}(I^k)$ onto $\mathrm{int}(Q^k)$.
  That is, given any $\mathbf{x} = (x_1,\ldots,x_k) \in \mathrm{int}(Q^k)$,
  we can pick
  \begin{align*}
    u_1 &= x_1 \\
    u_i &= x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
    \qquad
    (i = 2, \ldots, k)
  \end{align*}
  such that $\mathbf{u} \in \mathrm{int}(I^k)$ and $T(\mathbf{u}) = \mathbf{x}$ .

\item[(5)]
  Note that
  $T(\mathbf{u}) = (u_1, (1-u_1)u_2, \ldots, (1-u_1)\cdots(1-u_{k-1})u_{k})$
  on $\mathrm{int}(I^k)$.
  So
  \[
    T'(\mathbf{u})
    =
    \begin{bmatrix}
      1      & 0       & 0                      & \cdots & 0 \\
      *      & (1-u_1) & 0                      & \cdots & 0 \\
      *      & *       & \prod_{i=1}^{2}(1-u_i) & \cdots & 0 \\
      \vdots &  \vdots & \vdots                 & \ddots & \vdots \\
      *      & *       & *                      & \cdots & \prod_{i=1}^{k-1}(1-u_i)
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_T(\mathbf{u})
    &= \det T'(\mathbf{u}) \\
    &= 1 \cdot (1-u_1) \cdot \prod_{i=1}^{2}(1-u_i) \cdots \prod_{i=1}^{k-1}(1-u_i) \\
    &= \prod_{i=1}^{k-1}(1-u_i)^{k-i}.
  \end{align*}

\item[(6)]
  Similar to (5).
  $S(\mathbf{x}) = (x_1, x_2(1-x_1)^{-1}, \ldots, x_k(1-x_1-\cdots-x_{k-1})^{-1})$
  on $\mathrm{int}(Q^k)$.
  So
  \[
    S'(\mathbf{x})
    =
    \begin{bmatrix}
      1      & 0            & 0                & \cdots & 0 \\
      *      & (1-x_1)^{-1} & 0                & \cdots & 0 \\
      *      & *            & (1-x_1-x_2)^{-1} & \cdots & 0 \\
      \vdots &  \vdots      & \vdots           & \ddots & \vdots \\
      *      & *            & *                & \cdots & (1-x_1-\cdots-x_{k-1})^{-1}
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_S(\mathbf{x})
    &= \det S'(\mathbf{x}) \\
    &= 1 \cdot (1-x_1)^{-1} \cdot (1-x_1-x_2)^{-1} \cdots (1-x_1-\cdots-x_{k-1})^{-1} \\
    &= [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.13.}
\addcontentsline{toc}{subsection}{Exercise 10.13.}
\emph{Let $r_1, \ldots, r_k$ be nonnegative integers, and prove that
\[
  \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
  =
  \frac{r_1! \cdots r_k!}{(k+r_1+\cdots+r_k)!}
\]
(Hint: Use Exercise 10.12, Theorems 10.9 and 8.20.)
Note that the special case $r_1 = \cdots = r_k = 0$
shows that the volume of $Q^k$ is $\frac{1}{k!}$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define $T: I^k$ onto $Q^k$ as in Exercise 10.12,
  and $f: Q^k \to \mathbb{R}^1$ by
  \[
    f(\mathbf{x})
    = f(x_1, \ldots, x_k)
    = x_1^{r_1} \cdots x_k^{r_k}
    = \prod_{i=1}^{k} x_i^{r_i}.
  \]

\item[(2)]
  By Exercise 10.12, Example 10.4 and Theorems 10.9, we have
  \begin{align*}
    \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
    &=
    \int_{Q^k} f(\mathbf{x}) d\mathbf{x} \\
    &=
    \int_{I^k} f(T(\mathbf{u})) |J_T(\mathbf{u})| d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} \left( u_i \prod_{j=1}^{i-1}(1-u_j) \right)^{r_i}
      \prod_{i=1}^{k}(1-u_i)^{k-i} d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} d\mathbf{u} \\
    &=
    \prod_{i=1}^{k}
      \int_{0}^{1} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} du_i
      & (\text{Theorem 10.2}) \\
    &=
    \prod_{i=1}^{k}
      \frac{r_i! \left(k-i+\sum_{j=i+1}^{k}r_j\right)!}
        {\left(k-i+1+\sum_{j=i}^{k}r_j\right)!}
      & (\text{Theorem 8.20}) \\
    &=
    \frac{r_1! \cdots r_k!}{(k + r_1 + \cdots + r_k)!}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.14 (Levi-Civita symbol).}
\addcontentsline{toc}{subsection}{Exercise 10.14 (Levi-Civita symbol).}
\emph{Prove $\varepsilon(j_1, \ldots, j_k) = s(j_1, \ldots, j_k)$,
where}
\[
  s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p).
\] \\

It is usually to define the Levi-Civita symbol by
\begin{equation*}
\varepsilon(j_1, \ldots, j_k) =
  \begin{cases}
    1
      & \text{ if $(j_1,\cdots,j_k)$ is an even permutation of $J$}, \\
    -1
      & \text{ if $(j_1,\cdots,j_k)$ is an odd permutation of $J$}, \\
    0
      & \text{otherwise}
  \end{cases}
\end{equation*}
(Basic $k$-forms 10.14).
Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
So $\varepsilon(j_1, \ldots, j_k)$ is equivalent to an explicit expression
$s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p)$. \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Induction on $k$.
  Base case: \emph{Show that $\varepsilon(j_1,j_2) = s(j_1,j_2)$.}
  Since
  \begin{equation*}
  \varepsilon(j_1,j_2) =
    \begin{cases}
      1
        & \text{ if $j_1 < j_2$} \\
      -1
        & \text{ if $j_1 > j_2$},
    \end{cases}
  \end{equation*}
  $\varepsilon(j_1,j_2) = \mathrm{sgn}(j_2-j_1) = s(j_1,j_2)$.

\item[(2)]
  Inductive step: \emph{Show that for any $s \geq 2$,
  if $\varepsilon(j_1, \ldots, j_{s}) = s(j_1, \ldots, j_{s})$ holds,
  then $\varepsilon(j_1, \ldots, j_{s+1}) = s(j_1, \ldots, j_{s+1})$ also holds.}
  \begin{align*}
    \varepsilon(j_1, \ldots, j_{s+1})
    &= \varepsilon(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= s(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s} \mathrm{sgn}(j_q-j_p)
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s+1} \mathrm{sgn}(j_q - j_p) \\
    &= s(j_1, \ldots, j_{s+1}).
  \end{align*}

\item[(3)]
  Conclusion: Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement holds for every integer $k \geq 2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.15.}
\addcontentsline{toc}{subsection}{Exercise 10.15.}
\emph{If $\omega$ and $\lambda$ are $k$- and $m$-forms, respectively,
prove that}
\[
  \omega \wedge \lambda = (-1)^{km} \lambda \wedge \omega.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \omega = \sum_I b_I(\mathbf{x}) dx_I,
    \qquad
    \lambda = \sum_J c_J(\mathbf{x}) dx_J
  \]
  in the stardard presentations,
  where $I$ and $J$ range over all increasing $k$-indices
  and over all increasing $m$-indices taken from the set $\{1,\ldots,n\}$.

\item[(2)]
  \emph{Show that $dx_I \wedge dx_J = (-1)^{km} dx_J \wedge dx_I$.}
  \begin{align*}
    dx_I \wedge dx_J
    &= dx_{i_1} \wedge \cdots \wedge dx_{i_k}
      \wedge dx_J \\
    &= (-1)^m dx_{i_1} \wedge \cdots \wedge dx_{i_{k-1}}
      \wedge dx_J \wedge dx_{i_{k}} \\
    &= (-1)^{2m} dx_{i_1} \wedge \cdots \wedge dx_{i_{k-2}}
      \wedge dx_J \wedge dx_{i_{k-1}} \wedge dx_{i_{k}} \\
    &\cdots \\
    &= (-1)^{km} dx_J
      \wedge dx_{i_1} \wedge \cdots \wedge dx_{i_k} \\
    &= (-1)^{km} dx_J \wedge dx_I.
  \end{align*}

\item[(3)]
  \begin{align*}
    \omega \wedge \lambda
    &= \sum_{I,J} b_I(\mathbf{x}) c_J(\mathbf{x}) dx_I \wedge dx_J \\
    &= (-1)^{km} \sum_{J,I} c_J(\mathbf{x}) b_I(\mathbf{x}) dx_J \wedge dx_I \\
    &= (-1)^{km} \lambda \wedge \omega.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.16.}
\addcontentsline{toc}{subsection}{Exercise 10.16.}
\emph{If $k \geq 2$ and $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$
is an oriented affine $k$-simplex, prove that $\partial^2 \sigma = 0$,
directly from the definition of the boundary operator $\partial$.
Deduce from this that $\partial^2 \Psi = 0$ for every chain $\Psi$.
(Hint: For orientation, do it first for $k=2$, $k=3$.
In general, if $i < j$, let $\sigma_{ij}$ be the $(k-2)$-simplex obtained by
deleting $\mathbf{p}_i$ and $\mathbf{p}_j$ from $\sigma$.
Show that each $\sigma_{ij}$ occurs twice in $\partial^2\sigma$, with opposite sign.)} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Write the boundary of the oriented affine $k$-simplex
  $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$ as
  \[
    \partial \sigma
    = \sum_{i=0}^{k}(-1)^i
    [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]
  \]
  where where the oriented $(k-1)$-simplex
  $[\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]$
  is obtained by deleting $\sigma$'s $i$-th vertex (Boundaries 10.29).

\item[(2)]
  \begin{align*}
    \partial^2 \sigma
    =& \partial \left( \sum_{i}(-1)^{i}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \right) \\
    =& \sum_{i}(-1)^{i}
      \partial [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i}(-1)^{j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &+ \sum_{j>i} (-1)^{i}(-1)^{j-1}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &- \sum_{j>i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k].
  \end{align*}
  The latter two summations cancel since after switching $i$ and $j$ in the second sum.
  Therefore $\partial^2 \sigma = 0$.

\item[(3)]
  The boundary of a chain is the linear combination of boundaries of the simplices in the chain.
  Write $\Psi = \sum_{i=1}^{r} \sigma_i$. where $\sigma_i$ is an oriented affine simplex.
  Then
  \[
    \partial^2 \Psi
    = \partial \left(\partial \sum \sigma_i \right)
    = \partial \left( \sum \partial\sigma_i \right)
    = \sum \partial^2 \sigma_i
    = \sum 0
    = 0
  \]
  for any affine chain $\Psi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.17.}
\addcontentsline{toc}{subsection}{Exercise 10.17.}
\emph{Put $J^2 = \tau_1 + \tau_2$, where
\[
  \tau_1 = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2],
  \qquad
  \tau_2 = -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1].
\]
Explain why it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.
Show that $\partial J^2$ is the sum of $4$ oriented affine $1$-simplexes.
Find these.
What is $\partial(\tau_1 - \tau_2)$?} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that the unit square $I^2 \in \mathbb{R}^2$ is the union of
  $\tau_1(Q^2)$ and $\tau_2(Q_2)$, where
  \begin{align*}
    \tau_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2) \\
    &= \mathbf{0} + (\alpha_1+\alpha_2) \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 1 \\
        0 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  and
  \begin{align*}
    \tau_2(\mathbf{u})
    &= (-[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1])(\mathbf{u}) \\
    &= ([\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 (\mathbf{e}_1+\mathbf{e}_2) + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + (\alpha_1+\alpha_2) \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 0 \\
        1 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  where $\mathbf{u} = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \in \mathbb{R}^2$
  (as in Equation (78)).
  Both $\tau_1$ and $\tau_2$ have Jacobian $1 > 0$, or positively oriented
  (Affine simplexes 10.26).
  So it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.

\item[(2)]
  \begin{align*}
    \partial \tau_1
    &= [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_1], \\
    \partial \tau_2
    &= [\mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1] \\
    &= [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}]
      + [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2].
  \end{align*}

\item[(3)]
  By (2),
  \[
    \partial J^2
    = \partial \tau_1 + \partial \tau_2
    = [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}],
  \]
  which is the positively oriented boundary of $I^2$.

\item[(4)]
  By (2),
  \begin{align*}
    \partial(\tau_1 - \tau_2)
    =& \partial \tau_1 - \partial \tau_2 \\
    =& [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}] \\
      &+ [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}].
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.18.}
\addcontentsline{toc}{subsection}{Exercise 10.18.}
\emph{Consider the oriented affine $3$-simplex
\[
  \sigma_1
  = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3]
\]
in $\mathbb{R}^3$.
Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.
Thus $\sigma_1$ is positively oriented.} \\

\emph{Let $\sigma_2, \ldots, \sigma_6$ be five other oriented $3$-simplexes,
obtained as follows:
There are five permutations $(i_1, i_2, i_3)$ of $(1, 2, 3)$,
distinct from $(1, 2, 3)$.
Associate with each $(i_1, i_2, i_3)$ the simplex
\[
  s(i_1, i_2, i_3)
  [
    \mathbf{0},
    \mathbf{e}_{i_1},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
  ]
\]
where $s$ is the sign that occurs in the definition of the determinant.
(This is how $\tau_2$ was obtained from $\tau_1$ in Exercise 10.17.)
Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.} \\

\emph{Put $J^3 = \sigma_1+\cdots+\sigma_6$.
Then $J^3$ may be called the positively oriented unit cube in $\mathbb{R}^3$.
Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.
(These $12$ triangles cover the surface of the unit cube $I^3$.)} \\

\emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.} \\

\emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
and that their union covers $I^3$.
(Compared with Exercise 10.13; note that $3!=6$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.}
  Given any $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$,
  we have
  \begin{align*}
    \sigma_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3])
    (\mathbf{u}) \\
    &= \mathbf{0}
      + \alpha_1 \mathbf{e}_1
      + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2)
      + \alpha_3 (\mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3) \\
    &= \mathbf{0}
      + (\alpha_1+\alpha_2+\alpha_3) \mathbf{e}_1
      + (\alpha_2+\alpha_3) \mathbf{e}_2
      + \alpha_3 \mathbf{e}_3 \\
    &= \mathbf{0} +
      \underbrace{\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
      \end{bmatrix}}_{\text{say }A}
      \mathbf{u}.
  \end{align*}
  So
  \[
    \det(A)
    =
    \det
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    = 1.
  \]

\item[(2)]
  \emph{Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.}
  Define the permutation matrix $P_{(i_1,i_2,i_3)}$ corresponding to
  a permutation $(i_1,i_2,i_3)$ of $(1,2,3)$ by
  \[
    P_{(i_1,i_2,i_3)}
    =
    \begin{bmatrix}
      \mathbf{e}_{i_1} & \mathbf{e}_{i_2} & \mathbf{e}_{i_3}
    \end{bmatrix}.
  \]
  For example,
  \[
    P_{(2,3,1)}
    =
    \begin{bmatrix}
      \mathbf{e}_{2} & \mathbf{e}_{3} & \mathbf{e}_{1}
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}.
  \]
  Note that the sign $s(i_1,i_2,i_3)$ of the permutation $(i_1,i_2,i_3)$
  is exactly the same as the determinant of the permutation matrix $P_{(i_1,i_2,i_3)}$.
  Define a permutation $(j_1, j_2, 3)$ of $(1, 2, 3)$
  (for swapping the first and the second coordinates of $\mathbf{u}$)
  by
  \begin{equation*}
    (j_1, j_2, 3) =
      \begin{cases}
        (1, 2, 3) & \text{ if $s(i_1,i_2,i_3) = 1$}, \\
        (2, 1, 3) & \text{ if $s(i_1,i_2,i_3) = -1$}.
      \end{cases}
  \end{equation*}
  Write
  \[
    \sigma_{(i_1, i_2, i_3)}
    =
    s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ].
  \]
  (So that $\sigma_1 = \sigma_{(1,2,3)}$.)
  Hence,
  \begin{align*}
    &
    \sigma_{(i_1, i_2, i_3)}(\mathbf{u}) \\
    =& \mathbf{0}
      + \alpha_{j_1} \mathbf{e}_{i_1}
      + \alpha_{j_2} (\mathbf{e}_{i_1}+\mathbf{e}_{i_2})
      + \alpha_3 (\mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}) \\
    =& \mathbf{0}
      + (\alpha_{j_1}+\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_1}
      + (\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_2}
      + \alpha_3 \mathbf{e}_{i_3} \\
    =& \mathbf{0} + P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}
  \end{align*}
  where $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$.
  For example,
  \[
    P_{(2,3,1)} A P_{(1,2,3)}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 1 & 1 \\
      0 & 1 & 1
    \end{bmatrix}.
  \]
  So
  \begin{align*}
    \det(P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)})
    &= \det(P_{(i_1,i_2,i_3)}) \det(A) \det(P_{(j_1,j_2,3)}) \\
    &= s(i_1, i_2, i_3) \cdot 1 \cdot s(i_1, i_2, i_3) \\
    &= 1.
  \end{align*}

\item[(3)]
  \emph{Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.}
  Note that
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_1 < i_2}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_1}} -s(i_2, i_1, i_3)
      [
        \mathbf{0},
        \mathbf{e}_{i_2}+\mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =& \mathbf{0}
  \end{align*}
  and
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_2 < i_3}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_3 > i_2}} -s(i_1, i_3, i_2)
      [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =&
    \mathbf{0}.
  \end{align*}
  So
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} \partial \sigma_{(i_1, i_2, i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &+ s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &+ \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ].
  \end{align*}
  Thus,
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ]
  \end{align*}
  is the sum of $12$ oriented affine $2$-simplexes. (Note that $3! = 6$.)

\item[(4)]
  \emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
  if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.}
  \begin{enumerate}
  \item[(a)]
    By (1),
    $\mathbf{x}$ is in the range of $\sigma_1$ if and only if
    $\mathbf{x} = A\mathbf{u}$ for $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$, or
    \[
      \begin{bmatrix}
          x_1 \\
          x_2 \\
          x_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          1 & 1 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1
      \end{bmatrix}
      \begin{bmatrix}
          u_1 \\
          u_2 \\
          u_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          u_1+u_2+u_3 \\
          u_2+u_3 \\
          u_3
      \end{bmatrix}.
    \]

  \item[(b)]
    Since $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$,
    $u_1+u_2+u_3 \leq 1$ and $u_1,u_2,u_3 \geq 0$.
    Hence $0 \leq u_3 \leq u_2+u_3 \leq u_1+u_2+u_3 \leq 1$
    or $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.

  \item[(c)]
    Conversely, if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$,
    we define
    \[
      \mathbf{v}
      =
      \begin{bmatrix}
          v_1 \\
          v_2 \\
          v_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          x_1-x_2 \\
          x_2-x_3 \\
          x_3
      \end{bmatrix}.
    \]
    Clearly, $\mathbf{v} \in Q^3$.
  \end{enumerate}

\item[(5)]
  \emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
  and that their union covers $I^3$.}
  Similar to (4).
  By (2),
  $\mathbf{x} = P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  $P_{(i_1,i_2,i_3)^{-1}} \mathbf{x} = A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  \[
    \begin{bmatrix}
        x_{i_1} \\
        x_{i_2} \\
        x_{i_3}
    \end{bmatrix}
    =
    \begin{bmatrix}
        u_1+u_2+u_3 \\
        u_{j_2}+u_3 \\
        u_3
    \end{bmatrix}.
  \]
  In any case, we always have
  $0 \leq u_3 \leq u_{j_2}+u_3 \leq u_1+u_2+u_3 \leq 1$.
  Hence
  $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_{(i_1, i_2, i_3)}$
  if and only if
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1.
  \]
  The interior of $\sigma_{(i_1, i_2, i_3)}$ is
  \[
    \{ \mathbf{x} \in \mathbb{R}^3 : 0 < x_{i_3} < x_{i_2} < x_{i_1} < 1 \},
  \]
  and thus the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors.
  Also, any $\mathbf{x} \in I^3$ has the relation
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1
  \]
  for some permutation $(i_1,i_2,i_3)$ of $(1,2,3)$.
  Hence
  \[
    I^3
    = \bigcup_{(i_1,i_2,i_3)} \sigma_{(i_1,i_2,i_3)}(Q^3)
    = \bigcup_{i=1}^{6} \sigma_{i}(Q^3).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.19.}
\addcontentsline{toc}{subsection}{Exercise 10.19.}
\emph{Let $J^2$ and $J^3$ be as in Exercise 10.17 and Exercise 10.18.
Define
\begin{align*}
  B_{01}(u,v) = (0,u,v), &\qquad B_{11}(u,v) = (1,u,v), \\
  B_{02}(u,v) = (u,0,v), &\qquad B_{12}(u,v) = (u,1,v), \\
  B_{03}(u,v) = (u,v,0), &\qquad B_{13}(u,v) = (u,v,1).
\end{align*}
These are affine, and map $\mathbb{R}^2$ into $\mathbb{R}^3$.
Put $\beta_{ri} = B_{ri}(J^2)$, for $r=0,1$, $i=1,2,3$.
Each $\beta_{ri}$ is an affine-oriented $2$-chain. (See Section 10.30.)
Verify that
\[
  \partial J^3 = \sum_{i=1}^{3} (-1)^{i} (\beta_{0i}-\beta_{1i}),
\]
in agreement with Exercise 10.18.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  A direct calculation shows that
  \begin{align*}
    B_{01}(\tau_1) - B_{11}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_3]
      - [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_1) - B_{12}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3]
      - [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_1) - B_{13}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{01}(\tau_2) - B_{11}(\tau_2)
    =&
    -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_2) - B_{12}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_2) - B_{13}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3].
  \end{align*}

\item[(2)]
  To express the formula in (1) clearly, we define
  \[
    \omega_{(i_1,i_2,i_3)}
    =
    [
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ]
    -
    [
      \mathbf{0},
      \mathbf{e}_{i_2},
      \mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ],
  \]
  and thus
  \begin{align*}
    -(B_{01}(\tau_1) - B_{11}(\tau_1)) &= s(1,2,3) \omega_{(1,2,3)} \\
    B_{02}(\tau_1) - B_{12}(\tau_1) &= s(2,1,3) \omega_{(2,1,3)} \\
    -(B_{03}(\tau_1) - B_{13}(\tau_1)) &= s(3,1,2) \omega_{(3,1,2)} \\
    -(B_{01}(\tau_2) - B_{11}(\tau_2)) &= s(1,3,2) \omega_{(1,3,2)} \\
    B_{02}(\tau_2) - B_{12}(\tau_2) &= s(2,3,1) \omega_{(2,3,1)} \\
    -(B_{03}(\tau_2) - B_{13}(\tau_2)) &= s(3,2,1) \omega_{(3,2,1)}.
  \end{align*}

\item[(3)]
  Note that
  \begin{align*}
    \beta_{0i}-\beta_{1i}
    &= B_{0i}(J^2) - B_{1i}(J^2) \\
    &= B_{0i}(\tau_1+\tau_2) - B_{1i}(\tau_1+\tau_2) \\
    &= B_{0i}(\tau_1) + B_{0i}(\tau_2) - B_{1i}(\tau_1) - B_{1i}(\tau_2) \\
    &= (B_{0i}(\tau_1) - B_{1i}(\tau_1)) + (B_{0i}(\tau_2) - B_{1i}(\tau_2)).
  \end{align*}
  Thus,
  \begin{align*}
    &\sum_{i=1}^3 (-1)^{i} (\beta_{0i}-\beta_{1i}) \\
    =& \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_1) - B_{1i}(\tau_1))
      + \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_2) - B_{1i}(\tau_2)) \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1,i_2,i_3) \omega_{(i_1,i_2,i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \partial J^3.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.20.}
\addcontentsline{toc}{subsection}{Exercise 10.20.}
\emph{State conditions under which the formula
\[
  \int_{\Phi} fd\omega
  = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
\]
is valid, and show that it generalizes the formula for integration by parts.
(Hint: $d(f\omega) = (df) \wedge \omega + f d\omega$.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{If}
  \begin{enumerate}
  \item[(a)]
    \emph{$\Phi$ is a $k$-chain of class $\mathscr{C}''$ in an open set $V \subseteq \mathbb{R}^m$,}

  \item[(b)]
    \emph{$\omega$ is a $(k-1)$-form of class $\mathscr{C}'$ in $V$,}

  \item[(c)]
    \emph{$f$ is a $0$-form of class $\mathscr{C}'$ in $V$,}
  \end{enumerate}
  \emph{then}
  \[
    \int_{\Phi} fd\omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
  \]

\item[(2)]
  Theorem 10.20(a) implies that
  \[
    d(f\omega) = (df) \wedge \omega + f d\omega.
  \]

\item[(3)]
  The Stokes' theorem (Theorem 10.33) shows that
  \[
    \int_{\Phi} d(f\omega) = \int_{\partial\Phi} f\omega.
  \]
  Hence
  \[
    \int_{\Phi} fd\omega
    = \int_{\Phi} d(f\omega) - \int_{\Phi}(df) \wedge \omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega.
  \]

\item[(4)]
  Define $\Phi: Q^1 = [0,1] \to [a,b]$ by
  \[
    \Phi(\alpha) = a + \alpha (b - a).
  \]
  $\Phi$ is a $1$-simplex of class $\mathscr{C}''$ in an open set $V \supseteq [a,b]$.
  Also,
  \[
    \partial\Phi = [b] - [a].
  \]
  Let $\omega = g$ be a $0$-form of class $\mathscr{C}'(V)$.

\item[(5)]
  Note that
  \begin{align*}
    \int_{\Phi} fd\omega
    &= \int_{\Phi} fdg
    = \int_{0}^{1} f(\Phi(t)) g'(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f(u) g'(u) du, \\
    \int_{\partial\Phi} f\omega
    &= \int_{[b]} fg + \int_{-[a]} fg
    = f(b)g(b) + (-1) f(a)f(a), \\
    \int_{\Phi}(df) \wedge \omega
    &= \int_{\Phi}(df) g
    = \int_{0}^{1} f'(\Phi(t)) g(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f'(u) g(u) du.
  \end{align*}
  Hence
  \[
    \int_{a}^{b} f(u) g'(u) du = f(b)g(b) - f(a)f(a) - \int_{a}^{b} f'(u) g(u) du,
  \]
  which is the same as the integration by parts (Theorem 6.22).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.21.}
\addcontentsline{toc}{subsection}{Exercise 10.21.}
\emph{As in Example 10.36, consider the $1$-form
\[
  \eta = \frac{x dy - y dx}{x^2+y^2}
\]
in $\mathbb{R}^2-\{\mathbf{0}\}$.}
\begin{enumerate}
\item[(a)]
  \emph{Carry out the computation that leads to
  \[
    \int_{\gamma} \eta = 2\pi \neq 0,
  \]
  and prove that $d\eta = 0$.}

\item[(b)]
  \emph{Let $\gamma(t) = (r \cos t, r \sin t)$, for some $r > 0$,
  and let $\Gamma$ be a $\mathscr{C}''$-curve in $\mathbb{R}^2 - \{\mathbf{0}\}$,
  with parameter interval $[0,2\pi]$,
  with $\Gamma(0) = \Gamma(2\pi)$,
  such that the intervals $[\gamma(t),\Gamma(t)]$ do not contain $\mathbf{0}$
  for any $t \in [0,2\pi]$.
  Prove that
  \[
    \int_{\Gamma} \eta = 2\pi.
  \]
  (Hint: For $0 \leq t \leq 2\pi$, $0 \leq u \leq 1$, define
  \[
    \Phi(t,u) = (1-u)\Gamma(t) + u\gamma(t).
  \]
  Then $\Phi$ is a $2$-surface in $\mathbb{R}^2 - \{\mathbf{0}\}$
  whose parameter domain is the indicated rectangle.
  Because of cancellations (as in Example 10.32),
  \[
    \partial \Phi = \Gamma - \gamma.
  \]
  Use Stokes' theorem to deduce that
  \[
    \int_{\Gamma} \eta = \int_{\gamma} \eta
  \]
  because $d\eta = 0$.)}

\item[(c)]
  \emph{Take $\Gamma(t) = (a\cos t, b\sin t)$ where $a > 0$, $b > 0$ are fixed.
  Use part (b) to show that}
  \[
    \int_{0}^{2\pi} \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}dt = 2\pi.
  \]

\item[(d)]
  \emph{Show that
  \[
    \eta = d\left( \arctan\frac{y}{x} \right)
  \]
  in any convex open set in which $x \neq 0$, and that
  \[
    \eta = d\left( -\arctan\frac{x}{y} \right)
  \]
  in any convex open set in which $y \neq 0$.
  Explain why this justifies the notation $\eta = d\theta$,
  in spite of the fact that $\eta$ is not exact in $\mathbb{R}^2 - \{0\}$.}

\item[(e)]
  \emph{Show that (b) can be derived from (d).}

\item[(f)]
  \emph{If $\Gamma$ is any closed $\mathscr{C}'$-curve in $\mathbb{R}^2 - \{ \mathbf{0} \}$,
  prove that
  \[
    \frac{1}{2\pi} \int_{\Gamma} \eta = \mathrm{Ind}(\Gamma).
  \]
  (See Exercise 8.23 for the definition of the index of a curve.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    \int_{\gamma} \eta
    &= \int_{0}^{2\pi}
      \frac{(r\cos t) d(r\sin t) - (r\sin t) d(r\cos t)}{(r\cos t)^2 + (r\sin t)^2} \\
    &= \int_{0}^{2\pi}
      \frac{(r \cos t)(r \cos t) - (r\sin t) (-r\sin t)}{(r\cos t)^2 + (r\sin t)^2} dt \\
    &= \int_{0}^{2\pi} dt \\
    &= 2\pi.
  \end{align*}

\item[(2)]
  \begin{align*}
    d \eta
    =& d \left( \frac{x dy - y dx}{x^2+y^2} \right) \\
    =& d \left( \frac{x}{x^2+y^2} \right) \wedge dy
      - d \left( \frac{y}{x^2+y^2} \right) \wedge dx
      &(d^2 = 0) \\
    =& D_1\left(\frac{x}{x^2+y^2}\right) dx \wedge dy
      &(dy \wedge dy = 0) \\
      &- D_2\left(\frac{y}{x^2+y^2}\right) dy \wedge dx
      &(dx \wedge dx = 0) \\
    =& \left(\frac{1}{x^2+y^2} - \frac{2x^2}{(x^2+y^2)^2}\right) dx \wedge dy \\
      &+ \left(\frac{1}{x^2+y^2} - \frac{2y^2}{(x^2+y^2)^2}\right) dx \wedge dy \\
    =& 0
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
  $\eta$ is closed and locally exact, that is,
  $\eta = dt$ on $\mathbb{R}^2 - L$
  where $L$ is any line passing through $\mathbf{0}$.
  $\eta$ is not exact since $\int_{\gamma} \eta = 2\pi \neq 0$.
  (See Exercise 10.22(g).)

\item[(2)]
  \emph{(Poincar\'e's Lemma for $1$-form.)
  Let $\omega = \sum a_i dx_i$ be defined in an open set $U \subseteq \mathbb{R}^n$.
  Then $d\omega = 0$ if and only if for each $p \in U$ there is a neighborhood $V \subseteq U$
  of $p$ and a differentiable function $f: V \to \mathbb{R}^1$ with
  $df = \omega$ (i.e., $\omega$ is locally exact).} \\
\end{enumerate}



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  For $0 \leq t \leq 2\pi$, $0 \leq u \leq 1$, define
  \[
    \Phi(t,u) = (1-u)\Gamma(t) + u\gamma(t).
  \]
  Then $\Phi$ is a $2$-surface in $\mathbb{R}^2 - \{\mathbf{0}\}$
  whose parameter domain
  $D = \{(t,u) : 0 \leq t \leq 2\pi, 0 \leq u \leq 1 \}$ is the indicated rectangle.

\item[(2)]
  Similar to Example 10.32,
  \[
    \partial \Phi = \gamma_1 + \gamma_2 + \gamma_3 + \gamma_4
  \]
  where
  \begin{align*}
    \gamma_1(t) &= \Phi(t,0) = \Gamma(t), \\
    \gamma_2(u) &= \Phi(2\pi,u) = (1-u)\Gamma(2\pi) + u\gamma(2\pi), \\
    \gamma_3(t) &= \Phi(2\pi-t,1) = \gamma(2\pi-t), \\
    \gamma_4(u) &= \Phi(0,1-u) = u\Gamma(0) + (1-u)\gamma(0).
  \end{align*}
  Because of cancellations (as in Example 10.32), $\gamma(0) = \gamma(2\pi)$
  and $\Gamma(0) = \Gamma(2\pi)$,
  $\gamma_4 = -\gamma_2$ and $\gamma_3 = -\gamma$.
  Hence,
  \[
    \partial \Phi = \Gamma - \gamma.
  \]

\item[(3)]
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\Phi} d\eta
    = \int_{\partial\Phi} \eta
    = \int_{\Gamma - \gamma} \eta
    = \int_{\Gamma} \eta - \int_{\gamma} \eta.
  \]
  Hence,
  \[
    \int_{\Gamma} \eta = \int_{\gamma} \eta
  \]
  (since $d\eta = 0$ by (a)).
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  $\Gamma$ satisfies all conditions described in (b).
  So
  \[
    \int_{\Gamma} \eta = 2\pi.
  \]

\item[(2)]
  A direct calculation shows that
  \begin{align*}
    2\pi = \int_{\Gamma} \eta
    &= \int_{\Gamma} \frac{x dy - y dx}{x^2+y^2} \\
    &= \int_{0}^{2\pi}
      \frac{a \cos(t) d(b \sin(t)) - b \sin(t) d(a \cos(t))}{(a \cos(t))^2+(b \sin(t))^2} \\
    &= \int_{0}^{2\pi}
      \frac{ab (\cos^2 t + \sin^2 t)}{a^2\cos^2 t + b^2 \sin^2 t} \\
    &= \int_{0}^{2\pi}
      \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  In any convex open set in which $x \neq 0$,
  we have
  \begin{align*}
    d\left( \arctan\frac{y}{x} \right)
    &= \left( D_1\arctan\frac{y}{x} \right) dx
      + \left( D_2\arctan\frac{y}{x} \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(2)]
  In any convex open set in which $y \neq 0$,
  we have
  \begin{align*}
    d\left( -\arctan\frac{x}{y} \right)
    &= \left( D_1\left(-\arctan\frac{x}{y}\right) \right) dx
      + \left( D_2\left(-\arctan\frac{x}{y}\right) \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(3)]
  By (1)(2), $\eta$ is locally exact.
  Note that $\theta_1 = \arctan\frac{y}{x}$
  and $\theta_2 = -\arctan\frac{x}{y}$
  cannot be patched together to defined a global $0$-form $\theta$
  on $\mathbb{R}^2 - \{\mathbf{0}\}$.

\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
  Partition $[0,2\pi]$ into five subintervals
  \[
    I_i
    = \left[ \frac{(2i-3)\pi}{4}, \frac{(2i-1)\pi}{4} \right]
      \cap [0,2\pi].
  \]
  for $i = 1,2,3,4,5$.
  Hence
  \begin{align*}
    \int_{\gamma} \eta
    =& \: \sum_{i=1}^{5} \int_{\gamma(I_i)} \eta \\
    =& \: \sum_{i=1,3,5} \int_{\gamma(I_i)} d\left( \arctan\frac{y}{x} \right)
      + \sum_{i=2,4} \int_{\gamma(I_i)} d\left( -\arctan\frac{x}{y} \right).
  \end{align*}

\item[(2)]
  The Stokes' theorem (Theorem 10.33) implies that
  \begin{align*}
    \int_{\gamma(I_1)} d\left( \arctan\frac{y}{x} \right)
    &= \int_{\partial\gamma(I_1)} \arctan\frac{y}{x} \\
    &= \left[ \arctan\frac{r\cos t}{r\sin t} \right]_{t = 0}^{t = \frac{\pi}{4}} \\
    &= \left[ \arctan(\tan(t)) \right]_{t = 0}^{t = \frac{\pi}{4}} \\
    &= \frac{\pi}{4},
  \end{align*}
  and
  \begin{align*}
    \int_{\gamma(I_2)} d\left( -\arctan\frac{x}{y} \right)
    &= \int_{\partial\gamma(I_2)} -\arctan\frac{x}{y} \\
    &= \left[ \arctan\frac{r\sin t}{r\cos t} \right]_{t = \frac{\pi}{4}}^{t = \frac{3\pi}{4}} \\
    &= \left[ \arctan(\cot(t)) \right]_{t = \frac{\pi}{4}}^{t = \frac{3\pi}{4}} \\
    &= \frac{\pi}{2}.
  \end{align*}
  Similarly,
  \begin{align*}
    \int_{\gamma(I_3)} d\left( \arctan\frac{y}{x} \right)
    &= \frac{\pi}{2} \\
    \int_{\gamma(I_4)} d\left( -\arctan\frac{x}{y} \right)
    &= \frac{\pi}{2} \\
    \int_{\gamma(I_5)} d\left( \arctan\frac{y}{x} \right)
    &= \frac{\pi}{4}.
  \end{align*}

\item[(3)]
  Therefore,
  \[
    \int_{\gamma} \eta
    = \left( \frac{\pi}{4} + \frac{\pi}{2} + \frac{\pi}{4} \right)
      + \left( \frac{\pi}{2} + \frac{\pi}{2} \right)
    = 2\pi.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
  Regard $\Gamma(t)$ as a plane curve $(\Gamma_1(t), \Gamma_2(t))$ over $\mathbb{R}^2$
  or $\Gamma_1(t) + i \Gamma_2(t)$ over $\mathbb{C}^1$.
  Note that
  \begin{align*}
    \frac{\Gamma'(t)}{\Gamma(t)}
    &= \frac{\Gamma_1'(t) + i \Gamma_2'(t)}{\Gamma_1(t) + i \Gamma_2(t)} \\
    &= \frac{\Gamma_1'(t) \Gamma_1'(t) + \Gamma_2'(t) \Gamma_2'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}
      + i \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}.
  \end{align*}
  So
  \[
    \mathrm{Im}\left(\frac{\Gamma'(t)}{\Gamma(t)}\right)
    = \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}.
  \]

\item[(2)]
  By Exercise 8.23,
  \[
    \mathrm{Ind}(\Gamma)
    = \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{\Gamma'(t)}{\Gamma(t)} dt
  \]
  is always an integer.
  That is,
  \begin{align*}
    \mathrm{Ind}(\Gamma)
    &= \frac{1}{2\pi}\int_{0}^{2\pi} \mathrm{Im}\left(\frac{\Gamma'(t)}{\Gamma(t)}\right) dt \\
    &= \frac{1}{2\pi}\int_{0}^{2\pi}
      \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
        {\Gamma_1(t)^2 + \Gamma_2(t)^2} dt \\
    &= \frac{1}{2\pi}\int_{\Gamma} \frac{xdy - ydx}{x^2+y^2} \\
    &= \frac{1}{2\pi}\int_{\Gamma} \eta.
  \end{align*}
  (Note that $\mathrm{Ind}(\Gamma) = 1$ if $\Gamma$ is defined as in (c).
  Hence the integral in (c) is equal to $2\pi \mathrm{Ind}(\Gamma) = 2\pi$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.22.}
\addcontentsline{toc}{subsection}{Exercise 10.22.}
\emph{As in Example 10.37, define $\zeta$ in $\mathbb{R}^3-\{\mathbf{0}\}$ by
\[
  \zeta = \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3}
\]
where $r = (x^2+y^2+z^2)^{\frac{1}{2}}$,
let $D$ be the rectangle given by $0 \leq u \leq \pi$, $0 \leq v \leq 2\pi$,
and let $\Sigma$ be the $2$-surface in $\mathbb{R}^3$,
with parameter domain $D$, given by
\[
  x = \sin u \cos v,
  \qquad
  y = \sin u \sin v,
  \qquad
  z = \cos u.
\]}
\begin{enumerate}
\item[(a)]
  \emph{Prove that $d\zeta = 0$ in $\mathbb{R}^3 - \{ \mathbf{0} \}$.}

\item[(b)]
  \emph{Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subseteq D$.
  Prove that
  \[
    \int_{S} \zeta
    = \int_{E} \sin u \: du \: dv
    = A(S),
  \]
  where $A$ denotes area, as in Section 10.46.
  Note that this contains
  \[
    \int_{\Sigma} \zeta
    = \int_{D} \sin u \: du \: dv
    = 4\pi \neq 0
  \]
  as a special case.}

\item[(c)]
  \emph{Suppose $g, h_1, h_2, h_3$, are $\mathscr{C}''$-functions on $[0,1]$, $g > 0$.
  Let $(x,y,z) = \Phi(s,t)$ define a $2$-surface $\Phi$,
  with parameter domain $I^2$, by
  \[
    x = g(t)h_1(s),
    \qquad
    y = g(t)h_2(s),
    \qquad
    z = g(t)h_3(s).
  \]
  Prove that
  \[
    \int_{\Phi} \zeta = 0,
  \]
  directly from Equation (35) in Chapter 10.
  Note the shape of the range of $\Phi$:
  For fixed $s$, $\Phi(s,t)$ runs over an interval on a line through $\mathbf{0}$.
  The range of $\Phi$ thus lies in a ``cone'' with vertex at the origin.}

\item[(d)]
  \emph{Let $E$ be a closed rectangle in $D$, with edges parallel to those of $D$.
  Suppose $f \in \mathscr{C}''(D)$, $f > 0$.
  Let $\Omega$ be the $2$-surface with parameter domain $E$,
  defined by
  \[
    \Omega(u,v) = f(u,v)\Sigma(u,v).
  \]
  Define $S$ as in (b) and prove that
  \[
    \int_{\Omega} \zeta = \int_{S} \zeta = A(S).
  \]
  (Since $S$ is the ``radical projection'' of $\Omega$ into the unit sphere,
  this result makes it reasonable to call $\int_{\Omega} \zeta$ the ``solid angle''
  subtended by the range of $\Omega$ at the origin.)
  (Hint:
  Consider the $3$-surface $\Psi$ given by
  \[
    \Psi(t,u,v) = [1-t+tf(u,v)]\Sigma(u,v),
  \]
  where $(u,v) \in E$, $0 \leq t \leq 1$.
  For fixed $v$, the mapping $(t,u) \mapsto \Psi(t,u,v)$ is a $2$-surface $\Phi$
  to which (c) can be applied to show that $\int_{\Phi} \zeta = 0$.
  The same thing holds when $u$ is fixed.
  By (a) and Stokes' theorem,
  \[
    \int_{\partial \Psi} \zeta = \int_{\Psi} d\zeta = 0.)
  \]}

\item[(e)]
  \emph{Put $\lambda = -\frac{z}{r}\eta$, where
  \[
    \eta = \frac{xdy-ydx}{x^2+y^2},
  \]
  as in Exercise 10.21.
  Then $\lambda$ is a $1$-form in the open set $V \subseteq \mathbb{R}^3$ in which $x^2+y^2 > 0$.
  Show that $\zeta$ is exact in $V$ by showing that
  \[
    \zeta = d\lambda.
  \]}
\item[(f)]
  \emph{Derive (d) from (e), without using (c).
  (Hint: To begin with, assume $0 < u < \pi$ on $E$.
  By (e),
  \[
    \int_{\Omega} \zeta = \int_{\partial\Omega} \lambda
    \qquad
    \text{and}
    \qquad
    \int_{S} \zeta = \int_{\partial S} \lambda.
  \]
  Show that the two integrals of $\lambda$ are equal,
  by using part (d) of Exercise 10.21,
  and by noting that $\frac{z}{r}$ is the same at $\Sigma(u,v)$ as at $\Omega(u,v)$.)}

\item[(g)]
  \emph{Is $\zeta$ exact in the complement of every line through the origin?} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Note that $\zeta$ is well-defined on $\mathbb{R}^3 - \{ \mathbf{0} \}$.
  Hence,
  \begin{align*}
    d\zeta
    =& \: d\left( \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3} \right) \\
    =& \: d\left(\frac{x}{r^3}\right) \wedge dy \wedge dz
      + d\left(\frac{y}{r^3}\right) \wedge dz \wedge dx
      + d\left(\frac{z}{r^3}\right) \wedge dx \wedge dy \\
    =& \: D_1\left(\frac{x}{r^3}\right) dx \wedge dy \wedge dz
      + D_2\left(\frac{y}{r^3}\right) dy \wedge dz \wedge dx
      + D_3\left(\frac{z}{r^3}\right) dz \wedge dx \wedge dy \\
    =& \: \frac{r^3 - 3rx^2}{r^6} dx \wedge dy \wedge dz
      + \frac{r^3 - 3ry^2}{r^6} dy \wedge dz \wedge dx
      + \frac{r^3 - 3rz^2}{r^6} dz \wedge dx \wedge dy \\
    =& \: \left(\frac{r^3 - 3rx^2}{r^6}
      + \frac{r^3 - 3ry^2}{r^6}
      + \frac{r^3 - 3rz^2}{r^6}\right) dx \wedge dy \wedge dz \\
    =& \: 0 dx \wedge dy \wedge dz \\
    =& \: 0
  \end{align*}
  in $\mathbb{R}^3 - \{ \mathbf{0} \}$.

\item[(2)]
  Or write
  \[
    \mathbf{F}
    = \frac{x}{r^3} \mathbf{e}_1 + \frac{y}{r^3} \mathbf{e}_2 + \frac{z}{r^3}  \mathbf{e}_3
  \]
  as in Vector fields 10.42.
  So
  \[
    \omega_{\mathbf{F}} = \zeta
  \]
  and
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F}) dx \wedge dy \wedge dz
  \]
  as in the proof of the divergence theorem (Theorem 10.51).
  Note that the divergence of $\mathbf{F}$ is zero.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By Area elements in $\mathbb{R}^3$ 10.46,
  \begin{align*}
    \mathbf{N}(u,v)
    &= \frac{\partial(y,z)}{\partial(u,v)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(u,v)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(u,v)}\mathbf{e}_3 \\
    &= (\sin^2 u \cos v) \mathbf{e}_1
      + (\sin^2 u \sin v) \mathbf{e}_2
      + (\sin u \cos u) \mathbf{e}_3.
  \end{align*}
  Here
  $\abs{\mathbf{N}(u,v)} = \sin u \geq 0$ (by noting that $u \in [0,\pi]$),
  and
  \[
    \mathbf{n}(u,v)
    = \frac{\mathbf{N}(u,v)}{ \abs{\mathbf{N}(u,v)} }
    = (\sin u \cos v, \sin u \sin v, \cos u).
  \]

\item[(2)]
  Note that $\zeta = x dy \wedge dz + y dz \wedge dx + z dx \wedge dy$ on $S \subseteq \Sigma$.
  Hence,
  by Integrals of $2$-forms in $\mathbb{R}^3$ 10.49,
  \begin{align*}
    \int_{S} \zeta
    =& \: \int_{S} x dy \wedge dz + y dz \wedge dx + z dx \wedge dy \\
    =& \: \int_{E} (\sin u \cos v, \sin u \sin v, \cos u) \cdot \mathbf{N}(u,v) \: du \: dv \\
    =& \: \int_{E} \mathbf{n}(u,v) \cdot \mathbf{n}(u,v) \abs{\mathbf{N}(u,v)} \: du \: dv \\
    =& \: \int_{E} \abs{\mathbf{N}(u,v)} \: du \: dv \\
    =& \: A(S).
  \end{align*}

\item[(3)]
  In particular,
  \begin{align*}
    \int_{\Sigma} \zeta
    =& \: \int_{D} \sin u \: du \: dv \\
    =& \: \int_{0}^{\pi} \int_{0}^{2\pi} \sin u \: du \: dv \\
    =& \: \left( \int_{0}^{\pi} \sin u \: du \right)
      \left( \int_{0}^{2\pi} dv \right) \\
    =& \: 2 \cdot 2 \pi \\
    =& \: 4 \pi.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Similar to (b).
  \begin{align*}
    \mathbf{N}(s,t)
    =& \: \frac{\partial(y,z)}{\partial(s,t)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(s,t)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(s,t)}\mathbf{e}_3 \\
    =& \: g(t)g'(t)[ (h_1(s),h_2(s),h_3(s)) \times (h_1'(s),h_2'(s),h_3'(s)) ] \\
    =& \: g(t)g'(t)[ \mathbf{h}(s) \times \mathbf{h}'(s)],
  \end{align*}
  where $\mathbf{h}(s) = (h_1(s),h_2(s),h_3(s))$
  and $\mathbf{h}'(s) = (h_1'(s),h_2'(s),h_3'(s))$.
  (Here ``$\times$'' is the cross product in $\mathbb{R}^3$.)

\item[(2)]
  Assume $\zeta$ is well-defined, i.e., $\mathbf{h}(s) \neq \mathbf{0}$ for all $s \in [0,1]$.
  By Integrals of $2$-forms in $\mathbb{R}^3$ 10.49,
  \begin{align*}
    \int_{\Phi} \zeta
    =& \: \int_{\Phi} \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3} \\
    =& \: \int_{I^2} \frac{g(t)}{g(t)^3 |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot \mathbf{N}(s,t) \: ds \: dt \\
    =& \: \int_{I^2} \frac{g(t)}{g(t)^3 |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot g(t)g'(t)[ \mathbf{h}(s) \times \mathbf{h}'(s)] \: ds \: dt \\
    =& \: \int_{I^2} \frac{g'(t)}{g(t) |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot [ \mathbf{h}(s) \times \mathbf{h}'(s)] \: ds \: dt \\
    =& \: 0
  \end{align*}
  (since $\mathbf{h}(s) \cdot [ \mathbf{h}(s) \times \mathbf{h}'(s)] = 0$.)

\item[(3)]
  Note that $\Sigma$ in spherical coordinate system
  cannot be parameterized as
  $(x,y,z) = g(t)\mathbf{h}(s)$,
  and thus $\int_{S} \zeta$ could be nonzero as shown in (b).
\end{enumerate}
$\Box$ \\



\emph{Proof of (d) (Hint).}
\begin{enumerate}
\item[(1)]
  Consider the $3$-surface $\Psi$ given by
  \[
    \Psi(t,u,v) = [1-t+tf(u,v)]\Sigma(u,v),
  \]
  where $(u,v) \in E$, $0 \leq t \leq 1$.
  Write
  \[
    E = [a_1,b_1] \times [a_2,b_2] \subseteq D = [0,\pi] \times [0, 2\pi].
  \]
  Note that $\Psi(t,u,v) \subseteq \mathbb{R}^3 - \{ \mathbf{0} \}$.
  So the boundary of $\Psi$ is
  \begin{align*}
    \partial \Psi
    =& \: \Psi(0,u,v) - \Psi(1,u,v) \\
      &+ \Psi(t,a_1,v) - \Psi(t,b_1,v) \\
      &+ \Psi(t,u,a_2) - \Psi(t,u,b_2) \\
    =& \: S(u,v) - \Omega(u,v) \\
      &+ \Psi|_{u=a_1}(t,v) - \Psi|_{u=b_1}(t,v) \\
      &+ \Psi|_{v=a_2}(t,u) - \Psi|_{v=b_2}(t,u),
  \end{align*}
  where $\Psi|_{u=u_0}(t,v) = \Psi(t,u_0,v)$ and $\Psi|_{v=v_0}(t,u) = \Psi(t,u,v_0)$.

\item[(2)]
  \emph{Show that
  \[
    \int_{\Psi|_{v=v_0}} \zeta = 0
  \]
  for any fixed $v = v_0 \in [a_2, b_2]$.}
  Note that $\zeta$ is well-defined on $\Psi|_{v=v_0}$.
  Write $\Psi|_{v=v_0}(t,u) = (x,y,z) = (x(t,u),y(t,u),z(t,u))$.
  By definition of $\Psi$, we have
  \begin{align*}
    x &= g(t,u) \sin u \cos v_0 \\
    y &= g(t,u) \sin u \sin v_0 \\
    z &= g(t,u) \cos u,
  \end{align*}
  where $g(t,u) = 1-t+tf(u,v_0)$.
  Similar to (c),
  \begin{align*}
    \mathbf{N}(t,u)
    =& \: \frac{\partial(y,z)}{\partial(t,u)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(t,u)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(t,u)}\mathbf{e}_3 \\
    =& \: g(t,u) D_1 g(t,u) (-\sin v_0, \cos v_0, 0).
  \end{align*}
  Note that
  \[
    (x(t,u),y(t,u),z(t,u)) \cdot \mathbf{N}(t,u) = 0.
  \]
  So
  \begin{align*}
    \int_{\Psi|_{v=v_0}} \zeta
    =& \: \int_{\Psi|_{v=v_0}} r^{-3} (x dy \wedge dz + y dz \wedge dx + z dx \wedge dy) \\
    =& \: \int_{[0,1]\times[a_1,b_1]}
      r^{-3}(x(t,u),y(t,u),z(t,u)) \cdot \mathbf{N}(t,u) \: dt \: du \\
    =& \: \int_{[0,1]\times[a_1,b_1]} 0 \: dt \: du \\
    =& \: 0.
  \end{align*}

\item[(3)]
  \emph{Show that
  \[
    \int_{\Psi|_{u=u_0}} \zeta = 0
  \]
  for any fixed $u = u_0 \in [a_1, b_1]$.}
  Similar to (2).
  \begin{align*}
    \mathbf{N}(t,v)
    =& \: \frac{\partial(y,z)}{\partial(t,v)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(t,v)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(t,v)}\mathbf{e}_3 \\
    =& \: \sin u_0 g(t,v) D_1 g(t,v)(-\cos u_0 \cos v, -\cos u_0 \sin v, \sin u_0).
  \end{align*}
  where $g(t,v) = 1-t+tf(u_0,v)$.
  So $(x(t,v),y(t,v),z(t,v)) \cdot \mathbf{N}(t,v) = 0$ and thus
  $\int_{\Psi|_{u=u_0}} \zeta = 0$.

\item[(4)]
  So
  \begin{align*}
    0
    =& \: \int_{\Psi} d\zeta
      &(\text{$d\zeta = 0$ on $\mathbb{R}^3-\{\mathbf{0}\}$}) \\
    =& \: \int_{\partial \Psi} \zeta
      &(\text{Theorem 10.33}) \\
    =& \: \int_{S} \zeta - \int_{\Omega} \zeta \\
      &+ \underbrace{\int_{\Psi|_{u=a_1}} \zeta - \int_{\Psi|_{u=b_1}} \zeta}_{
        \text{all are zero by (2)}} \\
      &+ \underbrace{\int_{\Psi|_{v=a_2}} \zeta - \int_{\Psi|_{v=b_2}} \zeta}_{
        \text{all are zero by (3)}}
        &((1)) \\
    =& \: \int_{S} \zeta - \int_{\Omega} \zeta.
  \end{align*}
  Hence
  \[
    \int_{\Omega} \zeta = \underbrace{\int_{S} \zeta = A(S)}_{\text{by (b)}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    d \left( -\frac{z}{r} \right)
    = \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{r^2-z^2}{r^3} dz
    = \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{x^2+y^2}{r^3} dz
  \]
  since $r^2 = x^2+y^2+z^2$.

\item[(2)]
  \begin{align*}
    d\lambda
    &= d \left( -\frac{z}{r}\eta \right) \\
    &= \underbrace{d \left( -\frac{z}{r} \right)}_{\text{apply (1)}} \wedge \eta
      + (-1)^1 \left( -\frac{z}{r} \right) \wedge \underbrace{d\eta}_{= 0} \\
    &= \left( \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{x^2+y^2}{r^3} dz \right)
      \wedge \left( \frac{-ydx + xdy}{x^2+y^2} \right) \\
    &= \left( \frac{x(x^2+y^2)}{r^3(x^2+y^2)} \right) dy \wedge dz
      + \left( \frac{y(x^2+y^2)}{r^3(x^2+y^2)} \right) dz \wedge dx
      + \left( \frac{x^2 z + y^2 z}{r^3(x^2+y^2)} \right) dx \wedge dy \\
    &= \left( \frac{x}{r^3} \right) dy \wedge dz
      + \left( \frac{y}{r^3} \right) dz \wedge dx
      + \left( \frac{z}{r^3} \right) dx \wedge dy \\
    &= \zeta.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
  To ensure that $\eta$ is well-defined on $E$,
  we might assume $x^2+y^2 = \sin^2 u \neq 0$ or $0 < u < \pi$ on $E$.
  It is fine since
  $\int_{\Omega} \zeta$ and $\int_{S} \zeta$ is well-defined on any closed rectangle in $D$
  and we can apply the argument in Exercise 6.7 to remove the additional restriction.

\item[(2)]
  By the Stokes' theorem (Theorem 10.33) and (e),
  \[
    \int_{\Omega} \zeta = \int_{\partial\Omega} \lambda
    \qquad
    \text{and}
    \qquad
    \int_{S} \zeta = \int_{\partial S} \lambda.
  \]
  So it suffices to show that
  \[
    \int_{\partial\Omega} \lambda = \int_{\partial S} \lambda.
  \]
  Note that $\lambda = -\frac{z}{r} \eta$,
  and thus it suffices to show that
  $\left.\frac{z}{r}\right|_{\partial\Omega} = \left.\frac{z}{r}\right|_{\partial\Sigma}$
  and $\eta|_{\partial\Omega} = \eta|_{\partial S}$.

\item[(3)]
  \emph{Show that
  $\left.\frac{z}{r}\right|_{\partial\Omega} = \left.\frac{z}{r}\right|_{\partial\Sigma}$.}
  For any $(x_{\Omega}, y_{\Omega}, z_{\Omega}) \in \partial\Omega$,
  \[
    (x_{\Omega}, y_{\Omega}, z_{\Omega}) = f(u,v)(x_{\Sigma}, y_{\Sigma}, z_{\Sigma})
  \]
  where $(x_{\Sigma}, y_{\Sigma}, z_{\Sigma}) \in \partial S$.
  So
  \begin{align*}
    \left.\frac{z}{r}\right|_{\partial\Omega}
    &= \frac{z_{\Omega}}{(x_{\Omega}^2+y_{\Omega}^2+z_{\Omega}^2)^{\frac{1}{2}}} \\
    &= \frac{f(u,v)z_{\Sigma}}{f(u,v)(x_{\Sigma}^2+y_{\Sigma}^2+z_{\Sigma}^2)^{\frac{1}{2}}} \\
    &= \frac{z_{\Sigma}}{(x_{\Sigma}^2+y_{\Sigma}^2+z_{\Sigma}^2)^{\frac{1}{2}}} \\
    &= \left.\frac{z}{r}\right|_{\partial S}.
  \end{align*}
  (Note that $f > 0$.)

\item[(4)]
  \emph{Show that $\eta|_{\partial\Omega} = \eta|_{\partial S}$.}
  Similar to (3).
  If $x_{\Omega} \neq 0$ (or $x_{\Sigma} \neq 0$),
  then by Exercise 10.21(d)
  \begin{align*}
    \eta|_{\partial\Omega}
    &= d\left(\arctan\frac{y_{\Omega}}{x_{\Omega}}\right) \\
    &= d\left(\arctan\frac{f(u,v)y_{\Sigma}}{f(u,v)x_{\Sigma}}\right) \\
    &= d\left(\arctan\frac{y_{\Sigma}}{x_{\Sigma}}\right) \\
    &= \eta|_{\partial S}.
  \end{align*}
  Similarly, $\eta|_{\partial\Omega} = \eta|_{\partial S}$ is also true if $y_{\Omega} \neq 0$.
  Note that $(x_{\Omega},y_{\Omega}) \neq (0,0)$ by assumption.
  Therefore the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (g).}
\begin{enumerate}
\item[(1)]
  Yes.
  Given any line $L$ passing through $\mathbf{0}$,
  say
  \[
    (r \sin u \cos v, r \sin u \sin v, r \cos u) \in L
    \qquad
    (r \in \mathbb{R}^1),
  \]
  for some $u \in [0,\pi]$ and $v \in [0,2\pi]$.
  We will show that $\zeta$ is exact in $U = \mathbb{R}^3 - L$.

\item[(2)]
  Linear algebra says that all rotation matrices $T \in SO(3)$ can be obtained from
  \begin{align*}
    R_x(u)
    &=
    \begin{bmatrix}
      1 &      0 &       0 \\
      0 & \cos u & -\sin u \\
      0 & \sin u &  \cos u
    \end{bmatrix} \\
    R_y(v)
    &=
    \begin{bmatrix}
      \cos v & 0 & -\sin v \\
           0 & 1 &       0 \\
      \sin v & 0 &  \cos v
    \end{bmatrix} \\
    R_z(w)
    &=
    \begin{bmatrix}
      \cos w & -\sin w & 0 \\
      \sin w &  \cos w & 0 \\
           0 &       0 & 1
    \end{bmatrix}
  \end{align*}
  using matrix multiplication,
  say $T = R_x(u)R_y(v)R_z(w)$.
  For example, the rotation
  \[
    T = R_y\left(u-\frac{\pi}{2}\right)R_z(-v)
  \]
  maps $L$ to the $z$-axis
  (by showing that $T(r \sin u \cos v, r \sin u \sin v, r \cos u) = (0,0,r)$).
  By Theorem 10.22
  it suffices to show that $\zeta$ is invariant under $R_x(u)$, $R_x(v)$ and $R_z(w)$.
  By the symmetricity of $\zeta$, it suffices to show that $\zeta$ is invariant under $T = R_x(u)$.

\item[(3)]
  \emph{Show that $\zeta$ is invariant under $T = R_x(u)$.}
  By
  \[
    T: (x,y,z) \mapsto (x, y \cos u - z \sin u, y \sin u + z \cos u),
  \]
  we have
  \begin{align*}
    r &\mapsto r \\
    dx &\mapsto dx \\
    dy &\mapsto \cos u dy - \sin u dz \\
    dz &\mapsto \sin u dy + \cos u dz.
  \end{align*}
  So
  \begin{align*}
    dy \wedge dz
      &\mapsto (\cos u dy - \sin u dz) \wedge (\sin u dy + \cos u dz) \\
      &= dy \wedge dz, \\
    dz \wedge dx
      &\mapsto (\sin u dy + \cos u dz) \wedge dx \\
      &= -\sin u dx \wedge dy + \cos u dz \wedge dx, \\
    dx \wedge dy
      &\mapsto dx \wedge (\sin u dy + \cos u dz) \\
      &= \cos u dx \wedge dy + \sin u dz \wedge dx.
  \end{align*}
  Thus
  \begin{align*}
    \zeta
    \mapsto& \:
      r^{-3} \{ x dy \wedge dz \\
      &\qquad + (y \cos u - z \sin u)(-\sin u dx \wedge dy + \cos u dz \wedge dx) \\
      &\qquad + (y \sin u + z \cos u)(\cos u dx \wedge dy + \sin u dz \wedge dx) \} \\
    =& \:
      r^{-3} \{ x dy \wedge dz \\
      &\qquad + [\cos u(y \cos u - z \sin u) + \sin u(y \sin u + z \cos u)] dz \wedge dx \\
      &\qquad + [-\sin u(y \cos u - z \sin u) + \cos u(y \sin u + z \cos u)] dx \wedge dy \} \\
    =& \:
      r^{-3} \{ x dy \wedge dz + y dz \wedge dx + z dx \wedge dy \} \\
    =& \: \zeta.
  \end{align*}

\item[(4)]
  Let $V = \mathbb{R}^3 - \text{$z$-axis}$.
  Since $\zeta_T = \zeta$ (by (3)) is well-defined in $V$,
  $\zeta_T = \zeta = d\lambda$ by (e).
  Here $\lambda$ is in $V$, not necessary in $U$ (if $L \neq$ $z$-axis).
  Luckily, we can use $T^{-1}$ to pullback $\lambda$ in $U$.
  Thus
  \[
    \zeta
    = (\zeta_T)_{T^{-1}}
    = (d\lambda)_{T^{-1}}
    = d(\lambda_{T^{-1}})
  \]
  by Theorems 10.22 and 10.23.
  That is, $\zeta$ is exact in $U = \mathbb{R}^3 - L$.
  (Or $\zeta$ is locally exact in $\mathbb{R}^3 - \{ \mathbf{0} \}$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.23.}
\addcontentsline{toc}{subsection}{Exercise 10.23.}
\emph{Fix $n$.
Define $r_k = (x_1^2+\cdots+x_k^2)^{\frac{1}{2}}$ for $1 \leq k \leq n$,
let $E_k$ be the set of all $\mathbf{x} \in \mathbb{R}^n$ at which $r_k > 0$,
and let $\omega_k$ be the $(k-1)$-form defined in $E_k$ by
\[
  \omega_k
  = (r_k)^{-k}
    \sum_{i=1}^{k} (-1)^{i-1} x_i
    dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k
\]
Note that $\omega_2 = \eta$, $\omega_3 = \zeta$ in the terminology of
Exercise 10.21 and Exercise 10.22.
Note also that}
\[
  E_1 \subseteq E_2 \subseteq \cdots \subseteq E_n = \mathbb{R}^n.
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that $d\omega_k = 0$ in $E_k$.}

\item[(b)]
  \emph{For $k=2,\ldots,n$, prove that $\omega_k$ is exact in $E_{k-1}$,
  by showing that
  \[
    \omega_k = d(f_k\omega_{k-1}) = df_k \wedge \omega_{k-1}
  \]
  where $f_k(\mathbf{x}) = (-1)^k g_k\left( \frac{x_k}{r_k} \right)$
  where
  \[
    g_k(t) = \int_{-1}^{t} (1-s^2)^{\frac{k-3}{2}} ds
    \qquad
    (-1 < t < 1).
  \]
  (Hint: $f_k$ satisfies the differential equations
  \[
    \mathbf{x} \cdot (\nabla f_k)(\mathbf{x}) = 0
  \]
  and
  \[
    (D_k f_k)(\mathbf{x}) = \frac{(-1)^k(r_{k-1})^{k-1}}{(r_k)^k}.)
  \]}

\item[(c)]
  \emph{Is $\omega_n$ exact in $E_n$?}

\item[(d)]
  \emph{Note that (b) is a generalization of part (e) of Exercise 10.22.
  Try to extend some of the other assertions of Exercise 10.21 and Exercise 10.22
  to $\omega_n$, for arbitrary $n$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    D_i r_k = \frac{1}{2r_k} \cdot (2x_i) = \frac{x_i}{r_k}.
  \]
\item[(2)]
  \begin{align*}
    d\omega_k
    &= \sum_{i=1}^{k} d\left(
      (-1)^{i-1} (r_k)^{-k} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k\right) \\
    &= \sum_{i=1}^{k} D_i \left( (-1)^{i-1} (r_k)^{-k} x_i \right)
      dx_i \wedge dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k \\
    &= \sum_{i=1}^{k} (-1)^{i-1} \left(
      (r_k)^{-k} \cdot 1 + \underbrace{(-k)(r_k)^{-k-1} \frac{x_i}{r_k}}_{\text{chain rule}}
        \cdot x_i \right)
      \underbrace{(-1)^{i-1} dx_1 \wedge \cdots \wedge dx_k}_{\text{anticommutative relation}} \\
    &= (r_k)^{-k-2} \underbrace{\sum_{i=1}^{k} \left(
      (r_k)^2 - k x_i^2 \right)}_{= 0}
      dx_1 \wedge \cdots \wedge dx_k \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    D_i \left( \frac{x_k}{r_k} \right)
    = \frac{\delta_{ik}(r_k)^2 - x_i x_k}{(r_k)^3}
  \]
  where $\delta_{ik}$ is the Kronecker delta.
  So
  \begin{align*}
    (D_i f_k)(\mathbf{x})
    &= D_i \left( (-1)^k g_k\left(\frac{x_k}{r_k}\right) \right) \\
    &= D_i \left( (-1)^k \int_{-1}^{\frac{x_k}{r_k}}(1-s^2)^{\frac{k-3}{2}} ds \right) \\
    &= (-1)^k D_i\left( \frac{x_k}{r_k} \right)
      \left(1-\left(\frac{x_k}{r_k}\right)^2 \right)^{\frac{k-3}{2}} \\
    &= (-1)^k \frac{\delta_{ik}(r_k)^2 - x_i x_k}{(r_k)^3}
      \frac{(r_{k-1})^{k-3}}{(r_k)^{k-3}} \\
    &= (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}(\delta_{ik}(r_k)^2 - x_i x_k).
  \end{align*}
  In particular,
  \[
    (D_k f_k)(\mathbf{x})
    = (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}((r_k)^2 - (x_k)^2)
    = (-1)^k \frac{(r_{k-1})^{k-1}}{(r_k)^{k}}
  \]
  (since $(r_k)^2 - (x_k)^2 = (r_{k-1})^2$).

\item[(2)]
  Since
  \[
    \sum_{i} x_i (\delta_{ik}(r_k)^2 - x_i x_k)
    = (r_k)^2 \underbrace{\sum_{i} x_i\delta_{ik}}_{= x_k}
      - x_k \underbrace{\sum_{i} x_i^2}_{= (r_k)^2}
    = 0,
  \]
  we have
  \begin{align*}
    \mathbf{x} \cdot (\nabla f_k)(\mathbf{x})
    &= \sum_{i} x_i (D_i f_k)(\mathbf{x}) \\
    &= \sum_{i} x_i (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}(\delta_{ik}(r_k)^2 - x_i x_k) \\
    &= (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}} \sum_{i} x_i (\delta_{ik}(r_k)^2 - x_i x_k) \\
    &= 0.
  \end{align*}

\item[(3)]
  On $E_{k-1} \subsetneq E_k$, we write
  \begin{align*}
    & \: d(f_k \omega_{k-1}) \\
    =& \: (df_k) \wedge \omega_{k-1} + (-1)^{0} f_k \wedge \underbrace{(d\omega_{k-1})}_{=0} \\
    =& \: (df_k) \wedge \omega_{k-1} \\
    =& \: \left\{
        \sum_{i=1}^{k} D_i f_k(\mathbf{x}) dx_i
      \right\}
      \wedge \\
      &\left\{
        \frac{1}{(r_{k-1})^{k-1}}
        \sum_{j=1}^{k-1} (-1)^{j-1} x_j dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1}
      \right\} \\
    =& \: \frac{1}{(r_{k-1})^{k-1}}
      \sum_{\substack{1 \leq i \leq k \\ 1 \leq j \leq k-1}}
      (-1)^{j-1} x_j D_i f_k(\mathbf{x})
      dx_i \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \: \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_j f_k(\mathbf{x})
      dx_j \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
      &+
      \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_k f_k(\mathbf{x})
      dx_k \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1}.
  \end{align*}

\item[(4)]
  By (2),
  \begin{align*}
    &\frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_j f_k(\mathbf{x})
      dx_j \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      x_j D_j f_k(\mathbf{x})
      dx_1 \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{1}{(r_{k-1})^{k-1}}
      (-D_k f_k(x) x_k)
      dx_1 \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{-D_k f_k(\mathbf{x})}{(r_{k-1})^{k-1}}
      x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}} \\
    =& \:
    (r_{k})^{-k}
      (-1)^{k-1} x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}}
      & ((1)).
  \end{align*}
  Also,
  \begin{align*}
    &\frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_k f_k(\mathbf{x})
      dx_k \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{(-1)^{k} D_k f_k(\mathbf{x})}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \:
    (r_{k})^{-k}
      \sum_{j=1}^{k-1} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k}
      & ((1)).
  \end{align*}

\item[(5)]
  Hence,
  \begin{align*}
    & \: d(f_k \omega_{k-1}) \\
    =& \: (r_{k})^{-k}
      (-1)^{k-1} x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}} \\
      &+ \: (r_{k})^{-k}
        \sum_{j=1}^{k-1} (-1)^{j-1} x_j
          dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \: (r_{k})^{-k}
      \sum_{j=1}^{k} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \: \omega_{k}.
  \end{align*}
\end{enumerate}
$\Box$ \\



% https://scholar.rose-hulman.edu/cgi/viewcontent.cgi?article=1064&context=rhumj



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  $\omega_n$ is not exact in $E_n$ (though it is locally exact).

\item[(2)]
  Let
  \begin{align*}
    \mathbb{S}^{n-1}
    &= \{ (x_1,\ldots,x_n) \in \mathbb{R}^n : x_1^2+\cdots+x_n^2 = 1 \} \\
    \mathbb{B}^{n}
    &= \{ (x_1,\ldots,x_n) \in \mathbb{R}^n : x_1^2+\cdots+x_n^2 \leq 1 \}.
  \end{align*}
  \emph{It suffices to show that}
  \[
    \int_{\mathbb{S}^{n-1}} \omega_n
    = \frac{n \pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)} \neq 0.
  \]
  Therefore, $\omega_n$ is not exact in $E_n$.

\item[(3)]
  Define
  \[
    \omega = \frac{1}{n} \sum_{i=1}^{n} (-1)^{i-1} x_i
    dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_{n}
  \]
  on $\mathbb{S}^{n-1}$.
  Note that
  \[
    \omega = \frac{1}{n} \omega_n
  \]
  on $\mathbb{S}^{n-1}$ (and that's why we pick $\mathbb{S}^{n-1}$).
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\mathbb{S}^{n-1}} \frac{1}{n} \omega_n
    = \int_{\partial \mathbb{B}^{n}} \omega
    = \int_{\mathbb{B}^{n}} d\omega
    = \int_{\mathbb{B}^{n}} dx_1 \wedge \cdots \wedge dx_n
    = \mathrm{vol}(\mathbb{B}^n),
  \]
  where $\mathrm{vol}(\mathbb{B}^n)$ is the volume of $\mathbb{B}^n$.
  Thus
  \emph{it suffices to show that}
  \[
    \mathrm{vol}(\mathbb{B}^n)
    = \frac{\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)} \neq 0.
  \]
  There are many proofs for this.
  We give a direct integration in spherical coordinates.

\item[(4)]
  Similar to Exercise 10.9.
  The spherical coordinate system has a radial coordinate $r$ and
  angular coordinates $\bm{\varphi} = (\varphi_1, \ldots, \varphi_{n-1})$,
  where the domain of each $\varphi_1, \ldots, \varphi_{n-2}$ is $[0,\pi]$
  and the domain of $\varphi_{n-1}$ is $[0,2\pi]$.
  That is,
  \begin{align*}
    x_1 &= \cos\varphi_1 \\
    x_2 &= \sin\varphi_1 \cos\varphi_2 \\
    x_3 &= \sin\varphi_1 \sin\varphi_2 \cos\varphi_3 \\
    & \cdots \\
    x_{n-1} &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1}.
  \end{align*}
  (It is different from Exercise 10.22.)
  The spherical volume element is
  \[
    r^{n-1} \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
    dr \: d\bm{\varphi}.
  \]
  Thus by Some consequences 8.21,
  \begin{align*}
    \mathrm{vol}(\mathbb{B}^n)
    &= \int_{\mathbb{B}^{n}} d\mathbf{x} \\
    &= \int_{0}^{1} \int_{0}^{\pi} \cdots \int_{0}^{2\pi}
      r^{n-1} \sin^{n-2} \varphi_1 \cdots \sin \varphi_{n-2}
      dr \: d\bm{\varphi} \\
    &= \left( \int_{0}^{1} r^{n-1} dr \right)
      \left(\int_{0}^{\pi} \sin^{n-2} \varphi_1 d\varphi_1 \right)
      \cdots
      \left(\int_{0}^{2\pi} d\varphi_{n-1} \right) \\
    &= \frac{1}{n}
      \cdot \frac{\Gamma(\frac{n-1}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n}{2})}
      \cdot \frac{\Gamma(\frac{n-2}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n-1}{2})}
      \cdots
      \frac{\Gamma(1)\Gamma(\frac{1}{2})}{\Gamma(\frac{3}{2})} \cdot 2\pi \\
    &= \frac{\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)}.
  \end{align*}
  (Use the similar argument in (d)(ii) to get the spherical volume element.)

\item[(5)]
  Note that we can apply the spherical coordinate system
  to $\int_{\mathbb{S}^{n-1}} \omega_n$ directly (without the Stokes' theorem).
  The area element is
  \[
    \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
    d\bm{\varphi}.
  \]
  A long calculation shows that
  \begin{align*}
    &\int_{\mathbb{S}^{n-1}} \omega_n \\
    =& \: \int_{0}^{\pi} \cdots \int_{0}^{2\pi}
      \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
      d\bm{\varphi} \\
    =& \: \left(\int_{0}^{\pi} \sin^{n-2} \varphi_1 d\varphi_1 \right)
      \cdots
      \left(\int_{0}^{2\pi} d\varphi_{n-1} \right) \\
    =& \: \frac{\Gamma(\frac{n-1}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n}{2})}
      \cdot \frac{\Gamma(\frac{n-2}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n-1}{2})}
      \cdots
      \frac{\Gamma(1)\Gamma(\frac{1}{2})}{\Gamma(\frac{3}{2})} \cdot 2\pi \\
    =& \: \frac{n\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)}.
  \end{align*}
  (See (d)(ii) for more details.)
\end{enumerate}
$\Box$ \\




\emph{Outline of (d).}
\begin{enumerate}
\item[(i)]
  One generalization of Exercise 10.21(a) and 10.22(a).
  See Exercise 10.23(a).

\item[(ii)]
  One generalization of Exercise 10.22(b).
  \emph{Let $\Sigma = \mathbb{S}^{n-1}$ be the $(n-1)$-surface in $\mathbb{R}^n$,
  with parameter domain $D = [0,\pi]^{n-2} \times [0,2\pi]$,
  given by
  \begin{align*}
    x_1 &= \cos\varphi_1 \\
    x_2 &= \sin\varphi_1 \cos\varphi_2 \\
    x_3 &= \sin\varphi_1 \sin\varphi_2 \cos\varphi_3 \\
    & \cdots \\
    x_{n-1} &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1}.
  \end{align*}
  Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subseteq D$.
  Prove that
  \begin{align*}
    \int_{S} \omega_n
    &= \int_{E} \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}
      d\bm{\varphi} \\
    &= A(S),
  \end{align*}
  where $A$ denotes surface area.}

\item[(iii)]
  One generalization of Exercise 10.22(c).
  \emph{Suppose $g \in \mathscr{C}''([0,1])$,
  $\mathbf{h} = (h_1, \ldots, h_{n}) \in \mathscr{C}''([0,1]^{n-2})$, and $g > 0$.
  Write
  $\mathbf{x} = (x_1,\ldots,x_n)$ and $\mathbf{s} = (s_1,\ldots,s_{n-2})$.
  Let
  \[
    \mathbf{x}
    = \Phi(\mathbf{s},t)
  \]
  define a $(n-1)$-surface $\Phi$, with parameter domain $[0,1]^{n-1}$, by
  \[
    \mathbf{x} = g(t)\mathbf{h}(\mathbf{s}).
  \]
  Prove that
  \[
    \int_{\Phi} \omega_n = 0.
  \]}

\item[(iv)]
  One generalization of Exercise 10.21(b) and 10.22(d).
  \emph{Let $E$ be a closed cell in $D$,
  with edges parallel to those of $D$.
  Suppose $f \in \mathscr{C}''(D)$, $f > 0$.
  Let $\Omega$ be the $(n-1)$-surface with parameter domain $E$,
  defined by
  \[
    \Omega(\bm{\varphi}) = f(\bm{\varphi}) \Sigma(\bm{\varphi}).
  \]
  Define $S$ as in (ii) and prove that
  \[
    \int_{\Omega} \omega_n = \int_{S} \omega_n = A(S).
  \]}

\item[(v)]
  One generalization of Exercise 10.21(d) and 10.22(e).
  See Exercise 10.23(b).

\item[(vi)]
  One generalization of Examples 10.36 and 10.37.
  See Exercise 10.23(c).

\item[(vii)]
  One generalization of Exercise 10.21(e) and 10.22(f).
  \emph{Derive (iv) from Exercise 10.23(b), without using (iii).}

\item[(viii)]
  One generalization of Exercise 10.21(f).
  $\pi_{n-1}(\mathbb{S}^{n-1}) = \mathbb{Z}$ (without proof).

\item[(ix)]
  One generalization of Exercise 10.22(g).
  \emph{Show that $\omega_n$ is exact in the complement of every line $L$ passing
  through the origin.} \\
\end{enumerate}



\emph{Proof of (d)(ii).}
\begin{enumerate}
\item[(1)]
  On $S \subseteq \mathbb{S}^{n-1}$, we have
    \begin{align*}
    \int_{S} \omega_n
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i(\bm{\varphi})
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i(\bm{\varphi})
      \det
      \begin{bmatrix}
        \frac{\partial x_1}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
        \vdots & \ddots & \vdots \\
        \widehat{\frac{\partial x_i}{\partial \varphi_1}}
          & \cdots
          & \widehat{\frac{\partial x_i}{\partial \varphi_{n-1}}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial x_n}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_n}{\partial \varphi_{n-1}}
      \end{bmatrix}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{S}
      \det
      \underbrace{\begin{bmatrix}
        x_1
          & \frac{\partial x_1}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_i
          & \frac{\partial x_i}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_i}{\partial \varphi_{n-1}} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_n & \frac{\partial x_n}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_n}{\partial \varphi_{n-1}}
      \end{bmatrix}}_{\text{say $A_n$}}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1}.
  \end{align*}
  Hence, it suffices to show that
  \[
    \det(A_n)
    = \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}.
  \]



\item[(2)]
  \emph{Show that $\det(A_n)= \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}$.}
  Induction on $n$.
  \begin{enumerate}
  \item[(a)]
    When $n = 3$, a straightforward computation shows that the determinant is
    \begin{align*}
      &\det(A_3) \\
      =& \: \det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2
          & \cos\varphi_1\sin\varphi_2
          & \sin\varphi_1\cos\varphi_2
      \end{bmatrix} \\
      =& \: \sin\varphi_1.
    \end{align*}

  \item[(b)]
    When $n = 4$,
    \begin{align*}
      &\det(A_4) \\
      =& \: \det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2
          & 0 \\
        \sin\varphi_1\sin\varphi_2\cos\varphi_3
          & \cos\varphi_1\sin\varphi_2\cos\varphi_3
          & \sin\varphi_1\cos\varphi_2\cos\varphi_3
          & -\sin\varphi_1\sin\varphi_2\sin\varphi_3 \\
        \sin\varphi_1\sin\varphi_2\sin\varphi_3
          & \cos\varphi_1\sin\varphi_2\sin\varphi_3
          & \sin\varphi_1\cos\varphi_2\sin\varphi_3
          & \sin\varphi_1\sin\varphi_2\cos\varphi_3
      \end{bmatrix}.
    \end{align*}
    Expand along the last column to get
    \begin{align*}
      &\det(A_4) \\
      =& \: (-1)^{3+4} (-\sin\varphi_1\sin\varphi_2\sin\varphi_3) \\
      &\det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2\sin\varphi_3
          & \cos\varphi_1\sin\varphi_2\sin\varphi_3
          & \sin\varphi_1\cos\varphi_2\sin\varphi_3
      \end{bmatrix} \\
      &+ (-1)^{4+4} (\sin\varphi_1\sin\varphi_2\cos\varphi_3) \\
      &\det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2\cos\varphi_3
          & \cos\varphi_1\sin\varphi_2\cos\varphi_3
          & \sin\varphi_1\cos\varphi_2\cos\varphi_3
      \end{bmatrix} \\
      =& \: (\sin\varphi_1\sin\varphi_2\sin^2\varphi_3) \det(A_3)
        + (\sin\varphi_1\sin\varphi_2\cos^2\varphi_3) \det(A_3) \\
      =& \: \sin\varphi_1\sin\varphi_2 \det(A_3) \\
      =& \: \sin^2 \varphi_1\sin\varphi_2.
    \end{align*}

  \item[(c)]
    Now for large $n$, as (b) we expand along the last column to get
    \begin{align*}
      &\det(A_n) \\
      =& \: (-1)^{(n-1)+n} (-\sin\varphi_1\cdots\sin\varphi_{n-2}\sin\varphi_{n-1})
        (\sin\varphi_{n-1}\det(A_{n-1})) \\
      &+ (-1)^{n+n} (\sin\varphi_1\cdots\sin\varphi_{n-2}\cos\varphi_{n-1})
        (\cos\varphi_{n-1}\det(A_{n-1})) \\
      =& \: (\sin\varphi_1\cdots\sin\varphi_{n-2}) \det(A_{n-1}) \\
      =& \: (\sin\varphi_1\cdots\sin\varphi_{n-2})
        (\sin^{n-3}\varphi_1 \sin^{n-4}\varphi_2 \cdots \sin\varphi_{n-3}) \\
      =& \: \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}.
    \end{align*}
  \end{enumerate}

\item[(3)]
  (Area elements in $\mathbb{R}^3$ 10.46.)
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in S$.
  Define the vector $\mathbf{N}(\bm{\varphi})$ by
  \[
    \mathbf{N}(\bm{\varphi})
    = \sum_{i=1}^{n}
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})} \mathbf{e}_i.
  \]
  So the area of $S$ is defined by
  \[
    A(S) = \int_{E} \abs{ \mathbf{N}(\bm{\varphi}) } d\bm{\varphi}.
  \]

\item[(4)]
  By the similar proof in (2),
  \begin{align*}
    & \: \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(\varphi_1,\ldots,\varphi_{n-1})} \\
    =& \:
    (-1)^{i-1}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      x_i
  \end{align*}
  if $i = 1, \ldots, n$.
  Since $\sum x_i^2 = 1$ on $S$,
  \[
    \abs{ \mathbf{N}(\bm{\varphi}) }
    = \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}.
  \]
  Thus,
  \begin{align*}
    A(S)
    &=
      \int_{E} \abs{ \mathbf{N}(\bm{\varphi}) } d\bm{\varphi} \\
    &=
    \int_{E}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2} d\bm{\varphi}.
  \end{align*}

\item[(5)]
  Note that we can apply (3) on (2) to get the same conclusion.
  \begin{align*}
    \int_{S} \omega_n
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{E} \sum_{i=1}^{n} (-1)^{i-1} x_i \\
      & \qquad (-1)^{i-1}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      x_i d\bm{\varphi} \\
    =& \:
    \int_{E} \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      d\bm{\varphi}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(iii).}
\begin{enumerate}
\item[(1)]
  Similar to Exercise 10.22(c).
  Assume that $\omega_n$ is well-defined, i.e., $\mathbf{h}(\mathbf{s}) \neq 0$
  for all $\mathbf{s} \in [0,1]^{n-2}$.
  \begin{align*}
    \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(s_1,\ldots,s_{n-2},t)}
    =& \:
    \det
    \begin{bmatrix}
      \frac{\partial x_1}{\partial s_1}
        & \cdots
        & \frac{\partial x_1}{\partial s_{n-2}}
        & \frac{\partial x_1}{\partial t} \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{\frac{\partial x_i}{\partial s_1}}
        & \cdots
        & \widehat{\frac{\partial x_i}{\partial s_{n-2}}}
        & \widehat{\frac{\partial x_i}{\partial t}} \\
      \vdots & \ddots & \vdots & \vdots \\
      \frac{\partial x_1}{\partial s_1}
        & \cdots
        & \frac{\partial x_n}{\partial s_{n-2}}
        & \frac{\partial x_n}{\partial t}
    \end{bmatrix} \\
    =& \:
    \det
    \begin{bmatrix}
      g \frac{\partial h_1}{\partial s_1}
        & \cdots
        & g \frac{\partial h_1}{\partial s_{n-2}}
        & g' h_1 \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{g \frac{\partial h_i}{\partial s_1}}
        & \cdots
        & \widehat{g \frac{\partial h_i}{\partial s_{n-2}}}
        & \widehat{g' h_i} \\
      \vdots & \ddots & \vdots & \vdots \\
      g \frac{\partial h_n}{\partial s_1}
        & \cdots
        & g \frac{\partial h_n}{\partial s_{n-2}}
        & g' h_n
    \end{bmatrix} \\
    =& \:
    g^{n-2} g' \det
    \underbrace{\begin{bmatrix}
      \frac{\partial h_1}{\partial s_1}
        & \cdots
        & \frac{\partial h_1}{\partial s_{n-2}}
        & h_1 \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{\frac{\partial h_i}{\partial s_1}}
        & \cdots
        & \widehat{\frac{\partial h_i}{\partial s_{n-2}}}
        & \widehat{h_i} \\
      \vdots & \ddots & \vdots & \vdots \\
      \frac{\partial h_n}{\partial s_1}
        & \cdots
        & \frac{\partial h_n}{\partial s_{n-2}}
        & h_n
    \end{bmatrix}}_{\text{say $A$}}.
  \end{align*}
\item[(2)]
  So
  \begin{align*}
    \int_{\Phi} \omega_n
    =& \: \int_{[0,1]^{n-1}} \frac{1}{g(t)^{n}|\mathbf{h}(\mathbf{s})|^{n}}
      \sum_{i=1}^{n} (-1)^{i-1} g(t) h_i g(t)^{n-2} g'(t) \det(A) \: d\mathbf{s} \: dt \\
    =& \: \int_{[0,1]^{n-1}} \frac{g'(t)}{g(t)|\mathbf{h}(\mathbf{s})|^{n}}
      \sum_{i=1}^{n} (-1)^{i-1} h_i
      \det
      \begin{bmatrix}
        \frac{\partial h_1}{\partial s_1}
          & \cdots
          & \frac{\partial h_1}{\partial s_{n-2}}
          & h_1 \\
        \vdots & \ddots & \vdots & \vdots \\
        \widehat{\frac{\partial h_i}{\partial s_1}}
          & \cdots
          & \widehat{\frac{\partial h_i}{\partial s_{n-2}}}
          & \widehat{h_i} \\
        \vdots & \ddots & \vdots & \vdots \\
        \frac{\partial h_n}{\partial s_1}
          & \cdots
          & \frac{\partial h_n}{\partial s_{n-2}}
          & h_n
      \end{bmatrix}
      \: d\mathbf{s} \: dt \\
    =& \: \int_{[0,1]^{n-1}} \frac{g'(t)}{g(t)|\mathbf{h}(\mathbf{s})|^{n}}
      \det
      \underbrace{\begin{bmatrix}
        h_1
          & \frac{\partial h_1}{\partial s_1}
          & \cdots
          & \frac{\partial h_1}{\partial s_{n-2}}
          & h_1 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        h_n
          & \frac{\partial h_n}{\partial s_1}
          & \cdots
          & \frac{\partial h_n}{\partial s_{n-2}}
          & h_n
      \end{bmatrix}}_{\text{say $B$}}
      \: d\mathbf{s} \: dt.
  \end{align*}
  Since the first column is the same as the last column in $B$,
  $\det(B) = 0$ (Theorem 9.34(d)).
  Therefore, $\int_{\Phi} \omega_n = \int_{[0,1]^{n-1}} 0 \: d\mathbf{s} \: dt = 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(iv).}
\begin{enumerate}
\item[(1)]
  Consider the $n$-surface $\Psi$ given by
  \[
    \Psi(t,\bm{\varphi}) = [1-t+tf(\bm{\varphi})]\Sigma(\bm{\varphi}),
  \]
  where $\bm{\varphi} \in E \subseteq D$, $0 \leq t \leq 1$.

\item[(2)]
  Write
  \[
    E = [a_1,b_1] \times \cdots \times [a_{n-1},b_{n-1}] \subseteq D.
  \]
  Note that $\Psi(t,\bm{\varphi}) \subseteq \mathbb{R}^n - \{ \mathbf{0} \}$.
  So the boundary of $\Psi$ is
  \[
    \partial \Psi
    = \Psi(0,\bm{\varphi}) - \Psi(1,\bm{\varphi})
      + \sum_{i=1}^{n-1} ( \Psi|_{\varphi_i=a_i} - \Psi|_{\varphi_i=b_i}),
  \]
  where $\Psi|_{\varphi_i=\theta}:
  [a_1,b_1] \times \cdots \times \widehat{[a_i,b_i]} \times \cdots \times [a_{n-1},b_{n-1}]
  \to \Omega$ is a mapping defined by
  \begin{align*}
   \Psi|_{\varphi_i=\theta}(t,\varphi_1,\ldots,\widehat{\varphi_i},\ldots,\varphi_{n-1})
    &= \Psi(t,\varphi_1,\ldots,\varphi_{i-1},\theta,\varphi_{i+1},\ldots,\varphi_{n-1}) \\
    &= \Psi(t,\bm{\varphi} + (\theta-\varphi_i)\mathbf{e}_i).
  \end{align*}

\item[(3)]
  \emph{Show that
  \[
    \int_{\Psi|_{\varphi_1=\theta}} \omega_n = 0
  \]
  for any fixed $\varphi_1 = \theta \in [a_1, b_1]$.}
  Note that $\omega_n$ is well-defined on $\Psi|_{\varphi_1=\theta}$. \\
  Write
  \[
    \Psi|_{\varphi_1=\theta}(t,\widehat{\varphi_1},\varphi_2,\ldots,\varphi_{n-1})
    = \mathbf{x}(t,\widehat{\varphi_1},\varphi_2,\ldots,\varphi_{n-1}).
  \]
  By definition of $\Psi$, we have
  \begin{align*}
    x_1 &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \cos\theta \\
    x_2 &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cos\varphi_2 \\
    & \cdots \\
    x_{n-1} &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cdots \sin\varphi_{n-2} \sin\varphi_{n-1},
  \end{align*}
  where
  $g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
  = 1-t+tf(\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)$.

\item[(4)]
  Note that $r_n = g > 0$.
  Since
  \[
    \frac{\partial x_i}{\partial t}
    = \frac{\partial g}{\partial t} g^{-1} x_i,
  \]
  \begin{align*}
    \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(t,\widehat{\varphi_1}, \varphi_2,\ldots,\varphi_{n-1})}
    =& \: \det
    \begin{bmatrix}
      \frac{\partial x_1}{\partial t}
        & \frac{\partial x_1}{\partial \varphi_2}
        & \cdots
        & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{\frac{\partial x_i}{\partial t}}
        & \widehat{\frac{\partial x_i}{\partial \varphi_2}}
        & \cdots
        & \widehat{\frac{\partial x_i}{\partial \varphi_{n-1}}} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{\partial x_n}{\partial t}
        & \frac{\partial x_n}{\partial \varphi_2}
        & \cdots
        & \frac{\partial x_n}{\partial \varphi_{n-1}}
    \end{bmatrix} \\
    =& \:
    \det
    \begin{bmatrix}
      \frac{\partial g}{\partial t} g^{-1} x_1
        & *
        & \cdots
        & * \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{\frac{\partial g}{\partial t} g^{-1} x_i}
        & \widehat{*}
        & \cdots
        & \widehat{*} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{\partial g}{\partial t} g^{-1} x_n
        & *
        & \cdots
        & *
    \end{bmatrix} \\
    =& \:
    \frac{\partial g}{\partial t} g^{-1}
    \det
    \underbrace{\begin{bmatrix}
      x_1
        & *
        & \cdots
        & * \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{x_i}
        & \widehat{*}
        & \cdots
        & \widehat{*} \\
      \vdots & \vdots & \ddots & \vdots \\
      x_n
        & *
        & \cdots
        & *
    \end{bmatrix}}_{\text{say $A$}}.
  \end{align*}
  So
  \begin{align*}
    \int_{\Psi|_{\varphi_1=\theta}} \omega_n
    =& \: \int_{E} g^{-n}
      \sum_{i=1}^{n} (-1)^{i-1} x_i
      \frac{\partial g}{\partial t} g^{-1}
      \det(A)
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1} \\
    =& \: \int_{E} \frac{\partial g}{\partial t} g^{-n-1}
      \sum_{i=1}^{n} (-1)^{i-1} x_i
      \det
      \begin{bmatrix}
        x_1
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \ddots & \vdots \\
        \widehat{x_i}
          & \widehat{*}
          & \cdots
          & \widehat{*} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_n
          & *
          & \cdots
          & *
      \end{bmatrix}
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1} \\
    =& \: \int_{E} \frac{\partial g}{\partial t} g^{-n-1}
      \det
      \underbrace{\begin{bmatrix}
        x_1
          & x_1
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_i
          & x_i
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_n
          & x_n
          & *
          & \cdots
          & *
      \end{bmatrix}}_{\text{say $B$}}
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1}.
  \end{align*}
  Since the first column is the same as the second column in $B$,
  $\det(B) = 0$ (Theorem 9.34(d)).
  Therefore, $\int_{\Psi|_{\varphi_1=\theta}} \omega_n = 0$.

\item[(5)]
  $\int_{\Psi|_{\varphi_i=\theta}} \omega_n = 0$ is also true for all $i = 1,\ldots,n-1$
  by the same argument in (3)(4).
  Hence,
  \begin{align*}
    0
    =& \: \int_{\Psi} d\omega_n \\
    =& \: \int_{\partial \Psi} \omega_n \\
    =& \: \int_{S} \omega_n - \int_{\Omega} \omega_n
      + \sum_{i=1}^{n-1}
        \left( \int_{\Psi|_{\varphi_i=a_i}} \omega_n
        - \int_{\Psi|_{\varphi_i=b_i}} \omega_n \right) \\
    =& \: \int_{S} \omega_n - \int_{\Omega} \omega_n
  \end{align*}
  by (a) and the Stokes' theorem (Theorem 10.33),
  or
  \[
    \int_{\Omega} \omega_n = \underbrace{\int_{S} \omega_n = A(S)}_{\text{by (d)(ii)}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(vii).}
Similar to Exercise 10.22(f).
\begin{enumerate}
\item[(1)]
  To ensure that $\omega_n$ is well-defined on
  $E_1 \subseteq E_2 \subseteq \cdots \subseteq E_n = \mathbb{R}^n - \{ \mathbf{0} \}$,
  we might assume $0 < \varphi_1 < \pi$.
  It is fine since
  $\int_{\Omega} \omega_n$ and $\int_{S} \omega_n$ is well-defined on any closed rectangle in $D$
  and we can apply the argument in Exercise 6.7 to remove the additional restriction.

\item[(2)]
  By the Stokes' theorem (Theorem 10.33) and (b),
  \[
    \int_{\Omega} \omega_{n} = \int_{\partial\Omega} f_n \omega_{n-1}
    \qquad
    \text{and}
    \qquad
    \int_{S} \omega_{n} = \int_{\partial S} f_n \omega_{n-1}.
  \]
  So it suffices to show that
  \[
    \int_{\partial\Omega} f_n \omega_{n-1} = \int_{\partial S} f_n \omega_{n-1}.
  \]
  So it suffices to show that
  $\left.f_n\right|_{\partial\Omega} = \left.f_n\right|_{\partial S}$
  and $\omega_{n-1}|_{\partial\Omega} = \omega_{n-1}|_{\partial S}$.

\item[(3)]
  \emph{Show that
  $\left.f_n\right|_{\partial\Omega} = \left.f_n\right|_{\partial S}$.}
  For any $\mathbf{x}_{\Omega} \in \partial\Omega$,
  \[
    \mathbf{x}_{\Omega} = f(\bm{\varphi}) \mathbf{x}_{\Sigma}.
  \]
  So
  \begin{align*}
    f_n(\mathbf{x}_{\Omega})
    &= (-1)^n g_n\left(
      \frac{(x_n)_{\Omega}}
        {((x_1)_{\Omega}^2+\cdots+(x_n)_{\Omega}^2)^\frac{1}{2}}\right) \\
    &= (-1)^n g_n\left(
      \frac{f(\bm{\varphi})(x_n)_{\Sigma}}
        {f(\bm{\varphi})((x_1)_{\Sigma}^2+\cdots+(x_n)_{\Sigma}^2)^\frac{1}{2}}\right) \\
    &= (-1)^n g_n\left(
      \frac{(x_n)_{\Sigma}}
        {((x_1)_{\Sigma}^2+\cdots+(x_n)_{\Sigma}^2)^\frac{1}{2}}\right) \\
    &= f_n(\mathbf{x}_{\Sigma}).
  \end{align*}
  (Note that $f > 0$.)

\item[(4)]
  \emph{Show that $\omega_{n-1}|_{\partial\Omega} = \omega_{n-1}|_{\partial S}$.}
  Induction on $n$.
  When $n = 2$ or $n = 3$, it is proved in Exercise 10.22(f).
  Now for large $n-1$, (3) is also true for $n-1$.
  Hence,
  \[
    \left.\omega_{n-1}\right|_{\partial\Omega}
    = \left.d(f_{n-1}\omega_{n-2})\right|_{\partial\Omega}
    = \left.d(f_{n-1}\omega_{n-2})\right|_{\partial S}
    = \left.\omega_{n-1}\right|_{\partial S}.
  \]
  By induction, the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(ix).}
Similar to Exercise 10.22(g).
\begin{enumerate}
\item[(1)]
  Given any line $L$ passing through $\mathbf{0}$,
  say
  \[
    (r \cos\varphi_1, \cdots, \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1})
    \in L
    \subseteq \mathbb{R}^n
  \]
  where $r \in \mathbb{R}^1$
  for some $\bm{\varphi} \in [0,\pi]^{n-2} \times [0,2\pi]$.
  We will show that $\omega_n$ is exact in $U = \mathbb{R}^n - L$.

\item[(2)]
  Linear algebra says that all rotation matrices $T \in SO(n)$ can be obtained from
  \[
    R_i(u) = \begin{bmatrix}
      1 &               &   &      &   &               &   \\
        &        \ddots &   &      &   & \text{\huge0} &   \\
        &               & 1 &      &   &               &   \\
        &               &   & R(u) &   &               &   \\
        &               &   &      & 1 &               &   \\
        & \text{\huge0} &   &      &   &        \ddots &   \\
        &               &   &      &   &               & 1
    \end{bmatrix}
  \]
  using matrix multiplication.
  Here
  \[
    R(u) =
    \begin{bmatrix}
      \cos u & -\sin u \\
      \sin u &  \cos u
    \end{bmatrix}
  \]
  is a $2$-by-$2$ rotation matrix at the $i$th row and $i$th column.
  For example, the rotation
  \[
    T
    = R_1(-\varphi_1) R_2(-\varphi_2) \cdots
      R_{n-2}(-\varphi_{n-2}) R_{n-1}(-\varphi_{n-1})
  \]
  maps $L$ to the $x_n$-axis.
  Similar to Exercise 10.22(g), it suffices to show that $\omega_n$ is invariant under $T = R_1(u)$.

\item[(3)]
  \emph{Show that $\omega_n$ is invariant under $T = R_1(u)$.}
  By
  \[
    T: \mathbf{x} \mapsto (x_1 \cos u - x_2 \sin u, x_1 \sin u + x_2 \cos u, x_3, \ldots, x_n),
  \]
  we have
  \begin{align*}
    r_n &\mapsto r_n \\
    dx_1 &\mapsto \cos u dx_1 - \sin u dx_2 \\
    dx_2 &\mapsto \sin u dx_1 + \cos u dx_2 \\
    dx_3 &\mapsto dx_3 \\
    &\cdots \\
    dx_n &\mapsto dx_n.
  \end{align*}
  So $dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n$ maps to
  \begin{equation*}
    \begin{cases}
      \cos u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
      + \sin u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n
      & \text{if $i = 1$} \\
      -\sin u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
        + \cos u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n
      & \text{if $i = 2$} \\
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n
      & \text{otherwise}.
    \end{cases}
  \end{equation*}
  Thus
  \begin{align*}
    \omega_n
    \mapsto& \:
      (r_n)^{-n} (x_1 \cos u - x_2 \sin u) \\
        &\qquad \left(\cos u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
          + \sin u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n\right) \\
      & + (r_n)^{-n}(x_1 \sin u + x_2 \cos u) \\
        &\qquad \left(-\sin u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
          + \cos u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n\right) \\
      & + (r_n)^{-n}\sum_{i=3}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
      (r_n)^{-n} x_1 \widehat{dx_1} \wedge \cdots \wedge dx_n \\
      & - (r_n)^{-n} x_2 \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n \\
      & + (r_n)^{-n} \sum_{i=3}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
      (r_n)^{-n} \sum_{i=1}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \: \omega_n.
  \end{align*}

\item[(4)]
  Similar to Exercise 10.22(g),
  $\omega_n$ is exact in $\mathbb{R}^n - L$.
  (Or $\omega_n$ is locally exact in $\mathbb{R}^n - \{ \mathbf{0} \}$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.24.}
\addcontentsline{toc}{subsection}{Exercise 10.24.}
\emph{Let $\omega = \sum a_i(\mathbf{x}) dx_i$ be a $1$-form of class $\mathscr{C}''$
in a convex open set $E \subseteq \mathbb{R}^n$.
Assume $d\omega = 0$ and prove that $\omega$ is exact in $E$,
by completing the following outline:} \\

\emph{Fix $\mathbf{p} \in E$.
Define
\[
  f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
  \qquad
  (\mathbf{x} \in E).
\]
Apply Stokes' theorem to affine-oriented $2$-simplexs $[\mathbf{p},\mathbf{x},\mathbf{y}]$ in $E$.
Deduce that
\[
  f(\mathbf{y}) - f(\mathbf{x})
  = \sum_{i=1}^{n}(y_i - x_i) \int_{0}^{1} a_i((1-t)\mathbf{x} + t\mathbf{y}) dt
\]
for $\mathbf{x} \in E$, $\mathbf{y} \in E$.
Hence $(D_i f)(\mathbf{x}) = a_i(\mathbf{x})$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]

\item[(2)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  The affine-oriented $2$-simplexs $\Psi = [\mathbf{p},\mathbf{x},\mathbf{y}]$ is in $E$
  by the convexity of $E$.
  (If $E$ is open but not convex,
  we can show that $\omega = df$ \textbf{\emph{locally}} as the note in Exercise 10.21(a).
  That is why we say that $\omega$ is locally exact.
  The proof is exactly the same.)

\item[(3)]
  Note that
  \[
    \partial \Psi
    = \partial [\mathbf{p},\mathbf{x},\mathbf{y}]
    = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}].
  \]
  The Stokes' theorem (Theorem 10.33) implies that
  \begin{align*}
    \int_{\Psi} d\omega
    = \int_{\partial \Psi} \omega
    &\Longleftrightarrow
    \int_{\Psi} 0
    = \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &\Longleftrightarrow
    0 = \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}) \\
    &\Longleftrightarrow
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega.
  \end{align*}

\item[(4)]
  Define $\gamma: [0,1] \to E$ by
  \begin{align*}
    \gamma(t)
    &= \mathbf{x} + t(\mathbf{y}-\mathbf{x}) \\
    &= \sum_{i=1}^{n} x_i + t(y_i - x_i)
  \end{align*}
  (where $\mathbf{x} = (x_1, \ldots, x_n)$ and $\mathbf{y} = (y_1, \ldots, y_n)$).
  Hence $[0,1]$ is the parameter domain of $[\mathbf{x},\mathbf{y}]$ with respect to $\gamma$.
  So
  \begin{align*}
    \int_{[\mathbf{x},\mathbf{y}]} \omega
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\gamma(t))
      \frac{\partial (x_i + t(y_i - x_i))}{\partial t} dt \\
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x}))(y_i - x_i) dt \\
    &= \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \end{align*}
  Thus,
  \[
    f(\mathbf{y}) - f(\mathbf{x})
    = \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \]

\item[(5)]
  Note that
  \begin{align*}
    f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})
    &= \sum_{i=1}^{n} ((x_i + h\delta_{ij}) - x_i)
      \int_{0}^{1} a_i(\mathbf{x} + t((\mathbf{x} + h \mathbf{e}_j)-\mathbf{x})) dt \\
    &= \sum_{i=1}^{n} h\delta_{ij}
      \int_{0}^{1} a_i(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= h \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt.
  \end{align*}
  (Here $\delta_{ij}$ is the Kronecker delta.)
  So
  \begin{align*}
    (D_j f)(\mathbf{x})
    &= \lim_{h \to 0}
      \frac{f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})}{h} \\
    &= \lim_{h \to 0}
      \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= \int_{0}^{1} a_j(\mathbf{x}) dt
      &(a_j \in \mathscr{C}'') \\
    &= a_j(\mathbf{x}).
  \end{align*}
  Thus,
  \[
    df
    = \sum_{j=1}^{n} (D_j f)(\mathbf{x}) dx_j
    = \sum_{j=1}^{n} a_j(\mathbf{x}) dx_j
    = \omega,
  \]
  or $\omega$ is exact in $E$.
\end{enumerate}
$\Box$ \\

\emph{Note:}
\begin{enumerate}
\item[(1)]
  The first de Rham cohomology $H^{1}_{\mathrm{dR}}(E) = 0$ if $E$ is convex.

\item[(2)]
  $H^{1}_{\mathrm{dR}}(E) = 0$ if $E$ is simply connected.
  (The converse is not true.)

\item[(3)]
  $H^{n-1}_{\mathrm{dR}}(\mathbb{R}^n-\{\mathbf{0}\}) = \mathbb{R}^1$.
  (See Exercise 10.23.) \\\\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.25.}
\addcontentsline{toc}{subsection}{Exercise 10.25.}
\emph{Assume $\omega$ is a $1$-form in an open set $E \subseteq \mathbb{R}^n$
such that
\[
  \int_{\gamma} \omega = 0
\]
for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.
Prove that $\omega$ is exact in $E$,
by imitating part of the argument sketched in Exercise 10.24.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Assume that $E$ is a \textbf{connected} open subset of $\mathbb{R}^n$.
  Show that $\omega$ is exact in $E$
  if $\int_{\gamma} \omega = 0$
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.}

\item[(2)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]
  It is well-defined since $E$ is connected and
  $\int_{\gamma} \omega = 0$ for every closed curve $\gamma$ in $E$.

\item[(3)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  Let
  \[
    \gamma = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}]
  \]
  be a closed curve in $E$.
  Hence,
  \begin{align*}
    0
    &= \int_{\gamma} \omega
      &(\text{Assumption}) \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}).
  \end{align*}
  So
  \begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega
  \end{align*}

\item[(4)]
  Similar to (4)(5) in the proof of Exercise 10.24, we have $df = \omega$.
  So the statement in (1) is proved.
  In general, we can define each $f_{\alpha}$
  on each connected component $E_{\alpha}$ (which is open) of $E$
  such that $d f_{\alpha} = \omega$ on $E_{\alpha}$.
  Take
  \[
    f|_{E_{\alpha}} = f_{\alpha}
  \]
  on $E$.
  Hence, $df = \omega$ on the whole $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.26.}
\addcontentsline{toc}{subsection}{Exercise 10.26.}
\emph{Assume $\omega$ is a $1$-form in $\mathbb{R}^3 - \{\mathbf{0}\}$,
of class $\mathscr{C}'$ and $d\omega = 0$.
Prove that $\omega$ is exact in $\mathbb{R}^3 - \{\mathbf{0}\}$.
(Hint: Every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
Apply Stokes' theorem and Exercise 10.25.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Let $E = \mathbb{R}^3 - \{\mathbf{0}\}$.
  By Exercise 10.25,
  it suffices to show that
  \[
    \int_{\gamma} \omega = 0
  \]
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.

\item[(2)]
  Suppose that every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
  is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
  So there is some $2$-surface $\Psi$ such that $\partial \Psi = \gamma$.
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\gamma} \omega
    = \int_{\partial \Psi} \omega
    = \int_{\Psi} d\omega
    = \int_{\Psi} 0
    = 0.
  \]

\item[(3)]
  By algebraic topology theory,
  $\pi_1(E, x) = 0$ for any $x \in E$. That is, $E$ is simply connected.
  Given any closed curve $\gamma: [0,1] \to E$,
  we take $c \in (0,1)$ such that
  $\gamma(c) \neq \gamma(a) = \gamma(b)$.
  (If such $c$ does not exist, $\gamma$ is a trivial curve
  and thus $\int_{\gamma} \omega = 0$ holds trivially.)
  Consider two curves $\gamma_1:[0,1] \to E$ and $\gamma_2:[0,1] \to E$ defined by
  \begin{align*}
    \gamma_1(x) &= \gamma(cx), \\
    \gamma_2(x) &= \gamma((c-1)x+1).
  \end{align*}
  Thus $\gamma_1(0) = \gamma_2(0)$ and $\gamma_1(1) = \gamma_2(1)$.
  Since $E$ is simply connected, there is a homotopy $H(x,t): E \times [0,1] \to E$
  of $\gamma_1$ and $\gamma_2$ such that
  $H(x,0) = \gamma_1(x)$ and $H(x,1) = \gamma_2(x)$.
  $H$ defines a $2$-surface $\Psi$ such that
  $\partial \Psi = \gamma_1 - \gamma_2 = \gamma$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.27.}
\addcontentsline{toc}{subsection}{Exercise 10.27.}
\emph{Let $E$ be an open $3$-cell in $\mathbb{R}^3$,
with edges parallel to the coordinate axes.
Suppose $(a,b,c) \in E$, $f_i \in \mathscr{C}'(E)$ for $i=1,2,3$,
\[
  \omega
  = f_1 dy \wedge dz + f_2 dz \wedge dx + f_3 dx \wedge dy,
\]
and assume that $d\omega = 0$ in $E$.
Define
\[
  \lambda = g_1 dx + g_2 dy
\]
where
\begin{align*}
  g_1(x,y,z) &= \int_{c}^{z} f_2(x,y,s) ds - \int_{b}^{y} f_3(x,t,c)dt \\
  g_2(x,y,z) &= - \int_{c}^{z} f_1(x,y,s) ds,
\end{align*}
for $(x,y,z) \in E$.
Prove that $d\lambda = \omega$ in $E$.
Evaluate these integrals when $\omega = \zeta$ and thus find the form $\lambda$
that occurs in part (e) of Exercise 10.22.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{F} = f_1 \mathbf{e}_1 + f_2 \mathbf{e}_2 + f_3 \mathbf{e}_3$
  as in Vector fields 10.42.
  Then
  \[
    d\omega = (\nabla \cdot F) dx \wedge dy \wedge dz.
  \]
  As $d\omega = 0$ by assumption, $\nabla \cdot F = D_1 f_1 + D_2 f_2 + D_3 f_3 = 0$.

\item[(2)]
  As
  \begin{align*}
    d\lambda
    =& \: d(g_1 dx + g_2 dy) \\
    =& \: (D_1g_1 dx + D_2g_1 dy + D_3g_1 dz) \wedge dx \\
      &+ (D_1g_2 dx + D_2g_2 dy + D_3g_2 dz) \wedge dy \\
    =& \: (-D_3 g_2) dy \wedge dz
      + (D_3 g_1) dz \wedge dx + (D_1 g_2 - D_2 g_1) dx \wedge dy,
  \end{align*}
  it suffices to show that
  \begin{align*}
    f_1 &= -D_3 g_2, \\
    f_2 &= D_3 g_1, \\
    f_3 &= D_1 g_2 - D_2 g_1
  \end{align*}
  on $E$.

\item[(3)]
  Theorem 6.20 implies that
  \[
    -D_3 g_2 = D_3 \int_{c}^{z} f_1(x,y,s) ds = f_1(x,y,z)
  \]
  and
  \[
    D_3 g_1
    = D_3 \int_{c}^{z} f_2(x,y,s) ds - D_3 \int_{b}^{y} f_3(x,t,c)dt \\
    = f_2(x,y,z).
  \]
  Also,
  \begin{align*}
    & \: D_1 g_2 - D_2 g_1 \\
    =& \: D_1 \left( -\int_{c}^{z} f_1(x,y,s) ds \right) \\
      &- D_2 \left( \int_{c}^{z} f_2(x,y,s) ds - \int_{b}^{y} f_3(x,t,c)dt \right) \\
    =& - \int_{c}^{z} D_1 f_1(x,y,s) ds
      &(f_1 \in \mathscr{C}') \\
      &- \int_{c}^{z} D_2 f_2(x,y,s) ds + f_3(x,y,c)
        &(\text{$f_2 \in \mathscr{C}'$, Theorem 6.20}) \\
    =& \int_{c}^{z} D_3 f_3(x,y,s) ds + f_3(x,y,c)
      &((1)) \\
    =& f_3(x,y,z)
      &(\text{Theorem 6.21}).
  \end{align*}
  Therefore, $d\lambda = \omega$ in $E$.

\item[(4)]
  When $\omega = \zeta = r^{-3} (x dy \wedge dz + y dz \wedge dx + z dx \wedge dy)$,
  we get
  \begin{align*}
    f_1(x,y,z) &= x (x^2+y^2+z^2)^{-\frac{3}{2}}, \\
    f_2(x,y,z) &= y (x^2+y^2+z^2)^{-\frac{3}{2}}, \\
    f_3(x,y,z) &= z (x^2+y^2+z^2)^{-\frac{3}{2}}.
  \end{align*}
  So,
  \begin{align*}
    \int_{c}^{z} f_2(x,y,s) ds
    &= \left[ ys (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z}, \\
    \int_{b}^{y} f_3(x,t,c) dt
    &= \left[ ct (x^2+c^2)^{-1}(x^2+t^2+c^2)^{-\frac{1}{2}} \right]_{t=b}^{t=y}, \\
    \int_{c}^{z} f_1(x,y,s) ds
    &= \left[ xs (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z}.
  \end{align*}
  Hence,
  \begin{align*}
    \lambda
    =& \: g_1 dx + g_2 dy \\
    =& \: \left[ ys (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z} dx \\
      &- \left[ ct (x^2+c^2)^{-1}(x^2+t^2+c^2)^{-\frac{1}{2}} \right]_{t=b}^{t=y} dx \\
      &+ \left[ xs (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z} dy \\
    =& - \left[ zr^{-1} - c(x^2+y^2+c^2)^{-\frac{1}{2}} \right] \eta
      &(\text{Definition of $\eta$}) \\
      &- c(x^2+c^2)^{-1} \left[ y(x^2+y^2+c^2)^{-\frac{1}{2}}
        - b(x^2+b^2+c^2)^{-\frac{1}{2}} \right] dx.
  \end{align*}
  As we pick $(a,b,c) = (a,0,0) \in \mathbb{R}^3 - \{ \mathbf{0} \}$ (or $a \neq 0$),
  we have $\lambda = -zr^{-1} \eta$ such that $d\lambda = \omega = \zeta$,
  which is the same as part (e) in Exercise 10.22.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.28.}
\addcontentsline{toc}{subsection}{Exercise 10.28.}
\emph{Fix $b > a > 0$, define
\[
  \Phi(r,\theta) = (r\cos\theta, r\sin\theta)
\]
for $a \leq r \leq b$, $0 \leq \theta \leq 2\pi$.
(The range of $\Phi$ is an annulus in $\mathbb{R}^2$.)
Put $\omega = x^3 dy$,
and compute both
\[
  \int_{\Phi} d\omega
  \qquad
  \text{and}
  \qquad
  \int_{\partial\Phi} \omega
\]
to verify that they are equal.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    \frac{\partial(x,y)}{\partial(r,\theta)}
    =
    \det
    \begin{bmatrix}
      \cos\theta & -r\sin\theta \\
      \sin\theta & r\cos\theta
    \end{bmatrix}
    = r.
  \]
  So
  \begin{align*}
    \int_{\Phi} d\omega
    &= \int_{\Phi} 3x^2 dx \wedge dy
      &(dy \wedge dy = 0) \\
    &= \int_{[a,b]\times[0,2\pi]} 3(r\cos\theta)^2
      \frac{\partial(x,y)}{\partial(r,\theta)} dr d\theta \\
    &= \int_{a}^{b} \int_{0}^{2\pi} 3 r^3 (\cos\theta)^2 dr d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(2)]
  Similar to Exercise 10.21(b),
  write
  \[
    \partial \Phi = \Gamma - \gamma,
  \]
  where $\Gamma(t) = (b\cos t,b\sin t)$ on $[0,2\pi]$
  and $\gamma(t) = (a\cos t,a\sin t)$ on $[0,2\pi]$.
  Hence
  \begin{align*}
    \int_{\partial\Phi} \omega
    &= \int_{\Gamma} \omega - \int_{\gamma} \omega \\
    &= \int_{\Gamma} x^3 dy - \int_{\gamma} x^3 dy \\
    &= \int_{[0,2\pi]} (b\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta
      - \int_{[0,2\pi]} (a\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta \\
    &= \int_{0}^{2\pi} b^4 (\cos\theta)^4 d\theta
      - \int_{0}^{2\pi} a^4 (\cos\theta)^4 d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(3)]
  \[
    \int_{\Phi} d\omega
    = \int_{\partial\Phi} \omega
    = \frac{3\pi}{4}(b^4-a^4).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.29.}
\addcontentsline{toc}{subsection}{Exercise 10.29.}
\emph{Prove the existence of a function $\alpha$
with the properties needed in the proof of Theorem 10.38,
and prove that the resulting function $F$ is of class $\mathscr{C}'$.
(Both assertions become trivial if $E$ is an open cell or an open ball,
since $\alpha$ can then be taken to be a constant.
Refer to Theorem 9.42.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Suppose $E$ is a convex open set in $\mathbb{R}^n$,
  $p \in \mathbb{Z}$, $1 < p \leq n$.
  Write
  \[
    \mathbf{x} = (
      \underbrace{x_1, \ldots, x_{p-1}}_{= \mathbf{x}'},
      x_p,
      \underbrace{x_{p+1}, \ldots, x_n}_{= \mathbf{x}''}) \in E.
  \]
  Let $V$ be the set of all $(\mathbf{x}', x_p) \in \mathbb{R}^p$
  such that $(\mathbf{x}', x_p, \mathbf{x}'') \in E$ for some $\mathbf{x}''$.
  Let $U$ be the set of all $\mathbf{x}' \in \mathbb{R}^{p-1}$ such that
  $(\mathbf{x}',x_p) \in V$ for some $x_p$.
  Show that there is a function $\alpha \in \mathscr{C}'(U)$
  such that $(\mathbf{x}', \alpha(\mathbf{x}'))$ in V for every $\mathbf{x}' \in U$;
  in other words, the graph of $\alpha$ lies in $V$.}

\item[(2)]
  \emph{Show that (1) holds if $E$ is an open cell.}
  Say
  $E = (a_1,b_1) \times \cdots \times (a_n,b_n)$.
  Thus, $V = (a_1,b_1) \times \cdots \times (a_p,b_p)$
  and $U = (a_1,b_1) \times \cdots \times (a_{p-1},b_{p-1})$.
  Define a function $\alpha \in \mathscr{C}'(U)$
  by $\alpha(\mathbf{x}') = c_p$ for some $c_p \in (a_p,b_p)$.
  Hence,
  \[
    \{ (\mathbf{x}', \alpha(\mathbf{x}')) : \mathbf{x}' \in U \}
    = (a_1,b_1) \times \cdots \times (a_p,b_p) \times \{ c_p \}
    \subseteq V.
  \]

\item[(3)]
  \emph{Show that (1) holds if $E$ is an open ball.}
  Say
  $E = B(\mathbf{a};r) \subseteq \mathbb{R}^n$ for some
  \[
    \mathbf{a} = (
      \underbrace{a_1, \ldots, a_{p-1}}_{= \mathbf{a}'},
      a_p,
      \underbrace{a_{p+1}, \ldots, a_n}_{= \mathbf{a}''}) \in \mathbb{R}^n
  \]
  and $r > 0$.
  Thus, $V = B((\mathbf{a}',a_p);r) \subseteq \mathbb{R}^p$
  and $U = B(\mathbf{a}';r) \subseteq \mathbb{R}^{p-1}$.
  Define a function $\alpha \in \mathscr{C}'(U)$
  by $\alpha(\mathbf{x}') = a_p$.
  Hence,
  \[
    \{ (\mathbf{x}', \alpha(\mathbf{x}')) : \mathbf{x}' \in U \}
    = (a_1,b_1) \times \cdots \times (a_p,b_p) \times \{ a_p \}
    \subseteq V.
  \]

\item[(4)]
  \emph{(Partitions of unity.)
  Suppose $E$ is a subset of $\mathbb{R}^n$,
  and $\{U_{\beta}\}$ is a countable open covering of $E$.
  Then there exist functions $\psi_{\beta} \in \mathscr{C}(\mathbb{R}^n)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_{\beta} \leq 1$ for every $\beta$;}

  \item[(b)]
    \emph{each $\psi_{\beta}$ has its support in some $U_{\beta}$;}

  \item[(c)]
    \emph{each $\mathbf{x} \in E$ has an open neighborhood which intersects
    $\mathrm{supp}(\psi_{\beta})$ for only finitely many $\beta$, and}

  \item[(d)]
    \emph{$\sum_{\beta} \psi_{\beta}(\mathbf{x}) = 1$ for every $\mathbf{x} \in E$.}
  \end{enumerate}
  To prove this,
  we apply the similar argument in Theorem 10.8 and note that $E$ is paracompact.

\item[(6)]
  \emph{Show that (1) holds for arbitrary convex open set $E$ in $\mathbb{R}^n$.}
  \begin{enumerate}
  \item[(a)]
    Let $\{E_{\beta}\}$ is a countable open covering of $E$ in the sense of Exercises 2.22 and 2.23.
    (So that each $E_{\beta} = B(\mathbf{a}_{\beta};r_{\beta}) \subseteq E$.)
    Based on $\{E_{\beta}\}$,
    we can construct an open covering $\{U_{\beta}\}$ of $U$
    such that $U_{\beta} = B(\mathbf{a}_{\beta}';r_{\beta})$
    where $\mathbf{a}_{\beta} = (\mathbf{a}_{\beta}', a_{{\beta},p}, \mathbf{a}_{\beta}'')$
    as in Theorem 10.38.

  \item[(b)]
    Construct a partition of unity $\psi_{\beta} \in \mathscr{C}(U)$ of $U$ by (4).
    For any $\mathbf{x} \in U$,
    there is an open neighborhood $B(\mathbf{x})$ of $\mathbf{x}$ which intersects
    $\mathrm{supp}(\psi_{\beta})$ for only finitely many $\beta$
    and $\sum_{\beta} \psi_{\beta}(\mathbf{x}) = 1$ is a finite sum.

  \item[(c)]
    Define
    \[
      \alpha(\mathbf{x}) = \sum_{\beta} a_{\beta,p} \psi_{\beta}(\mathbf{x}).
    \]
    (See (a) for the definition of $a_{\beta,p}$.)
    $\alpha$ is well-defined and $\alpha \in \mathscr{C}(U)$ (by (b)).
    Write
    \[
      (\mathbf{x}, \alpha(\mathbf{x}))
      = \sum_{\beta} \psi_{\beta}(\mathbf{x})(\mathbf{x}, a_{\beta}).
    \]
    Note that
    $(\mathbf{x}, a_{\beta}) \in B((\mathbf{a}_{\beta}', a_{{\beta},p});r_{\beta}) \subseteq V$.
    Also, $V$ is convex since $E$ is convex.
    Therefore, $(\mathbf{x}, \alpha(\mathbf{x})) \in V$.
  \end{enumerate}

\item[(7)]
  \emph{Show that $F \in \mathscr{C}'$.}
  Since the partial derivatives $(D_j F)(\mathbf{x})$ exist and are continuous on $E$,
  $F \in \mathscr{C}'$ by Theorem 9.21.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.30.}
\addcontentsline{toc}{subsection}{Exercise 10.30.}
\emph{If $\mathbf{N}$ is the vector given by
\[
  \mathbf{N}
  = (\alpha_2 \beta_3 - \alpha_3 \beta_2) \mathbf{e}_1
    + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \mathbf{e}_2
    + (\alpha_1 \beta_2 - \alpha_2 \beta_1) \mathbf{e}_3
\]
(Equation (135)), prove that
\[
  \det\begin{bmatrix}
    \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
    \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
    \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
  \end{bmatrix}
  = \abs{\mathbf{N}}^2
\]
Also, verify
\[
  \mathbf{N} \cdot (T\mathbf{e}_1) = \mathbf{N} \cdot (T\mathbf{e}_2)
\]
(Equation (137)).} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Laplace's expansion along the third column,
    \begin{align*}
    &\det\begin{bmatrix}
      \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
      \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
      \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
    \end{bmatrix} \\
    =& (-1)^{1+3} (\alpha_2 \beta_3 - \alpha_3 \beta_2)
      \det\begin{bmatrix}
        \alpha_2 & \beta_2 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{2+3} (\alpha_3 \beta_1 - \alpha_1 \beta_3)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{3+3} (\alpha_1 \beta_2 - \alpha_2 \beta_1)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_2 & \beta_2
      \end{bmatrix} \\
    =& (\alpha_2 \beta_3 - \alpha_3 \beta_2)^2
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3)^2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)^2 \\
    =& \abs{\mathbf{N}}^2.
  \end{align*}

\item[(2)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_1)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\alpha_1, \alpha_2, \alpha_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \alpha_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \alpha_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \alpha_3 \\
    &= (\alpha_3 \alpha_2 - \alpha_2 \alpha_3) \beta_1
      + (\alpha_1 \alpha_3 - \alpha_3 \alpha_1) \beta_2
      + (\alpha_2 \alpha_1 - \alpha_1 \alpha_2)\beta_3 \\
    &= 0.
  \end{align*}

\item[(3)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_2)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\beta_1, \beta_2, \beta_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \beta_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \beta_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \beta_3 \\
    &= (\beta_2 \beta_3 - \beta_3 \beta_2) \alpha_1
      + (\beta_3 \beta_1 - \beta_1 \beta_3) \alpha_2
      + (\beta_1 \beta_2 - \beta_2 \beta_1) \alpha_3 \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 10.31.}
\addcontentsline{toc}{subsection}{Exercise 10.31.}
\emph{Let $E \subseteq \mathbb{R}^3$ be open,
suppose $g \in \mathscr{C}''(E)$, $h \in \mathscr{C}''(E)$,
and consider the vector field}
\[
    \mathbf{F} = g \nabla h
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  where $\nabla^2 h = \nabla \cdot (\nabla h) = \sum\frac{\partial^2 h}{\partial x_i^2}$
  is the so-called ``Laplacian'' of $h$.}

\item[(b)]
  \emph{If $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$
  (as in Theorem 10.51), prove that
  \[
    \int_{\Omega} [ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  where (as is customary) we have written $\frac{\partial h}{\partial n}$
  in place of $(\nabla h) \cdot \mathbf{n}$.
  (Thus $\frac{\partial h}{\partial n}$ is the directional derivative of $h$
  in the direction of the outward normal to $\partial \Omega$,
  the so-called \textbf{normal derivative} of $h$.)
  Interchange $g$ and $h$,
  substract the resulting formula from the first one, to obtain
  \[
    \int_{\Omega} ( g\nabla^2 h - h \nabla^2 g) dV
    = \int_{\partial \Omega}
      \left( g \frac{\partial h}{\partial n} - h \frac{\partial g}{\partial n} \right) dA.
  \]
  These two formulas are usually called \textbf{Green's identities}.}

\item[(c)]
  \emph{Assume that $h$ is \textbf{harmonic} in $E$;
  this means that $\nabla^2 h = 0$.
  Take $g = 1$ and conclude that
  \[
    \int_{\partial \Omega} \frac{\partial h}{\partial n} dA = 0.
  \]
  Take $g = h$, and conclude that $h = 0$ in $\Omega$ if $h = 0$ on $\partial \Omega$.}

\item[(d)]
  \emph{Show that Green's identities are also valid in $\mathbb{R}^2$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since
  \[
    \mathbf{F}
    = g \nabla h
    = g \left(\sum (D_i h)\mathbf{e}_i \right)
    = \sum g(D_i h)\mathbf{e}_i,
  \]
  we have
  \begin{align*}
    \nabla \cdot \mathbf{F}
    &= \nabla \cdot \left( \sum g(D_i h)\mathbf{e}_i \right) \\
    &= \sum D_i(g(D_i h)) \\
    &= \sum \{ (D_i g)(D_i h) + g D_i(D_i h) \} \\
    &= \sum (D_i g)(D_i h) + g \sum D_i(D_i h).
  \end{align*}

\item[(2)]
  Also,
  \begin{align*}
    g \nabla^2 h + (\nabla g) \cdot (\nabla h)
    &= g \nabla \cdot (\nabla h) + (\nabla g) \cdot (\nabla h) \\
    &= g \nabla \cdot \left(\sum (D_i h)\mathbf{e}_i \right)
      + \left(\sum (D_i g)\mathbf{e}_i \right) \cdot \left(\sum (D_i h)\mathbf{e}_i \right) \\
    &= g \sum D_i(D_i h) + \sum (D_i g)(D_i h).
  \end{align*}

\item[(3)]
  By (1)(2), the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  The divergence theorem (Theorem 10.51) implies that
  \begin{align*}
    &\int_{\Omega} (\nabla \cdot \mathbf{F}) dV
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) dA \\
    \Longrightarrow&
    \int_{\Omega} [g \nabla^2 h + (\nabla g) \cdot (\nabla h)] dV
    = \int_{\partial\Omega} g \underbrace{\nabla h \cdot \mathbf{n}}_{=\frac{\partial h}{\partial n}} dA.
  \end{align*}

\item[(2)]
  Green's identities are a set of three identities in vector calculus
  relating the bulk with the boundary of a region on which differential operators act.
  \emph{(Green's third identity.)
  Assume that $h$ is harmonic in $E$.
  If $G(\mathbf{x},\mathbf{x}_0)$ is the Green's function,
  then}
  \[
    h(\mathbf{x}_0)
    = \int_{\partial \Omega}
      \left[ h(\mathbf{x}) \frac{\partial G(\mathbf{x},\mathbf{x}_0)}{\partial n}
      - G(\mathbf{x},\mathbf{x}_0) \frac{\partial h(\mathbf{x})}{\partial n} \right] dA.
  \]
  For example, in $\mathbb{R}^3$
  \[
    G(\mathbf{x},\mathbf{x}_0) = -\frac{1}{4\pi\norm{\mathbf{x} - \mathbf{x}_0}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
Assume $\nabla^2 h = 0$.
\begin{enumerate}
\item[(1)]
  Take $g = 1$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get the conclusion.
  (Here $\nabla g = \mathbf{0}$ as $g = 1$.)

\item[(2)]
  Assume $h = 0$ on $\partial\Omega$.
  Take $g = h$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get
  \[
    \int_{\Omega} |\nabla h|^2 dV
    = \int_{\partial \Omega} h \frac{\partial h}{\partial n} dA
    = 0
  \]
  (since $h = 0$ on $\partial\Omega$).
  Since $h \in \mathscr{C}'(\Omega)$, Exercise 6.2 implies that
  $|\nabla h|^2 = 0$ on $\Omega$.
  So $D_1 h = D_2 h = D_3 h = 0$ on $\Omega$.
  Since $h \in \mathscr{C}'(\Omega)$, Theorem 9.21 implies that
  $h = 0$ on $\Omega$, or
  $h$ is locally constant in $\Omega$ (Exercise 9.9).
  Note that $h = 0$ globally on $\partial \Omega$,
  and thus $h = 0$ globally on $\Omega$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \emph{(The divergence theorem in $\mathbb{R}^2$.)
  If $\mathbf{F} = F_1 \mathbf{e}_1 + F_2 \mathbf{e}_2$ is a vector field of class
  $\mathscr{C}'$ in an open set $E \subseteq \mathbb{R}^2$,
  and if $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial\Omega$
  then}
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]
  Define a $1$-form by
  \[
    \omega_{\mathbf{F}} = F_1 dy - F_2 dx.
  \]
  So
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F})dx \wedge dy
    = (\nabla \cdot \mathbf{F})dA.
  \]
  Hence the Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\Omega} d\omega_{\mathbf{F}}
    = \int_{\partial\Omega} \omega_{\mathbf{F}}
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]

\item[(2)]
  Note that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  is also true in $\mathbb{R}^2$.
  Similar to (b), two Green's identities are also true in $\mathbb{R}^2$.
  (In $\mathbb{R}^1$, the Green's first identity is
  the integration by parts (Theorem 6.22).)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% http://ms.mcmaster.ca/gabardo/moebius.pdf



\subsection*{Exercise 10.32 (M\"obius band).}
\addcontentsline{toc}{subsection}{Exercise 10.32 (M\"obius band).}
\emph{Fix $\delta$, $0 < \delta < 1$.
Let $D$ be the set of all $(\theta,t) \in \mathbb{R}^2$ such that $0 \leq \theta \leq \pi$,
$-\delta \leq t \leq \delta$.
Let $\Phi$ be the $2$-surface in $\mathbb{R}^3$, with parameter domain $D$, given by
\begin{align*}
  x &= (1-t\sin\theta) \cos(2\theta) \\
  y &= (1-t\sin\theta) \sin(2\theta) \\
  z &= t \cos\theta
\end{align*}
where $(x,y,z) = \Phi(\theta,t)$.
Note that $\Phi(\pi,t) = \Phi(0,-t)$, and that $\Phi$ is one-to-one on the rest of $D$.} \\

\emph{The range $M = \Phi(D)$ of $\Phi$ is known as a \textbf{M\"obius band}.
It is the simplest example of a nonorientable surface.} \\

\emph{Prove the various assertions made in the following description:
Put
$\mathbf{p}_1 = (0,-\delta)$,
$\mathbf{p}_2 = (\pi,-\delta)$,
$\mathbf{p}_3 = (\pi,\delta)$,
$\mathbf{p}_4 = (0,\delta)$,
$\mathbf{p}_5 = \mathbf{p}_1$.
Put $\gamma_i = [\mathbf{p}_i,\mathbf{p}_{i+1}]$, $i=1,\ldots,4$,
and put $\Gamma_i = \Phi \circ \gamma_i$.
Then
\[
  \partial \Phi = \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4.
\]
Put $\mathbf{a} = (1,0,-\delta)$, $\mathbf{b} = (1,0,\delta)$.
Then
\[
  \Phi(\mathbf{p}_1) = \Phi(\mathbf{p}_3) = \mathbf{a},
  \qquad
  \Phi(\mathbf{p}_2) = \Phi(\mathbf{p}_4) = \mathbf{b},
\]
and $\partial \Phi$ can be described as follows.}
\begin{enumerate}
\item[(1)]
  \emph{$\Gamma_1$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $+1$ around the origin.
  (See Exercise 8.23.)}

\item[(2)]
  \emph{$\Gamma_2 = [\mathbf{b}, \mathbf{a}]$.}

\item[(3)]
  \emph{$\Gamma_3$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $-1$ around the origin.}

\item[(4)]
  \emph{$\Gamma_4 = [\mathbf{b}, \mathbf{a}]$.}
\end{enumerate}
\emph{Thus $\partial \Phi = \Gamma_1 + \Gamma_3 + 2 \Gamma_2.$} \\

\emph{If we go from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_1$
and continue along the ``edge'' of $M$ until we return to $\mathbf{a}$,
the curve traced out is
\[
  \Gamma = \Gamma_1 - \Gamma_3,
\]
which may also be represented on the parameter interval $[0,2\pi]$ by the equations
\begin{align*}
  x &= (1+\delta\sin\theta) \cos(2\theta) \\
  y &= (1+\delta\sin\theta) \sin(2\theta) \\
  z &= -\delta\cos\theta.
\end{align*}
It should be emphasized that $\Gamma \neq \partial \Phi$:
Let $\eta = \frac{xdy-ydx}{x^2+y^2}$
be the $1$-form discussed in Exercise 10.21 and Exercise 10.22.
Since $d\eta = 0$, Stokes' theorem shows that
\[
  \int_{\partial \Phi} \eta = 0.
\]
But although $\Gamma$ is the ``geometric'' boundary of $M$, we have
\[
  \int_{\Gamma} \eta = 4 \pi.
\]
In order to avoid this possible source of confusion,
Stokes' formula (Theorem 10.50) is frequently stated only for orientable surfaces $\Phi$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\partial \Phi = \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4$.}
  \begin{align*}
    \partial \Phi
    &= \Phi \circ (\partial D) \\
    &= \Phi \circ (\gamma_1 + \gamma_2 + \gamma_3 + \gamma_4) \\
    &= \Phi \circ \gamma_1 + \Phi \circ \gamma_2 + \Phi \circ \gamma_3 + \Phi \circ \gamma_4 \\
    &= \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4.
  \end{align*}

\item[(2)]
  It is trivial that
  $\Phi(\mathbf{p}_1) = \Phi(\mathbf{p}_3) = \mathbf{a} = (1,0,-\delta)$
  and
  $\Phi(\mathbf{p}_2) = \Phi(\mathbf{p}_4) = \mathbf{b} = (1,0,\delta)$
  by the definition of $\Phi$.

\item[(3)]
  \emph{Show that $\Gamma_1$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $+1$ around the origin.}
  By definition,
  $\Gamma_1 = \Phi \circ \gamma_1 = \Phi([\mathbf{p}_1, \mathbf{p}_2])$.
  That is,
  $\Gamma_1$ spirals up from $\Phi(\mathbf{p}_1) = \mathbf{a}$
  to $\Phi(\mathbf{p}_2) = \mathbf{b}$.
  Besides, the projection $P_{\Gamma_1}$ of $\Gamma_1$ into the $(x,y)$-plane ($z = 0$)
  can be parameterized as
  \begin{align*}
    x &= \left(1+\delta\sin\frac{t}{2}\right) \cos t \\
    y &= \left(1+\delta\sin\frac{t}{2}\right) \sin t
  \end{align*}
  for $0 \leq t \leq 2\pi$.
  Note that $P_{\Gamma_1}$ satisfies the condition in Exercise 10.21(b).
  Hence $\int_{P_{\Gamma_1}} \eta = 2\pi$.
  (Here $\eta$ is well-defined.)
  Apply Exercise 10.21(f) to get
  \[
    \mathrm{Ind}(P_{\Gamma_1})
    = \frac{1}{2\pi} \int_{P_{\Gamma_1}} \eta
    = \frac{1}{2\pi} \cdot 2\pi
    = 1.
  \]

\item[(4)]
  \emph{Show that $\Gamma_2 = [\mathbf{b}, \mathbf{a}]$.}
  By definition,
  $\Gamma_2 = \Phi \circ \gamma_2 = \Phi([\mathbf{p}_2, \mathbf{p}_3])$
  is $[\mathbf{b}, \mathbf{a}]$ exactly.

\item[(5)]
  \emph{Show that $\Gamma_3$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $-1$ around the origin.}
  Similar to (3),
  $\Gamma_3$ spirals up from $\Phi(\mathbf{p}_3) = \mathbf{a}$
  to $\Phi(\mathbf{p}_4) = \mathbf{b}$.
  Now we consider $-\Gamma_3$ instead of $\Gamma_3$.
  The projection $P_{-\Gamma_3}$ of $-\Gamma_3$ into the $(x,y)$-plane ($z = 0$)
  can be parameterized as
  \begin{align*}
    x &= \left(1-\delta\sin\frac{t}{2}\right) \cos t \\
    y &= \left(1-\delta\sin\frac{t}{2}\right) \sin t
  \end{align*}
  for $0 \leq t \leq 2\pi$.
  Similar to (3), $\mathrm{Ind}(P_{-\Gamma_3}) = 1$. Therefore,
  \[
    \mathrm{Ind}(P_{\Gamma_3})
    = -\mathrm{Ind}(-P_{\Gamma_3})
    = -\mathrm{Ind}(P_{-\Gamma_3})
    = -1.
  \]

\item[(6)]
  \emph{Show that $\Gamma_4 = [\mathbf{b}, \mathbf{a}]$.}
  Similar to (4).

\item[(7)]
  \emph{Show that $\Gamma = \Gamma_1 - \Gamma_3$
  is the trace of from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_1$
  and continue along the ``edge'' of $M$ until we return to $\mathbf{a}$.}
  By definition, $\Gamma$ can be parameterized as
  \begin{align*}
    x &= (1+\delta\sin t) \cos(2t) \\
    y &= (1+\delta\sin t) \sin(2t) \\
    z &= -\delta \cos t
  \end{align*}
  for $t \in [0,2\pi]$.
  Thus, $\Gamma$ is $\Gamma_1$ if $t \in [0,\pi]$
  and $\Gamma$ is $-\Gamma_3$ if $t \in [\pi,2\pi]$ by (3)(5).
  So $\Gamma = \Gamma_1 - \Gamma_3$.

\item[(8)]
  \emph{Show that $\int_{\partial \Phi} \eta = 0$.}
  Note that $\eta$ is well-defined since $M$ does not intersect the $z$-axis.
  So the Stokes' theorem (Theorem 10.33) and $d\eta = 0$ on $M$ implies that
  \[
    \int_{\partial \Phi} \eta
    = \int_{\Phi} d\eta
    = 0.
  \]

\item[(9)]
  \emph{Show that $\int_{\Gamma} \eta = 4 \pi$.}
  \begin{align*}
    \int_{\Gamma} \eta
    &= \int_{\Gamma} \frac{xdy-ydx}{x^2+y^2} \\
    &= \int_{0}^{2\pi} \frac{x(t)y'(t) - y(t)x'(t)}{x(t)^2+y(t)^2} dt
      &((7)) \\
    &= \int_{0}^{2\pi} 2 \: dt \\
    &= 4 \pi.
  \end{align*}
  (So the winding number of $\Gamma$ around of $\mathbf{0}$ is $2$.)

\item[(10)]
  By (8)(9), $\Gamma \neq \partial \Phi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section*{Chapter 11: The Lebesuge Theory \\}
\addcontentsline{toc}{section}{Chapter 11: The Lebesuge Theory}



\subsection*{Exercise 11.1.}
\addcontentsline{toc}{subsection}{Exercise 11.1.}
\emph{If $f \geq 0$ and $\int_{E} f d\mu = 0$,
prove that $f(x) = 0$ almost everywhere on $E$.
(Hint: Let $E_n$ be the subset of $E$ on which $f(x) > \frac{1}{n}$.
Write $A = \bigcup E_n$.
Then $\mu(A) = 0$ if and only if $\mu(E_n) = 0$ for every $n$.)} \\

Might assume that $f$ is measurable on $E$. \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
Define $A = \{ x \in E : f(x) > 0 \}$.
So $f(x) = 0$ almost everywhere on $E$ if and only if $\mu(A) = 0$.

\item[(2)]
Define
\[
  E_n = \left\{ x \in E : f(x) > \frac{1}{n} \right\}
\]
for $n = 1,2,3,\ldots$.
Note that $E_1 \subseteq E_2 \subseteq E_3 \subseteq \cdots$ and
\[
  A = \bigcup_{n=1}^{\infty} E_n.
\]
Since $\mu$ is a measure,
\[
  \lim_{n \to \infty} \mu(E_n) = \mu(A)
\]
(Theorem 11.3).

\item[(3)]
(Reductio ad absurdum)
If $\mu(A)> 0$, there is an integer $N$ such that
$\mu(E_n) \geq \frac{\mu(A)}{2}$ whenever $n \geq N$ (by (2)).
In particular, take $n = N$ to get
\begin{align*}
  \int_E f d\mu
  &\geq \int_{E_N} f d\mu
    &\text{($\mu$ is a measure and $E_N \subseteq E$)}\\
  &\geq \frac{1}{N} \cdot \mu(E_N)
    &\text{(Remarks 11.23(b))} \\
  &\geq \frac{1}{N} \cdot \frac{\mu(A)}{2} \\
  & > 0,
\end{align*}
contrary to the assumption that $\int_{E} f d\mu = 0$.
\end{enumerate}
$\Box$ \\

\emph{Note.}
Compare with Exercise 6.2. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.2.}
\addcontentsline{toc}{subsection}{Exercise 11.2.}
\emph{If $\int_A f d\mu = 0$ for every measurable subset $A$ of a measurable set $E$,
then $f(x) = 0$ almost everywhere on $E$.} \\

Might assume that $f$ is measurable on $E$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define
  \[
    A = \{ x \in E : f(x) \geq 0 \}
    \qquad \text{ and } \qquad
    B = \{ x \in E : f(x) \leq 0 \}.
  \]
  $A$ and $B$ are measurable subsets of a measurable set $E$ since $f$ is measurable.

\item[(2)]
  Apply Exercise 11.1 to the fact that $f \geq 0$ on $A$ (by construction)
  and $\int_A f d\mu = 0$ (by assumption),
  we have $f(x) = 0$ almost everywhere on $A$.

\item[(3)]
  Similarly,
  apply Exercise 11.1 to the fact that $-f \geq 0$ on $B$
  and $\int_B (-f) d\mu = -\int_B f d\mu = 0$,
  we have $f(x) = 0$ almost everywhere on $B$.

\item[(4)]
  As $E = A \cup B$, $f(x) = 0$ almost everywhere on $E$ by (2)(3).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.3.}
\addcontentsline{toc}{subsection}{Exercise 11.3.}
\emph{If $\{f_n\}$ is a sequence of measurable functions,
prove that the set of points $x$ at which $\{f_n(x)\}$ converges is measurable.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  It suffices to show that
  \[
    E
    = \{ x : \{ f_n(x) \} \text{ is convergent } \}
    = \{ x : \{ f_n(x) \} \text{ is Cauchy } \}
  \]
  is measurable (since $\mathbb{R}^1$ is complete).

\item[(2)]
  Write
  \[
    E
    =
    \bigcap_{k=1}^{\infty}
    \bigcup_{N=1}^{\infty}
    \bigcap_{n,m \geq N} \left\{ x : |f_n(x)-f_m(x)| \leq \frac{1}{k} \right\}
  \]
  Since $\{f_n\}$ is a sequence of measurable functions,
  $x \mapsto |f_n(x)-f_m(x)|$ is measurable
  (Theorem 11.16 and Theorem 11.18).
  Hence
  \[
    \left\{ x : |f_n(x)-f_m(x)| \leq \frac{1}{k} \right\}
  \]
  is measurable (Theorem 11.15).
  Therefore $E$ is measurable.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.4.}
\addcontentsline{toc}{subsection}{Exercise 11.4.}
\emph{If $f \in \mathscr{L}(\mu)$ on $E$ and $g$ is bounded and measurable on $E$,
then $fg \in \mathscr{L}(\mu)$ on $E$.} \\

\emph{Proof (Theorem 11.27).}
\begin{enumerate}
\item[(1)]
  $fg$ is measurable since both $f$ and $g$ are measurable (Theorem 11.18).

\item[(2)]
  $|g| \leq M$ for some real $M \in \mathbb{R}^1$ by the boundedness of $g$.
  Hence
  \[
    |fg| \leq M|f|
  \]
  on $E$.

\item[(3)]
  To apply Theorem 11.27, it suffices to show that
  $M|f| \in \mathscr{L}(\mu)$ on $E$.
  Theorem 11.26 implies that $|f| \in \mathscr{L}(\mu)$ if $f \in \mathscr{L}(\mu)$.
  And Remarks 11.23(d) implies that $M|f| \in \mathscr{L}(\mu)$ if $|f| \in \mathscr{L}(\mu)$.
\end{enumerate}
$\Box$ \\

\emph{Note} (Riemann integral).
  \emph{If $f \in \mathscr{R}$ on $[a,b]$ and $g$ is bounded and measurable on $[a,b]$,
  then $fg$ might be not Riemann integrable.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.5.}
\addcontentsline{toc}{subsection}{Exercise 11.5.}
\emph{Put
  \begin{equation*}
    g(x) =
    \begin{cases}
      0 & (0 \leq x \leq \frac{1}{2}), \\
      1 & (\frac{1}{2} < x \leq 1),
    \end{cases}
  \end{equation*}
  and
  \begin{align*}
    f_{2k}(x) &= g(x) &(0 \leq x \leq 1), \\
    f_{2k+1}(x) &= g(1-x) &(0 \leq x \leq 1).
  \end{align*}
Show that
\[
  \liminf_{n \to \infty} f_n(x) = 0
  \qquad
  (0 \leq x \leq 1),
\]
but
\[
  \int_{0}^{1} f_n(x)dx = \frac{1}{2}.
\]
(Compare with the Fatou's theorem.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\liminf_{n \to \infty} f_n(x) = 0$.}
  Note that
  \begin{equation*}
    g(1-x) =
    \begin{cases}
      1 & (0 \leq x < \frac{1}{2}), \\
      0 & (\frac{1}{2} < x \leq 1).
    \end{cases}
  \end{equation*}
  Since $f_n(x) \geq 0$ by definition,
  $\liminf_{n \to \infty} f_n(x) \geq 0$.
  Since $f_{2k}(0) = f_{2k+1}(1) = 0$ for all positive integers $k$,
  $\liminf_{n \to \infty} f_n(x) \leq 0$.
  Therefore the result is established.

\item[(2)]
  \emph{Show that $\int_{0}^{1} f_n(x)dx = \frac{1}{2}$.}
  Since
  \begin{align*}
    \int_{0}^{1} f_{2k}(x)dx
    &= \int_{0}^{1} g(x)dx = \frac{1}{2}, \\
    \int_{0}^{1} f_{2k+1}(x)dx
    &= \int_{0}^{1} g(1-x)dx = \frac{1}{2},
  \end{align*}
  in any case $\int_{0}^{1} f_n(x)dx = \frac{1}{2}$ for all positive integers $n$.

\item[(3)]
  This example shows that we may have the strict inequality in the Fatou's theorem.
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Similar exercise).}
  \emph{Consider the sequence $\{f_n\}$ defined by $f_n(x) = 1$ if $n \leq x < n+1$,
  with $f_n(x) = 0$ otherwise.
  Show that we may have the strict inequality in the Fatou's theorem.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.6.}
\addcontentsline{toc}{subsection}{Exercise 11.6.}
\emph{Let
\begin{equation*}
f_n(x) =
  \begin{cases}
    \frac{1}{n}
      & (|x| \leq n), \\
    0
      & (|x| > n).
  \end{cases}
\end{equation*}
Then $f_n(x) \to 0$ uniformly on $\mathbb{R}^1$,
but
\[
  \int_{-\infty}^{\infty} f_n(x) dx = 2
  \qquad
  (n = 1,2,3,\ldots).
\]
(We write $\int_{-\infty}^{\infty}$ in place of $\int_{\mathbb{R}^1}$.)
Thus uniform convergence does not imply dominated convergence
in the sense of Theorem 11.32.
However, on sets of finite measure,
uniformly convergent sequences of bounded functions do satisfy Theorem 11.32.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $f_n(x) \to 0$ uniformly on $\mathbb{R}^1$.}
  Given any $\varepsilon > 0$, there is an integer $N > \frac{1}{\varepsilon}$
  such that
  \[
    |f_n(x) - 0| \leq \frac{1}{n} \leq \frac{1}{N} < \varepsilon
  \]
  whenever $n \geq N$ and $x \in \mathbb{R}^1$.
  Hence $f_n(x) \to 0$ uniformly.

\item[(2)]
  \emph{Show that $\int_{-\infty}^{\infty} f_n(x) dx = 2$.}
  \[
    \int_{-\infty}^{\infty} f_n(x) dx
    = \int_{-n}^{n} \frac{1}{n} dx
    = 2.
  \]

\item[(3)]
  By (1)(2),
  \[
    \lim_{n \to \infty} \int_{-\infty}^{\infty} f_n(x) dx
    \neq \int_{-\infty}^{\infty} \lim_{n \to \infty} f_n(x) dx
  \]
  suggests that the Lebesgue's dominated convergence theorem (Theorem 11.32)
  does not hold in this case.
  In fact,
  if there were $g \in \mathscr{L}$ such that $|f_n(x)| \leq g(x)$,
  then
  \begin{align*}
    \int_{-\infty}^{\infty} g(x) dx
    &\geq \int_{0}^{\infty} g(x) dx
      &(\text{Theorem 11.24}) \\
    &= \sum_{n=1}^{\infty} \int_{n-1}^{n} g(x) dx
      &(\text{Theorem 11.24}) \\
    &\geq \sum_{n=1}^{\infty} \int_{n-1}^{n} |f_n(x)| dx \\
    &= \sum_{n=1}^{\infty} \int_{n-1}^{n} \frac{1}{n} dx \\
    &= \sum_{n=1}^{\infty} \frac{1}{n} \\
    &= \infty,
  \end{align*}
  which is absurd.

\item[(4)]
  \emph{Show that on sets of finite measure,
  uniformly convergent sequences of bounded functions $\{f_n\}$
  do satisfy Theorem 11.32.}
  \begin{enumerate}
  \item[(a)]
    Since $\{f_n\}$ is uniformly convergent,
    $\{f_n\}$ is uniformly bounded (Exercise 7.1), or
    there exists a real number $M$ such that
    \[
      |f_n(x)| \leq M
    \]
    for all positive integer $n$ and $x \in E$.

  \item[(b)]
    Define $g(x) = M$ on $E$.
    It is clear that
    \[
      \int_E g(x) dx = M \mu(E) < +\infty.
    \]
    Now we can apply the Lebesgue's dominated convergence theorem (Theorem 11.32)
    to get
    \[
      \lim_{n \to \infty} \int_E f_n d\mu = \int_E \lim_{n \to \infty} f_n d\mu.
    \]
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.7.}
\addcontentsline{toc}{subsection}{Exercise 11.7.}
\emph{Find a necessary and sufficient condition that $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
(Hint: Consider Example 11.6(b) and Theorem 11.33.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Defines the regular measure $\mu$ by
  \begin{align*}
    \mu([a,b)) &= \alpha(b-) - \alpha(a-) \\
    \mu([a,b]) &= \alpha(b+) - \alpha(a-) \\
    \mu((a,b]) &= \alpha(b+) - \alpha(a+) \\
    \mu((a,b)) &= \alpha(b-) - \alpha(a+)
  \end{align*}
  where $\alpha: \mathbb{R}^1 \to \mathbb{R}^1$ might be defined by
  \begin{equation*}
    \alpha(x) =
      \begin{cases}
        \alpha(a) & \text{ if $x < a$}, \\
        \alpha(x) & \text{ if $a \leq x \leq b$}, \\
        \alpha(b) & \text{ if $x > b$}.
      \end{cases}
  \end{equation*}
  (Example 11.6(b)).

\item[(2)]
  \emph{Suppose $f$ is bounded on $[a,b]$.
  Then $f \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if
  $f$ and $\alpha$ satisfy both properties (I) and (II) below.}
  \begin{enumerate}
  \item[(I)]
    \emph{$f$ and $\alpha$
    cannot be both left-discontinuous, or both right-discontinuous at same point.}
  \item[(II)]
    \emph{$f$ is continuous almost everywhere with respect to $\mu$ on $[a,b] - D_{\alpha}$
    where $D_{\alpha}$ is the set of discontinuities of $\alpha$ on $[a,b]$.}
  \end{enumerate}

\item[(3)]
  Similar to Theorem 11.33.
  By Definition 6.2 and Theorem 6.4 there is a sequence $\{P_k\}$ of partitions of $[a,b]$,
  such that $P_{k+1}$ is a refinement of $P_k$,
  such that the distance between adjacent points of $P_k$ is less than $\frac{1}{k}$,
  and such that
  \[
    \lim_{k \to \infty} L(P_k,f,\alpha) = \mathscr{R}\lowint f d\alpha,
    \qquad
    \lim_{k \to \infty} L(P_k,f,\alpha) = \mathscr{R}\upint f d\alpha.
  \]
  (In this proof, all integrals are taken over $[a,b]$.)

\item[(4)]
  If $P_k = \{ x_0, x_1, \ldots, x_n \}$, with $x_0 = a$, $x_n = b$, define
  \[
    L_k(a) = U_k(a) = f(a);
  \]
  put $U_k(x) = M_i$ and $L_k(x) = m_i$ for $x_{i-1} < x \leq x_{i}$, $1 \leq i \leq n$,
  using the notation introduced in Definition 6.1.
  Then
  \[
    L(P_k,f,\alpha) = \int L_k d\mu,
    \qquad
    U(P_k,f,\alpha) = \int U_k d\mu
  \]
  and
  \[
    L_1(x) \leq L_2(x) \leq \cdots \leq f(x) \leq \cdots \leq U_2(x) \leq U_1(x)
  \]
  for all $x \in [a,b]$, since $P_{k+1}$ refines $P_k$.

\item[(5)]
  So there exist
  \[
    L(x) = \lim_{k \to \infty} L_k(x),
    \qquad
    U(x) = \lim_{k \to \infty} U_k(x).
  \]
  Observe that $L$ and $U$ are bounded $\mu$-measurable function on $[a,b]$,
  that
  \[
    L(x) \leq f(x) \leq U(x)
    \qquad
    (a \leq x \leq b),
  \]
  and that
  \begin{align*}
    \int L d\mu &= \lim \int L_k d\mu = \lim L(P_k,f,\alpha) = \mathscr{R}\lowint f d\alpha, \\
    \int U d\mu &= \lim \int U_k d\mu = \lim U(P_k,f,\alpha) = \mathscr{R}\upint f d\alpha
  \end{align*}
  by the Lebesgue's monotone convergence theorem (Theorem 11.28).

\item[(6)]
  So $f \in \mathscr{R}(\alpha)$ on $[a,b]$
  if and only if
  $\int L d\mu = \int U d\mu$
  if and only if
  \begin{enumerate}
  \item[(III)]
    $L(x) = U(x)$ almost everywhere with respect to $\mu$
  \end{enumerate}
  by Exercise 11.1 and the fact that $U(x) - L(x) \geq 0$.

\item[(7)]
  \emph{Show that $f \in \mathscr{R}(\alpha)$ on $[a,b]$ implies the property (I).}
  It is independent of the Lebesgue theory.
  \begin{enumerate}
  \item[(a)]
    Suppose both $f$ and $\alpha$ are discontinuous from the right at $x = c$;
    that is,
    suppose that there exists an $\varepsilon > 0$ such that for every $\delta > 0$
    there are values of $x, y \in (c,c+\delta) \subseteq [a,b]$ for which
    \[
      |f(x)-f(c)| \geq \sqrt{\varepsilon},
      \qquad
      |\alpha(y)-\alpha(c)| \geq \sqrt{\varepsilon}.
    \]

  \item[(b)]
    Let $P = \{x_0 < \cdots < x_n \}$ be any partition of $[a,b]$ containing $c$,
    say $c = x_{i-1}$ for some $i = 1,\ldots,n$.
    Then
    \begin{align*}
      U(P,f,\alpha) - L(P,f,\alpha)
      &= \sum_{j=1}^{n} (M_j - m_j)(\alpha(x_j) - \alpha(x_{j-1})) \\
      &\geq (M_i - m_i)(\alpha(x_i) - \alpha(x_{i-1})).
    \end{align*}
    Take $\delta = x_i - x_{i-1}$. $x_i = x_{i-1}+\delta = c+\delta$.
    Then
    \[
      \alpha(x_i) - \alpha(x_{i-1})
      = \alpha(c+\delta) - \alpha(c)
      \geq \alpha(y) - \alpha(c)
      \geq \sqrt{\varepsilon}
    \]
    (by the monotonicity of $\alpha$).
    Besides,
    \[
      M_i - m_i \geq |f(x)-f(c)| \geq \sqrt{\varepsilon}.
    \]
    Hence,
    \[
      U(P,f,\alpha) - L(P,f,\alpha) \geq \varepsilon.
    \]
    Therefore,
    Theorem 6.6 implies that $f \not\in \mathscr{R}(\alpha)$ on $[a,b]$.

  \item[(c)]
    The argument is similar if $c$ is a common discontinuity from the left.
  \end{enumerate}

\item[(8)]
  \emph{Show that (III) implies (II).}
  \begin{enumerate}
  \item[(a)]
    \emph{Show that $f$ is continuous at $x \in [a,b] - D_{\alpha}$ if
    $U(x) = L(x)$ and $x \not\in \bigcup_{k=1}^{\infty} P_k$.}
    (Reductio ad absurdum)
    If $f$ were not continuous at $x$,
    then there exists an $\varepsilon > 0$ such that
    there is a sequence $\{ y_m \}$ in $[a,b]$
    such that $|y_m - x| < \frac{1}{m}$ but
    \[
      |f(y_m) - f(x)| > \varepsilon
    \]
    for $m = 1,2,3,\ldots$ (Theorem 4.2).

  \item[(b)]
    Given any $P_k$.
    Since $x \not \in P_k$, $x \in (x_{i-1},x_{i})$ for some $i$.
    Since $(x_{i-1},x_{i})$ is open,
    there exists an integer $N_k$ such that $y_m \in (x_{i-1},x_{i})$ whenever $m \geq N_k$.
    Hence,
    \[
      U_k(x) - L_k(x) = M_i - m_i \geq |f(y_m) - f(x)| > \varepsilon.
    \]
    Take the limit to get
    \[
      U(x) - L(x) \geq \varepsilon,
    \]
    which is absurd. Therefore, the statement in (a) is proved.

  \item[(c)]
    Now it suffices to show that both sets
    \begin{align*}
      E_1 &= \left\{ x \in [a,b] - D_{\alpha} : L(x) \neq U(x) \right\} \\
      E_2 &= \left\{ x \in [a,b] - D_{\alpha} : x \in \bigcup_{k=1}^{\infty} P_k \right\}
    \end{align*}
    are $\mu$-measure zero.
    $E_1$ is $\mu$-measure zero by (III).
    $E_2$ is $\mu$-measure zero since $E_2$ is countable and defined on $[a,b] - D_{\alpha}$.
  \end{enumerate}
  Therefore, (II) is established.

\item[(9)]
  \emph{Show that (I)(II) implies (III).}
  Use the notation in (8).
  \begin{enumerate}
  \item[(a)]
    It suffices to show that
    \begin{align*}
      &\{ x \in [a,b] : L(x) \neq U(x) \} \\
      =& \underbrace{\{ x \in [a,b] - D_{\alpha} : L(x) \neq U(x) \}}_{= E_1}
      \bigcup
        \underbrace{\{ x \in D_{\alpha} : L(x) \neq U(x) \}}_{= E_3}
    \end{align*}
    is $\mu$-measure zero.

  \item[(b)]
    Note that $E_2$ is $\mu$-measure zero.
    Hence (II) and (8)(a) imply that $E_1$ is $\mu$-measure zero.
    So it suffices to show that $E_3$ is $\mu$-measure zero.
    In fact, we will show that $E_3 = \varnothing$,
    or $L(x) = U(x)$ whenever $x \in D_{\alpha}$.

  \item[(c)]
    Write
    \[
      D_{\alpha} = \{ y_1, \ldots, y_m, \ldots \}
    \]
    since $D_{\alpha}$ is at most countable (Theorem 4.30).
    (Set $y_m = y_{m+1} = \cdots$ if $D_{\alpha}$ is finite.)
    Define a refinement of $P_k$ by
    \[
      P_k \bigcup \{ y_1, \ldots, y_k \}
    \]
    and use the same symbol $P_k$ to denote this refinement.

  \item[(d)]
    Given any $x \in D_{\alpha}$.
    Suppose $\alpha$ is discontinuous from the right at $x$.
    By the construction in (c),
    there is an integer $N_1$ such that
    $x = x_{i-1}$ is in some subinterval $[x_{i-1},x_{i}]$ of $P_k$
    whenever $k \geq N_1$.

  \item[(e)]
    Note that $f$ is continuous from the right at $x$ by (I).
    Given an $\varepsilon > 0$, there exists a $\delta > 0$ such that
    \[
      |f(y) - f(x)| \leq \frac{\varepsilon}{2}
    \]
    whenever $y \in [x,x+\delta)$.
    So
    \[
      |f(y) - f(z)|
      \leq |f(y) - f(x)| + |f(x) - f(z)|
      \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2}
      = \varepsilon
    \]
    whenever $y, z \in [x,x+\delta)$.

  \item[(f)]
    Take an integer $N_2 > \frac{1}{\delta}$ such that
    for any any subinterval $[x_{i-1},x_{i}]$ of $P_k$ we have
    $[x_{i-1},x_{i}] \subseteq [x,x+\delta)$ whenever $k \geq N_2$.

  \item[(g)]
    Take $N = \max\{ N_1, N_2 \}$.
    As $k \geq N$, $[x_{i-1},x_{i}] \subseteq [x,x+\delta)$ and
    \[
      U_k(x) - L_k(x)
      = M_i - m_i
      = \sup_{y,z \in [x_{i-1},x_{i}]} |f(y) - f(z)|
      \leq \varepsilon.
    \]
    Take the limit to get
    \[
      U(x) - L(x) \leq \varepsilon.
    \]
    Since $\varepsilon$ is arbitrary, $L(x) = U(x)$.

  \item[(h)]
    The argument is similar if $\alpha$ is discontinuous from the left at $x$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.8.}
\addcontentsline{toc}{subsection}{Exercise 11.8.}
\emph{If $f \in \mathscr{R}$ on $[a,b]$ and if $F(x) = \int_{a}^{x}f(t)dt$,
prove that $F'(x) = f(x)$ almost everywhere on $[a,b]$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Theorem 6.20 implies that
  $F'(x_0) = f(x_0)$ if $f$ is continuous at $x_0 \in [a,b]$.

\item[(2)]
  Since $f \in \mathscr{R}$ on $[a,b]$, $f$ is bounded on $[a,b]$.
  Theorem 11.33 implies that
  $f$ is continuous almost everywhere on $[a,b]$.
\end{enumerate}
By (1)(2), $F'(x) = f(x)$ almost everywhere on $[a,b]$.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.9.}
\addcontentsline{toc}{subsection}{Exercise 11.9.}
\emph{Prove that the function $F$ given by
\[
  F(x) = \int_{a}^{x} f dt
  \qquad
  (a \leq x \leq b)
\]
(where $f \in \mathscr{L}$ on $[a,b]$) is continuous on $[a,b]$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Let $f \in \mathscr{L}$ on $E$.
  Show that given any $\varepsilon > 0$ there is a $\delta > 0$ such that
  \[
    \int_{A} f d\mu < \varepsilon
  \]
  whenever $A \subseteq E$ with $\mu(A) < \delta$.}
  \begin{enumerate}
  \item[(a)]
    Define $f_n(x) = \min\{ f(x), n \}$ on $E$ for $n=1,2,3,\ldots$.
    Then $\{f_n\}$ is a sequence of measurable functions such that
    \[
      0 \leq f_1(x) \leq f_2(x) \leq \cdots.
    \]
    Also, $f_n \to f$.
    Then by the Lebesuge's monotone convergenece theorem (Theorem 11.28),
    \[
      \lim_{n \to \infty} \int_{E} f_n d\mu = \int_{E} f d\mu.
    \]

  \item[(b)]
    For such $\varepsilon > 0$, there is an integer $N \geq 1$ such that
    \[
      \int_{E} (f - f_N) d\mu < \frac{\varepsilon}{2}.
    \]
    Choose $\delta > 0$ so that $\delta < \frac{\varepsilon}{2N}$.
    If $\mu(A) < \delta$, we have
    \begin{align*}
      \int_{A} f d\mu
      &= \int_{A} (f-f_N) d\mu + \int_{A} f_N d\mu \\
      &\leq \int_{E} (f-f_N) d\mu + N \mu(A) \\
      &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
      &= \varepsilon.
    \end{align*}
  \end{enumerate}

\item[(2)]
  Apply (1) to $f^{+}$ and $f^{-}$ on $E = [a,b]$.
  Given any $\varepsilon > 0$, there is a common $\delta > 0$
  such that
  \[
    \abs{ \int_{x}^{y} f^{+} dt } < \frac{\varepsilon}{2}
    \qquad \text{and} \qquad
    \abs{ \int_{x}^{y} f^{-} dt } < \frac{\varepsilon}{2}
  \]
  whenever $|y - x| < \delta$.
  So
  \[
    \abs{ F(y) - F(x) }
    \leq \abs{\int_{x}^{y} f^{+} dt} + \abs{\int_{x}^{y} f^{-} dt}
    < \varepsilon
  \]
  whenever $|y - x| < \delta$.
  Hence $F$ is uniformly continuous.
  (In fact, $F$ is absolutely continuous by the same argument.)
\end{enumerate}
$\Box$ \\

\emph{Note.}
Compare with Theorem 6.20. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.10.}
\addcontentsline{toc}{subsection}{Exercise 11.10.}
\emph{If $\mu(X) < +\infty$ and $f \in \mathscr{L}^2(\mu)$ on $X$,
prove that $f \in \mathscr{L}$ on $X$.
If
\[
  \mu(X) = +\infty,
\]
this is false. For instance, if
\[
  f(x) = \frac{1}{1+|x|},
\]
then $f^2 \in \mathscr{L}$ on $\mathbb{R}^1$,
but $f \not\in \mathscr{L}$ on $\mathbb{R}^1$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Since $\mu(X) < +\infty$, $1 \in \mathscr{L}^2(\mu)$ on $X$.
By Theorem 11.35, $f \in \mathscr{L}(\mu)$, and
\[
  \int_X |f| d\mu
  \leq \norm{f} \norm{1}.
\]

\item[(2)]
  \emph{Show that $f^2 \in \mathscr{L}$ on $\mathbb{R}^1$.}
  To apply Theorem 11.33,
  we might restrict the measure space $X = \mathbb{R}^1$ to some interval $[a,b]$.
  Then apply the Lebesgue's monotone convergence theorem (Theorem 11.28) to get the conclusion.
  \begin{enumerate}
  \item[(a)]
  Write
  \[
    f(x)^2
    = \left(\frac{1}{1+|x|}\right)^2
    = \frac{1}{1 + 2|x| + x^2}
    \leq \frac{1}{1+x^2}.
  \]
  By Theorem 11.27,
  \emph{it suffices to show that $\frac{1}{1+x^2} \in \mathscr{L}$ on $\mathbb{R}^1$.}

  \item[(b)]
  Consider the sequence $\{f_n\}$ defined by
  \[
    f_n(x) = \frac{1}{1+x^2} \chi_{[-n,n]}(x).
  \]
  (Here $\chi_{[-n,n]} = K_{[-n,n]}$ is the characteristic function of $[-n,n]$
  defined in Definition 11.19.)
  By construction,
  \[
    0 \leq f_1(x) \leq f_2(x) \leq \cdots
    \qquad
    (x \in \mathbb{R}^1)
  \]
  and
  \[
    f_n(x) \to \frac{1}{1+x^2}
    \qquad
    (x \in \mathbb{R}^1).
  \]

  \item[(c)]
  Hence
  \begin{align*}
    \int_{\mathbb{R}^1} \frac{1}{1+x^2} dx
    &= \lim_{n \to \infty} \int_{\mathbb{R}^1} f_n(x) dx
      &(\text{Theorem 11.28}) \\
    &= \lim_{n \to \infty} \int_{\mathbb{R}^1} \frac{1}{1+x^2} \chi_{[-n,n]}(x) dx \\
    &= \lim_{n \to \infty} \int_{-n}^{n} \frac{1}{1+x^2} dx \\
    &= \lim_{n \to \infty} \mathscr{R}\int_{-n}^{n} \frac{1}{1+x^2} dx
      &(\text{Theorem 11.33}) \\
    &= \lim_{n \to \infty} 2 \arctan(n) \\
    &= \pi < \infty.
  \end{align*}
  \end{enumerate}

\item[(4)]
  \emph{Show that $f \not\in \mathscr{L}$ on $\mathbb{R}^1$.}
  \begin{enumerate}
  \item[(a)]
  Consider the sequence $\{f_n\}$ defined by
  \[
    f_n(x) = f(x) \chi_{[-n,n]}(x) = \frac{1}{1+|x|} \chi_{[-n,n]}(x).
  \]
  By construction,
  \[
    0 \leq f_1(x) \leq f_2(x) \leq \cdots
    \qquad
    (x \in \mathbb{R}^1)
  \]
  and
  \[
    f_n(x) \to f(x)
    \qquad
    (x \in \mathbb{R}^1).
  \]

  \item[(b)]
  Hence
  \begin{align*}
    \int_{\mathbb{R}^1} f(x) dx
    &= \lim_{n \to \infty} \int_{\mathbb{R}^1} f_n(x) dx
      &(\text{Theorem 11.28}) \\
    &= \lim_{n \to \infty} \int_{\mathbb{R}^1} \frac{1}{1+|x|} \chi_{[-n,n]}(x) dx \\
    &= \lim_{n \to \infty} \int_{-n}^{n} \frac{1}{1+|x|} dx \\
    &= \lim_{n \to \infty} \mathscr{R}\int_{-n}^{n} \frac{1}{1+|x|} dx
      &(\text{Theorem 11.33}) \\
    &= \lim_{n \to \infty} 2 \log(n+1) \\
    &= \infty,
  \end{align*}
  or $f \not\in \mathscr{L}$ on $\mathbb{R}^1$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Note.}
Compare with Exercise 6.5. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.11.}
\addcontentsline{toc}{subsection}{Exercise 11.11.}
\emph{If $f, g \in \mathscr{L}(\mu)$ on $X$, defined the distance between $f$ and $g$ by
\[
  \int_{X} |f-g| d\mu.
\]
Prove that $\mathscr{L}(\mu)$ is a complete metric space.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define
  \[
    \norm{f-g}_1 = \int_{X} |f-g| d\mu.
  \]
  Thus $\norm{f-g}_1 = 0$ if and only if $f = g$ almost everywhere on $X$ (Exercise 11.1).
  As in Remark 11.37, we identify two functions to be equivalent
  if they are equal almost everywhere.

\item[(2)]
  \emph{Show that $\mathscr{L}(\mu)$ is a metric space.}
  \begin{enumerate}
  \item[(a)]
    By definition, $\norm{f-g}_1 \geq 0$.
    Besides, $\norm{f-g}_1 = 0$ if and only if $f = g$ almost everywhere by (1).

  \item[(b)]
    $\norm{f-g}_1 = \norm{g-f}_1$ since $|f(x)-g(x)| = |g(x)-f(x)|$ for all $x \in X$.

  \item[(c)]
    Since $|f(x)-g(x)| \leq |f(x)-h(x)| + |h(x)-g(x)|$ for all $x \in X$,
    Remarks 11.23(c) and Theorem 11.29 imply that
    \[
      \norm{f-g}_1 \leq \norm{f-h}_1 + \norm{h-g}_1.
    \]
  \end{enumerate}

\item[(3)]
  \emph{Show that $\mathscr{L}(\mu)$ is complete.}
  Similar to the proof of Theorem 11.42.
  \begin{enumerate}
  \item[(a)]
    \emph{Let $\{f_n\}$ be a Cauchy sequence in $\mathscr{L}(\mu)$,
    show that there exists a function $f \in \mathscr{L}(\mu)$ such that
    $\{f_n\}$ converges to $f \in \mathscr{L}(\mu)$.}

  \item[(b)]
    Since $\{f_n\}$ is a Cauchy sequence,
    we can find a sequence $\{n_k\}$, $k = 1,2,3,\ldots$, such that
    \[
      \norm{ f_{n_k} - f_{n_{k+1}} }_1
      = \int_{X} \abs{f_{n_k} - f_{n_{k+1}}} d\mu
      < \frac{1}{2^k}
      \qquad
      (k = 1,2,3,\ldots).
    \]
    Hence
    \[
      \sum_{k=1}^{\infty} \int_{X} \abs{f_{n_k} - f_{n_{k+1}}} d\mu
      \leq \sum_{k=1}^{\infty} \frac{1}{2^k}
      = 1
      < +\infty.
    \]

  \item[(c)]
    By Theorem 11.30, we may interchange the summation and integration to get
    \[
      \int_{X} \sum_{k=1}^{\infty} \abs{f_{n_k} - f_{n_{k+1}}} d\mu < +\infty,
    \]
    or
    \[
      \sum_{k=1}^{\infty} \abs{f_{n_k}(x) - f_{n_{k+1}}(x)}
      = \sum_{k=1}^{\infty} \abs{f_{n_{k+1}}(x) - f_{n_{k}}(x)}
      < +\infty
    \]
    almost everywhere on $X$.

  \item[(d)]
    Since the $k$th partial sum of the series
    \[
      \sum_{k=1}^{\infty} (f_{n_{k+1}}(x) - f_{n_{k}}(x))
    \]
    which converges almost everywhere on $X$ (Theorem 3.45), is
    \[
      f_{n_{k+1}}(x) - f_{n_{1}}(x),
    \]
    we see that the equation
    \[
      f(x) = \lim_{k \to \infty} f_{n_k}(x)
    \]
    defines $f(x)$ for almost all $x \in X$,
    and it does not matter how we define $f(x)$ at the remaining points of $X$.

  \item[(e)]
    We shall now show that this function $f$ has the desired properties.
    Let $\varepsilon > 0$ be given, and choose $N$ such that
    \[
      \norm{ f_n-f_m }_1 \leq \varepsilon
    \]
    whenever $n, m \geq N$.
    If $n_k > N$, Fatou's theorem shows that
    \[
      \norm{ f-f_{n_k} }_1
      \leq \liminf_{i \to \infty} \norm{ f_{n_i}-f_{n_k} }_1
      \leq \varepsilon.
    \]
    Thus $f-f_{n_k} \in \mathscr{L}(\mu)$,
    and since $f = (f-f_{n_k}) + f_{n_k} \in \mathscr{L}(\mu)$,
    we see that $f \in \mathscr{L}(\mu)$.
    Also, since $\varepsilon$ is arbitrary,
    \[
      \lim_{k \to \infty} \norm{f-f_{n_k}}_1 = 0.
    \]

  \item[(f)]
    Finally, the inequality
    \[
      \norm{f-f_n}_1 \leq \norm{f-f_{n_k}}_1 + \norm{f_{n_k}-f_n}_1
    \]
    shows that $\{f_n\}$ converges to $f \in \mathscr{L}(\mu)$;
    for if we take $n$ and $n_k$ large enough,
    each of the two terms can be made arbitrary small.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.12.}
\addcontentsline{toc}{subsection}{Exercise 11.12.}
\emph{Suppose}
\begin{enumerate}
\item[(a)]
  \emph{$|f(x,y)| \leq 1$ if $0 \leq x \leq 1$, $0 \leq y \leq 1$.}

\item[(b)]
  \emph{for fixed $x$, $f(x,y)$ is a continuous function of $y$.}

\item[(c)]
  \emph{for fixed $y$, $f(x,y)$ is a continuous function of $x$.}
\end{enumerate}
\emph{Put
\[
  g(x) = \int_{0}^{1} f(x,y) dy
  \qquad
  (0 \leq x \leq 1).
\]
Is $g$ continuous?} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $g$ is continuous.}

\item[(2)]
  Let $\{ x_n \}$ be a sequence in $[0,1]$ such that
  $x_n \neq x$ and $\lim x_n = x$.
  It suffices to show that
  \begin{align*}
    \lim_{n \to \infty} g(x_n)
    &= \lim_{n \to \infty} \int_{0}^{1} f(x_n,y) dy \\
    &= \int_{0}^{1} \lim_{n \to \infty} f(x_n,y) dy \\
    &= \int_{0}^{1} f(x,y) dy \\
    &= g(x)
  \end{align*}
  (Theorem 4.2).
  Since $\lim_{n \to \infty} f(x_n,y) = f(x,y)$ for any fixed $y$ (by (c)),
  it suffices to show that
  \[
    \lim_{n \to \infty} \int_{0}^{1} f(x_n,y) dy
    = \int_{0}^{1} \lim_{n \to \infty} f(x_n,y) dy.
  \]

\item[(3)]
  Define $\{f_n\}$ by $f_n(y) = f(x_n,y)$.
  $f_n(y)$ is a continuous function of $y$ for every fixed $n$ (by (b)).
  Thus $f_n(y)$ is measurable (Example 11.14).
  Besides, $|f_n(y)| \leq 1$ and $1 \in \mathscr{L}$ on $[0,1]$ (by (a)).
  The Lebesgue's dominated convergence theorem (Theorem 11.32) implies that
  \[
    \lim_{n \to \infty} \int_{0}^{1} f(x_n,y) dy
    = \int_{0}^{1} \lim_{n \to \infty} f(x_n,y) dy.
  \]
\end{enumerate}
$\Box$ \\



\textbf{Supplement (Similar exercise).}
\emph{Suppose}
\begin{enumerate}
\item[(a)]
  \emph{$|f(x,y)| \leq g(y)$ if $0 \leq x \leq 1$, $0 \leq y \leq 1$,
  where $g \in \mathscr{L}$ on $[0,1]$.}

\item[(b)]
  \emph{for fixed $x$, $f(x,y)$ is a measurable function of $y$.}

\item[(c)]
  \emph{for fixed $y$, $f(x,y)$ is a continuous function of $x$.}
\end{enumerate}
\emph{Show that
\[
  h(x) = \int_{0}^{1} f(x,y) dy
  \qquad
  (0 \leq x \leq 1).
\]
is continuous.} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.13.}
\addcontentsline{toc}{subsection}{Exercise 11.13.}
\emph{Consider the functions
\[
  f_n(x) = \sin(nx)
  \qquad
  (n=1,2,3,\ldots, -\pi \leq x \leq \pi)
\]
as points of $\mathscr{L}^2$.
Prove that the set of these points is closed and bounded, but not compact.} \\


\emph{Proof.}
Define $E = \{ f_n \}$ as a set in $\mathscr{L}^2$.
\begin{enumerate}
\item[(1)]
  \emph{Show that $E$ is bounded.}
  Note that
  \[
    \norm{f_n}_2
    = \left( \int_{-\pi}^{\pi} \sin(nx)^2 dx \right)^{\frac{1}{2}}
    = \sqrt{\pi}
  \]
  for all $n$ (Definition 8.10).
  So $E$ is bounded by $\sqrt{\pi}$.

\item[(2)]
  \emph{Show that $E$ is closed.}
  It suffices to show that $E$ has no limit points.
  \begin{align*}
    \norm{f_n - f_m}_2
    &= \left( \int_{-\pi}^{\pi} (\sin(nx)-\sin(mx))^2 dx \right)^{\frac{1}{2}} \\
    &= \left( \int_{-\pi}^{\pi}
      \sin(nx)^2 - 2\sin(nx)\sin(mx) + \sin(mx)^2 dx \right)^{\frac{1}{2}} \\
    &= (\pi + 0 + \pi)^{\frac{1}{2}} \\
    &= \sqrt{2\pi}
  \end{align*}
  for all $n \neq m$ (Definition 8.10).
  So all points of $E$ are isolated.

\item[(3)]
  \emph{Show that $E$ is not compact.}
  \begin{enumerate}
  \item[(a)]
    Take a collection
    \[
      \mathscr{G} = \left\{ G_n = B\left(f_n;1\right) \right\}
    \]
    of open subsets ($n = 1,2,3,\ldots$).

  \item[(b)]
    $\mathscr{G}$ is an open covering of $E \subseteq \mathscr{L}^2$
    since $f_n \in G_n$ for each $n = 1,2,3,\ldots$.

  \item[(c)]
    \emph{Show that there is no finite subcoverings of $\mathscr{G}$.}
    (Reductio ad absurdum)
    If
    \[
      \mathscr{G}' = \left\{ G_{n_1}, G_{n_2}, \ldots, G_{n_k} \right\}
    \]
    were a finite subcovering of $\mathscr{G}$ with $n_1 < n_2 < \cdots < n_k$,
    then $f_{n_k+1}$ is not in any open sets from $\mathscr{G}'$ (by (2)),
    which is absurd.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.14.}
\addcontentsline{toc}{subsection}{Exercise 11.14.}
\emph{Prove that a complex function $f$ is measurable
if and only if $f^{-1}(V)$ is measurable for every open set $V$ in the plane.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write $f=u+iv$, where $u$ and $v$ are real.
  $f$ is measurable if and only if both $u$ and $v$ are measurable.

\item[(2)]
  \emph{Show that $f$ is measurable
  if $f^{-1}(V)$ is measurable for every open set $V \subseteq \mathbb{C}$.}
  For every real $a$,
  both sets
  \begin{align*}
    f^{-1}((a,\infty) \times \mathbb{R}^1) &= \{ x : u(x) > a \}, \\
    f^{-1}(\mathbb{R}^1 \times (a,\infty)) &= \{ x : v(x) > a \}
  \end{align*}
  are measurable.
  By Definition 11.13, both $u$ and $v$ are measurable, or $f$ is measurable.

\item[(3)]
  \emph{Show that $f^{-1}(V)$ is measurable for every open set $V \subseteq \mathbb{C}$
  if $f$ is measurable.}
  Since $\mathbb{R}^2$ is separable (Exercise 2.22),
  $V$ is the union of a subcollection of a countable base (Exercise 2.23).
  In particular,
  define
  \[
    I((a,b);r)
    = (a-r,a+r) \times (b-r,b+r)
    \subseteq \mathbb{R}^2
  \]
  for all $a, b, r \in \mathbb{Q}$.
  The collection
  \[
    \{ I((a,b);r) \}
  \]
  is a countable base for $\mathbb{R}^2$.
  Write $V$ as a countable union of $I((a,b);r)$:
  \[
    V = \bigcup I((a,b);r).
  \]

\item[(4)]
  \emph{Show that $f^{-1}(V)$ is measurable for every open set $V \subseteq \mathbb{C}$.}
  \begin{align*}
    f^{-1}(V)
    &= f^{-1} \left( \bigcup I((a,b);r) \right) \\
    &= \bigcup f^{-1}(I((a,b);r)) \\
    &= \bigcup \{ x : f(x) \in I((a,b);r) \} \\
    &= \bigcup \{ x : (u(x), v(x)) \in (a-r,a+r) \times (b-r,b+r) \} \\
    &= \bigcup \left( \{ x : u(x) \in (a-r,a+r) \} \bigcap \{ x : v(x) \in (b-r,b+r) \} \right) \\
    &= \bigcup \left( u^{-1}((a-r,a+r)) \bigcap v^{-1}((b-r,b+r)) \right).
  \end{align*}
  Since both $u$ and $v$ are measurable,
  \[
    u^{-1}((a-r,a+r)) \bigcap v^{-1}((b-r,b+r))
  \]
  is measurable, and thus $f^{-1}(V)$ is measurable (since the union is countable).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.15.}
\addcontentsline{toc}{subsection}{Exercise 11.15.}
\emph{Let $\mathscr{R}$ be the ring of all elementary subsets of $(0,1]$.
If $0 < a \leq b \leq 1$, define
\[
  \phi([a,b])
  = \phi([a,b))
  = \phi((a,b])
  = \phi((a,b))
  = b-a,
\]
but define
\[
  \phi((0,b)) = \phi((0,b]) = 1+b
\]
if $0 < b \leq 1$.
Show that this gives an additive set function $\phi$ on $\mathscr{R}$,
which is not regular and
which cannot be extended to a countably additive set function on a $\sigma$-ring.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define $\phi: \mathscr{R} \to \mathbb{R} \cup \{ \pm \infty \}$ by
  \[
    \phi(A) = \sum_{i=1}^{n} \phi(I_i)
  \]
  where $A$ is a finite number of disjoint intervals $I_1, \ldots, I_n$ (Definition 11.4).

\item[(2)]
  \emph{Show that $\phi$ is an additive set function.}
  Given any two elementary sets $A, B \in \mathscr{R}$ with $A \bigcap B = \varnothing$.
  By Definition 11.4,
  \[
    A = \bigcup_{i=1}^{n} I_i,
    \qquad
    B = \bigcup_{j=1}^{m} J_j
  \]
  where $I_i \bigcap J_j = \varnothing$ for all $1 \leq i \leq n$ and $1 \leq j \leq m$
  (since $A \bigcap B = \varnothing$).
  Hence,
  \begin{align*}
    \phi\left(A \bigcup B\right)
    &= \phi\left( \left\{ \bigcup_{i=1}^{n} I_i \right\}
      \bigcup \left\{ \bigcup_{j=1}^{m} J_j \right\} \right) \\
    &= \sum_{i=1}^{n} \phi(I_i) + \sum_{j=1}^{m} \phi(J_j) \\
    &= \phi(A) + \phi(B).
  \end{align*}

\item[(3)]
  \emph{Show that $\phi$ is not countably additive.}
  Write
  \[
    (0,1]
    = \bigcup_{i=1}^{\infty} A_i
  \]
  where $A_i = \left( 2^{-i},2^{-i+1} \right]$.
  Note that $A_i \bigcap A_j = \varnothing$ if $i \neq j$.
  So
  \[
    \sum_{i=1}^{\infty} \phi(A_i)
    = \sum_{i=1}^{\infty} 2^{-i}
    = 1
    \neq 2
    = \phi((0,1]).
  \]

\item[(4)]
  \emph{Show that $\phi$ is not regular.}
  \begin{enumerate}
  \item[(a)]
    \emph{Given any closed set $F \in \mathscr{R}$. Show that $\phi(F) \leq 1$.}
    Write $F = \bigcup_{i=1}^{n} I_i$ where each $I_i$ are disjoint by Definition 11.4.
    Here every $I_i$ is never of the form $(0,b]$ or $(0,b)$ where $b > 0$.
    (Otherwise $0$ is a limit point of $F$, or $0 \in \overline{F} = F$,
    which is absurd.)
    Hence $\phi(F) = \sum \phi(I_i) \leq 1$.

  \item[(b)]
    Take $A = \left(0,\frac{1}{2}\right] \in \mathscr{R}$
    and $\varepsilon = \frac{1}{64} > 0$.
    Then for every closed set $F \in \mathscr{R}$,
    we have
    \[
      \phi(A) = \frac{3}{2} > 1 + \frac{1}{64} \geq \phi(F) + \varepsilon.
    \]
    That is, $\phi$ cannot be regular (Definition 11.5).
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.16.}
\addcontentsline{toc}{subsection}{Exercise 11.16.}
\emph{Suppose $\{n_k\}$ is an increasing sequence of positive integers and
$E$ is the set of all $x \in (-\pi,\pi)$ at which $\{ \sin(n_k x) \}$ converges.
Prove that $m(E) = 0$.
(Hint: For every $A \subseteq E$,
\[
  \int_A \sin(n_k x) dx \to 0,
\]
and
\[
  2 \int_A (\sin(n_k x))^2 dx = \int_A (1-\cos(2n_k x)) dx \to m(A)
\]
as $k \to \infty$.)} \\

\emph{Proof (Hint).}
\begin{enumerate}
\item[(1)]
  Define $\{f_k\}$ by $f_k(x) = \sin(n_k x)$ on $[-\pi,\pi]$ for $k = 1,2,3,\ldots$.
  $\{f_k\}$ is a sequence of measurable functions on $[-\pi,\pi]$
  since each $f_k: x \to \sin(n_k x)$ is continuous (Example 11.14).
  By Exercise 11.3, $E$ is measurable.
  Given any measurable subset $A$ of $E$,
  $\{f_k\}$ is a sequence of measurable functions on $A$
  and $f(x) = \lim_{k \to \infty} f_k(x)$ is well-defined by the definition of $A \subseteq E$.

\item[(2)]
  Apply the Bessel inequality (Theorem 8.12 and Definition 11.39) to
  the function $\chi_{A} \in \mathscr{L}^2$ on $[-\pi,\pi]$,
  we have
  \[
    c_{-n}
    = \int_{[-\pi,\pi]} \chi_{A} e^{inx} dx
    \to 0
  \]
  as $n \to \infty$.
  Hence
  \[
    \lim_{k \to \infty} \int_{A} \sin(n_k x) dx
    = 0
  \]
  for any measurable subset $A$ of $E$.

\item[(3)]
  \emph{Show that $f(x) = 0$ almost everywhere on $E$.}
  Note that
  \[
    |f_k(x)| = |\sin(n_k x)| \leq 1
  \]
  on $A$ and
  \[
    \int_{A} dx
    = m(A)
    \leq m([-\pi,\pi])
    = 2\pi
    < \infty.
  \]
  By (2) and the Lebesgue's dominated convergence theorem (Theorem 11.32),
  \[
    \int_{A} f dx
    = \lim_{k \to \infty} \int_{A} f_k dx
    = \lim_{k \to \infty} \int_{A} \sin(n_k x) dx
    = 0
  \]
  for any measurable subset $A$ of $E$.
  By Exercise 11.2, the conclusion holds.

\item[(4)]
  Apply (1)(2)(3) to the sequence of measurable functions $\{f_k^2\}$ on $[-\pi,\pi]$,
  we have
  \begin{align*}
    0
    &= 2 \int_{A} f^2 dx
      &(\text{$f^2(x) = 0$ a.e. on $A$}) \\
    &= \lim_{k \to \infty} 2 \int_{A} f_k^2 dx \\
    &= \lim_{k \to \infty} 2 \int_{A} \sin(n_k x)^2 dx \\
    &= \lim_{k \to \infty} \int_A (1-\cos(2n_k x)) dx \\
    &= m(A) - \lim_{k \to \infty} \int_A \cos(2n_k x) dx \\
    &= m(A)
  \end{align*}
  for any measurable subset $A$ of $E$.
  In particular, take $A = E$ to get $m(E) = 0$.
\end{enumerate}
$\Box$ \\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.17.}
\addcontentsline{toc}{subsection}{Exercise 11.17.}
\emph{Suppose $E \subseteq (-\pi,\pi)$, $m(E) > 0$, $\delta > 0$.
Use the Bessel inequality to prove that there are at most finitely many integers $n$
such that $\sin(nx) \geq \delta$ for all $x \in E$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  (Reductio ad absurdum)
  If there were infinitely many integers $n$
  such that $\sin(nx) \geq \delta$ for all $x \in E$,
  then there exists an increasing sequence of positive integers $\{n_k\}$
  such that $\sin(n_k x) \geq \delta$ for all $x \in E$ and $n_k$.

\item[(2)]
  Since $E$ is measurable,
  we apply the Bessel inequality (Theorem 8.12 and Definition 11.39) to
  the function $\chi_{E} \in \mathscr{L}^2$ on $[-\pi,\pi]$:
  \[
    c_{-n}
    = \int_{[-\pi,\pi]} \chi_{E} e^{inx} dx
    \to 0
  \]
  as $n \to \infty$.
  Hence
  \[
    \lim_{k \to \infty} \int_{E} \sin(n_k x) dx
    = 0.
  \]

\item[(3)]
  Note that for all $n_k$, we have
  \[
    \int_{E} \sin(n_k x) dx
    \geq m(E) \delta.
  \]
  Here $m(E) \delta > 0$ is a constant, contrary to
  $\lim_{k \to \infty} \int_{E} \sin(n_k x) dx = 0$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Exercise 11.18.}
\addcontentsline{toc}{subsection}{Exercise 11.18.}
\emph{Suppose $f \in \mathscr{L}^2(\mu)$, $g \in \mathscr{L}^2(\mu)$.
Prove that
\[
  \abs{ \int f\overline{g} d\mu }^2 = \int |f|^2 d\mu \int |g|^2 d\mu
\]
if and only if $f(x) = 0$ almost everywhere or there is a constant $c$ such that
$g(x) = cf(x)$ almost everywhere.
(Compare Theorem 11.35.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that
  \[
    \abs{ \int f\overline{g} d\mu }^2 = \int |f|^2 d\mu \int |g|^2 d\mu
  \]
  if $f(x) = 0$ almost everywhere or
  there is a constant $c$ such that $g(x) = cf(x)$ almost everywhere.}
  \begin{enumerate}
  \item[(a)]
    If $f(x) = 0$ almost everywhere,
    then
    \[
      \abs{ \int f\overline{g} d\mu }^2 = 0
    \]
    and
    \[
      \int |f|^2 d\mu \int |g|^2 d\mu = 0.
    \]

  \item[(b)]
    If there is a constant $c \in \mathbb{C}$ such that $g(x) = cf(x)$ almost everywhere,
    then
    \[
      \abs{ \int f\overline{g} d\mu }^2
      = \abs{ \int \overline{c}|f|^2 d\mu }^2
      = |\overline{c}|^2 \left\{ \int |f|^2 d\mu \right\}^2
    \]
    and
    \[
      \int |f|^2 d\mu \int |g|^2 d\mu
      = \int |f|^2 d\mu \int |c|^2 |f|^2 d\mu
      = |c|^2 \left\{ \int |f|^2 d\mu \right\}^2.
    \]
    Since $|\overline{c}| = |c|$, the conclusion holds.
  \end{enumerate}

\item[(2)]
  \emph{Show that $f(x) = 0$ almost everywhere
  or there is a constant $c$ such that $g(x) = cf(x)$ almost everywhere
  if}
  \[
    \abs{ \int f\overline{g} d\mu }^2 = \int |f|^2 d\mu \int |g|^2 d\mu.
  \]
  \begin{enumerate}
  \item[(a)]
    We might assume that $\int |f|^2 d\mu > 0$.
    (The case $\int |f|^2 d\mu = 0$ implies that $f(x) = 0$ almost everywhere (Exercise 11.1).)

  \item[(b)]
    Let
    \[
      c = \frac{\overline{\int f\overline{g} d\mu}}{\int |f|^2 d\mu} \in \mathbb{C}.
    \]
    $c$ is well-defined since $\int |f|^2 d\mu \neq 0$ by (a).
    Then it suffices to show that $\int |g-cf|^2 d\mu = 0$
    since this conclusion implies that $g(x) = cf(x)$ almost everywhere by Exercise 11.1.

  \item[(c)]
    \emph{Show that $\int |c|^2|f|^2 d\mu = \int |g|^2 d\mu$.}
    \begin{align*}
      \int |c|^2|f|^2 d\mu
      &= |c|^2 \int |f|^2 d\mu \\
      &= \frac{\abs{ \int f\overline{g} d\mu }^2}{ \left\{ \int |f|^2 d\mu \right\}^2 }
        \int |f|^2 d\mu \\
      &= \frac{\abs{ \int f\overline{g} d\mu }^2}{\int |f|^2 d\mu} \\
      &= \int |g|^2 d\mu
    \end{align*}

  \item[(d)]
    \emph{Show that $\int \Re(cf\overline{g}) d\mu = \int |g|^2 d\mu$.}
    \begin{align*}
      \int \Re(cf\overline{g}) d\mu
      &= \Re\left( \int cf\overline{g} d\mu \right) \\
      &= \Re\left( c \int f\overline{g} d\mu \right) \\
      &= \Re\left( \frac{\overline{\int f\overline{g} d\mu}}{\int |f|^2 d\mu}
        \int f\overline{g} d\mu \right) \\
      &= \Re\left( \frac{\abs{\int f\overline{g} d\mu}^2}{\int |f|^2 d\mu} \right) \\
      &= \Re\left( \int |g|^2 d\mu \right) \\
      &= \int |g|^2 d\mu.
    \end{align*}

  \item[(e)]
    By (c)(d), we have
    \begin{align*}
      \int |g-cf|^2 d\mu
      &= \int |g|^2 - 2\Re(cf\overline{g}) + |c|^2|f|^2 d\mu \\
      &= \int |g|^2 d\mu - 2 \int \Re(cf\overline{g}) d\mu + \int |c|^2|f|^2 d\mu \\
      &= \int |g|^2 d\mu - 2 \int |g|^2 d\mu + \int |g|^2 d\mu \\
      &= 0.
    \end{align*}
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Note.}
Compare with Exercise 1.15. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}