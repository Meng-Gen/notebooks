\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt

\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\begin{document}



\textbf{\Large Chapter 8: Some Special Functions} \\\\



\emph{Author: Meng-Gen Tsai} \\
\emph{Email: plover@gmail.com} \\\\



% http://math.ucsd.edu/~lni/math140/HW140B_7_solutions.pdf
% http://www.math.ucsd.edu/~ibejenar/teaching/2019/140B/HW1S.pdf
% https://minds.wisconsin.edu/bitstream/handle/1793/67009/rudin%20ch%208.pdf?sequence=4
% https://math.mit.edu/~gs/cse/websections/cse41.pdf
% https://julypraise.files.wordpress.com/2012/07/rudin-solution-collection1.pdf



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Supplement.} Fourier coefficients in Definition 8.9.
\begin{enumerate}
\item[(1)]
Write $$f(x) = a_0 + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}$$
(as the textbook Rudin, Principles of Mathematical Analysis, Third Edition).
Then
\begin{align*}
a_0 &= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx. \\
a_n &= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(nx) dx, n \in \mathbb{Z}^+. \\
b_n &= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(nx) dx, n \in \mathbb{Z}^+.
\end{align*}

\item[(2)]
One might write in one different form,
$$f(x) = \frac{a_0}{2} + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
The only difference between the new one and the old one is $a_0$,
so $a_0$ should be
$$a_0 = \frac{1}{\pi} \int_{-\pi}^\pi f(x) dx.$$

\item[(3)]
Again, one might write in one different form,
$$f(x) = \frac{a_0}{\sqrt{2}} + \sum_{n = 1}^{N}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$ Similarly, $a_0$ should be
$$a_0 = \frac{1}{\pi} \int_{-\pi}^\pi \frac{f(x)}{\sqrt{2}} dx.$$

\item[(4)]
Recall $f(x) = \sum_{-N}^{N} c_n e^{inx}$ ($x \in \mathbb{R}$) where
$$c_n = \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) e^{-inx} dx.$$
The relations among $a_n$, $b_n$ of this textbook and $c_n$ are
\begin{align*}
c_0 &= a_0 \\
c_n &= \frac{1}{2} \left( a_n + i b_n \right), n \in \mathbb{Z}^+.
\end{align*}

\item[(5)]
In some textbooks (Henryk Iwaniec, Topics in Classical Automorphic Forms),
it is convenient to consider periodic functions $f$ of period $1$.
Define $$e(n) = e^{2 \pi i x} = \cos(2 \pi x) + i \sin(2 \pi x).$$
Any periodic and piecewise continuous function $f$ has the Fourier series representation
$$f(x) = \sum_{-\infty}^{\infty} a_n e(nx)$$
with coefficients given by
$$a_n = \int_{0}^{1} f(x) e(-nx) dx.$$
Here is one exercise for this representation.
\emph{Show that the fractional part of $x$, $\{x\} = x - [x]$, is given by
$$\{x\} = \frac{1}{2} - \sum_{n = 1}^{\infty} \frac{\sin(2 \pi nx)}{\pi n}.$$}
\end{enumerate}



\textbf{Supplement.} Parseval's theorem 8.16.
\begin{enumerate}
\item[(1)]
Given $$f(x) = a_0 + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= 2 a_0^2 + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$
\item[(2)]
Given $$f(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= \frac{a_0^2}{2} + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$
\item[(3)]
Given $$f(x) = \frac{a_0}{\sqrt{2}} + \sum_{n = 1}^{\infty}(a_n \cos(nx) + b_n \sin(nx)),
x \in \mathbb{R}.$$
Then
$$\frac{1}{\pi} \int_{-\pi}^\pi |f(x)|^2 dx
= a_0^2 + \sum_{n = 1}^{\infty}(a_n^2 + b_n^2).$$ \\
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.1.}
\emph{Define
\begin{equation*}
  f(x) =
    \begin{cases}
      e^{-\frac{1}{x^2}} & (x \neq 0), \\
      0                  & (x = 0).
    \end{cases}
\end{equation*}
Prove that $f$ has derivatives of all orders at $x = 0$,
and that $f^{(n)}(0) = 0$ for $n = 1,2,3,\ldots$} \\

$f(x)$ is an example of non-analytic smooth function, that is,
infinitely differentiable functions are not necessarily analytic.
In this exercise, we will show that Taylor series of $f$ at the origin
converges everywhere to the zero function.
So the Taylor series does not equal $f(x)$ for $x \neq 0$.
Consequently, $f$ is not analytic at $x = 0$. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\emph{Show that
\[
  \lim_{x \rightarrow 0} g(x) e^{-\frac{1}{x^2}} = 0
\]
for any rational function $g(x) \in \mathbb{R}(x)$.}
  \begin{enumerate}
  \item[(a)]
  Write $g(x) = \frac{p(x)}{q(x)}$ for some $p(x), q(x) \in \mathbb{R}[x]$,
  $g(x) \neq 0$.

  \item[(b)]
  Write $q(x) = b_m x^m + b_{m - 1} x^{m - 1} + \cdots + b_0$.
  $q(x)$ is not identically zero, that is, there exists the unique coefficient
  of the least power of $x$ in $q(x)$ which is non-zero, say $b_M \neq 0$.

  \item[(c)]
  Thus,
  \[
    g(x) = \frac{p(x)/x^M}{q(x)/x^M}.
  \]
  The denominator of $g(x)$ tends to $b_M \neq 0$ as $x \rightarrow 0$.
  By the similar argument in Theorem 8.6(f), we have
  \[
    \frac{p(x)}{x^M} e^{-\frac{1}{x^2}} \rightarrow 0 \text{ as } x \rightarrow 0.
  \]
  Hence, $\lim_{x \rightarrow 0} g(x) e^{-\frac{1}{x^2}} = 0$
  for any $g(x) \in \mathbb{R}(x)$.
  \end{enumerate}

\item[(2)]
\emph{Given any real $x \neq 0$, show that
\[
  f^{(n)}(x) = g_n(x) e^{-\frac{1}{x^2}}
\]
for some rational function $g(x) \in \mathbb{R}(x)$.}
  \begin{enumerate}
  \item[(a)]
  Say $g_0(x) = 1 \in \mathbb{R}(x)$.

  \item[(b)]
  $\mathbb{R}(x)$ is a field.
  \emph{Show that $g'(x) \in \mathbb{R}(x)$ for any $g(x) \in \mathbb{R}(x)$.}
  Write $g(x) = \frac{p(x)}{q(x)}$ for some $p(x), q(x) \in \mathbb{R}[x]$, $q(x) \neq 0$.
  Thus
  \[
    g'(x) = \frac{p'(x)q(x) - p(x)q'(x)}{q(x)^2}.
  \]
  The numerator of $g'(x)$ is in $\mathbb{R}[x]$ since
  the differentiation operator on $\mathbb{R}[x]$ is closed in $\mathbb{R}[x]$.
  Also, the denominator of $g'(x) = q(x)^2 \neq 0$
  since $\mathbb{R}[x]$ is an integral domain.
  Therefore, $g'(x) \in \mathbb{R}(x)$.

  \item[(c)]
  Induction on $n$.
  For $n = 1$, we have
  \begin{align*}
    f'(x)
    &= g_0'(x) e^{-\frac{1}{x^2}}
      + g_0(x) \cdot \left( -\frac{1}{x^2} \right)' e^{-\frac{1}{x^2}} \\
    &= \left( g_0'(x) + g_0(x) \cdot \left( -\frac{1}{x^2} \right)' \right) e^{-\frac{1}{x^2}} \\
    &= g_1(x) e^{-\frac{1}{x^2}}
  \end{align*}
  where
  \[
    g_1(x) = g_0'(x) + g_0(x) \cdot \left(-\frac{1}{x^2}\right)' \in \mathbb{R}(x).
  \]
  Now assume that the conclusion holds for $n = k$.
  As $n = k + 1$, similar to the case $n = 1$,
  \[
    f^{(k + 1)}(x) = g_{k + 1}(x) e^{-\frac{1}{x^2}}
  \]
  where
  \[
    g_{k + 1}(x)
    = g_k'(x) + g_k(x) \cdot \left( -\frac{1}{x^2} \right)' \in \mathbb{R}(x).
  \]
  By induction, the conclusion is true.
  \end{enumerate}

\item[(3)]
Induction on $n$.
For $n = 1$, by (1) we have
\[
  f'(0) = \lim_{t \rightarrow 0} \frac{e^{- \frac{1}{t^2}} - 0}{t} = 0.
\]
Now assume that the statement holds for $n = k$.
As $n = k + 1$, by (1)(2) we have
\[
  f^{(k + 1)}(0)
  = \lim_{t \rightarrow 0} \frac{f^{(k)}(t) - f^{(k)}(0)}{t}
  = \lim_{t \rightarrow 0} \frac{g_k(t) e^{- \frac{1}{t^2}} - 0}{t} = 0.
\]
Thus, $f^{(n)}(0) = 0$ for $n \in \mathbb{Z}^+$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.2.}
\emph{Let $a_{ij}$ be the number in the $i$th row and $j$th column of the array
\[
\begin{array}{rrrrr}
           -1 &           0 &           0 &      0 & \cdots \\
  \frac{1}{2} &          -1 &           0 &      0 & \cdots \\
  \frac{1}{4} & \frac{1}{2} &          -1 &      0 & \cdots \\
  \frac{1}{8} & \frac{1}{4} & \frac{1}{2} &     -1 & \cdots \\
       \vdots &      \vdots &      \vdots & \vdots & \ddots
\end{array}
\]
so that
\begin{equation*}
  a_{ij} =
    \begin{cases}
      0       & (i < j), \\
      -1      & (i = j), \\
      2^{j-i} & (i > j).
    \end{cases}
\end{equation*}
Prove that
\[
  \sum_{i} \sum_{j} a_{ij} = -2, \:\:\:\:\:\:\:\: \sum_{j} \sum_{i} a_{ij} = 0.
\]
} \\

Also see Theorem 8.3. \\

\emph{Proof (Brute-force).}
\begin{align*}
  \sum_{i} \sum_{j} a_{ij}
  &= \sum_{i=1}^{\infty} \left( \sum_{j = i} a_{ij} + \sum_{j < i} a_{ij} \right) \\
  &= \sum_{i=1}^{\infty} \left( -1 + \sum_{j=1}^{i-1} 2^{j-i} \right) \\
  &= \sum_{i=1}^{\infty} ( -1 + (1 - 2^{1-i}) ) \\
  &= \sum_{i=1}^{\infty} -2^{1-i} \\
  &= -2.
\end{align*}
\begin{align*}
  \sum_{j} \sum_{i} a_{ij}
  &= \sum_{j=1}^{\infty} \left( \sum_{i = j} a_{ij} + \sum_{i > j} a_{ij} \right) \\
  &= \sum_{j=1}^{\infty} \left( -1 + \sum_{i=j+1}^{\infty} 2^{j-i} \right) \\
  &= \sum_{j=1}^{\infty} ( -1 + 1 ) \\
  &= \sum_{j=1}^{\infty} 0 \\
  &= 0.
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.3.}
\emph{Prove that
\[
  \sum_{i} \sum_{j} a_{ij} = \sum_{j} \sum_{i} a_{ij}
\]
if $a_{ij} \geq 0$ for all $i$ and $j$ (the case $+\infty = +\infty$ may occur).} \\

\emph{Note.}
It can be proved by Theorem 8.3 if both summations are finite. \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let $\mathscr{F}(I)$ be the collection of all finite subsets of $I$.

\item[(2)]
Let
\[
  s = \sup \left\{
    \sum_{(i,j) \in E} a_{ij} : E \in \mathscr{F}(\mathbb{N}^2)
  \right\}
\]

(the case $s = +\infty$ may occur).
\emph{It suffices to show that $\sum_{i}\sum_{j} a_{ij} = s$.}
The case $\sum_{j}\sum_{i} a_{ij} = s$ is similar, and thus
$\sum_{i}\sum_{j} a_{ij} = \sum_{j}\sum_{i} a_{ij}.$

\item[(3)]
\emph{Show that $\sum_{i}\sum_{j} a_{ij} \geq s$.}
Given any $E \in \mathscr{F}(\mathbb{N}^2)$.
It is clear that
\[
  \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij} \geq \sum_{(i,j) \in E} a_{ij}
\]

(since $a_{ij} \geq 0$).
Thus,
\[
  \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}a_{ij}
  \geq
  \sup \left\{
    \sum_{(i,j) \in E} a_{ij} : E \in \mathscr{F}(\mathbb{N}^2)
  \right\}
  = s.
\]

\item[(4)]
\emph{Show that $\sum_{i}\sum_{j} a_{ij} \leq s$.}
(Reductio ad absurdum)
If $\sum_{i}\sum_{j} a_{ij} > s$,
especially $s < \infty$,
then there exists $\varepsilon > 0$ such that
\[
  \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} a_{ij} > s + \varepsilon,
\]
or
\[
  \sum_{i=1}^{n} \sum_{j=1}^{\infty} a_{ij} > s + \varepsilon
\]
for some integer $n$.
Consider two possible cases.
  \begin{enumerate}
  \item[(a)]
  If there is some $1 \leq i_0 \leq n$
  such that
  \[
    \sum_{j=1}^{\infty} a_{i_0 j} = \infty,
  \]
  then there is some $m$ such that
  \[
    \sum_{j=1}^{m} a_{i_0 j} > s.
  \]
  For $E = \{ (i_0,1),\ldots,(i_0,m) \} \in \mathscr{F}(\mathbb{N}^2)$,
  \[
    \sum_{(i,j) \in E} a_{ij} = \sum_{j=1}^{m} a_{i_0 j} > s,
  \]
  contrary to the supremum of $s$.

  \item[(b)]
  Otherwise, for each  $1 \leq i \leq n$
  we have
  \[
    \sum_{j=1}^{\infty} a_{ij} < \infty,
  \]
  or there exists some $m_i$ such that
  \[
    \sum_{j=1}^{m_i} a_{ij} > \sum_{j=1}^{\infty} a_{ij} - \frac{\varepsilon}{n}.
  \]
  For $E = \bigcup_{1 \leq i \leq n} \{ (i,1),\ldots,(i,m_i) \} \in \mathscr{F}(\mathbb{N}^2)$,
  \begin{align*}
    \sum_{(i,j) \in E} a_{ij}
    &= \sum_{i=1}^{n} \sum_{j=1}^{m_i} a_{ij} \\
    &> \sum_{i=1}^{n} \left( \sum_{j=1}^{\infty} a_{ij} - \frac{\varepsilon}{n} \right) \\
    &= \sum_{i=1}^{n}\sum_{j=1}^{\infty} a_{ij} - \sum_{i=1}^{n}\frac{\varepsilon}{n} \\
    &> s + \varepsilon - \varepsilon \\
    &= s,
  \end{align*}
  contrary to the supremum of $s$.
  \end{enumerate}
  Therefore, $\sum_{i}\sum_{j} a_{ij} \leq s$.

  \item[(5)]
  By (3)(4), $\sum_{i}\sum_{j} a_{ij} = s$.
  Similarly, $\sum_{j}\sum_{i} a_{ij} = s$.
  Hence, $\sum_{i}\sum_{j} a_{ij} = \sum_{j}\sum_{i} a_{ij}$
  (including the case $+\infty = +\infty$).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.4.}
\emph{Prove the following limit relations:}
\begin{enumerate}
  \item[(a)]
  $\lim_{x \to 0} \frac{b^x - 1}{x} = \log b \:\:\:\:\:\:\:\: (b > 0).$
  \item[(b)]
  $\lim_{x \to 0} \frac{\log(1+x)}{x} = 1.$
  \item[(c)]
  $\lim_{x \to 0} (1+x)^{\frac{1}{x}} = e.$
  \item[(d)]
  $\lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n} = e^x.$ \\
\end{enumerate}

\emph{Proof of (a).}
\begin{align*}
  \lim_{x \to 0} \frac{b^x - 1}{x}
  &= \lim_{x \to 0} \frac{\exp(x \log b) - 1}{x} \\
  &= \left. \frac{d}{dx} \exp(x \log b) \right\vert_{x=0} \\
  &= \left. \exp(x \log b) \cdot \log b \right\vert_{x=0} \\
  &= \log b.
\end{align*}
$\Box$ \\

\emph{Proof of (b).}
\begin{align*}
  \lim_{x \to 0} \frac{\log(1+x)}{x}
  &= \left. \frac{d}{dx} \log(1+x) \right\vert_{x=0} \\
  &= \left. \frac{1}{x+1} \right\vert_{x=0} \\
  &= 1.
\end{align*}
$\Box$ \\

\emph{Proof of (c).}
\begin{align*}
  \lim_{x \to 0} (1+x)^{\frac{1}{x}}
  &= \lim_{x \to 0} \exp\left( \frac{\log(1+x)}{x} \right) \\
  &= \exp\left( \lim_{x \to 0} \frac{\log(1+x)}{x} \right) \\
  &= \exp(1) \\
  &= e.
\end{align*}
$\Box$ \\

\emph{Proof of (d).}
\begin{align*}
  \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{n}
  &= \lim_{n \to \infty} \left( \left( 1 + \frac{x}{n} \right)^{\frac{n}{x}} \right)^{x} \\
  &= \left( \lim_{n \to \infty} \left( 1 + \frac{x}{n} \right)^{\frac{n}{x}} \right)^{x} \\
  &= \left( \lim_{y \to 0} (1+y)^{\frac{1}{y}} \right)^{x} \\
  &= \exp(x).
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.5.}
\emph{Find the following limits}
\begin{enumerate}
  \item[(a)]
  $\lim_{x \to 0} \frac{e-(1+x)^{\frac{1}{x}}}{x}.$
  \item[(b)]
  $\lim_{n \to \infty} \frac{n}{\log n}\left[ n^{\frac{1}{n}} - 1 \right].$
  \item[(c)]
  $\lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}.$
  \item[(d)]
  $\lim_{x \to 0} \frac{x - \sin x}{\tan x - x}.$ \\
\end{enumerate}

\emph{Proof of (a).}
By L'Hospital's rule (Theorem 5.13),
\begin{align*}
  \lim_{x \to 0} \frac{e-(1+x)^{\frac{1}{x}}}{x}
  &= \lim_{x \to 0} \frac{-(1+x)^{\frac{1}{x}} \cdot
    \frac{ \frac{x}{x+1} - \log(x+1) }{x^2}}{1} \\
  &= \lim_{x \to 0} \left( -(1+x)^{\frac{1}{x}} \cdot
    \frac{ \frac{x}{x+1} - \log(x+1) }{x^2} \right) \\
  &= - \lim_{x \to 0}(1+x)^{\frac{1}{x}} \cdot
    \lim_{x \to 0} \frac{ \frac{x}{x+1} - \log(x+1) }{x^2} \\
  &= -e \cdot
    \lim_{x \to 0} \frac{ \frac{x}{x+1} - \log(x+1) }{x^2}
    &\text{(Exercise 8.4(c))} \\
  &= -e \cdot \lim_{x \to 0} \frac{ -\frac{x}{(x+1)^2} }{2x} \\
  &= e \cdot \lim_{x \to 0} \frac{1}{2(x+1)^2} \\
  &= e \cdot \frac{1}{2} \\
  &= \frac{e}{2}.
\end{align*}
Here
\begin{align*}
  \frac{d}{dx}\left( e-(1+x)^{\frac{1}{x}} \right)
  &= \frac{d}{dx}\left( e-\exp\left( \frac{\log(x+1)}{x} \right) \right) \\
  &= -\exp\left( \frac{1}{x} \log(x+1) \right) \cdot
    \frac{ \frac{1}{x+1} \cdot x - \log(x+1) \cdot 1 }{x^2} \\
  &= -(1+x)^{\frac{1}{x}} \cdot \frac{ \frac{x}{x+1} - \log(x+1) }{x^2},
\end{align*}
and
\begin{align*}
  \frac{d}{dx}\left( \frac{x}{x+1} - \log(x+1) \right)
  &= \frac{(x+1) - x}{(x+1)^2} - \frac{1}{x+1} \\
  &= -\frac{x}{(x+1)^2}.
\end{align*}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Let $x = \frac{\log n}{n}$.
Note that $\lim_{n \to \infty} \frac{\log n}{n} = 0$.

\item[(2)]
\begin{align*}
  \lim_{n \to \infty} \frac{n}{\log n}\left[ n^{\frac{1}{n}} - 1 \right]
  &= \lim_{n \to \infty} \frac{n}{\log n}\left[ \exp\left( \frac{\log n}{n} \right) - 1 \right] \\
  &= \lim_{x \to 0} \frac{\exp(x) - 1}{x}
    &\text{((1))} \\
  &= \left.\frac{d}{dx} \exp(x) \right\vert_{x=0} \\
  &= \left.\exp(x) \right\vert_{x=0} \\
  &= 1.
\end{align*}
\end{enumerate}
$\Box$ \\

\emph{Proof of (c) (L'Hospital's rule).}
By L'Hospital's rule (Theorem 5.13) three times,
\begin{align*}
  \lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}
  &= \lim_{x \to 0} \frac{\sec^2 x - 1}{1 - \cos x + x \sin x} \\
  &= \lim_{x \to 0} \frac{2 \sec x (\tan x \sec x)}{\sin x + \sin x + x \cos x} \\
  &= \lim_{x \to 0} \frac{2 \tan x \sec^2 x}{2 \sin x + x \cos x} \\
  &= \lim_{x \to 0}
    \frac{2 [\sec^2 x \sec^2 x + \tan x \cdot 2 \sec x (\tan x \sec x)]}
    {2 \cos x + \cos x - x \sin x} \\
  &= \lim_{x \to 0} \frac{2 \sec^4 x + 2 \sec^2 x \tan^2 x}{3 \cos x - x \sin x} \\
  &= \frac{2}{3}.
\end{align*}
$\Box$ \\

\emph{Proof of (c) (Taylor series).}
Since
\begin{align*}
  \cos x &= 1 - \frac{x^2}{2} + O(x^4) \\
  \tan x &= x + \frac{x^3}{3} + O(x^5),
\end{align*}
we have
\[
  \lim_{x \to 0} \frac{\tan x - x}{x(1 - \cos x)}
  = \lim_{x \to 0} \frac{\frac{x^3}{3} + O(x^5)}{\frac{x^3}{2} + O(x^5)}
  = \frac{2}{3}.
\]
$\Box$ \\

\emph{Proof of (d) (L'Hospital's rule).}
By L'Hospital's rule (Theorem 5.13) three times,
\begin{align*}
  \lim_{x \to 0} \frac{x - \sin x}{\tan x - x}
  &= \lim_{x \to 0} \frac{1 - \cos x}{\sec^2 x - 1} \\
  &= \lim_{x \to 0} \frac{\sin x}{2 \sec x (\tan x \sec x)} \\
  &= \lim_{x \to 0} \frac{\sin x}{2 \tan x \sec^2 x} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 \tan x \sec^2 x} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 [\sec^2 x \sec^2 x + \tan x \cdot 2 \sec x (\tan x \sec x)]} \\
  &= \lim_{x \to 0} \frac{\cos x}{2 \sec^4 x + 2 \sec^2 x \tan^2 x} \\
  &= \frac{1}{2}.
\end{align*}
$\Box$ \\

\emph{Proof of (d) (Taylor series).}
Since
\begin{align*}
  \sin x &= x - \frac{x^3}{6} + O(x^5) \\
  \tan x &= x + \frac{x^3}{3} + O(x^5),
\end{align*}
we have
\[
  \lim_{x \to 0} \frac{x - \sin x}{\tan x - x}
  = \lim_{x \to 0} \frac{\frac{x^3}{6} + O(x^5)}{\frac{x^3}{2} + O(x^5)}
  = \frac{1}{2}.
\]
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.6.}
\emph{Suppose $f(x)f(y) = f(x + y)$ for all real $x$ and $y$.}
\begin{enumerate}
\item[(a)]
\emph{Assuming that $f$ is differentiable and not zero, prove that
$$f(x) = e^{cx}$$
where $c$ is a constant.}
\item[(b)]
\emph{Prove the same thing, assuming only that $f$ is continuous.} \\
\end{enumerate}

Part (b) implies part (a). We prove part (b) directly. \\

\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
Since $f(x)$ is not zero, there exists $x_0 \in \mathbb{R}$ such that $f(x_0) \neq 0$.
So $f(0)f(x_0) = f(x_0)$, or $f(0) = 1$ by cancelling $f(x_0) \neq 0$.

\item[(2)]
Next, $f(\frac{n}{m}) = f(\frac{1}{m})^n$ for $m \in \mathbb{Z}$, $n \in \mathbb{Z}^{+}$.
Since $f$ is continuous at $x = 0$, $f$ is positive in the neighborhood of $x = 0$.
That is, there exists $N \in \mathbb{Z}^{+}$ such that $f(\frac{1}{m}) > 0$
whenever $|m| \geq N$.
So, $f(\frac{n}{m}) = f(\frac{1}{m})^n > 0$.
(Since $f(\frac{n}{m}) = f(\frac{kn}{km})$ for any $k \in \mathbb{Z}^{+}$,
we can rescale $m$ to $km$ such that $|km| \geq N$.)
That is, $f$ is positive on $\mathbb{Q}$.
Since $\mathbb{Q}$ is dense in $\mathbb{R}$ and $f$ is continuous on $\mathbb{R}$,
$f$ is positive on $\mathbb{R}$.

\item[(3)]
Now let $c = \log f(1)$ (which is well-defined since $f > 0$).
We write $f(1)$ in the two ways.
Firstly, $f(1) = f(\frac{n}{n}) = f(\frac{1}{n})^n$ where $n \in \mathbb{Z}^{+}$.
Secondly, $f(1) = e^c = (e^{\frac{c}{n}})^n$.
Since the positive $n$-th root is unique (Theorem 1.21),
$f(\frac{1}{n}) = e^{\frac{c}{n}}$ for $n \in \mathbb{Z}^{+}$.
By $f(x)f(-x) = f(0) = 1$ or $f(-x) = \frac{1}{f(x)}$,
$f(-\frac{1}{n}) = \frac{1}{e^{\frac{c}{n}}} = e^{-\frac{c}{n}}$ for $n \in \mathbb{Z}^{+}$.
Therefore,
\[
  f\left( \frac{1}{m} \right) = e^{\frac{c}{m}} \text{ where } m \in \mathbb{Z}.
\]

\item[(4)]
By using
$f(\frac{n}{m}) = f(\frac{1}{m})^n$ for $m \in \mathbb{Z}$, $n \in \mathbb{Z}^{+}$ again,
$f(\frac{n}{m}) = e^{c \frac{n}{m}}$ where $m \in \mathbb{Z}, n \in \mathbb{Z}^{+}$, or
$$f(x) = e^{cx} \text{ where } x \in \mathbb{Q}.$$
Since $g(x) = f(x) - e^{cx}$ vanishes on a dense set of $\mathbb{Q}$
and $g$ is continuous on $\mathbb{R}$, $g$ vanishes on $\mathbb{R}$.
Therefore, $f(x) = e^{cx}$ for $x \in \mathbb{R}$.
\end{enumerate}
$\Box$ \\



\textbf{Supplement.}
\emph{Proof of (a).}

\begin{enumerate}
\item[(1)]
Since $f(x)$ is not zero, there exists $x_0 \in \mathbb{R}$ such that $f(x_0) \neq 0$.
So $f(0)f(x_0) = f(x_0)$, or $f(0) = 1$ by cancelling $f(x_0) \neq 0$.
\item[(2)]
Since $f$ is differentiable, for any $x \in \mathbb{R}$,
\begin{align*}
f'(x)
=& \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h} \\
=& \lim_{h \rightarrow 0} \frac{f(x)f(h) - f(x)}{h} \\
=& f(x) \lim_{h \rightarrow 0} \frac{f(h) - 1}{h} \\
=& f(x) \lim_{h \rightarrow 0} \frac{f(h) - f(0)}{h} \\
=& f(x) f'(0).
\end{align*}
Let $c = f'(0)$ be a constant. Then $f'(x) = c f(x)$.
So $f(x) = e^{cx}$ for $x \in \mathbb{R}$.
(To see this, let $g(x) = \frac{f(x)}{e^{cx}}$ be well-defined on $\mathbb{R}$. $g(0) = 1$.
$g'(x) = 0$ since $f'(x) = c f(x)$. So $g(x)$ is a constant, or $g(x) = 1$ since $g(0) = 1$.
Therefore, $f(x) = e^{cx}$ on $\mathbb{R}$.)
\end{enumerate}
$\Box$ \\



\textbf{Supplement.} Cauchy's functional equation.
\begin{enumerate}
\item[(1)]
\emph{(Cauchy's functional equation.) Suppose $f(x) + f(y) = f(x + y)$ for all real $x$ and $y$.
Assuming that $f$ is continuous, prove that $f(x) = cx$ where $c$ is a constant}. \\

Notice that we cannot let $g(x) = \log f(x)$
and apply Cauchy's functional equation on $g(x)$
to prove Exercise 8.6 since $f(x)$ is not necessary positive and thus
$g(x) = \log f(x)$ might be meaningless.
However, this wrong approach gives you some useful ideas such as
you need to prove that $f(x)$ is positive first,
and $f(x)$ should be equal to $e^{cx}$ where $c = g(1) = \log f(1)$.

\item[(2)]
\emph{Suppose $f(xy) = f(x) + f(y)$ for all positive real $x$ and $y$.
Assuming that $f$ is continuous, prove that $f(x) = c \log x$ where $c$ is a constant}.

\item[(3)]
\emph{Suppose $f(xy) = f(x)f(y)$ for all positive real $x$ and $y$.
Assuming that $f$ is continuous and positive,
prove that $f(x) = x^c$ where $c$ is a constant}.

\item[(4)]
\emph{Suppose $f(x + y) = f(x) + f(y) + xy$ for all real $x$ and $y$.
Assuming that $f$ is continuous,
prove that $f(x) = \frac{1}{2}x^2 + cx$ where $c$ is a constant}.

\item[(5)]
\emph{(USA 2002.) Suppose $f(x^2 - y^2) = x f(x) - y f(y)$ for all real $x$ and $y$.
Assuming that $f$ is continuous,
prove that $f(x) = cx$ where $c$ is a constant}. \\
\end{enumerate}



\textbf{Supplement.}
\emph{Show that the only automorphism of $\mathbb{Q}$ is the identity.} \\

\emph{Proof.}
Given any $\sigma \in \text{Aut}(\mathbb{Q})$.
\begin{enumerate}
\item[(1)]
\emph{Show that $\sigma(1) = 1$.}
Since $1^2 = 1$, $\sigma(1)\sigma(1) = \sigma(1)$. $\sigma(1) = 0$ or $1$.
There are only two possible cases.
  \begin{enumerate}
  \item[(a)]
  Assume that $\sigma(1) = 0$. So
  $$\sigma(a) = \sigma(a \cdot 1) = \sigma(a)\cdot \sigma(1) = \sigma(a) \cdot 0 = 0$$
  for any $a \in \mathbb{Q}$.
  That is, $\sigma = 0 \in \text{Aut}(\mathbb{Q})$, which is absurd.
  \item[(b)]
  Therefore, $\sigma(1) = 1$.
  \end{enumerate}
\item[(2)]
\emph{Show that $\sigma(n) = n$ for all $n \in \mathbb{Z}^+$.}
Write $n = 1 + 1 + \cdots + 1$ ($n$ times $1$).
Applying the additivity of $\sigma$, we have
$$\sigma(n) = \sigma(1) + \sigma(1) + \cdots + \sigma(1) = 1 + 1 + \cdots + 1 = n.$$
(Might use induction on $n$ to eliminate $\cdots$ symbols.)
\item[(3)]
\emph{Show that $\sigma(n) = n$ for all $n \in \mathbb{Z}$.}
By the additivity of $\sigma$, $\sigma(-n) = -\sigma(n) = -n$ for $n \geq 0$.
The result is established.
\end{enumerate}
For any $a = \frac{n}{m} \in \mathbb{Q}$ ($m, n \in \mathbb{Z}$, $n \neq 0$),
applying the multiplication of $\sigma$ on $am = n$,
that is,
$\sigma(a) \sigma(m) = \sigma(n)$. By (3), we have $\sigma(a)m = n$,
or $$\sigma(a) = \frac{m}{n} = a$$
provided $n \neq 0$,
or $\sigma$ is the identity.
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.7.}
\emph{If $0 < x < \frac{\pi}{2}$, prove that
\[
  \frac{2}{\pi} < \frac{\sin x}{x} < 1.
\]} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
Let
\begin{equation*}
  f(x) =
    \begin{cases}
      \frac{\sin x}{x} & \text{ if } x \neq 0 \\
      1                & \text{ if } x = 0
    \end{cases}
\end{equation*}
be a continuous function on $\left[ 0, \frac{\pi}{2} \right]$
(since $\lim_{x \to 0+} f(x) = 1$).
So
\[
  f'(x) = \frac{x \cos x - \sin x}{x^2} < 0
\]
on $\left( 0, \frac{\pi}{2} \right)$
since $\tan x > x$ on $\left( 0, \frac{\pi}{2} \right)$.

\item[(2)]
\emph{Show that $\frac{\sin x}{x} < 1$ on $\left( 0, \frac{\pi}{2} \right)$.}
Given any $x \in \left( 0, \frac{\pi}{2} \right)$, there exists $\xi_1 \in (0,x)$ such that
\[
  \frac{f(x) - f(0)}{x - 0} = f'(\xi_1) < 0
\]
by the mean value theorem (Theorem 5.10).
So $f(x) < f(0) = 1$,
or $\frac{\sin x}{x} < 1$.

\item[(3)]
\emph{Show that $\frac{\sin x}{x} > \frac{2}{\pi}$ on $\left( 0, \frac{\pi}{2} \right)$.}
Given any $x \in \left( 0, \frac{\pi}{2} \right)$, there exists $\xi_2 \in (0,x)$ such that
\[
  \frac{f(\frac{\pi}{2}) - f(x)}{\frac{\pi}{2} - x} = f'(\xi_2) < 0
\]
by the mean value theorem (Theorem 5.10).
So $f(x) > f(\frac{\pi}{2}) = \frac{2}{\pi}$,
or $\frac{\sin x}{x} > \frac{2}{\pi}$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.8.}
\emph{For $n=0,1,2,\ldots$, and $x$ real, prove that
\[
  |\sin(nx)| \leq n |\sin x|.
\]
Note that this inequality may be false for other values of $n$.
For instance,
\[
  \abs{ \sin(\frac{1}{2} \pi) } > \frac{1}{2}|\sin \pi|.
\]} \\

\emph{Proof.}
Induction on $n$.
\begin{enumerate}
\item[(1)]
Note that
\[
  \sin(a+b) = \sin a \cos b + \cos a \sin b
\]
for any $a, b \in \mathbb{R}$.
\item[(2)]
$n = 0,1$ are clearly true.
\item[(3)]
Assume the induction hypothesis that for the single case $n = k$ holds,
meaning
\[
  |\sin(kx)| \leq k |\sin x|
\]
is true.
It follows that
\begin{align*}
  |\sin((k+1)x)|
  &= |\sin(kx) \cos x + \cos(kx) \sin x|
    &\text{((1))} \\
  &\leq |\sin(kx)| |\cos x| + |\cos(kx)| |\sin x|
    &\text{(Triangle inequality)} \\
  &\leq |\sin(kx)| + |\sin x|
    &\text{($|\cos(\cdot)| \leq 1$)} \\
  &\leq k |\sin x| + |\sin x|
    &\text{(Induction hypothesis)} \\
  &\leq (k+1)|\sin x|.
\end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% https://rivoal.perso.math.cnrs.fr/articles/cteuler.pdf
% - Lots of identities about the Euler-Mascheroni constant (without proof)
% https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant
% - Wikipedia.



\textbf{Exercise 8.9 (The Euler-Mascheroni constant).}
\begin{enumerate}
\item[(a)]
\emph{Put $s_N = 1 + \frac{1}{2} + \cdots + \frac{1}{N}$.
Prove that
\[
  \lim_{N \to \infty}(s_N - \log N)
\]
exists.
(The limit, often denoted by $\gamma$, is called Euler's constant.
Its numerical value is $0.5772\ldots$.
It is not known whether $\gamma$ is rational or not.)}
\item[(b)]
\emph{Roughly how large must $m$ be so that $N = 10^m$ satisfies $s_N > 100$?} \\
\end{enumerate}

\emph{Proof of (a) (Theorem 3.14).}
\begin{enumerate}
\item[(1)]
Note that
\begin{align*}
  &\frac{1}{1+\frac{1}{n}} \leq \frac{1}{x} \leq 1
    \text{ for } x \in \left[ 1,1+\frac{1}{n} \right] \\
  \Longrightarrow&
  \int_{1}^{1+\frac{1}{n}} \frac{dx}{1+\frac{1}{n}}
  \leq \int_{1}^{1+\frac{1}{n}} \frac{dx}{x}
  \leq \int_{1}^{1+\frac{1}{n}} dx
    &\text{(Theorem 6.12(b))} \\
  \Longrightarrow&
  \frac{1}{n+1}
  \leq \int_{1}^{1+\frac{1}{n}} \frac{dx}{x}
  \leq \frac{1}{n} \\
  \Longrightarrow&
  \frac{1}{n+1}
  \leq \log \left(1 + \frac{1}{n} \right)
  \leq \frac{1}{n}.
    &\text{(Equation (39) on page 180)}
\end{align*}

\item[(2)]
Define
\[
  \gamma_n = s_n - \log n.
\]
It suffices to show that $\{\gamma_n\}$ is monotonic and bounded (Theorem 3.14).

\item[(3)]
\emph{Show that $\{\gamma_n\}$ is decreasing.}
\begin{align*}
  \gamma_{n+1} - \gamma_n
  &= (s_{n+1} - \log(n+1)) - (s_n - \log n) \\
  &= (s_{n+1} - s_n) - (\log(n+1)-\log n) \\
  &= \frac{1}{n+1} - \log \left( \frac{n+1}{n} \right) \\
  &= \frac{1}{n+1} - \log \left(1 + \frac{1}{n} \right) \\
  &\leq 0.
    &\text{((1))}
\end{align*}
\emph{Note.} $\gamma_n \leq \cdots \leq \gamma_1 = 1$ for all $n=1,2,3,\ldots$.

\item[(4)]
\emph{Show that $\gamma_n \geq 0$ for all $n=1,2,3,\ldots$.}
Since
\begin{align*}
  \log n
  &= \sum_{k=1}^{n-1} (\log (k+1) - \log k) \\
  &= \sum_{k=1}^{n-1} \log \frac{k+1}{k} \\
  &= \sum_{k=1}^{n-1} \log \left( 1 + \frac{1}{k} \right) \\
  &\leq \sum_{k=1}^{n-1} \frac{1}{k}
    &\text{((1))} \\
  &= s_{n-1},
\end{align*}
we have
\[
  \gamma_n = s_n - \log n \geq s_n - s_{n-1} = \frac{1}{n} > 0.
\]
\end{enumerate}
By (3)(4), $\{\gamma_n\}$ converges to $\lim_{N \to \infty} (s_N - \log N) = \gamma$.
$\Box$ \\



\textbf{Supplement.}
  \emph{Show that if $f \geq 0$ on $[0,\infty)$ and $f$ is monotonically decreasing,
  and if $$c_n = \sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx,$$
  then $\lim_{n \to \infty} c_n$ exists.}
  (Exercise 10 of Section 5.2 in the textbook:
  \emph{R Creighton Buck, Advanced Calculus, 3rd edition}. See page 235.)
  If this exercise is true,
  we can get the existence of $\gamma$ by taking $f(x) = \frac{1}{x}$.
  \begin{enumerate}
    \item[(1)]
    Note that
    \[
       f(n+1)
      \leq \int_{n}^{n+1} f(x) dx
      \leq f(n).
    \]

    \item[(2)]
    \emph{Show that $\{c_n\}$ is decreasing.}
    \[
      c_{n+1} - c_n
      = f(n+1) - \int_{n}^{n+1} f(x)dx
      \leq 0.
    \]

    \item[(3)]
    \emph{Show that $c_n \geq 0$.}
    Since $f(k) \geq \int_{k}^{k+1} f(x) dx$,
    \begin{align*}
      \sum_{k=1}^{n} f(k)
      &\geq \sum_{k=1}^{n} \int_{k}^{k+1} f(x) dx \\
      &= \int_{1}^{n+1} f(x) dx \\
      &\geq \int_{1}^{n} f(x) dx.
        &(f \geq 0)
    \end{align*}
    So that $c_n = \sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx \geq 0$.

    \item[(4)]
    By (2)(3), $\{c_n\}$ converges (Theorem 3.14).
  \end{enumerate}
  $\Box$ \\



\emph{Proof of (a) (Limit comparison test).}
Inspired by this paper:
\emph{Philippe Flajolet and Ilan Vardi,
Zeta Function Expansions of Classical Constants}.
\begin{enumerate}
\item[(1)]
Rewrite
\[
  \gamma_n + \log n - \log(n+1)
  = \sum_{k=1}^{n} \left( \frac{1}{k} - \log \left(1+\frac{1}{k}\right) \right)
\]
(similar to the argument in (a)(4)(Theorem 3.14)).
Let
\[
  c_k = \frac{1}{k} - \log \left(1+\frac{1}{k}\right).
\]
\item[(2)]
\emph{Show that
\[
  \lim_{k \to \infty}
  \frac{c_k}{ \frac{1}{k^2} }
  = \frac{1}{2}.
\]}

In fact,
\begin{align*}
  &\lim_{k \to \infty}
  \frac{c_k}{ \frac{1}{k^2} } \\
  =&
  \lim_{x \to 0} \frac{x - \log(1+x)}{ x^2 }
    &\text{(Put $x = \frac{1}{k}$)} \\
  =&
  \lim_{x \to 0} \frac{1 - \frac{1}{1+x}}{ 2x }
    &\text{(L'Hospital's rule)} \\
  =&
  \lim_{x \to 0} \frac{1}{ 2(x+1) } \\
  =& \frac{1}{2}.
\end{align*}
\item[(3)]
By limit comparison test or comparison test,
$\sum c_k$
converges since $\sum \frac{1}{k^2}$ converges.
Also, $$\lim_{n \to \infty} \log n - \log(n+1) = 0.$$
Therefore, $\lim_{n \to \infty} \gamma_n$ exists.
\end{enumerate}
$\Box$ \\

\emph{Note.}
This proof is based on \textbf{limit comparison test} (Theorem 8.21) in this textbook:
\emph{Tom. M. Apostol, Mathematical Analysis, 2nd edition.}
It is easy to prove by the original comparison test. \\



\emph{Proof of (a) (Comparison test).}
\begin{enumerate}
\item[(1)]
Note that
\[
  0 \leq x - \log(x+1) \leq \frac{x^2}{2}
\]
for all $x \geq 0$.

\item[(2)]
Write
\[
  c_n = \frac{1}{n} - \log \left(1+\frac{1}{n}\right).
\]
as in the the proof of (a) (Limit comparison test).
By (1),
\[
  |c_n| \leq \frac{1}{2n^2}
\]
for all $n=1,2,\ldots$.
Hence, by the comparison test (Theorem 3.25(a),
$\sum c_n$ converges since $\sum \frac{1}{n^2}$ converges (to $\frac{\pi^2}{6}$).
Use the same argument in the proof of (a) (Limit comparison test), since
\[
  \gamma_n + \log n - \log(n+1) = \sum c_n
  \text{ and }
  \lim_{n \to \infty} \log n - \log(n+1) = 0,
\]
we have the existence of $\lim \gamma_n = \gamma$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (a) (Uniformly convergence of $\sum \frac{x}{n(x+n)}$).}
(One example to Exercise 7 of Section 6.2 in the textbook:
\emph{R Creighton Buck, Advanced Calculus, 3rd edition}.
See pages 270 to 271.)
\begin{enumerate}
\item[(1)]
Let
\[
  f_n(x) = \frac{x}{n(x+n)} = \frac{1}{n} - \frac{1}{x+n}
\]
defined on $E = [0,1]$.

\item[(2)]
Note that
\[
  |f_n(x)| \leq \frac{1}{n^2}
\]
for all $x \in [0,1]$. Since $\sum \frac{1}{n^2}$ converges,
$\sum f_n$ converges uniformly on $[0,1]$ (Theorem 7.10).

\item[(3)]
Corollary to Theorem 7.16 implies that
\begin{align*}
  \int_{0}^{1} \sum_{n=1}^{\infty} \frac{x}{n(x+n)} dx
  &= \sum_{n=1}^{\infty} \int_{0}^{1} \frac{x}{n(x+n)} dx \\
  &= \sum_{n=1}^{\infty} \int_{0}^{1} \left( \frac{1}{n} - \frac{1}{x+n} \right) dx \\
  &= \sum_{n=1}^{\infty} \left( \frac{1}{n} - \log\frac{n+1}{n} \right) \\
  &= \lim_{N \to \infty}
    \left( \sum_{n=1}^{N} \frac{1}{n} - \log(N+1) \right) \\
  &= \lim_{N \to \infty}
    \left( s_N - \log(N+1) \right)
\end{align*}
exists.
Since $\lim_{N \to \infty}(\log(N+1) - \log N) = 0$,
\begin{align*}
  \gamma
  &= \lim_{N \to \infty}( s_N - \log N ) \\
  &= \lim_{N \to \infty}( s_N - \log(N+1) ) + \lim_{N \to \infty}(\log(N+1) - \log N)
\end{align*}
exists.
\end{enumerate}
$\Box$ \\



\emph{Proof of (a) (Existence of $\int_{1}^{\infty} \frac{\{x\}}{x^2} dx$).}
\begin{enumerate}
\item[(1)]
Define $\{x\} = x - [x]$ where $[x]$ is the greatest integer $\leq x$
(Exercise 6.16).
\emph{Show that
\[
  \int_{1}^{\infty} \frac{\{x\}}{x^2} dx < \infty.
\]}

Use the similar argument in Exercise 6.16(b).
Since
$\frac{\{x\}}{x^2} \leq \frac{1}{x^2}$ on $[1,\infty)$
and $\int_{1}^{\infty} \frac{1}{x^2} dx = 1$ exists,
the result is established (Theorem 6.12(b)).

\item[(2)]
\emph{Show that
\[
  \int_{1}^{N} \frac{[x]}{x^2} dx = s_N - 1.
\]}

Use the similar argument in Exercise 6.16(a),
\begin{align*}
  \int_{1}^{N} \frac{[x]}{x^2} dx
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{[x]}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{k}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \int_{k}^{k+1} \frac{k}{x^2} dx \\
  &= \sum_{k=1}^{N-1} \frac{1}{k+1} \\
  &= \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{N} \\
  &= s_N - 1.
\end{align*}

\textbf{Supplement (Euler's summation formula).}
(Theorem 7.13 in the textbook:
\emph{Tom. M. Apostol, Mathematical Analysis, 2nd edition.})
\emph{If $f$ has a continuous derivative $f'$ on $[a,b]$, then we have
\[
  \sum_{a < n \leq b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\{x\}dx + f(a)\{a\} - f(b)\{b\},
\]
where $\sum_{a < n \leq b}$ means the sum from $n=[a]+1$ to $n=[b]$.
When $a$ and $b$ are integers, this becomes
\[
  \sum_{n=a}^{b} f(n)
  = \int_{a}^{b} f(x)dx
  + \int_{a}^{b} f'(x)\left( \{x\}-\frac{1}{2} \right)dx
  + \frac{f(a)+f(b)}{2}.
\]}
By taking $f(x) = \frac{1}{x}$ we can get the same result. \\

\item[(3)]
\emph{Show that
\[
  \int_{1}^{N} \frac{\{x\}}{x^2} dx = \log N - s_N + 1 = 1 - \gamma_N.
\]}

In fact,
\begin{align*}
  \int_{1}^{N} \frac{\{x\}}{x^2} dx
  &= \int_{1}^{N} \frac{x-[x]}{x^2} dx \\
  &= \int_{1}^{N} \frac{1}{x} dx - \int_{1}^{N} \frac{[x]}{x^2} dx \\
  &= \log N - (s_N - 1) \\
  &= \log N - s_N + 1 \\
  &= 1 - \gamma_N.
\end{align*}

\item[(4)]
Since
\[
  \lim_{N \to \infty} \int_{1}^{N} \frac{\{x\}}{x^2} dx
  = \int_{1}^{\infty} \frac{\{x\}}{x^2} dx
\]
exists (by (1)),
$\gamma = \lim \gamma_N$ exists.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
By $s_n - \log n > 0$ in (a)(4)(Theorem 3.14),
it suffices to choose $N = 10^m$ such that
$s_N \geq \log(N+1) > 100$, or
\[
  m > \frac{\log(\exp(100) - 1)}{\log 10},
\]
or choose $m$ satisfying
\[
  m
  > \frac{100}{\log 10}
  > \frac{\log(\exp(100) - 1)}{\log 10},
\]
or $m = 44$.
$\Box$ \\

\emph{Note.}
The exact value of $N$ is
\[
  15092688622113788323693563264538101449859497 \approx 1.509 \times 10^{43}.
\] \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.10.}
\emph{Prove that $\sum \frac{1}{p}$ diverges; the sum extends over all primes.} \\

There are many proofs of this result. We provide some of them. \\

\emph{Proof (Due to hint).}
Given $N$.
\begin{enumerate}
\item[(1)]
\emph{Show that
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}.
\]}

By the unique factorization theorem on $n \leq N$,
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \prod_{p \leq N} \left( 1 + \frac{1}{p} + \frac{1}{p^2} + \cdots \right)
  = \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}.
\]

\item[(2)]
By (1) and the fact that $\sum \frac{1}{n}$ diverges,
there are infinitely many primes.

\item[(3)]
\emph{Show that
\[
  \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]}

By applying the inequality $(1 - x)^{-1} < e^{2x}$ where $x \in (0, \frac{1}{2}]$
on any prime $p$,
\[
  \left( 1 - \frac{1}{p} \right)^{-1} < \exp \left( \frac{2}{p} \right).
\]
Now multiplying the inequality over all primes $p \leq N$ and noticing that
$\exp(x) \cdot \exp(y) = \exp(x + y)$, we have
\[
  \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]

\item[(4)]
By (1)(3),
\[
  \sum_{n \leq N} \frac{1}{n}
  \leq \exp \left( \sum_{p \leq N} \frac{2}{p} \right).
\]
Since $\sum_{n \leq N} \frac{1}{n}$ diverges, the result holds.
\end{enumerate}
$\Box$ \\



\emph{Proof (Due to Kenneth Ireland and Michael Rosen).}
The proof in Kenneth Ireland and Michael Rosen,
A Classical Introduction to Modern Number Theory, Second Edition (Theorem 3 in Chapter 2)
does not use the inequality $(1 - x)^{-1} < e^{2x}$ ($x \in (0, \frac{1}{2}]$) directly.
Instead, the authors take the logarithm on $(1 - p^{-1})^{-1}$ and estimate it.
(So the length of proof is longer than the proof due to hint.)
That is,
\begin{align*}
- \log(1 - p^{-1})
&= \sum_{n = 1}^{\infty} \frac{p^{-n}}{n} \\
&= \frac{1}{p} + \sum_{n = 2}^{\infty} \frac{p^{-n}}{n} \\
&< \frac{1}{p} + \sum_{n = 2}^{\infty} p^{-n} \\
&= \frac{1}{p} + \frac{p^{-2}}{1 - p^{-1}} \\
&< \frac{1}{p} + 2 \cdot \frac{1}{p^2}.
\end{align*}
Now we sum over all primes $p \leq N$,
$$\log \left( \prod_{p \leq N} \left( 1 - \frac{1}{p} \right)^{-1} \right)
< \sum_{p \leq N} \frac{1}{p} + 2 \sum_{p \leq N} \frac{1}{p^2}.$$
So
$$\log \sum_{n \leq N} \frac{1}{n}
< \sum_{p \leq N} \frac{1}{p} + 2 \sum_{p \leq N} \frac{1}{p^2}.$$
Notice that $\sum \frac{1}{n}$ diverges and $\sum \frac{1}{p^2}$ converges
(since $\sum \frac{1}{n^2}$ converges).
Therefore, $\sum \frac{1}{p}$ diverges.
$\Box$ \\



\emph{Proof (Due to I. Niven).}
It is an exercise in Kenneth Ireland and Michael Rosen,
A Classical Introduction to Modern Number Theory, Second Edition. See Exercise 27 in Chapter 2.

\begin{enumerate}
\item[(1)]
\emph{Show that ${\sum}' \frac{1}{n}$, the sum being over square free integers, diverges.}
For any positive integers $n$, we can write $n = a^2 b$ where $a \in \mathbb{Z}^+$ and
$b$ is a square free integer.
Given $N$,
$$\sum_{n \leq N} \frac{1}{n}
\leq \left(\sum_{a = 1}^{\infty} \frac{1}{a^2} \right)
\left( {\sum_{b \leq N}}' \frac{1}{b} \right).$$
Notice that $\sum_{a = 1}^{\infty} \frac{1}{a^2}$ converges.
Since $\sum_{n \leq N} \frac{1}{n} \rightarrow \infty$ as $N \rightarrow \infty$,
$\sum'_{b \leq N}\frac{1}{b} \rightarrow \infty$ as $N \rightarrow \infty$.

\item[(2)]
\emph{Show that
\[
  \prod_{p \leq N} ( 1 + \frac{1}{p} ) \rightarrow \infty \text{ as } N \rightarrow \infty.
\]}

By the unique factorization theorem on $n \leq N$,
$$\prod_{p \leq N} \left( 1 + \frac{1}{p} \right)
\geq {\sum_{n \leq N}}' \frac{1}{n}.$$
Since ${\sum_{n \leq N}}' \frac{1}{n} \rightarrow \infty$ as $N \rightarrow \infty$ by (1),
the conclusion is established.

\item[(3)]
By applying the inequality $e^x > 1 + x$ on any prime $p$,
$$\exp\left(\frac{1}{p}\right) > 1 + \frac{1}{p}.$$
Now multiplying the inequality over all primes $p \leq N$ and noticing that
$\exp(x) \cdot \exp(y) = \exp(x + y)$, we have
$$\exp\left(\sum_{p \leq N} \frac{1}{p} \right)
> \prod_{p \leq N} \left( 1 + \frac{1}{p} \right).$$
By (2),
$\exp\left(\sum_{p \leq N} \frac{1}{p} \right) \rightarrow \infty$ as $N \rightarrow \infty$, or
$\sum_{p \leq N} \frac{1}{p} \rightarrow \infty$ as $N \rightarrow \infty$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.11.}
\emph{Suppose $f \in \mathscr{R}$ on $[0,A]$ for all $A < \infty$,
and $f(x) \to 1$ as $x \to +\infty$.
Prove that
\[
  \lim_{t \to 0} t \int_{0}^{\infty} e^{-tx}f(x)dx = 1
  \:\:\:\:\:\:\:\: (t > 0),
\]} \\

It is similar to Exercise 3.14(a). \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
The integral $\int_{0}^{\infty} e^{-tx}f(x)dx$ is well-defined.
(It suffices to show that $\int_{0}^{\infty} e^{-tx}f(x)dx$
converges absolutely in the sense of Exercise 6.8.
It is quite easy since $f(x) \to 1$ as $x \to +\infty$
and well-behavior of $\int_{A_0}^{\infty} e^{-tx}f(x)dx$ for any $A_0 > 0$.)

\item[(2)]
Note that
\[
  t \int_{0}^{\infty} e^{-tx} dx = 1
\]
for any $t > 0$.

\item[(3)]
Since $f(x) \to 1$ as $x \to +\infty$,
there is $A_0 > 0$ such that
\[
  |f(x) - 1| < \frac{\varepsilon}{64} \text{ whenever } x \geq A_0.
\]

\item[(4)]
Since $f \in \mathscr{R}$ on $[0,A_0]$,
$f$ is bounded on $[0,A_0]$,
or $|f| \leq M$ on $[0,A_0]$ for some $M$ (Theorem 6.7(c)).

\item[(5)]
As $t > 0$,
\begin{align*}
  &\abs{\left( t \int_{0}^{\infty} e^{-tx}f(x)dx \right) - 1} \\
  =& \abs{t \int_{0}^{\infty} e^{-tx}(f(x)-1)dx }
    &((2)) \\
  \leq& t \int_{0}^{\infty} e^{-tx}|f(x)-1|dx
    &\text{((1) with Theorem 6.13)} \\
  =& t \int_{0}^{A_0} e^{-tx}|f(x)-1|dx
    + t \int_{A_0}^{\infty} e^{-tx}|f(x)-1|dx \\
  \leq& t \int_{0}^{A_0}(M + 1) dx + t \int_{A_0}^{\infty} e^{-tx}|f(x)-1|dx
    &\text{((3) and $e^{-tx} \leq 1$)} \\
  \leq& t \int_{0}^{A_0}(M + 1) dx + t \int_{A_0}^{\infty} e^{-tx} \frac{\varepsilon}{64} dx
    &\text{((4))} \\
  =& t A_0 (M + 1) + \exp(-A_0t) \frac{\varepsilon}{64} \\
  \leq& t A_0 (M + 1) + \frac{\varepsilon}{64}.
    &\text{($e^{-tx} \leq 1$)}
\end{align*}
Since $t$ is arbitrary, take $t = \frac{\varepsilon}{89 A_0 (M+1)} > 0$ to get
\[
  \abs{\left( t \int_{0}^{\infty} e^{-tx}f(x)dx \right) - 1}
  < \frac{\varepsilon}{89} + \frac{\varepsilon}{64}
  < \varepsilon,
\]
or
\[
  \lim_{t \to 0^+} t \int_{0}^{\infty} e^{-tx}f(x)dx = 1.
\]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.12.}
\emph{Suppose $0 < \delta < \pi$,
\begin{equation*}
  f(x) =
    \begin{cases}
      1 & \text{ if } |x| \leq \delta, \\
      0 & \text{ if } \delta < |x| \leq \pi,
    \end{cases}
\end{equation*}
and $f(x + 2\pi) = f(x)$ for all $x$.}

\begin{enumerate}
\item[(a)]
\emph{Compute the Fourier coefficients of $f$.}

\item[(b)]
\emph{Compute that
$$\sum_{n = 1}^{\infty} \frac{\sin(n\delta)}{n} = \frac{\pi - \delta}{2}
\:\:\:\:\:\:\:\:
(0 < \delta < \pi).$$}

\item[(c)]
\emph{Deduce from Parseval's theorem that
$$\sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \delta} = \frac{\pi - \delta}{2}.$$}

\item[(d)]
\emph{Let $\delta \rightarrow 0$ and prove that
$$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
= \frac{\pi}{2}.$$}

\item[(e)]
\emph{Put $\delta = \frac{\pi}{2}$ in (c). What do you get?} \\
\end{enumerate}

It is a centered square pulse around $x = 0$ with shift $\delta$.
Besides, $f(x)$ is an even function. \\

\emph{Proof of (a).}
\begin{align*}
c_0
&= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx \\
&= \frac{1}{2 \pi} \int_{-\delta}^\delta dx \\
&= \frac{\delta}{\pi}.
\end{align*}
For $0 \neq n \in \mathbb{Z}$,
\begin{align*}
c_n
&= \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) e^{-inx} dx \\
&= \frac{1}{2 \pi} \int_{-\delta}^\delta e^{-inx} dx \\
&= \frac{1}{2 \pi} \cdot \frac{2 \sin(n \delta)}{n} \\
&= \frac{\sin(n \delta)}{n \pi}.
\end{align*}
$\Box$ \\

\textbf{Supplement.} Find $a_n$ and $b_n$ of this textbook. \\
By (a), $a_0 = \frac{\delta}{\pi}$,
$a_n = \frac{2 \sin(n \delta)}{n \pi}$, $b_n = 0$ for $n \in \mathbb{Z}^+$.
Surely, we can compute $a_n$ and $b_n$ ($n > 0$) directly.
Since $f(x)$ is an even function, $b_n = 0$.
And
\begin{align*}
a_n
&= \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(nx) dx \\
&= \frac{2}{\pi} \int_{0}^\delta \cos(nx) dx \\
&= \frac{2 \sin(n \delta)}{n \pi}.
\end{align*}

\emph{Proof of (b).}
Given $x = 0$, there are constants $\delta' = \delta > 0$ and $M = 1 < \infty$ such that
$$|f(0 + t) - f(0)| \leq M|t|$$ for all $t \in (-\delta', \delta')$.
By Theorem 8.14,
$$\sum_{-\infty}^{\infty} c_n = f(0).$$
Notice that $c_{-n} = c_n$ for $n \in \mathbb{Z}^+$, so
\begin{align*}
\frac{\delta}{\pi} + 2 \sum_{n = 1}^{\infty} \frac{\sin(n \delta)}{n \pi}
&= 1 \\
\sum_{n = 1}^{\infty} \frac{\sin(n \delta)}{n}
&= \frac{\pi - \delta}{2}.
\end{align*}
$\Box$ \\

We can also use the expression $a_n$ and $b_n$ to prove the same thing.
Besides, taking $\delta = 1$ yields
$$\sum_{n = 1}^{\infty} \frac{\sin n}{n} = \frac{\pi - 1}{2}.$$ \\

\emph{Proof of (c).}
Since $f(x)$ is a Riemann-integrable function with period $2 \pi$,
by Parseval's theorem
$$\frac{1}{2 \pi} \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{-\infty}^{\infty} |c_n|^2.$$
So
$$\frac{\delta}{\pi}
= \frac{\delta^2}{\pi^2} + 2 \sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \pi^2}, $$
or
$$\sum_{n = 1}^{\infty} \frac{(\sin(n\delta))^2}{n^2 \delta}
= \frac{\pi - \delta}{2}.$$
$\Box$ \\

Notices that
$$\sum_{n = 1}^{\infty} \frac{(\sin n)^2}{n^2} = \frac{\pi - 1}{2}$$
as $\delta = 1$. \\

\emph{Proof of (d).}
Given $\varepsilon > 0$.
By Exercise 6.8,
$$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx$$ exists.
So there exists $b > 0$ such that
$$\left| \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
- \int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
\right| < \frac{\varepsilon}{4}$$

By Supplement in Chapter 6, there exists
$\delta > 0$ such that for any partition
$P_m = \{ 0, \frac{b}{m}, \frac{2b}{m}, \ldots, \frac{(m - 1)b}{m}, b\}$ of $[0, b]$
with $\Vert P \Vert = \frac{b}{m} < \delta$, or $m > \frac{b}{\delta}$,
we have
\begin{align*}
\left|
\sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{(n \frac{b}{m})^2} \cdot \frac{b}{m}
- \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
\right|
&< \frac{\varepsilon}{4}, \\
\left|
\sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
- \int_{0}^{b} \left( \frac{\sin x}{x} \right)^2 dx
\right| &< \frac{\varepsilon}{4}.
\end{align*}
For simplicity we resize $\delta$ to $\delta < \pi$ to make $0 < \frac{b}{m} < \delta < \pi$.
Besides, since $\sum_{n = 1}^{\infty} \frac{1}{n^2}$ converges,
there exists $N > 0$ such that
$$\left|
\sum_{n = 1}^{\infty} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
- \sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
\right|
< \frac{\varepsilon}{4}$$
whenever $m \geq N$.
By (c),
$$\left|
\frac{\pi - \frac{b}{m}}{2}
- \sum_{n = 1}^{m} \frac{(\sin(n \frac{b}{m}))^2}{n^2 \frac{b}{m}}
\right|
< \frac{\varepsilon}{4}$$
whenever $m \geq N$.
Last, it is easy to get
$$\left|
\frac{\pi}{2}
- \frac{\pi - \frac{b}{m}}{2}
\right|
< \frac{\varepsilon}{4}$$
whenever $m > \frac{2b}{\varepsilon}$.
Now we have
$$\left|
\frac{\pi}{2}
- \int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
\right|
< \varepsilon
$$
whenever $m > \max(\frac{b}{\delta}, N, \frac{2b}{\varepsilon})$.
Since $\varepsilon$ is arbitrary,
$\int_{0}^{\infty} \left( \frac{\sin x}{x} \right)^2 dx
= \frac{\pi}{2}$.
$\Box$ \\


\emph{Proof of (e).}
$$\sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} = \frac{\pi^2}{8}.$$
Write
\begin{align*}
\sum_{n = 1}^{\infty} \frac{1}{n^2}
&= \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} + \sum_{n = 1}^{\infty} \frac{1}{(2n)^2} \\
&= \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2} + \frac{1}{4} \sum_{n = 1}^{\infty} \frac{1}{n^2},
\end{align*}
so
$$\sum_{n = 1}^{\infty} \frac{1}{n^2}
= \frac{4}{3} \sum_{n = 1}^{\infty} \frac{1}{(2n - 1)^2}
= \frac{\pi^2}{6}.$$
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.13.}
\emph{Put $f(x) = x$ if $0 \leq x < 2 \pi$, and apply Parseval's theorem to conclude that
$$\sum_{n = 1}^{\infty} \frac{1}{n^2} = \frac{\pi}{6}.$$}
\emph{Proof.}
\begin{align*}
c_0
&= \frac{1}{2 \pi} \int_{0}^{2 \pi} x dx \\
&= \pi,
\end{align*}
For $n \neq 0$,
\begin{align*}
c_n
&= \frac{1}{2 \pi} \int_{0}^{2 \pi} x e^{-inx} dx \\
&= \frac{1}{2 \pi} \left(
\left[ - \frac{1}{i n} x e^{-inx} \right]_{x = 0}^{x = 2 \pi}
- \int_{0}^{2 \pi} - \frac{1}{i n} e^{-inx} dx \right) \\
&= \frac{i}{n}.
\end{align*}
Since $f(x)$ is a Riemann-integrable function with period $2 \pi$,
by Parseval's theorem
$$\frac{1}{2 \pi} \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{-\infty}^{\infty} |c_n|^2.$$
So
$$\frac{1}{2 \pi} \cdot \frac{(2 \pi)^3}{3}
= \pi^2 + 2 \sum_{n = 1}^{\infty} \frac{1}{n^2}, $$
or
$$\sum_{n = 1}^{\infty} \frac{1}{n^2}
= \frac{\pi^2}{6}.$$
$\Box$ \\



\textbf{Supplement.} \emph{
Put $f(x) = x^n$ if $n \in \mathbb{Z}^+$ and $0 \leq x < 2 \pi$.
Might get
\[
  \zeta(2n)
  = \sum_{k = 1}^{\infty} \frac{1}{k^{2n}}
  = \frac{(-1)^{n+1} B_{2n} (2\pi)^{2n}}{2(2n)!}.
\]} \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.14.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.15.}
\emph{With the Dirichlet kernel $D_n$ as defined by
\[
  D_n(x)
  = \sum_{k=-n}^{n} \exp(ikx)
  = \frac{\sin(n+\frac{1}{2})x}{\sin(\frac{x}{2})},
\]
put the \textbf{Fej\'er kernel}
\[
  K_N(x) = \frac{1}{N+1} \sum_{n=0}^{N} D_n(x).
\]
Prove that
\[
  K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}
\]
and that}
\begin{enumerate}
  \item[(a)]
  $K_N \geq 0$,

  \item[(b)]
  $\frac{1}{2\pi} \int_{-\pi}^{\pi} K_N(x) dx = 1$,

  \item[(c)]
  \emph{$K_N(x) \leq \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta}$
  if $0 < \delta \leq |x| \leq \pi$.}
\end{enumerate}

\emph{If $s_N = s_N(f;x)$ is the $N$th partial sum of the Fourier series of $f$,
consider the arithmetic means
\[
  \sigma_N = \frac{s_0+s_1+\cdots+s_N}{N+1}.
\]
Prove that
\[
  s_N(f;x) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)K_N(t)dt,
\]
and hence prove \textbf{Fej\'er's theorem:}}
\begin{quote}
  \emph{If $f$ is continuous, with period $2\pi$,
  then $\sigma_N(f;x) \to f(x)$ uniformly on $[-\pi,\pi]$.}
\end{quote}
\emph{(Hint: Use properties (a),(b),(c) to proceed as in Theorem 7.26.)} \\



\emph{Proof of $K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}$.}
Since
  \begin{align*}
    (1-\cos x) K_N(x)
    =& 2 \left( \sin \frac{x}{2} \right)^2 \frac{1}{N+1}
      \sum_{n=0}^{N} \frac{\sin(n+\frac{1}{2})x}{\sin(\frac{x}{2})} \\
    =& \frac{1}{N+1}
      \sum_{n=0}^{N} 2 \sin \frac{x}{2} \sin(n+\frac{1}{2})x \\
    =& \frac{1}{N+1}
      \sum_{n=0}^{N} (\cos(nx) - \cos(n+1)x) \\
    =& \frac{1 - \cos(N+1)x}{N+1},
  \end{align*}
\[
  K_N(x) = \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x}
\]
if $x \neq 2k\pi$ for $k \in \mathbb{Z}$.
$\Box$ \\

\emph{Proof of (a).}
It is clear since $\cos x \leq 1$ for all $x \in \mathbb{R}$.
Or we may write
\[
  K_N(x) = \frac{1}{N+1} \left( \frac{\sin \frac{(N+1)x}{2}}{\sin \frac{x}{2}} \right)^2 \geq 0.
\]
$\Box$ \\

\emph{Proof of (b).}
By the definition of $D_n(x)$,
  \begin{align*}
     \frac{1}{2\pi} \int_{-\pi}^{\pi} K_N(x) dx
     =& \frac{1}{2\pi} \int_{-\pi}^{\pi} \frac{1}{N+1} \sum_{n=0}^{N} D_n(x) dx \\
     =& \frac{1}{N+1} \sum_{n=0}^{N} \frac{1}{2\pi} \int_{-\pi}^{\pi} D_n(x) dx \\
     =& \frac{1}{N+1} \sum_{n=0}^{N} 1 \\
     =& 1.
  \end{align*}
$\Box$ \\

\emph{Proof of (c).}
Since $\cos x$ is bounded by $1$ and monotonically decreasing on $(0, \pi]$,
  \begin{align*}
    K_N(x)
    =& \frac{1}{N+1} \cdot \frac{1-\cos(N+1)x}{1-\cos x} \\
    \leq& \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta}.
  \end{align*}
$\Box$ \\

\emph{Proof of $s_N(f;x) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)K_N(t)dt$.}
  \begin{align*}
    \sigma_N(f;x)
    &= \frac{1}{N+1} \sum_{n=0}^{N} s_N(f;x) \\
    &= \frac{1}{N+1} \sum_{n=0}^{N} \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)D_N(t)dt \\
    &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t)
      \left( \frac{1}{N+1} \sum_{n=0}^{N} D_N(t) \right) dt \\
    &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t) K_N(t) dt.
  \end{align*}
$\Box$ \\

\emph{Proof of Fej\'er's theorem.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
  \begin{align*}
    \abs{ \sigma_N(f;x) - f(x) }
    =& \abs{ \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t) K_N(t) dt
      - \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) K_N(t) dt } \\
    =& \abs{ \frac{1}{2\pi} \int_{-\pi}^{\pi} (f(x-t)-f(x)) K_N(t) dt } \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{\pi} |f(x-t)-f(x)| K_N(t) dt.
  \end{align*}

\item[(2)]
Since $f$ is continuous on a compact set $[-\pi,\pi]$, $f$ is continuous uniformly.
For such $\varepsilon > 0$, there exists $\delta > 0$
such that
\[
  |f(y) - f(x)| < \frac{\varepsilon}{2}
\]
whenever $x,y \in [-\pi,\pi]$ and $|y-x| < \delta$.

\item[(3)]
Since $f$ is continuous on a compact set $[-\pi,\pi]$, $f$ is bounded
on $[-\pi,\pi]$, say $M = \sup|f(x)|$.

\item[(4)]
Therefore,
  \begin{align*}
    &\abs{ \sigma_N(f;x) - f(x) } \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{\pi} |f(x-t)-f(x)| K_N(t) dt \\
    =& \frac{1}{2\pi} \int_{-\pi}^{-\delta} |f(x-t)-f(x)| K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{-\delta}^{\delta} |f(x-t)-f(x)| K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi} |f(x-t)-f(x)| K_N(t) dt \\
    \leq& \frac{1}{2\pi} \int_{-\pi}^{-\delta}
        2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
      &+ \frac{1}{2\pi} \int_{-\delta}^{\delta} \frac{\varepsilon}{2} K_N(t) dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi}
        2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
    =& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi}
      + \frac{\varepsilon}{2} \cdot \frac{1}{2\pi} \int_{-\delta}^{\delta} K_N(t) dt \\
    \leq& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi} + \frac{\varepsilon}{2}.
  \end{align*}

\item[(5)]
Since $N$ is arbitrary,
we can take an integer
$N > \frac{4M(\pi-\delta)}{(1-\cos\delta)\pi\varepsilon} - 1$
so that
  \begin{align*}
    \abs{ \sigma_N(f;x) - f(x) }
    \leq& \frac{4M (\pi - \delta)}{(N+1)(1 - \cos \delta) \pi} + \frac{\varepsilon}{2} \\
    <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    =& \varepsilon.
  \end{align*}
Therefore, the conclusion holds.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.16.}
\emph{Prove a pointwise version of Fej\'er's theorem:
If $f \in \mathscr{R}$ and $f(x+)$, $f(x-)$ exist for some $x$, then
\[
  \lim_{N \to \infty} \sigma_N(f;x) = \frac{1}{2}[f(x+)+f(x-)].
\]
} \\

\emph{Proof.}
Given any $\varepsilon > 0$.
\begin{enumerate}
\item[(1)]
Since $K_N(-x) = K_N(x)$, we have
\[
  \sigma_N(f;x)
  = \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt
    + \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_N(t)dt
\]
and
\[
  \frac{1}{2\pi} \int_{0}^{\pi} K_N(t)dt = \frac{1}{2}.
\]

\item[(2)]
Since $f \in \mathscr{R}$,
$f$ is bounded on $[-\pi,\pi]$, say $M = \sup|f(x)|$.

\item[(3)]
Therefore,
  \begin{align*}
    &\abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt - \frac{1}{2} f(x-) } \\
    =& \abs{ \frac{1}{2\pi} \int_{0}^{\pi} (f(x-t)-f(x-))K_N(t)dt } \\
    \leq& \frac{1}{2\pi} \int_{0}^{\pi} |f(x-t)-f(x-)|K_N(t)dt.
  \end{align*}
Since $f(x-)$ exists,
for fixed $\varepsilon > 0$, there exists $\delta > 0$
such that
\[
  |f(y) - f(x-)| < \frac{\varepsilon}{2}
\]
whenever $y \in (x-\delta,x) \cap [-\pi,\pi]$.
Hence,
  \begin{align*}
    &\abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_N(t)dt - \frac{1}{2} f(x-) } \\
    \leq& \frac{1}{2\pi} \int_{0}^{\pi} |f(x-t)-f(x-)|K_N(t)dt \\
    =& \frac{1}{2\pi} \int_{0}^{\delta} |f(x-t)-f(x-)|K_N(t)dt \\
      &+ \frac{1}{2\pi} \int_{\delta}^{\pi} |f(x-t)-f(x-)|K_N(t)dt \\
    \leq& \frac{1}{2\pi} \int_{0}^{\delta} \frac{\varepsilon}{2} K_N(t)dt
      + \frac{1}{2\pi} \int_{\delta}^{\pi} 2M \cdot \frac{1}{N+1} \cdot \frac{2}{1-\cos \delta} dt \\
    =& \frac{\varepsilon}{2} \cdot \frac{1}{2\pi} \int_{0}^{\delta} K_N(t)dt
      + \frac{2M(\pi-\delta)}{(N+1)(1-\cos \delta)\pi} \\
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(N+1)(1-\cos \delta)\pi}. \\
  \end{align*}

\item[(4)]
Since $N$ is arbitrary,
we can take an integer
$N_1 > \frac{8M(\pi-\delta)}{(1-\cos\delta)\pi\varepsilon} - 1$
such that
  \begin{align*}
    \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_{n}(t)dt - \frac{1}{2} f(x-) }
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(n+1)(1-\cos \delta)\pi} \\
    <& \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
    =& \frac{\varepsilon}{2}
  \end{align*}
whenever $n \geq N_1$.
Similarly,
we can take an integer $N_2$ such that
  \begin{align*}
    \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_{n}(t)dt - \frac{1}{2} f(x+) }
    \leq& \frac{\varepsilon}{4} + \frac{2M(\pi-\delta)}{(n+1)(1-\cos \delta)\pi} \\
    <& \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
    =& \frac{\varepsilon}{2}.
  \end{align*}
whenever $n \geq N_2$.

\item[(5)]
Hence,
  \begin{align*}
    &\abs{ \sigma_N(f;x) - \frac{1}{2}[f(x+)+f(x-)] } \\
    \leq& \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x-t)K_{n}(t)dt - \frac{1}{2} f(x-) } \\
      &+ \abs{ \frac{1}{2\pi} \int_{0}^{\pi} f(x+t)K_{n}(t)dt - \frac{1}{2} f(x+) } \\
    <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    =& \varepsilon.
  \end{align*}
whenever $n \geq \max\{N_1,N_2\}$.
Hence, $\lim \sigma_N(f;x) = \frac{1}{2}[f(x+)+f(x-)]$.
\end{enumerate}
$\Box$ \\

\textbf{Supplement.} Poisson's equation.
(Theorem 1 of Section 2.2 in the textbook:
\emph{Lawrence C. Evans, Partial Differential Equations.})
\emph{Let the fundamental solution of Laplace's equation be
\begin{equation*}
  \Phi(x) =
    \begin{cases}
      -\frac{1}{2\pi} \log|x|                       & (n = 2) \\
      \frac{1}{n(n-2)\alpha(n)} \frac{1}{|x|^{n-2}} & (n \geq 3),
    \end{cases}
\end{equation*}
where $x \in \mathbb{R}^n$, $x \neq 0$.
Let
\[
  u(x) = \int_{\mathbb{R}^n} \Phi(x-y)f(y) dy.
\]
Then $-\Delta u = f$ in $\mathbb{R}^n$.}
Note that $\Phi(x)$ blows up at $0$.
To calculate $\Delta u(x)$,
we need to isolate this singularity inside a small ball, say $B(0;\varepsilon)$.
Therefore,
\[
  \Delta u(x)
  = \int_{B(0;\varepsilon)} \Phi(y) \Delta_x f(x-y)dy
    + \int_{\mathbb{R}^n - B(0;\varepsilon)} \Phi(y) \Delta_x f(x-y)dy,
\]
and we can continue estimating two integrals individually as the textbook did. \\\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.17.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.18.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.19.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.20.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.21.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.22.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.23.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.24.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.25.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.26.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.27.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.28.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.29.}
PLACEHOLDER. \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.30.}
\emph{Use Stirling's formula to prove that
\[
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)} = 1
\]
for every real constant $c$.} \\

\emph{Proof.}
By Stirling's formula,
\begin{align*}
  \lim_{x \to \infty}
  \frac{\Gamma(x+c)}{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}
  &= 1 \\
  \lim_{x \to \infty}
  \frac{\Gamma(x)}{\left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}}
  &= 1,
\end{align*}
we have
\begin{align*}
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)}
  =&
  \lim_{x \to \infty} \frac{\Gamma(x+c)}{x^c \Gamma(x)} \\
  &\times
  \lim_{x \to \infty}
  \frac{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}{\Gamma(x+c)} \\
  &\times
  \lim_{x \to \infty}
  \frac{\Gamma(x)}{\left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}} \\
  =&
  \lim_{x \to \infty} \frac{\left( \frac{x+c-1}{e} \right)^{x+c-1} \sqrt{2\pi(x+c-1)}}
    {x^c \left( \frac{x-1}{e} \right)^{x-1} \sqrt{2\pi(x-1)}} \\
  =&
  \lim_{x \to \infty}
  \frac{\left( \frac{x+c-1}{e} \right)^{c}}{x^c}
  \frac{\left( \frac{x+c-1}{e} \right)^{x-1}}{\left( \frac{x-1}{e} \right)^{x-1}}
  \sqrt{\frac{x+c-1}{x-1}} \\
  =& \frac{1}{e^c} \cdot e^c \cdot 1 \\
  =& 1
\end{align*}
since
\begin{enumerate}
\item[(1)]
\[
  \lim_{x \to \infty}
    \frac{\left( \frac{x+c-1}{e} \right)^{c}}{x^c}
  = \frac{1}{e^c} \lim_{x \to \infty} \left( \frac{x+c-1}{x} \right)^{c}
  = \frac{1}{e^c}.
\]

\item[(2)]
\[
  \lim_{x \to \infty}
    \frac{\left( \frac{x+c-1}{e} \right)^{x-1}}{\left( \frac{x-1}{e} \right)^{x-1}}
  = \lim_{x \to \infty} \left( \frac{x+c-1}{x-1} \right)^{x-1}
  = \lim_{x \to \infty} \left( 1+\frac{c}{x-1} \right)^{x-1}
  = e^c.
\]

\item[(3)]
and
\[
  \lim_{x \to \infty} \sqrt{\frac{x+c-1}{x-1}}
  = \lim_{x \to \infty} \sqrt{1+\frac{c}{x-1}} = 1.
\]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 8.31.}
\emph{In the proof of Theorem 7.26 it was shown that
\[
  \int_{-1}^{1} (1-x^2)^n dx \geq \frac{4}{3\sqrt{n}}
\]
for $n=1,2,3,\ldots$.
Use Theorem 8.20 and Exercise 8.30 to show the more precise result
\[
\lim_{n \to \infty} \sqrt{n} \int_{-1}^{1} (1-x^2)^n dx = \sqrt{\pi}.
\]} \\

\emph{Proof.}
\begin{align*}
  &\lim_{n \to \infty} \sqrt{n} \int_{-1}^{1} (1-x^2)^n dx \\
  =& \lim_{n \to \infty} \sqrt{n} \int_{0}^{1} u^{-\frac{1}{2}} (1-u)^n dx
    &(u = x^2) \\
  =& \lim_{n \to \infty} \sqrt{n}
    \frac{\Gamma\left( \frac{1}{2} \right) \Gamma(n+1)}{\Gamma\left( n+\frac{3}{2} \right)}
    &\text{(Theorem 8.20)} \\
  =& \Gamma\left( \frac{1}{2} \right) \lim_{n \to \infty}
    \frac{n^{\frac{1}{2}} \Gamma(n+1)}{\Gamma\left( n+\frac{3}{2} \right)} \\
  =& \Gamma\left( \frac{1}{2} \right)
    &\text{(Exercise 8.30)} \\
  =& \sqrt{\pi}.
    &\text{(Some consequences 8.21)}
\end{align*}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}