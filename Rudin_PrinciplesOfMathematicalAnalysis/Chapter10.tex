\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt

\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\begin{document}

\textbf{\Large Chapter 10: Integration of Differential Forms} \\\\



\emph{Author: Meng-Gen Tsai} \\
\emph{Email: plover@gmail.com} \\\\



% http://pages.cs.wisc.edu/~wentaowu/other-docs/POMA_Solution_Sheet.pdf
% https://linearalgebras.com/baby-rudin-chapter-10.html
% https://www.researchgate.net/publication/248817777_Partitions_of_Unity_for_Countable_Covers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.1.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.2.}
\emph{For $i=1,2,3,\ldots$, let $\varphi_i \in \mathscr{C}(\mathbb{R}^1)$ have support
in $(2^{-i},2^{1-i})$, such that $\int \varphi_i = 1$.
Put
\[
  f(x,y) = \sum_{i=1}^{\infty}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
\]
Then $f$ has compact support in $\mathbb{R}^2$,
$f$ is continuous except at $(0,0)$,
and
\[
  \int dy \int f(x,y) dx = 0
  \qquad
  \text{ but }
  \qquad
  \int dx \int f(x,y) dy = 1.
\]
Observe that $f$ is unbounded in every neighborhood of $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.3.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.4.}
\emph{For $(x,y) \in \mathbb{R}^2$, define
\[
  \mathbf{F}(x,y) = (e^x \cos y - 1, e^x \sin y)
\]
Prove that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$, where
\begin{align*}
  \mathbf{G}_1(x,y) &= (e^x \cos y - 1, y) \\
  \mathbf{G}_2(u,v) &= (u, (1+u) \tan v)
\end{align*}
are primitive in some neighborhood of $(0,0)$.
Compute the Jacobians of $\mathbf{G}_1$, $\mathbf{G}_2$, $\mathbf{F}$ at $(0,0)$.
Define
\[
  \mathbf{H}_2(x,y) = (x, e^x \sin y)
\]
and find
\[
  \mathbf{H}_1(u,v) = (h(u,v),v)
\]
so that $\mathbf{F} = \mathbf{H}_1 \circ \mathbf{H}_2$
is in some neighborhood of $(0,0)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Definition 10.5,
  \begin{align*}
    \mathbf{G}_1(x,y) &= (e^x \cos y - 1) \mathbf{e}_1 + y \mathbf{e}_2, \\
    \mathbf{G}_2(u,v) &= u \mathbf{e}_1 + ((1+u) \tan v) \mathbf{e}_2
  \end{align*}
  are primitive in some neighborhood of $(0,0)$.

\item[(2)]
  \emph{Show that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$.}
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{G}_2 \circ \mathbf{G}_1)(x,y)
    &= \mathbf{G}_2(\mathbf{G}_1(x,y)) \\
    &= \mathbf{G}_2(e^x \cos y - 1, y) \\
    &= (e^x \cos y - 1, (1+(e^x \cos y - 1)) \tan y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\item[(3)]
  Since
  \begin{align*}
    J_{\mathbf{G}_1}(x,y)
    &=
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      0 & 1
    \end{bmatrix} \\
    J_{\mathbf{G}_2}(x,y)
    &=
    \begin{bmatrix}
      1 & 0 \\
      \tan y & (1+x)\sec^2 y
    \end{bmatrix} \\
    J_{\mathbf{F}}(x,y)
    &=
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y & e^x \cos y
    \end{bmatrix},
  \end{align*}
  \begin{align*}
    J_{\mathbf{G}_1}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix} \\
     J_{\mathbf{G}_2}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix} \\
     J_{\mathbf{F}}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}.
  \end{align*}

\item[(4)]
  Define $h(u,v) = \sqrt{e^{2u} - v^{2}} - 1$ on
  \[
    B\left((0,0);\frac{1}{64}\right) \subseteq \mathbb{R}^2.
  \]
  $h(u,v)$ is well-defined since $e^{2u}-v^2 > 0$
  for all $(u,v) \in B\left((0,0);\frac{1}{64}\right)$.

\item[(5)]
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{H}_1 \circ \mathbf{H}_2)(x,y)
    &= \mathbf{H}_1(\mathbf{H}_2(x,y)) \\
    &= \mathbf{H}_1(x, e^x \sin y) \\
    &= (\sqrt{e^{2x} - (e^x \sin y)^2} - 1, e^x \sin y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.5.}
\emph{Formulate and prove an analogue of Theorem 10.8,
in which $K$ is a compact subset of an arbitrary metric space.
(Replace the functions $\varphi_i$ that occur in the proof of Theorem 10.8
by functions of the type constructed in Exercise 4.22.)} \\

\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  \emph{(Partitions of unity.)
  Suppose $K$ is a compact subset of a metric space $X$,
  and $\{V_{\alpha}\}$ is an open cover of $K$.
  Then there exist function $\psi_1, \ldots, \psi_s \in \mathscr{C}(X)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_i \leq 1$ for $1 \leq i \leq s$.}

  \item[(b)]
    \emph{each $\psi_i$ has its support in some $V_{\alpha}$, and}

  \item[(c)]
    \emph{$\psi_1(x) + \cdots + \psi_s(x) = 1$ for every $x \in K$.}
  \end{enumerate}

\item[(2)]
  It is trivial that some $V_{\alpha} = X$
  by taking $s = 1$ and $\psi_1(x) = 1 \in \mathscr{C}(X)$.
  Now we assume that all $V_{\alpha} \subsetneq X$.

\item[(3)]
  Associate with each $x \in K$ an index $\alpha(x)$ so that $x \in V_{\alpha(x)}$.
  Then there are open balls $B(x)$ and $W(x)$, centered at $x$,
  with
  \[
    x
    \in B(x)
    \subseteq \overline{B(x)}
    \subseteq W(x)
    \subseteq \overline{W(x)}
    \subseteq V_{\alpha(x)}
  \]
  (Since $V_{\alpha(x)}$ is open, there exists $r > 0$
  such that $B(x;r) \subseteq V_{\alpha(x)}$.
  Take $B(x) = B\left(x;\frac{r}{89}\right)$
  and $W(x) = B\left(x;\frac{r}{64}\right)$.)

\item[(4)]
  Since $K$ is compact, there are finitely many points
  $x_1, \ldots, x_s \in K$ such that
  \[
    K \subseteq B(x_1) \cup \cdots \cup B(x_s).
  \]
  Note that
  \begin{enumerate}
  \item[(a)]
    $\overline{B(x_i)}$ is a nonempty closed set since $x_i \in B(x_i) \subseteq \overline{B(x_i)}$.

  \item[(b)]
    $X - W(x_i) \supseteq X - V_{\alpha(x_i)}$ is a nonempty closed set by the assumption in (2).

  \item[(c)]
    $\overline{B(x_i)} \cap (X - W(x_i)) \subseteq W(x_i) \cap (X - W(x_i)) = \varnothing$.

  \end{enumerate}
  By Exercise 4.22, there is a function
  \[
    \varphi_i(x)
    = \frac{\rho_{\overline{B(x_i)}}(x)}{\rho_{\overline{B(x_i)}}(x) + \rho_{X-W(x_i)}(x)}
    \in \mathscr{C}(X)
  \]
  such that $\varphi_i(x) = 1$ on $\overline{B(x_i)}$,
  $\varphi_i(x) = 0$ outside $W(x_i)$, and $0 \leq \varphi_i(x) \leq 1$ on $X$
  for $1 \leq i \leq s$.

\item[(5)]
  Define $\psi_{1} = \varphi_{1}$ and
  \[
    \psi_{i+1} = (1-\varphi_{1}) \cdots (1-\varphi_{i})\varphi_{i+1}
  \]
  for $1 \leq i \leq s-1$.
  Properties (a) and (b) in (1) are clear.
  Also,
  \[
    \psi_1(x) + \cdots + \psi_s(x) = 1 - (1-\varphi_1(x)) \cdots (1-\varphi_s(x))
  \]
  by the construction of $\psi_i$.
  If $x \in K$, then $x \in B(x_i)$ for some $i$, hence $\varphi_i(x)=1$,
  and the product $(1-\varphi_1(x)) \cdots (1-\varphi_s(x)) = 0$.
  This proves property (c) in (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.6.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.7.}
\begin{enumerate}
\item[(a)]
  \emph{Show that the simplex $Q^k$ is the smallest convex subset of $\mathbb{R}^k$
  such that contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}

\item[(b)]
  \emph{Show that affine mappings take convex sets to convex sets.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $Q^k$ contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}
  Recall
  \[
    Q^k = \{ (x_1,\ldots,x_k) \in \mathbb{R}^k :
      x_1 + \cdots + x_k \leq 1 \text{ and }
      x_1, \ldots, x_k \geq 0 \}
  \]
  (Example 10.14).
  Hence $\mathbf{0} = (0,\ldots,0) \in Q^k$ and
  \[
    \mathbf{e}_i = (0,\ldots,\underbrace{1}_{\text{$i$th coordinate}},\ldots,0) \in Q^k.
  \]

\item[(2)]
  \emph{Show that $Q^k$ is a convex subset of $\mathbb{R}^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$,
  $\mathbf{y} = (y_1,\ldots,y_k) \in Q^k$ and $0 < \lambda < 1$.
  Hence
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda x_1 + (1-\lambda)y_1, \ldots, \lambda x_k + (1-\lambda)y_k) \in Q^k
  \]
  since each $\lambda x_i + (1-\lambda)y_i \geq 0$
  and
  \[
    \sum_{i=1}^{k} (\lambda x_i + (1-\lambda)y_i)
    = \lambda \sum_{i=1}^{k} x_i + (1-\lambda) \sum_{i=1}^{k} y_i
    \leq \lambda + (1-\lambda)
    = 1.
  \]

\item[(3)]
  \emph{Given any convex set $E \subseteq \mathbb{R}^k$ containing
  $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.
  Show that $E \supseteq Q^k$.}
  \begin{enumerate}
  \item[(a)]
    Induction on $k$.
    Base case: $k = 1$. Given any $\mathbf{x} = (x_1) \in Q^1$.
    We have $0 \leq x_1 \leq 1$ by the definition of $Q^1$.
    So that $\mathbf{x} = x_1 \mathbf{e}_1 + (1-x_1) \mathbf{0} \in E$
    since $\mathbf{0}, \mathbf{e}_1 \in E$ and $E$ is convex.

  \item[(b)]
    Inductive step: suppose the statement holds for $k = n$.
    Given any $\mathbf{x} = (x_1,\ldots,x_n,x_{n+1}) \in Q^{n+1}$.
    If $x_{n+1} = 1$, then $x_1 = \cdots = x_n = 0$ by the definition of $Q^{n+1}$.
    So $\mathbf{x} = \mathbf{e}_{n+1} \in E$ by the assumption of $E$.
    If $0 \leq x_{n+1} < 1$, then $x_1 + \cdots + x_n \leq 1 - x_{n+1}$ or
    \[
      \frac{x_1}{1-x_{n+1}} + \cdots + \frac{x_{n}}{1-x_{n+1}} \leq 1.
    \]
    So the point
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}} \right)
      \in Q^n,
    \]
    or
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}}, 0 \right),
      \text{ say } \widehat{\mathbf{x}}, \in E
    \]
    by the induction hypothesis.
    Note that $\mathbf{e}_{n+1} \in E$.
    Hence
    \[
      \mathbf{x}
      = x_{n+1} \mathbf{e}_{n+1} + (1-x_{n+1})\widehat{\mathbf{x}}
      \in E
    \]
    by the convexity of $E$.

  \item[(c)]
    Conclusion: Since both the base case and the inductive step have been proved as true,
    by mathematical induction the statement holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{f}$ be an affine mapping that carries a vector space $X$ into a vector space $Y$
  such that
  \[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{0}) + A\mathbf{x}
  \]
  for some $A \in L(X,Y)$.

\item[(2)]
  Given any convex subset $C$ of $X$.
  To show that $\mathbf{f}(C)$ is convex, it suffices to show that
  \[
    \lambda \mathbf{y}_1 + (1-\lambda) \mathbf{y}_2 \in \mathbf{f}(C)
  \]
  for any $\mathbf{y}_1, \mathbf{y}_2 \in \mathbf{f}(C)$ and $0 < \lambda < 1$.
  Write $\mathbf{y}_1 = \mathbf{f}(\mathbf{x}_1)$,
  $\mathbf{y}_2 = \mathbf{f}(\mathbf{x}_2)$ for some $\mathbf{x}_1, \mathbf{x}_2 \in C$.
  Note that $\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2 \in C$ by the convexity of $C$.
  Hence
  \begin{align*}
    &\mathbf{f}(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + A(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + \lambda A\mathbf{x}_1 + (1-\lambda) A\mathbf{x}_2
      &(A \in L(X,Y)) \\
    =& \lambda(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_1)
      + (1-\lambda)(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_2) \\
    =& \lambda \mathbf{f}(\mathbf{x}_1) + (1-\lambda)\mathbf{f}(\mathbf{x}_2) \\
    =& \lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 \in \mathbf{f}(C).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.8.}
\emph{Let $H$ be the parallelogram in $\mathbb{R}^2$ whose vertices are
$(1,1)$, $(3,2)$, $(4,5)$, $(2,4)$.
Find the affine map $T$ which sends
$(0,0)$ to $(1,1)$, $(1,0)$ to $(3,2)$, $(1,1)$ to $(4,5)$, $(0,1)$ to $(2,4)$.
Show that $J_{T} = 5$.
Use $T$ to convert the integral
\[
  \alpha = \int_{H} e^{x-y} dxdy
\]
to an integral over $I^2$ and thus compute $\alpha$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Affine simplexes 10.26,
  \[
    T(\mathbf{x}) = T(\mathbf{0}) + A\mathbf{x},
  \]
  where $A \in L(\mathbb{R}^2, \mathbb{R}^2)$, say
  $A = \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}$.
  Note that $T:
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} \mapsto
  \begin{bmatrix}
    1 \\
    1
  \end{bmatrix}$.
  Thus
  \[
    T:
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix} \mapsto
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
    +
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix}
    =
    \begin{bmatrix}
      1+ax+by \\
      1+cx+dy
    \end{bmatrix}.
  \]

\item[(2)]
  By $T: (1,0) \mapsto (3,2)$ and $T: (0,1) \mapsto (2,4)$,
  we can solve $A$ as
  \[
    A = \begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}.
  \]
  It is easy to verify such
  \[
    T:
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    \mapsto
    \underbrace{\begin{bmatrix}
      1 \\
      1
    \end{bmatrix}}_{T(\mathbf{0})}
    +
    \underbrace{\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    =
    \begin{bmatrix}
      1+2x+y \\
      1+x+3y
    \end{bmatrix}
  \]
  satisfying our requirement.

\item[(3)]
  \[
    J_T
    =
    \det\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}
    = 5.
  \]

\item[(4)]
  \begin{align*}
    \int_{H} e^{x-y} dxdy
    &= \int_{[0,1]^2} e^{(1+2u+v)-(1+u+3v)} \abs{J_T} du dv \\
    &= 5 \int_{[0,1]^2} e^{u-2v} du dv \\
    &= 5 \left\{ \int_{0}^{1} e^u du \right\}\left\{ \int_{0}^{1} e^{-2v} dv \right\}
      &(\text{Theorem 10.2}) \\
    &= \frac{5}{2}(e-1)(1-e^{-2}).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.9.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.10.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.11.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.12.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.13.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.14 (Levi-Civita symbol).}
\emph{Prove $\varepsilon(j_1, \ldots, j_k) = s(j_1, \ldots, j_k)$,
where}
\[
  s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p).
\] \\

It is usually to define the Levi-Civita symbol by
\begin{equation*}
\varepsilon(j_1, \ldots, j_k) =
  \begin{cases}
    1
      & \text{ if $(j_1,\cdots,j_k)$ is an even permutation of $J$}, \\
    -1
      & \text{ if $(j_1,\cdots,j_k)$ is an odd permutation of $J$}, \\
    0
      & \text{otherwise}
  \end{cases}
\end{equation*}
(Basic $k$-forms 10.14).
Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
So $\varepsilon(j_1, \ldots, j_k)$ is equivalent to an explicit expression
$s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p)$. \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Induction on $k$.
  Base case: \emph{Show that $\varepsilon(j_1,j_2) = s(j_1,j_2)$.}
  Since
  \begin{equation*}
  \varepsilon(j_1,j_2) =
    \begin{cases}
      1
        & \text{ if $j_1 < j_2$} \\
      -1
        & \text{ if $j_1 > j_2$},
    \end{cases}
  \end{equation*}
  $\varepsilon(j_1,j_2) = \mathrm{sgn}(j_2-j_1) = s(j_1,j_2)$.

\item[(2)]
  Inductive step: \emph{Show that for any $s \geq 2$,
  if $\varepsilon(j_1, \ldots, j_{s}) = s(j_1, \ldots, j_{s})$ holds,
  then $\varepsilon(j_1, \ldots, j_{s+1}) = s(j_1, \ldots, j_{s+1})$ also holds.}
  \begin{align*}
    \varepsilon(j_1, \ldots, j_{s+1})
    &= \varepsilon(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= s(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s} \mathrm{sgn}(j_q-j_p)
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s+1} \mathrm{sgn}(j_q - j_p) \\
    &= s(j_1, \ldots, j_{s+1}).
  \end{align*}

\item[(3)]
  Conclusion: Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement holds for every integer $k \geq 2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.15.}
\emph{If $\omega$ and $\lambda$ are $k$- and $m$-forms, respectively,
prove that}
\[
  \omega \wedge \lambda = (-1)^{km} \lambda \wedge \omega.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \omega = \sum_I b_I(\mathbf{x}) dx_I,
    \qquad
    \lambda = \sum_J c_J(\mathbf{x}) dx_J
  \]
  in the stardard presentations,
  where $I$ and $J$ range over all increasing $k$-indices
  and over all increasing $m$-indices taken from the set $\{1,\ldots,n\}$.

\item[(2)]
  \emph{Show that $dx_I \wedge dx_J = (-1)^{km} dx_J \wedge dx_I$.}
  \begin{align*}
    dx_I \wedge dx_J
    &= dx_{i_1} \wedge \cdots \wedge dx_{i_k}
      \wedge dx_J \\
    &= (-1)^m dx_{i_1} \wedge \cdots \wedge dx_{i_{k-1}}
      \wedge dx_J \wedge dx_{i_{k}} \\
    &= (-1)^{2m} dx_{i_1} \wedge \cdots \wedge dx_{i_{k-2}}
      \wedge dx_J \wedge dx_{i_{k-1}} \wedge dx_{i_{k}} \\
    &\cdots \\
    &= (-1)^{km} dx_J
      \wedge dx_{i_1} \wedge \cdots \wedge dx_{i_k} \\
    &= (-1)^{km} dx_J \wedge dx_I.
  \end{align*}

\item[(3)]
  \begin{align*}
    \omega \wedge \lambda
    &= \sum_{I,J} b_I(\mathbf{x}) c_J(\mathbf{x}) dx_I \wedge dx_J \\
    &= (-1)^{km} \sum_{J,I} c_J(\mathbf{x}) b_I(\mathbf{x}) dx_J \wedge dx_I \\
    &= (-1)^{km} \lambda \wedge \omega.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.16.}
\emph{If $k \geq 2$ and $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$
is an oriented affine $k$-simplex, prove that $\partial^2 \sigma = 0$,
directly from the definition of the boundary operator $\partial$.
Deduce from this that $\partial^2 \Psi = 0$ for every chain $\Psi$.
(Hint: For orientation, do it first for $k=2$, $k=3$.
In general, if $i < j$, let $\sigma_{ij}$ be the $(k-2)$-simplex obtained by
deleting $\mathbf{p}_i$ and $\mathbf{p}_j$ from $\sigma$.
Show that each $\sigma_{ij}$ occurs twice in $\partial^2\sigma$, with opposite sign.)} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Write the boundary of the oriented affine $k$-simplex
  $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$ as
  \[
    \partial \sigma
    = \sum_{i=0}^{k}(-1)^i
    [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]
  \]
  where where the oriented $(k-1)$-simplex
  $[\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]$
  is obtained by deleting $\sigma$'s $i$-th vertex (Boundaries 10.29).

\item[(2)]
  \begin{align*}
    \partial^2 \sigma
    =& \partial \left( \sum_{i}(-1)^{i}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \right) \\
    =& \sum_{i}(-1)^{i}
      \partial [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i}(-1)^{j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &+ \sum_{j>i} (-1)^{i}(-1)^{j-1}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &- \sum_{j>i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k].
  \end{align*}
  The latter two summations cancel since after switching $i$ and $j$ in the second sum.
  Therefore $\partial^2 \sigma = 0$.

\item[(3)]
  The boundary of a chain is the linear combination of boundaries of the simplices in the chain.
  Write $\Psi = \sum_{i=1}^{r} \sigma_i$. where $\sigma_i$ is an oriented affine simplex.
  Then
  \[
    \partial^2 \Psi
    = \partial \left(\partial \sum \sigma_i \right)
    = \partial \left( \sum \partial\sigma_i \right)
    = \sum \partial^2 \sigma_i
    = \sum 0
    = 0
  \]
  for any affine chain $\Psi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.17.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.18.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.19.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.20.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.21.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.22.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.23.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.24.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.25.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.26.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.27.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.28.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.29.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.30.}
\emph{If $\mathbf{N}$ is the vector given by
\[
  \mathbf{N}
  = (\alpha_2 \beta_3 - \alpha_3 \beta_2) \mathbf{e}_1
    + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \mathbf{e}_2
    + (\alpha_1 \beta_2 - \alpha_2 \beta_1) \mathbf{e}_3
\]
(Equation (135)), prove that
\[
  \det\begin{bmatrix}
    \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
    \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
    \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
  \end{bmatrix}
  = \abs{\mathbf{N}}^2
\]
Also, verify
\[
  \mathbf{N} \cdot (T\mathbf{e}_1) = \mathbf{N} \cdot (T\mathbf{e}_2)
\]
(Equation (137)).} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Laplace's expansion along the third column,
    \begin{align*}
    &\det\begin{bmatrix}
      \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
      \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
      \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
    \end{bmatrix} \\
    =& (-1)^{1+3} (\alpha_2 \beta_3 - \alpha_3 \beta_2)
      \det\begin{bmatrix}
        \alpha_2 & \beta_2 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{2+3} (\alpha_3 \beta_1 - \alpha_1 \beta_3)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{3+3} (\alpha_1 \beta_2 - \alpha_2 \beta_1)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_2 & \beta_2
      \end{bmatrix} \\
    =& (\alpha_2 \beta_3 - \alpha_3 \beta_2)^2
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3)^2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)^2 \\
    =& \abs{\mathbf{N}}^2.
  \end{align*}

\item[(2)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_1)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\alpha_1, \alpha_2, \alpha_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \alpha_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \alpha_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \alpha_3 \\
    &= (\alpha_3 \alpha_2 - \alpha_2 \alpha_3) \beta_1
      + (\alpha_1 \alpha_3 - \alpha_3 \alpha_1) \beta_2
      + (\alpha_2 \alpha_1 - \alpha_1 \alpha_2)\beta_3 \\
    &= 0.
  \end{align*}

\item[(3)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_2)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\beta_1, \beta_2, \beta_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \beta_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \beta_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \beta_3 \\
    &= (\beta_2 \beta_3 - \beta_3 \beta_2) \alpha_1
      + (\beta_3 \beta_1 - \beta_1 \beta_3) \alpha_2
      + (\beta_1 \beta_2 - \beta_2 \beta_1) \alpha_3 \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.31.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.32.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}