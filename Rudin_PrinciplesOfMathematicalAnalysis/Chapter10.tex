\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt

\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\begin{document}

\textbf{\Large Chapter 10: Integration of Differential Forms} \\\\



\emph{Author: Meng-Gen Tsai} \\
\emph{Email: plover@gmail.com} \\\\



% http://pages.cs.wisc.edu/~wentaowu/other-docs/POMA_Solution_Sheet.pdf
% https://linearalgebras.com/baby-rudin-chapter-10.html
% https://www.researchgate.net/publication/248817777_Partitions_of_Unity_for_Countable_Covers
% https://www.math.lsu.edu/~lawson/Chapter4.pdf
% https://www2.math.upenn.edu/~ryrogers/HW7-solutions.pdf
% https://www.math.ucla.edu/~tao/preprints/forms.pdf
% https://en.wikipedia.org/wiki/N-sphere



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.1.}
\emph{Let $H$ be a compact convex set in $\mathbb{R}^k$, with nonempty interior.
Let $f \in \mathscr{C}(H)$, put $f(\mathbf{x}) = 0$ in the complement of $H$,
and define $\int_{H} f$ as in Definition 10.3.
Prove that $\int_{H} f$ is independent of the order in which the $k$ integrations are carried out.
(Hint: Approximate $f$ by functions that are continuous on $\mathbb{R}^k$
and whose supports are in $H$,
as was done in Example 10.4.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.2.}
\emph{For $i=1,2,3,\ldots$, let $\varphi_i \in \mathscr{C}(\mathbb{R}^1)$ have support
in $(2^{-i},2^{1-i})$, such that $\int \varphi_i = 1$.
Put
\[
  f(x,y) = \sum_{i=1}^{\infty}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
\]
Then $f$ has compact support in $\mathbb{R}^2$,
$f$ is continuous except at $(0,0)$,
and
\[
  \int dy \int f(x,y) dx = 0
  \qquad
  \text{ but }
  \qquad
  \int dx \int f(x,y) dy = 1.
\]
Observe that $f$ is unbounded in every neighborhood of $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  If $f, g: \mathbb{R}^{n} \to \mathbb{R}^{m}$ are two functions,
  then
  \begin{enumerate}
  \item[(a)]
    $\mathrm{supp}(fg) \subseteq \mathrm{supp}(f) \cap \mathrm{supp}(g)$.
  \item[(b)]
    $\mathrm{supp}(f+g) \subseteq \mathrm{supp}(f) \cup \mathrm{supp}(g)$.
  \end{enumerate}

\item[(2)]
  Note that $f(x,y)$ is well-defined on $\mathbb{R}^2$
  since only finitely many terms are nonzero for each fixed point $(x,y) \in \mathbb{R}^2$ (by (1)).
  Besides,
  \begin{align*}
    &\mathrm{supp}([ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)) \\
    \subseteq \: &
    \{ (x,y) :
      x \in \mathrm{supp}(\varphi_i) \cup \mathrm{supp}(\varphi_{i+1}),
      y \in \mathrm{supp}(\varphi_i)
    \} \\
    \subseteq \: &
    \{ (x,y) :
      x \in (2^{-i},2^{-i+1}) \cup (2^{-i-1},2^{-i}),
      y \in (2^{-i},2^{-i+1})
    \} \\
    \subseteq \: &
    \{ (x,y) :
      x \in (0,1),
      y \in (0,1)
    \}
  \end{align*}
  for all $i=1,2,3,\ldots$.
  So $\mathrm{supp}(f) \subseteq (0,1)^2$, or $\mathrm{supp}(f)$ is bounded.
  As $\mathrm{supp}(f)$ is closed (by definition),
  $\mathrm{supp}(f)$ is compact (Theorem 2.41).

\item[(3)]
  \emph{Show that $f(x,y)$ is not continuous at $(0,0)$.}
  \begin{enumerate}
  \item[(a)]
    Note that $f(0,0) = 0$ since $(0,0) \not\in \mathrm{supp}(f) \subseteq (0,1)^2$.
    It suffices to show that there exists a sequence
    $\{ (t_n,t_n) \}$ in $\mathbb{R}^2$ such that
    $(t_n,t_n) \neq (0,0)$, $\lim_{n \to \infty}(t_n,t_n) = (0,0)$
    but $\lim_{n \to \infty} f(t_n,t_n)$ does not converge to $0$
    (Theorem 4.2).

  \item[(b)]
    For any $n = 1,2,3,\ldots$,
    \begin{align*}
      1
      = \int \varphi_n
      = \int_{2^{-n}}^{2^{-n+1}} \varphi(t)dt
      \leq 2^{-n} \sup_{t \in [2^{-n},2^{-n+1}]} \varphi(t),
    \end{align*}
    or $\sup_{t \in [2^{-n},2^{-n+1}]} \varphi(t) \geq 2^n$.
    By the continunity of $\varphi_n$, there exists $t_n \in [2^{-n},2^{-n+1}]$
    such that $\varphi_n(t_n) \geq 2^n$ (Theorem 4.16).

  \item[(c)]
    We construct $\{ (t_n,t_n) \}$ in $\mathbb{R}^2$ by (b) for all $n=1,2,3,\ldots$.
    Clearly, $(t_n,t_n) \neq (0,0)$ and $\lim_{n \to \infty}(t_n,t_n) = (0,0)$.
    However,
    \[
      f(t_n,t_n)
      = [\varphi_{n}(t_n) - \varphi_{n+1}(t_n)]\varphi_{n}(t_n)
      = \varphi_{n}(t_n)^2
      \geq 2^{2n}
    \]
    does not converge to $0$ as $n \to \infty$.
  \end{enumerate}

\item[(4)]
  \emph{Show that $f(x,y)$ is continuous at $\mathbf{x}_0 = (x_0,y_0) \neq (0,0)$.}
  Consider an open neighborhood
  $B(\mathbf{x}_0;r)$ of $\mathbf{x}_0$ with $r=\frac{\norm{\mathbf{x}_0}}{64} > 0$.
  Hence,
  \[
    f(x,y)|_{B(\mathbf{x}_0;r)}
    = \sum_{i=1}^{N}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
  \]
  is the sum of finitely many terms
  where $N = \log_2 \frac{89}{\norm{\mathbf{x}_0}} \geq 1$
  (since $[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) = 0$
  on ${B(\mathbf{x}_0;r)}$ whenever $i \geq N$).
  Therefore,
  $f(x,y)|_{B(\mathbf{x}_0;r)}$ is continuous
  by the continunity of $\varphi_i$.

\item[(5)]
  \emph{Show that $\int dy \int f(x,y) dx = 0$.}
  For any fixed $y$,
  there is a positive integer $N(y)$ such that
  $\varphi_{N(y)+1}(y) = \varphi_{N(y)+2}(y) = \ldots = 0$ and
  \[
    f(x,y) = \sum_{i=1}^{N(y)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y).
  \]
  So
  \begin{align*}
    \int f(x,y) dx
    &= \int \sum_{i=1}^{N(y)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) dx \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \int [ \varphi_i(x)-\varphi_{i+1}(x) ] dx \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \left( \int \varphi_i(x) dx - \int \varphi_{i+1}(x) dx \right) \\
    &= \sum_{i=1}^{N(y)} \varphi_i(y) \left( 1 - 1 \right) \\
    &= 0,
  \end{align*}
  and thus
  \[
    \int dy \int f(x,y) dx = \int 0 dy = 0.
  \]

\item[(6)]
  \emph{Show that $\int dx \int f(x,y) dy = 0$.}
  For any fixed $x$,
  there is a positive integer $N(x)$ such that
  $\varphi_{N(x)+1}(x) = \varphi_{N(x)+2}(x) = \ldots = 0$ and
  \[
    f(x,y) = \sum_{i=1}^{N(x)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y).
  \]
  So
  \begin{align*}
    \int f(x,y) dy
    &= \int \sum_{i=1}^{N(x)}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y) dy \\
    &= \sum_{i=1}^{N(x)} [ \varphi_i(x)-\varphi_{i+1}(x) ] \int \varphi_i(y) dy \\
    &= \sum_{i=1}^{N(x)} [ \varphi_i(x)-\varphi_{i+1}(x) ] \\
    &= \varphi_1(x), \\
  \end{align*}
  and thus
  \[
    \int dx \int f(x,y) dy = \int \varphi_1(x) dx = 1.
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.3.}
\begin{enumerate}
\item[(a)]
  \emph{If $\mathbf{F}$ is as in Theorem 10.7,
  put $\mathbf{A} = \mathbf{F}'(\mathbf{0})$,
  $\mathbf{F}_1(\mathbf{x}) = \mathbf{A}^{-1} \mathbf{F}(\mathbf{x})$.
  Then $\mathbf{F}_1(\mathbf{0}) = \mathbf{I}$.
  Show that
  \[
    \mathbf{F}_1(\mathbf{x})
    = \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x})
  \]
  in some neighborhood of $\mathbf{0}$,
  for certain primitive mappings $\mathbf{G}_{1}, \ldots, \mathbf{G}_{n}$.
  This gives another version of Theorem 10.7:}
  \[
    \mathbf{F}(\mathbf{x})
    =
    \mathbf{F}'(\mathbf{0})
    \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x}).
  \]

\item[(b)]
  \emph{Prove that the mapping $(x,y) \mapsto (y,x)$ of $\mathbb{R}^2$ onto $\mathbb{R}^2$
  is not the composition of any two primitive mappings,
  in any neighborhood of the origin.
  (This shows that the flips $B_i$ cannot be omitted from the statement of Theorem 10.7.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Suppose $\mathbf{F}$ is a $\mathscr{C}'$-mapping of an open set $E \subseteq \mathbb{R}^n$
  into $\mathbb{R}^n$, $\mathbf{0} \in E$, $\mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'(\mathbf{0})$ is invertible.

\item[(2)]
  Similar to the proof of Theorem 10.7.
  Put $\mathbf{F}_1 = \mathbf{F}$.

\item[(3)]
  As $m = 1$, there is an open neighborhood $V_1 \subseteq E$ of $\mathbf{0}$
  such that
  $\mathbf{F}_1(\mathbf{0}) = (\mathbf{F}'(\mathbf{0}))^{-1} \mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  $\mathbf{F}'_1(\mathbf{0}) = \mathbf{I}$ is invertible, and
  \[
    \mathbf{F}_1(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_1$.
  Hence
  \[
    \mathbf{F}'_1(\mathbf{0})\mathbf{e}_1
    = \sum_{i=1}^{n} (D_1 \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_1 \alpha_1)(\mathbf{0}) = 1 \neq 0$, and we might pick $B_1 = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_1(\mathbf{x})
    = \mathbf{x} + [\alpha_1(\mathbf{x}) - x_1] \mathbf{e}_1
    \qquad
    (\mathbf{x} \in V_1).
  \]
  Then $\mathbf{G}_1 \in \mathscr{C}'(V_1)$, $\mathbf{G}_1$ is primitive,
  and $\mathbf{G}'_1(\mathbf{0}) = \mathbf{I}$ is invertible.

\item[(4)]
  Now we make the induction hypothesis for $1 \leq m \leq n-1$.

\item[(5)]
  Since $\mathbf{G}'_{m}(\mathbf{0}) = \mathbf{I}$ is invertible,
  the inverse function theorem shows that there is an open set $U_{m}$,
  with $\mathbf{0} \in U_{m} \subseteq V_{m}$, such that
  $\mathbf{G}_m$ is an injective mapping of $U_{m}$ onto a neighborhood $V_{m+1}$ of $\mathbf{0}$,
  in which $\mathbf{G}_m^{-1} \in \mathscr{C}'(V_{m+1})$.
  Define $\mathbf{F}_{m+1}$ by
  \[
    \mathbf{F}_{m+1}(\mathbf{y}) = \mathbf{F}_{m} \circ \mathbf{G}_m^{-1}(\mathbf{y})
    \qquad
    (\mathbf{y} \in V_{m+1}).
  \]
  Then $\mathbf{F}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{F}_m(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible by the chain rule
  and the inverse function theorem.
  So
  \[
    \mathbf{F}_{m+1}(\mathbf{x})
    = P_m \mathbf{x}
      + \sum_{i=m+1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_{m+1}$.
  Hence
  \[
    \mathbf{F}'_{m+1}(\mathbf{0})\mathbf{e}_{m+1}
    = \sum_{i=m+1}^{n} (D_{m+1} \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_{m+1} \alpha_{m+1})(\mathbf{0}) = 1 \neq 0$,
  and we might pick $B_{m+1} = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_{m+1}(\mathbf{x})
    = \mathbf{x} + [\alpha_{m+1}(\mathbf{x}) - x_{m+1}] \mathbf{e}_{m+1}
    \qquad
    (\mathbf{x} \in V_{m+1}).
  \]
  Then $\mathbf{G}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{G}_{m+1}$ is primitive,
  and $\mathbf{G}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible.
  Our induction hypothesis holds therefore with $m+1$ in place of $m$.

\item[(6)]
  Note that
  \[
    \mathbf{F}_{m}(\mathbf{x}) = \mathbf{F}_{m+1}(\mathbf{G}_{m}(\mathbf{x}))
    \qquad
    (\mathbf{x} \in U_{m}).
  \]
  If we apply this with $m = 1, \ldots, n-1$, we successively obtain
  \[
    \mathbf{F}_1
    = \mathbf{F}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1
  \]
  in some open neighborhood of $\mathbf{0}$.
  Note that $\mathbf{F}_{n}$ is primitive since
  \[
    \mathbf{F}_{n}(\mathbf{x})
    = P_{n-1} \mathbf{x} + \alpha_n(\mathbf{x}) \mathbf{e}_n.
  \]
  This completes the proof.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  For $(x,y) \in \mathbb{R}^2$, define
  \[
    \mathbf{F}(x,y) = (y,x).
  \]

\item[(2)]
  (Reductio ad absurdum)
  If $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$ for some primitive mappings
  $\mathbf{G}_{i}$ ($i = 1,2$) in some neighborhood $V_{i}$ of the origin,
  $\mathbf{G}_{i}(\mathbf{0}) = \mathbf{0}$ and $\mathbf{G}_{i}'$ is invertible,
  then we may assume that
  \[
    \mathbf{G}_1(x,y) = (x, g_1(x,y))
    \qquad
    \text{and}
    \qquad
    \mathbf{G}_2(x,y) = (g_2(x,y),y).
  \]
  Here the case $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  is similar to the above case.
  Besides, $\mathbf{G}_1(x,y) = (x,g_1(x,y))$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  implies that
  \[
    \mathbf{G}_2 \circ \mathbf{G}_1(x,y) = (x,g_2(x,g_1(x,y)))
    \neq (y,x) = \mathbf{F}(x,y).
  \]
  Same reason for
  $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (g_2(x,y),y)$.

\item[(3)]
  Note that
  \[
    \mathbf{F}'(\mathbf{0})
    =
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
  \]
  Since
  \[
    \mathbf{F}'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{G}_1(\mathbf{0})) \mathbf{G}_1'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{0}) \mathbf{G}_1'(\mathbf{0}),
  \]
  we have
  \begin{align*}
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      D_1 g_2(0,0) & D_2 g_2(0,0) \\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      * & * \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix}.
  \end{align*}
  So $D_1 g_1(0,0) = 1$ and $D_2 g_1(0,0) = 0$, and thus
  $\mathbf{G}_1'(\mathbf{0})
  =
  \begin{bmatrix}
    1 & 0 \\
    1 & 0
  \end{bmatrix}$
  is not invertible, which is absurd.


\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.4.}
\emph{For $(x,y) \in \mathbb{R}^2$, define
\[
  \mathbf{F}(x,y) = (e^x \cos y - 1, e^x \sin y)
\]
Prove that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$, where
\begin{align*}
  \mathbf{G}_1(x,y) &= (e^x \cos y - 1, y) \\
  \mathbf{G}_2(u,v) &= (u, (1+u) \tan v)
\end{align*}
are primitive in some neighborhood of $(0,0)$.
Compute the Jacobians of $\mathbf{G}_1$, $\mathbf{G}_2$, $\mathbf{F}$ at $(0,0)$.
Define
\[
  \mathbf{H}_2(x,y) = (x, e^x \sin y)
\]
and find
\[
  \mathbf{H}_1(u,v) = (h(u,v),v)
\]
so that $\mathbf{F} = \mathbf{H}_1 \circ \mathbf{H}_2$
is in some neighborhood of $(0,0)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Definition 10.5,
  \begin{align*}
    \mathbf{G}_1(x,y) &= (e^x \cos y - 1) \mathbf{e}_1 + y \mathbf{e}_2, \\
    \mathbf{G}_2(u,v) &= u \mathbf{e}_1 + ((1+u) \tan v) \mathbf{e}_2
  \end{align*}
  are primitive in some neighborhood of $(0,0)$.

\item[(2)]
  \emph{Show that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$.}
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{G}_2 \circ \mathbf{G}_1)(x,y)
    &= \mathbf{G}_2(\mathbf{G}_1(x,y)) \\
    &= \mathbf{G}_2(e^x \cos y - 1, y) \\
    &= (e^x \cos y - 1, (1+(e^x \cos y - 1)) \tan y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\item[(3)]
  Since
  \begin{align*}
    J_{\mathbf{G}_1}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      0 & 1
    \end{bmatrix}
    = e^x \cos y \\
    J_{\mathbf{G}_2}(x,y)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      \tan y & (1+x)\sec^2 y
    \end{bmatrix}
    = (1+x)\sec^2 y \\
    J_{\mathbf{F}}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y & e^x \cos y
    \end{bmatrix}
    = e^{2x},
  \end{align*}
  \begin{align*}
    J_{\mathbf{G}_1}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{G}_2}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{F}}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1.
  \end{align*}

\item[(4)]
  Define $h(u,v) = \sqrt{e^{2u} - v^{2}} - 1$ on
  \[
    B\left((0,0);\frac{1}{64}\right) \subseteq \mathbb{R}^2.
  \]
  $h(u,v)$ is well-defined since $e^{2u}-v^2 > 0$
  for all $(u,v) \in B\left((0,0);\frac{1}{64}\right)$.

\item[(5)]
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{H}_1 \circ \mathbf{H}_2)(x,y)
    &= \mathbf{H}_1(\mathbf{H}_2(x,y)) \\
    &= \mathbf{H}_1(x, e^x \sin y) \\
    &= (\sqrt{e^{2x} - (e^x \sin y)^2} - 1, e^x \sin y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.5.}
\emph{Formulate and prove an analogue of Theorem 10.8,
in which $K$ is a compact subset of an arbitrary metric space.
(Replace the functions $\varphi_i$ that occur in the proof of Theorem 10.8
by functions of the type constructed in Exercise 4.22.)} \\

\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  \emph{(Partitions of unity.)
  Suppose $K$ is a compact subset of a metric space $X$,
  and $\{V_{\alpha}\}$ is an open cover of $K$.
  Then there exist functions $\psi_1, \ldots, \psi_s \in \mathscr{C}(X)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_i \leq 1$ for $1 \leq i \leq s$.}

  \item[(b)]
    \emph{each $\psi_i$ has its support in some $V_{\alpha}$, and}

  \item[(c)]
    \emph{$\psi_1(x) + \cdots + \psi_s(x) = 1$ for every $x \in K$.}
  \end{enumerate}

\item[(2)]
  It is trivial that some $V_{\alpha} = X$
  by taking $s = 1$ and $\psi_1(x) = 1 \in \mathscr{C}(X)$.
  Now we assume that all $V_{\alpha} \subsetneq X$.

\item[(3)]
  Associate with each $x \in K$ an index $\alpha(x)$ so that $x \in V_{\alpha(x)}$.
  Then there are open balls $B(x)$ and $W(x)$, centered at $x$,
  with
  \[
    x
    \in B(x)
    \subseteq \overline{B(x)}
    \subseteq W(x)
    \subseteq \overline{W(x)}
    \subseteq V_{\alpha(x)}
  \]
  (Since $V_{\alpha(x)}$ is open, there exists $r > 0$
  such that $B(x;r) \subseteq V_{\alpha(x)}$.
  Take $B(x) = B\left(x;\frac{r}{89}\right)$
  and $W(x) = B\left(x;\frac{r}{64}\right)$.)

\item[(4)]
  Since $K$ is compact, there are finitely many points
  $x_1, \ldots, x_s \in K$ such that
  \[
    K \subseteq B(x_1) \cup \cdots \cup B(x_s).
  \]
  Note that
  \begin{enumerate}
  \item[(a)]
    $\overline{B(x_i)}$ is a nonempty closed set since $x_i \in B(x_i) \subseteq \overline{B(x_i)}$.

  \item[(b)]
    $X - W(x_i) \supseteq X - V_{\alpha(x_i)}$ is a nonempty closed set by the assumption in (2).

  \item[(c)]
    $\overline{B(x_i)} \cap (X - W(x_i)) \subseteq W(x_i) \cap (X - W(x_i)) = \varnothing$.

  \end{enumerate}
  By Exercise 4.22, there is a function
  \[
    \varphi_i(x)
    = \frac{\rho_{\overline{B(x_i)}}(x)}{\rho_{\overline{B(x_i)}}(x) + \rho_{X-W(x_i)}(x)}
    \in \mathscr{C}(X)
  \]
  such that $\varphi_i(x) = 1$ on $\overline{B(x_i)}$,
  $\varphi_i(x) = 0$ outside $W(x_i)$, and $0 \leq \varphi_i(x) \leq 1$ on $X$
  for $1 \leq i \leq s$.

\item[(5)]
  Define $\psi_{1} = \varphi_{1} \in \mathscr{C}(X)$ and
  \[
    \psi_{i+1} = (1-\varphi_{1}) \cdots (1-\varphi_{i})\varphi_{i+1} \in \mathscr{C}(X)
  \]
  for $1 \leq i \leq s-1$.
  Properties (a) and (b) in (1) are clear.
  Also,
  \[
    \psi_1(x) + \cdots + \psi_s(x) = 1 - (1-\varphi_1(x)) \cdots (1-\varphi_s(x))
  \]
  by the construction of $\psi_i$.
  If $x \in K$, then $x \in B(x_i)$ for some $i$, hence $\varphi_i(x)=1$,
  and the product $(1-\varphi_1(x)) \cdots (1-\varphi_s(x)) = 0$.
  This proves property (c) in (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.6.}
\emph{Strengthen the conclusion of Theorem 10.8 by showing that
the functions $\psi_i$ can be made differentiable, and even infinitely differentiable.
(Use Exercise 8.1 in the construction of the auxiliary functions $\psi_i$.)} \\



\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  It is trivial that some $V_{\alpha} = \mathbb{R}^n$
  by taking $s = 1$ and $\psi_1(\mathbf{x}) = 1 \in \mathscr{C}^{\infty}(\mathbb{R}^n)$.
  Now we assume that all $V_{\alpha} \subsetneq \mathbb{R}^n$.

\item[(2)]
  Associate with each $\mathbf{x} \in K$ an index $\alpha(x)$ so that $\mathbf{x} \in V_{\alpha(x)}$.
  Then there are open $n$-cells $B(\mathbf{x})$ and $W(\mathbf{x})$ (Definition 10.1),
  centered at $\mathbf{x}$,
  with
  \[
    \mathbf{x}
    \in B(\mathbf{x})
    \subseteq \overline{B(\mathbf{x})}
    \subseteq W(\mathbf{x})
    \subseteq \overline{W(\mathbf{x})}
    \subseteq V_{\alpha(\mathbf{x})}
  \]
  (Since $V_{\alpha(\mathbf{x})}$ is open, there exists $r > 0$
  such that $B(\mathbf{x};r) \subseteq V_{\alpha(\mathbf{x})}$.
  Take
  \[
    B(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{89\sqrt{n}}\right),
    \qquad
    W(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{64\sqrt{n}}\right)
  \]
  where $I(\mathbf{p};r)$ is the open $n$-cell centered at $\mathbf{p} = (p_1,\ldots,p_n)$
  defined by
  \[
    I(\mathbf{p};r)
    = (p_1-r,p_1+r) \times \cdots \times (p_n-r,p_n+r) \subseteq \mathbb{R}^n.)
  \]

\item[(3)]
  Define
  \begin{equation*}
    f(y) =
    \begin{cases}
      e^{-\frac{1}{y^2}} & (y > 0), \\
      0 & (y \leq 0).
    \end{cases}
  \end{equation*}
  $f(y) \in \mathscr{C}^{\infty}(\mathbb{R}^1)$
  by applying the similar argument in Exercise 8.1.

\item[(4)]
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in K$
  and construct $B(\mathbf{x})$ and $W(\mathbf{x})$ as in (2).
  Define
  \[
    g_{x_j}(y_j)
    = \frac{f(y_j)}{f(y_j)+f\left(\frac{r}{64\sqrt{n}}-\frac{r}{89\sqrt{n}}-y_j\right)}
  \]
  for $1 \leq j \leq n$.
  $g_{x_j}$ is well-defined and $g_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    g_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq 0, \\
      \text{strictly increasing}
        & \text{if } 0 \leq y_j \leq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } y_j \geq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Next, define
  \[
    h_{x_j}(y_j)
    = g_{x_j}\left(y_j-x_j+\frac{r}{64\sqrt{n}}\right)
      g_{x_j}\left(x_j+\frac{r}{64\sqrt{n}}-y_j\right)
  \]
  for $1 \leq j \leq n$.
  $h_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    h_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq x_j - \frac{r}{64\sqrt{n}}, \\
      \text{strictly increasing}
        & \text{if } x_j - \frac{r}{64\sqrt{n}} \leq y_j \leq x_j - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } x_j - \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{89\sqrt{n}}, \\
      \text{strictly decreasing}
        & \text{if } x_j + \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{64\sqrt{n}}, \\
      0
        & \text{if } y_j \geq x_j + \frac{r}{64\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Finally we define $\mathbf{h}_{\mathbf{x}}: \mathbb{R}^n \to \mathbb{R}^1$ by
  \[
    \mathbf{h}_{\mathbf{x}}(\mathbf{y})
    = \prod_{j=1}^{n} h_{x_j}(y_j)
  \]
  where $\mathbf{y} = (y_1,\ldots,y_n) \in \mathbb{R}^n$.
  Hence, $\mathbf{h}_{\mathbf{x}} \in \mathscr{C}^{\infty}(\mathbb{R}^n)$ (Theorem 9.21).
  Also, $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 1$ on $\overline{B(\mathbf{x})}$,
  $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 0$ outside $W(\mathbf{x})$,
  and $0 \leq \mathbf{h}_{\mathbf{x}}(\mathbf{y}) \leq 1$.

\item[(5)]
  Since $K$ is compact, there are finitely many points
  $\mathbf{x}_1, \ldots, \mathbf{x}_s \in K$ such that
  \[
    K \subseteq B(\mathbf{x}_1) \cup \cdots \cup B(\mathbf{x}_s).
  \]
  Take
  \[
    \varphi_i(\mathbf{x})
    = \mathbf{h}_{\mathbf{x}_i}(\mathbf{x})
    \in \mathscr{C}^{\infty}(\mathbb{R}^n)
  \]
  for $1 \leq i \leq s$.

\item[(6)]
  The rest are the same as the proof of Theorem 10.8 or Exercise 10.5.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.7.}
\begin{enumerate}
\item[(a)]
  \emph{Show that the simplex $Q^k$ is the smallest convex subset of $\mathbb{R}^k$
  such that contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}

\item[(b)]
  \emph{Show that affine mappings take convex sets to convex sets.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $Q^k$ contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}
  Recall
  \[
    Q^k = \{ (x_1,\ldots,x_k) \in \mathbb{R}^k :
      x_1 + \cdots + x_k \leq 1 \text{ and }
      x_1, \ldots, x_k \geq 0 \}
  \]
  (Example 10.14).
  Hence $\mathbf{0} = (0,\ldots,0) \in Q^k$ and
  \[
    \mathbf{e}_i = (0,\ldots,\underbrace{1}_{\text{$i$th coordinate}},\ldots,0) \in Q^k.
  \]

\item[(2)]
  \emph{Show that $Q^k$ is a convex subset of $\mathbb{R}^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$,
  $\mathbf{y} = (y_1,\ldots,y_k) \in Q^k$ and $0 < \lambda < 1$.
  Hence
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda x_1 + (1-\lambda)y_1, \ldots, \lambda x_k + (1-\lambda)y_k) \in Q^k
  \]
  since each $\lambda x_i + (1-\lambda)y_i \geq 0$
  and
  \[
    \sum_{i=1}^{k} (\lambda x_i + (1-\lambda)y_i)
    = \lambda \sum_{i=1}^{k} x_i + (1-\lambda) \sum_{i=1}^{k} y_i
    \leq \lambda + (1-\lambda)
    = 1.
  \]

\item[(3)]
  \emph{Given any convex set $E \subseteq \mathbb{R}^k$ containing
  $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.
  Show that $E \supseteq Q^k$.}
  \begin{enumerate}
  \item[(a)]
    Induction on $k$.
    Base case: $k = 1$. Given any $\mathbf{x} = (x_1) \in Q^1$.
    We have $0 \leq x_1 \leq 1$ by the definition of $Q^1$.
    So that $\mathbf{x} = x_1 \mathbf{e}_1 + (1-x_1) \mathbf{0} \in E$
    since $\mathbf{0}, \mathbf{e}_1 \in E$ and $E$ is convex.

  \item[(b)]
    Inductive step: suppose the statement holds for $k = n$.
    Given any $\mathbf{x} = (x_1,\ldots,x_n,x_{n+1}) \in Q^{n+1}$.
    If $x_{n+1} = 1$, then $x_1 = \cdots = x_n = 0$ by the definition of $Q^{n+1}$.
    So $\mathbf{x} = \mathbf{e}_{n+1} \in E$ by the assumption of $E$.
    If $0 \leq x_{n+1} < 1$, then $x_1 + \cdots + x_n \leq 1 - x_{n+1}$ or
    \[
      \frac{x_1}{1-x_{n+1}} + \cdots + \frac{x_{n}}{1-x_{n+1}} \leq 1.
    \]
    So the point
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}} \right)
      \in Q^n,
    \]
    or
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}}, 0 \right),
      \text{ say } \widehat{\mathbf{x}}, \in E
    \]
    by the induction hypothesis.
    Note that $\mathbf{e}_{n+1} \in E$.
    Hence
    \[
      \mathbf{x}
      = x_{n+1} \mathbf{e}_{n+1} + (1-x_{n+1})\widehat{\mathbf{x}}
      \in E
    \]
    by the convexity of $E$.

  \item[(c)]
    Conclusion: Since both the base case and the inductive step have been proved as true,
    by mathematical induction the statement holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{f}$ be an affine mapping that carries a vector space $X$ into a vector space $Y$
  such that
  \[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{0}) + A\mathbf{x}
  \]
  for some $A \in L(X,Y)$.

\item[(2)]
  Given any convex subset $C$ of $X$.
  To show that $\mathbf{f}(C)$ is convex, it suffices to show that
  \[
    \lambda \mathbf{y}_1 + (1-\lambda) \mathbf{y}_2 \in \mathbf{f}(C)
  \]
  for any $\mathbf{y}_1, \mathbf{y}_2 \in \mathbf{f}(C)$ and $0 < \lambda < 1$.
  Write $\mathbf{y}_1 = \mathbf{f}(\mathbf{x}_1)$,
  $\mathbf{y}_2 = \mathbf{f}(\mathbf{x}_2)$ for some $\mathbf{x}_1, \mathbf{x}_2 \in C$.
  Note that $\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2 \in C$ by the convexity of $C$.
  Hence
  \begin{align*}
    &\mathbf{f}(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + A(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + \lambda A\mathbf{x}_1 + (1-\lambda) A\mathbf{x}_2
      &(A \in L(X,Y)) \\
    =& \lambda(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_1)
      + (1-\lambda)(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_2) \\
    =& \lambda \mathbf{f}(\mathbf{x}_1) + (1-\lambda)\mathbf{f}(\mathbf{x}_2) \\
    =& \lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 \in \mathbf{f}(C).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.8.}
\emph{Let $H$ be the parallelogram in $\mathbb{R}^2$ whose vertices are
$(1,1)$, $(3,2)$, $(4,5)$, $(2,4)$.
Find the affine map $T$ which sends
$(0,0)$ to $(1,1)$, $(1,0)$ to $(3,2)$, $(1,1)$ to $(4,5)$, $(0,1)$ to $(2,4)$.
Show that $J_{T} = 5$.
Use $T$ to convert the integral
\[
  \alpha = \int_{H} e^{x-y} dx \: dy
\]
to an integral over $I^2$ and thus compute $\alpha$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Affine simplexes 10.26,
  \[
    T(\mathbf{x}) = T(\mathbf{0}) + A\mathbf{x},
  \]
  where $A \in L(\mathbb{R}^2, \mathbb{R}^2)$, say
  $A = \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}$.
  Note that $T:
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} \mapsto
  \begin{bmatrix}
    1 \\
    1
  \end{bmatrix}$.
  Thus
  \[
    T:
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix} \mapsto
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
    +
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix}
    =
    \begin{bmatrix}
      1+ax+by \\
      1+cx+dy
    \end{bmatrix}.
  \]

\item[(2)]
  By $T: (1,0) \mapsto (3,2)$ and $T: (0,1) \mapsto (2,4)$,
  we can solve $A$ as
  \[
    A = \begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}.
  \]
  It is easy to verify such
  \[
    T:
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    \mapsto
    \underbrace{\begin{bmatrix}
      1 \\
      1
    \end{bmatrix}}_{T(\mathbf{0})}
    +
    \underbrace{\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    =
    \begin{bmatrix}
      1+2x+y \\
      1+x+3y
    \end{bmatrix}
  \]
  satisfying our requirement.

\item[(3)]
  \[
    J_T
    =
    \det\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}
    = 5.
  \]

\item[(4)]
  By Example 10.4 and Theorem 10.9, we have
  \begin{align*}
    \int_{H} e^{x-y} dx \: dy
    &= \int_{I^2} e^{(1+2u+v)-(1+u+3v)} \abs{J_T} du \: dv \\
    &= 5 \int_{I^2} e^{u-2v} du \: dv \\
    &= 5 \left\{ \int_{0}^{1} e^u du \right\}\left\{ \int_{0}^{1} e^{-2v} dv \right\}
      &(\text{Theorem 10.2}) \\
    &= \frac{5}{2}(e-1)(1-e^{-2}).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.9.}
\emph{Define $(x,y) = T(r,\theta)$ one the rectangle
\[
  0 \leq r \leq a,
  \qquad
  0 \leq \theta \leq 2\pi
\]
by the equations
\[
  x = r \cos\theta,
  \qquad
  y = r \sin\theta.
\]
Show that $T$ maps this rectangle onto the closed disc $D$ with center at $(0,0)$ and radius $a$,
that $T$ is one-to-one in the interior of the rectangle,
and that $J_T(r,\theta) = r$.
If $f \in \mathscr{C}(D)$, prove the formula for integration in polar coordinates:}
\[
  \int_{D} f(x,y)dx \: dy
  = \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta.
\] \\

\emph{(Hint: Let $D_0$ be the interior of $D$,
minus the interval from $(0,0)$ to $(a,0)$.
As it stands, Theorem 10.9 applies to continuous functions $f$
whose support lies in $D_0$.
To remove this restriction,
proceed as in Example 10.4.)} \\

\emph{Proof.}
Define $E = [0,a] \times [0,2\pi]$.
\begin{enumerate}
\item[(1)]
  \emph{Show that $T$ maps $E$ onto $D$.}
  Given any $(x,y) \in D$.
  \begin{enumerate}
  \item[(a)]
    It is equivalent to solve $(r,\theta)$ from
    \begin{align*}
      x &= r\cos\theta \\
      y &= r\sin\theta
    \end{align*}
    in terms of $(x,y)$.
    Let $L$ be the closed interval from $(0,0)$ to $(a,0)$,
    say $L = \{ (r,0) \in \mathbb{R}^2 : 0 \leq r \leq a \} \subseteq D$.

  \item[(b)]
    If $(x,y) \in L$, say $(x,y) = (r,0)$ for some $r \in [0,a]$,
    then there exists $(r,0)$ or $(r,2\pi)$
    such that $T(r,0) = T(r,2\pi) = (r,0)$.
    (Therefore, $T$ is not one-to-one on $L$.)

  \item[(c)]
    If $(x,y) \in D - L$, then there is $r = (x^2+y^2)^{\frac{1}{2}}$ in $[0,a]$
    (since $(x,y) \in D = \{ (x,y) \in \mathbb{R}^2 : (x^2+y^2)^{\frac{1}{2}} \leq a \}$).
    Define
    \begin{equation*}
      \theta =
        \begin{cases}
          \arccos\left(\frac{x}{r}\right) & \text{ if $y \geq 0$}, \\
          2\pi - \arccos\left(\frac{x}{r}\right) & \text{ if $y < 0$}.
        \end{cases}
    \end{equation*}
    It is well-defined since $r \neq 0$.
    Besides, $\theta \in [0,2\pi]$ and $T(r,\theta) = (x,y)$.
  \end{enumerate}

\item[(2)]
  \emph{Show that $T$ is one-to-one in the interior of the rectangle $E$.}
  Suppose $(r_1,\theta_1), (r_2,\theta_2) \in \mathrm{int}(E) = (0,a) \times (0,2\pi)$
  and $T(r_1,\theta_1) = T(r_2,\theta_2) = (x,y) \in D$.
  Then
  \begin{align*}
    x &= r_1 \cos\theta_1 = r_2 \cos\theta_2, \\
    y &= r_1 \sin\theta_1 = r_2 \sin\theta_2.
  \end{align*}
  Note that $r_1^2 = r_2^2 = x^2+y^2$ and $r_1, r_2 > 0$,
  we have $r_1 = r_2$.
  Solve $\cos\theta_1 = \cos\theta_2$ and $\sin\theta_1 = \sin\theta_2$
  to get $\theta_1 = \theta_2 + 2m\pi$ for all $m \in \mathbb{Z}$.
  Here $m$ must be zero since $\theta_1, \theta_2 \in (0,2\pi)$.
  Therefore, $(r_1,\theta_1) = (r_2,\theta_2)$.

\item[(3)]
  $T(\mathrm{int}(E)) = D_0 = \mathrm{int}(D) - L$ (by (1)(2)).

\item[(4)]
  \emph{Show that $J_T(r,\theta) = r$.}
  \[
    J_T(r,\theta)
    =
    \det
    \begin{bmatrix}
      \cos\theta & -r\sin\theta \\
      \sin\theta & r\cos\theta
    \end{bmatrix}
    = r.
  \]

\item[(5)]
  \emph{If $f \in \mathscr{C}(D)$, show that}
  \[
    \int_{D} f(x,y)dx \: dy
  \]
  is well-defined.
  Similar to Example 10.4.
  \begin{enumerate}
  \item[(a)]
    Extend $f$ to a function on $I^2 = [-a,a]^2$
    by setting $f(x,y) = 0$ off $D$, and define
    \[
      \int_{D} f = \int_{I^2} f.
    \]
    Since $f$ may be discontinuous on $I^2$,
    the existence of the integral $\int_{I^2} f$.
    We also wish to show that this integral is independent of
    the order in which the $2$ integrations are carried out.

  \item[(b)]
    To do this, suppose $0 < \delta < 1$, put
    \begin{equation*}
      \varphi_{\delta}(t) =
      \begin{cases}
        1 & \text{ if $t \leq 1 - \delta$}, \\
        \frac{1-t}{\delta} & \text{ if $1 - \delta \leq t \leq 1$}, \\
        0 & \text{ if $t \geq 1$},
      \end{cases}
    \end{equation*}
    and define
    \[
      F_{\delta}(x,y) = \varphi_{\delta}\left(\frac{\sqrt{x^2+y^2}}{a}\right)f(x,y).
    \]
    Then $F_{\delta} \in \mathscr{C}(I^2)$ (or $\mathscr{C}(\mathbb{R}^2)$).

  \item[(c)]
    For each $x \in [-a,a]$, the set of all $y$ such that $F_{\delta}(x,y) \neq f(x,y)$
    is contained a union of two segment whose length does not exceed
    \[
      a\sqrt{1^2 - (1-\delta)^2}
      = a\sqrt{2\delta - \delta^2}
      < a\sqrt{2\delta}.
    \]
    Since $0 \leq \varphi_{\delta} \leq 1$, it follows that
    \[
      \abs{
        \int_{-a}^{a} F_{\delta}(x,y) dy - \int_{-a}^{a} f(x,y) dy
      }
      \leq 2a \sqrt{2\delta}\norm{f}
    \]
    where
    $\norm{f} = \max_{(x,y) \in I^2}|f(x,y)|$.
    So the sequence of continuous function
    \[
      \left\{ \int_{-a}^{a} F_{\delta}(x,y) dy \right\}
      \to
      \int_{-a}^{a} f(x,y) dy
      := g(x)
    \]
    uniformly as $\delta \to 0$.
    ($\delta = \frac{1}{n}$ for example.)
    So $g(x) \in \mathscr{C}([-a,a])$,
    and the further integrations present no problem, that is,
    $\int_{I^2} f$ is existed.

  \item[(d)]
    Moreover,
    \[
      \abs{ \int_{I^2} F_{\delta}(x,y) dxdy - \int_{I^2} f(x,y) dxdy }
      \leq 4a^2 \sqrt{2\delta} \norm{f}
    \]
    It is true, regardless of the order in which the $2$ single integrations
    are carried out.
    Since $F_{\delta} \in \mathscr{C}(I^2)$,
    $\int F_{\delta}$ is unaffected by any change in this order.
    Hence the inequality shows that the same is true of $\int f$.
  \end{enumerate}

\item[(6)]
  \emph{Show that}
  \[
    \int_{D} f(x,y)dx \: dy
    = \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta.
  \]
  \begin{enumerate}
  \item[(a)]
    Note that $T$ is a one-to-one $\mathscr{C}'$-mapping of an open set $D_0$
    such that $J_T(r,\theta) = r \neq 0$ for all $(r,\theta) \in \mathrm{int}(E)$.
    To apply Theorem 10.9, we will leverage Example 10.4 again.

  \item[(b)]
    Given any $\min\left\{ \frac{a}{89}, \frac{\pi}{64} \right \} > \delta > 0$.
    Define
    \[
      E_{\delta}
      =
      [\delta,a-\delta] \times [\delta,2\pi-\delta] \subseteq \textrm{int}(E)
    \]
    and $D_{\delta} = T(E_{\delta})$.
    Similar to Example 10.4,
    let $\varphi_{\delta}$ be a continuous function on $\mathbb{R}^2$
    such that $\varphi_{\delta}(x,y) = 1$ on $D_{\delta}$
    and $\varphi_{\delta}(x,y) = 0$ off $D_{\frac{\delta}{2}}$.
    Consider $f_{\delta}(x,y) = \varphi_{\delta}(x,y)f(x,y)$ on $\mathbb{R}^2$.
    By construction, $f_{\delta}$ is a continuous function on $\mathbb{R}^2$
    whose support is compact and lies in $D_{\frac{\delta}{2}} \subseteq D_0$.
    Hence, by Theorem 10.9
    \begin{align*}
      \int_{\mathbb{R}^2} f_{\delta}(x,y) dx dy
      &= \int_{D_{\frac{\delta}{2}}} f_{\delta}(x,y) dx dy \\
      &= \int_{E_{\frac{\delta}{2}}} f_{\delta}(T(r,\theta)) \abs{J_T(r,\theta)} dr d\theta \\
      &= \int_{\frac{\delta}{2}}^{a-\frac{\delta}{2}}
        \int_{\frac{\delta}{2}}^{2\pi-\frac{\delta}{2}}
        f(T(r,\theta)) r dr d\theta.
    \end{align*}

  \item[(c)]
    Since $f \in \mathscr{C}(D)$ and Exercise 6.7,
    \[
      \lim_{\delta \to 0} \int_{\frac{\delta}{2}}^{a-\frac{\delta}{2}}
        \int_{\frac{\delta}{2}}^{2\pi-\frac{\delta}{2}}
        f(T(r,\theta)) r dr d\theta
      = \int_{0}^{a}
        \int_{0}^{2\pi}
        f(T(r,\theta)) r dr d\theta.
    \]
    Therefore, it suffices to show that
    \[
      \lim_{\delta \to 0} \int_{\mathbb{R}^2} f_{\delta} = \int_{\mathbb{R}^2} f.
    \]

  \item[(d)]
    Note that $\int_{\mathbb{R}^2} f = \int_{D} f$ (by (5))
    and $\int_{\mathbb{R}^2} f_{\delta} = \int_{D_{\frac{\delta}{2}}} f_{\delta}
    = \int_{D} f_{\delta}$ (by (b)).
    So
    \[
      \abs{ \int_{\mathbb{R}^2} f - \int_{\mathbb{R}^2} f_{\delta} }
      = \abs{ \int_{D} f - \int_{D} f_{\delta} }.
    \]
    To estimate the difference between $\int_{D} f$ and $\int_{D} f_{\delta}$,
    we notice that $f$ and $f_{\delta}$ are coincide on $D_{\delta}$ by construction.
    Given any $(x,y) \in D - D_{\delta}$.
    Fix $y$, the set of all $x$ such that $f(x,y) \neq f_{\delta}(x,y)$
    is contained a union of two segment whose length does not exceed
    $\sqrt{a^2 - (a-\delta)^2 \cos^2\delta}$.
    Similarly as in (5), we have
    \[
      \abs{ \int_{D} f - \int_{D} f_{\delta} }
      \leq 4a \sqrt{a^2 - (a-\delta)^2 \cos^2\delta} \norm{f}.
    \]
    Hence, $\lim \int_{D} f_{\delta} = \int_{D} f$.
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.10.}
\emph{Let $a \to \infty$ in Exercise 10.9 and prove that
\[
  \int_{\mathbb{R}^2} f(x,y) dx dy
  = \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta,
\]
for continuous functions $f$ that decrease sufficiently rapidly
as $|x|+|y| \to \infty$.
(Find a more precise formulation.)
Apply this to
\[
  f(x,y) = \exp(-x^2-y^2)
\]
to derive formula}
\[
  \int_{-\infty}^{\infty} e^{-s^2} ds = \sqrt{\pi}.
\]



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define
  \begin{align*}
    D_a &= \{ (x,y) \in \mathbb{R}^2 : x^2+y^2 \leq a^2 \}, \\
    I_a &= \{ (x,y) \in \mathbb{R}^2 : |x| \leq a, |y| \leq a \},
  \end{align*}
  for any $a > 0$.
  Similar to Definition 10.1,
  define
  \[
    \int_{\mathbb{R}^2} f(x,y) dx dy
    = \lim_{a \to \infty} \int_{I_a} f(x,y) dx dy.
  \]
  Besides, since $f \in \mathscr{C}(\mathbb{R}^2)$, we have
  \begin{align*}
    \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta
    &= \lim_{a \to \infty} \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta \\
    &= \lim_{a \to \infty} \int_{D_a} f(x,y) dx dy
  \end{align*}
  by the definition in Exercise 6.8 and Exercise 10.9.
  So it suffices to show that
  both $\lim_{a \to \infty} \int_{I_a} f(x,y) dx dy$
  and $\lim_{a \to \infty} \int_{D_a} f(x,y) dx dy$ exist and they are equal
  if $f$ decreases sufficiently rapidly as $|x|+|y| \to \infty$.

\item[(2)]
  Say that $f$ decreases sufficiently rapidly as $|x|+|y| \to \infty$
  if there is a real number $M > 0$ such that
  \[
    |f(x,y)| \leq \alpha (x^2+y^2)^{-1-\beta}
  \]
  for some $\alpha > 0$ and $\beta > 0$
  whenever $|x|+|y| \geq M$.
  Similar to Theorem 3.29, we can give other weaker hypotheses:
  \[
    |f(x,y)|
    \leq
    \alpha (x^2+y^2)^{-1} (\log(x^2+y^2))^{-1-\beta}
  \]
  or
  \[
    |f(x,y)|
    \leq
    \alpha (x^2+y^2)^{-1} (\log(x^2+y^2))^{-1} (\log\log(x^2+y^2))^{-1-\beta}
  \]
  and so on.

\item[(3)]
  Write
  \begin{align*}
    & \: \int_{0}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta \\
    =& \:
      \int_{0}^{M} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta
      + \int_{M}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta.
  \end{align*}
  Note that
  \[
    \int_{0}^{M} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta < +\infty
  \]
  since $f \in \mathscr{C}(\mathbb{R}^2)$.
  Besides,
  \begin{align*}
    \int_{M}^{a} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta
    \leq& \:
    \int_{M}^{a} \int_{0}^{2\pi} \alpha (x^2+y^2)^{-1-\beta} r dr d\theta \\
    =& \:
    \int_{M}^{a} \int_{0}^{2\pi} \alpha r^{-1-2\beta} dr d\theta \\
    =& \:
    \pi \alpha \beta^{-1} (M^{-2\beta} - a^{-2\beta})
  \end{align*}
  is finite as $a \to \infty$.
  Hence, $\int_{0}^{\infty} \int_{0}^{2\pi} \abs{f(T(r,\theta))} r dr d\theta$
  converges absolutely,
  or $\lim_{a \to \infty} \int_{D_a} f(x,y) dx dy$ exists.

\item[(4)]
  Note that
  \[
    \int_{D_a} |f(x,y)| dx dy
    \leq \int_{I_a} |f(x,y)| dx dy
    \leq \int_{D_{\sqrt{2}a}} |f(x,y)| dx dy.
  \]
  Let $a \to \infty$,
  we have
  \[
    \lim_{a \to \infty} \int_{I_a} |f(x,y)| dx dy
    = \lim_{a \to \infty} \int_{D_a} |f(x,y)| dx dy
  \]
  also exists.

\item[(5)]
  \emph{Show that $\int_{-\infty}^{\infty} e^{-s^2} ds = \sqrt{\pi}$.}
  Take $f(x,y) = \exp(-x^2-y^2) \in \mathscr{C}(\mathbb{R}^2)$.
  So
  \[
    \lim_{x^2+y^2 \to \infty} |f(x,y)|(x^2+y^2)^2
    = \lim_{x^2+y^2 \to \infty} e^{-(x^2+y^2)} (x^2+y^2)^2
    = 0
  \]
  by Theorem 8.6(f).
  Note that
  \begin{align*}
    \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta
    &= \int_{0}^{\infty} \int_{0}^{2\pi} e^{-r^2} r dr d\theta \\
    &= 2 \pi \int_{0}^{\infty} e^{-r^2} r dr \\
    &= \pi \left[-e^{-r^2}\right]_{r=0}^{r=\infty} \\
    &= \pi
  \end{align*}
  and
  \begin{align*}
    \int_{\mathbb{R}^2} f(x,y) dx dy
    &=
    \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
      e^{-x^2-y^2} dx dy \\
    &=
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}
    \left\{ \int_{-\infty}^{\infty} e^{-y^2} dy \right\} \\
    &=
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}^2.
  \end{align*}
  Hence,
  \begin{align*}
    & \:
    \int_{\mathbb{R}^2} f(x,y) dx dy
    = \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr d\theta \\
    \Longrightarrow& \:
    \left\{ \int_{-\infty}^{\infty} e^{-x^2} dx \right\}^2 = \pi \\
    \Longrightarrow& \:
    \int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}.
  \end{align*}
  (Here $\int_{-\infty}^{\infty} e^{-x^2} dx$ is always nonnegative.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.11.}
\emph{Define $(u,v)=T(s,t)$ on the strip
\[
  0 < s < \infty,
  \qquad
  0 < t < 1
\]
by setting $u=s-st$, $v=st$.
Show that $T$ is a $1$-$1$ mapping of the strip onto the positive quadrant $Q$ in $\mathbb{R}^2$.
Show that $J_T(s,t) = s$.
For $x > 0$, $y > 0$, integrate
\[
  u^{x-1} e^{-u} v^{y-1} e^{-v}
\]
over $Q$, use Theorem 10.9 to convert the integral to one over the strip,
and derive
\[
  \int_{0}^{1} t^{x-1}(1-t)^{y-1} dt = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\]
in this way.
(For this application,
Theorem 10.9 has to be extended so as to cover certain improper integrals.
Provide this extension.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \begin{align*}
    S &= \{ (s,t) \in \mathbb{R}^2 : 0 < s < \infty, 0 < t < 1 \} \\
    Q &= \{ (u,v) \in \mathbb{R}^2 : u > 0, v > 0 \}.
  \end{align*}

\item[(2)]
  \emph{Show that $T$ maps $S$ onto $Q$.}
  Given any $(u,v) \in Q$.
  It is equivalent to solve $(s,t)$ from
  \begin{align*}
    u &= s-st \\
    v &= st
  \end{align*}
  in terms of $(u,v)$.
  It is trivial since $s = (s-st)+st = u+v \in (0,\infty)$
  and $t = \frac{v}{s} = \frac{v}{u+v} \in (0,1)$.
  Hence, $T: (u+v,\frac{v}{u+v}) \mapsto (u,v)$ is onto.

\item[(3)]
  \emph{Show that $T$ is one-to-one.}
  Suppose $(s_1,t_1), (s_2,t_2) \in S$
  and $T(s_1,t_1) = T(s_2,t_2) = (u,v) \in Q$.
  Then
  \begin{align*}
    u &= s_1 - s_1 t_1 = s_2 - s_2 t_2, \\
    v &= s_1 t_1 = s_2 t_2.
  \end{align*}
  Note that $s_1 = s_2 = u + v > 0$.
  As $u+v \neq 0$, $t_1 = \frac{v}{s_1} = \frac{v}{s_2} = t_2$.
  Therefore, $(s_1,t_1) = (s_2,t_2)$.

\item[(4)]
  \emph{Show that $J_T(s,t) = s$.}
  \[
    J_T(s,t)
    =
    \det
    \begin{bmatrix}
      1-t & -s \\
      t & s
    \end{bmatrix}
    = s.
  \]

\item[(5)]
  Let $f(u,v) = u^{x-1} e^{-u} v^{y-1} e^{-v}$ be defined on $Q$.
  Note that
  \begin{align*}
    \int_{Q} f(u,v) du dv
    &= \int_{0}^{\infty} \int_{0}^{\infty} u^{x-1} e^{-u} v^{y-1} e^{-v} du dv \\
    &= \left\{ \int_{0}^{\infty} u^{x-1} e^{-u} du \right\}
      \left\{ \int_{0}^{\infty} v^{y-1} e^{-v} dv \right\} \\
    &= \Gamma(x) \Gamma(y)
  \end{align*}
  and
  \begin{align*}
    \int_{S} f(T(s,t))s ds dt
    &= \int_{0}^{\infty} \int_{0}^{1} s^{x+y-1} e^{-s} t^{y-1}(1-t)^{x-1} ds dt \\
    &= \left\{ \int_{0}^{\infty} s^{x+y-1} e^{-s} ds \right\}
      \left\{ \int_{0}^{1} t^{y-1}(1-t)^{x-1} dt \right\} \\
    &= \Gamma(x+y)
      \left\{ \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt \right\}.
  \end{align*}
  Now we assume that $\int_{Q} f(u,v) du dv = \int_{S} f(T(s,t))s ds dt$ holds, then
  \[
    \Gamma(x) \Gamma(y)
    =
    \Gamma(x+y)
    \left\{ \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt \right\}
  \]
  or
  \[
    \int_{0}^{1} t^{x-1} (1-t)^{y-1} dt = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}.
  \]

\item[(6)]
  \emph{Given $f \in \mathscr{C}(Q)$.
  Suppose both $\int_{Q} f(u,v) du dv$ and $\int_{S} f(T(s,t))s ds dt$ converge
  as improper integrals in the sense of Exercises 6.7 and 6.8.
  Show that}
  \[
    \int_{Q} f(u,v) du dv = \int_{S} f(T(s,t))s ds dt
  \]
  \begin{enumerate}
  \item[(a)]
    Given $a > 64$ and $1 > \delta > 0$.
    Define
    \begin{align*}
      Q_{a,\delta} &= [\delta, a-\delta]^2 \subseteq Q \\
      S_{a,\delta} &= [2\delta, 2a-2\delta] \times
        \left[ \frac{\delta}{2a-\delta}, 1-\frac{\delta}{2a-\delta} \right] \subseteq S.
    \end{align*}
    So $T(S_{a,\delta}) = Q_{a,\delta}$.
  
  \item[(b)]
    Similar to (6) in the proof of Exercise 10.9,
    there is a continuous function $\varphi_{a,\delta}$ on $\mathbb{R}^2$
    such that $\varphi_{a,\delta}(x,y) = 1$ on $Q_{a,\delta}$
    and $\varphi_{a,\delta}(x,y) = 0$ off $Q_{a,\frac{\delta}{2}}$.
    Consider $f_{a,\delta}(x,y) = \varphi_{a,\delta}(x,y)f(x,y)$ on $\mathbb{R}^2$.
    By construction, $f_{a,\delta}$ is a continuous function on $\mathbb{R}^2$
    whose support is compact and lies in $Q_{a,\frac{\delta}{2}} \subseteq Q$.
    Hence, by Theorem 10.9
    \begin{align*}
      \int_{\mathbb{R}^2} f_{a,\delta}(u,v) du dv
      &= \int_{Q_{a,\frac{\delta}{2}}} f_{a,\delta}(u,v) du dv \\
      &= \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) \abs{J_T(s,t)} ds dt \\
      &= \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) s ds dt \\
      &= \int_{\mathbb{R}^2} f_{a,\delta}(T(s,t)) s ds dt.
    \end{align*}

  \item[(c)]
    Given any $\varepsilon > 0$.
    Since $\int_{Q} f(u,v) du dv$ converges,
    there exists a real number $a > 0$ such that
    \[
      \abs{ \int_{[0,a]^2} f(u,v) du dv - \int_{Q} f(u,v) du dv }
      \leq \frac{\varepsilon}{89}.
    \]
    Write
    \begin{align*}
      \abs{ \int_{\mathbb{R}^2} f_{a,\delta} \: du dv - \int_{\mathbb{R}^2} f \: du dv }
      =& \: \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{Q} f \: du dv } \\
      \leq& \:
        \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv } \\
        &+ \abs{ \int_{[0,a]^2} f \: du dv - \int_{Q} f \: du dv }.
    \end{align*}
    So
    \[
      \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv }
      \leq
      \int_{[0,a]^2} \abs{f_{a,\delta} - f} \: du dv.
    \]
    Notice that $f$ and $f_{a,\delta}$ are coincide on $Q_{a,\delta}$ by construction.
    A simple calculation shows that
    \[
      \int_{[0,a]^2} \abs{f_{a,\delta} - f} \: du dv
      \leq
      2a \delta \norm{f}.
    \]
    Choose $\delta
    = \min \left\{ \frac{1}{2}, \frac{\varepsilon}{64 \cdot 2a(\norm{f} + 1)} \right\} > 0$
    so that
    \[
      \abs{ \int_{Q} f_{a,\delta} \: du dv - \int_{[0,a]^2} f \: du dv }
      \leq
      \frac{\varepsilon}{64}.
    \]
    Hence,
    \[
      \abs{ \int_{\mathbb{R}^2} f_{a,\delta} \: du dv - \int_{\mathbb{R}^2} f \: du dv }
      \leq \frac{\varepsilon}{89} + \frac{\varepsilon}{64}
      < \varepsilon.
    \]
    Therefore,
    \[
      \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta} \: du dv
      = \int_{\mathbb{R}^2} f \: du dv.
    \]

  \item[(d)]
    Similar to (c),
    \[
      \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{S_{a,\frac{\delta}{2}}} f_{a,\delta}(T(s,t)) s \: ds dt
      = \int_{\mathbb{R}^2} f(T(s,t)) s \: ds dt.
    \]
    Hence
    \begin{align*}
      \int_{Q} f(u,v) \: du dv
      &= \int_{\mathbb{R}^2} f(u,v) \: du dv \\
      &= \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta}(u,v) \: du dv \\
      &= \lim_{\substack{a \to \infty \\ \delta \to 0}}
        \int_{\mathbb{R}^2} f_{a,\delta}(T(s,t)) s \: ds dt \\
      &= \int_{\mathbb{R}^2} f(T(s,t)) s \: ds dt \\
      &= \int_{S} f(T(s,t)) s \: ds dt.
    \end{align*}
  \end{enumerate}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.12.}
\emph{Let $I^k$ be the set of all $\mathbf{u} = (u_1,\ldots,u_k) \in \mathbb{R}^k$
with $0 \leq u_i \leq 1$ for all $i$;
let $Q^k$ be the set of all $\mathbf{x} = (x_1,\ldots,x_k) \in \mathbb{R}^k$
with $x_i \geq 0$, $\sum x_i \leq 1$.
($I^k$ is the unit cube; $Q^k$ is the standard simplex in $\mathbb{R}^k$.)
Define $\mathbf{x} = T(\mathbf{u})$ by
\begin{align*}
  x_1 &= u_1 \\
  x_2 &= (1-u_1)u_2 \\
  &\cdots \\
  x_k &= (1-u_1) \cdots (1-u_{k-1})u_k.
\end{align*}
Show that
\[
  \sum_{i=1}^{k} x_i = 1 - \prod_{i=1}^{k} (1-u_i).
\]
Show that $T$ maps $I^k$ onto $Q^k$, that $T$ is $1$-$1$ in the interior of $I^k$,
and that its inverse $S$ is defined in the interior of $Q^k$ by $u_1 = x_1$ and
\[
  u_i = \frac{x_i}{1-x_1-\cdots-x_{i-1}}
\]
for $i = 2, \ldots, k$.
Show that
\[
  J_T(\mathbf{u}) = (1-u_1)^{k-1}(1-u_2)^{k-2} \cdots (1-u_{k-1}),
\]
and}
\[
  J_S(\mathbf{x}) = [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
\] \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that
  \[
    \sum_{i=1}^{m} x_i = 1 - \prod_{i=1}^{m} (1-u_i)
  \]
  for all $1 \leq m \leq k$.}
  Induction on $m$.
  Base case: $x_1 = 1 - (1-u_1)$.
  Inductive step: Suppose the case $m = h$ is true.
  Consider the the case $m = h+1$:
  \begin{align*}
    \sum_{i=1}^{h+1} x_i
    &= \left( \sum_{i=1}^{h} x_i \right) + x_{h+1} \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + x_{h+1}
      &(\text{Induction hypothesis}) \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + u_{h+1}\prod_{i=1}^{h} (1-u_i)
      &(\text{Definition of $x_{h+1}$}) \\
    &= 1 - (1-u_{h+1})\prod_{i=1}^{h} (1-u_i) \\
    &= 1 - \prod_{i=1}^{h+1} (1-u_i).
  \end{align*}
  Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement is established.

\item[(2)]
  \emph{Show that $T$ maps $I^k$ onto $Q^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$.
  It is equivalent to solve $\mathbf{u} = (u_1, \ldots, u_k)$ from
  \begin{align*}
    x_1 &= u_1 \\
    x_2 &= (1-u_1)u_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k
  \end{align*}
  in terms of $\mathbf{x} = (x_1, \ldots, x_k)$.
  It is clear that $u_1 = x_1$
  and
  \begin{equation*}
    u_i =
      \begin{cases}
        x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
          & \text{ if $x_1+\cdots+x_{i-1} \neq 1$}, \\
        0
          & \text{ if $x_1+\cdots+x_{i-1} = 1$}.
      \end{cases}
  \end{equation*}
  for $i = 2, \ldots, k$.
  (If $x_1+\cdots+x_{i-1} \neq 1$, by (1) we have
  \[
    \prod_{j=1}^{i-1}(1-u_j)
    = 1 - \sum_{j=1}^{i-1} x_i
    \neq 0
  \]
  and thus
  \[
    u_i
    = x_i \left\{ \prod_{j=1}^{i-1}(1-u_j) \right\}^{-1}
    = x_i(1 - x_1 - \cdots - x_{i-1})^{-1}.
  \]
  If $x_1+\cdots+x_{i-1} = 1$,
  then $x_{i} = \cdots = x_{k} = 0$.
  We may take $u_{i} = 0$ to set the expression $x_{i} = (1-u_1)\cdots(1-u_{i-1})u_i$ to zero.)
  Note that the solution $\mathbf{u} \in I^k$ is well-defined by construction,
  or $T(I^k) = Q^k$.

\item[(3)]
  \emph{Show that $T$ is $1$-$1$ in the interior of $I^k$.}
  Suppose $T(\mathbf{u}) = T(\mathbf{v}) = \mathbf{x}$ with
  $\mathbf{u}, \mathbf{v} \in \mathrm{int}(I^k)$.
  Then we consider the following equation:
  \begin{align*}
    x_1 &= u_1 = v_1 \\
    x_2 &= (1-u_1)u_2 = (1-v_1)v_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k = (1-v_1) \cdots (1-v_{k-1})v_k.
  \end{align*}
  By (1),
  \[
    \mathbf{x} \in \mathrm{int}(Q^k)
    = \left\{ (x_1,\ldots,x_k) \in \mathbb{R}^k : x_i > 0, \sum x_i < 1 \right\}.
  \]
  Hence,
  \begin{align*}
    u_1 = v_1 &= x_1 \\
    u_2 = v_1 &= x_2(1-x_1)^{-1} \\
    &\cdots \\
    u_k = v_k &= x_k(1-x_1-\cdots-x_{k-1})^{-1}.
  \end{align*}
  Here all $(1-x_1)^{-1}, \ldots, (1-x_1-\cdots-x_{i})^{-1}$
  are well-defined since $\mathbf{x} \in \mathrm{int}(Q^k)$.
  Therefore, $T$ is injective on $\mathrm{int}(I^k)$.

\item[(4)]
  By (2)(3), $T$ maps $\mathrm{int}(I^k)$ onto $\mathrm{int}(Q^k)$.
  That is, given any $\mathbf{x} = (x_1,\ldots,x_k) \in \mathrm{int}(Q^k)$,
  we can pick
  \begin{align*}
    u_1 &= x_1 \\
    u_i &= x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
    \qquad
    (i = 2, \ldots, k)
  \end{align*}
  such that $\mathbf{u} \in \mathrm{int}(I^k)$ and $T(\mathbf{u}) = \mathbf{x}$ .

\item[(5)]
  Note that
  $T(\mathbf{u}) = (u_1, (1-u_1)u_2, \ldots, (1-u_1)\cdots(1-u_{k-1})u_{k})$
  on $\mathrm{int}(I^k)$.
  So
  \[
    T'(\mathbf{u})
    =
    \begin{bmatrix}
      1      & 0       & 0                      & \cdots & 0 \\
      *      & (1-u_1) & 0                      & \cdots & 0 \\
      *      & *       & \prod_{i=1}^{2}(1-u_i) & \cdots & 0 \\
      \vdots &  \vdots & \vdots                 & \ddots & \vdots \\
      *      & *       & *                      & \cdots & \prod_{i=1}^{k-1}(1-u_i)
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_T(\mathbf{u})
    &= \det T'(\mathbf{u}) \\
    &= 1 \cdot (1-u_1) \cdot \prod_{i=1}^{2}(1-u_i) \cdots \prod_{i=1}^{k-1}(1-u_i) \\
    &= \prod_{i=1}^{k-1}(1-u_i)^{k-i}.
  \end{align*}

\item[(6)]
  Similar to (5).
  $S(\mathbf{x}) = (x_1, x_2(1-x_1)^{-1}, \ldots, x_k(1-x_1-\cdots-x_{k-1})^{-1})$
  on $\mathrm{int}(Q^k)$.
  So
  \[
    S'(\mathbf{x})
    =
    \begin{bmatrix}
      1      & 0            & 0                & \cdots & 0 \\
      *      & (1-x_1)^{-1} & 0                & \cdots & 0 \\
      *      & *            & (1-x_1-x_2)^{-1} & \cdots & 0 \\
      \vdots &  \vdots      & \vdots           & \ddots & \vdots \\
      *      & *            & *                & \cdots & (1-x_1-\cdots-x_{k-1})^{-1}
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_S(\mathbf{x})
    &= \det S'(\mathbf{x}) \\
    &= 1 \cdot (1-x_1)^{-1} \cdot (1-x_1-x_2)^{-1} \cdots (1-x_1-\cdots-x_{k-1})^{-1} \\
    &= [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.13.}
\emph{Let $r_1, \ldots, r_k$ be nonnegative integers, and prove that
\[
  \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
  =
  \frac{r_1! \cdots r_k!}{(k+r_1+\cdots+r_k)!}
\]
(Hint: Use Exercise 10.12, Theorems 10.9 and 8.20.)
Note that the special case $r_1 = \cdots = r_k = 0$
shows that the volume of $Q^k$ is $\frac{1}{k!}$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define $T: I^k$ onto $Q^k$ as in Exercise 10.12,
  and $f: Q^k \to \mathbb{R}^1$ by
  \[
    f(\mathbf{x})
    = f(x_1, \ldots, x_k)
    = x_1^{r_1} \cdots x_k^{r_k}
    = \prod_{i=1}^{k} x_i^{r_i}.
  \]

\item[(2)]
  By Exercise 10.12, Example 10.4 and Theorems 10.9, we have
  \begin{align*}
    \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
    &=
    \int_{Q^k} f(\mathbf{x}) d\mathbf{x} \\
    &=
    \int_{I^k} f(T(\mathbf{u})) |J_T(\mathbf{u})| d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} \left( u_i \prod_{j=1}^{i-1}(1-u_j) \right)^{r_i}
      \prod_{i=1}^{k}(1-u_i)^{k-i} d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} d\mathbf{u} \\
    &=
    \prod_{i=1}^{k}
      \int_{0}^{1} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} du_i
      & (\text{Theorem 10.2}) \\
    &=
    \prod_{i=1}^{k}
      \frac{r_i! \left(k-i+\sum_{j=i+1}^{k}r_j\right)!}
        {\left(k-i+1+\sum_{j=i}^{k}r_j\right)!}
      & (\text{Theorem 8.20}) \\
    &=
    \frac{r_1! \cdots r_k!}{(k + r_1 + \cdots + r_k)!}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.14 (Levi-Civita symbol).}
\emph{Prove $\varepsilon(j_1, \ldots, j_k) = s(j_1, \ldots, j_k)$,
where}
\[
  s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p).
\] \\

It is usually to define the Levi-Civita symbol by
\begin{equation*}
\varepsilon(j_1, \ldots, j_k) =
  \begin{cases}
    1
      & \text{ if $(j_1,\cdots,j_k)$ is an even permutation of $J$}, \\
    -1
      & \text{ if $(j_1,\cdots,j_k)$ is an odd permutation of $J$}, \\
    0
      & \text{otherwise}
  \end{cases}
\end{equation*}
(Basic $k$-forms 10.14).
Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
So $\varepsilon(j_1, \ldots, j_k)$ is equivalent to an explicit expression
$s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p)$. \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Induction on $k$.
  Base case: \emph{Show that $\varepsilon(j_1,j_2) = s(j_1,j_2)$.}
  Since
  \begin{equation*}
  \varepsilon(j_1,j_2) =
    \begin{cases}
      1
        & \text{ if $j_1 < j_2$} \\
      -1
        & \text{ if $j_1 > j_2$},
    \end{cases}
  \end{equation*}
  $\varepsilon(j_1,j_2) = \mathrm{sgn}(j_2-j_1) = s(j_1,j_2)$.

\item[(2)]
  Inductive step: \emph{Show that for any $s \geq 2$,
  if $\varepsilon(j_1, \ldots, j_{s}) = s(j_1, \ldots, j_{s})$ holds,
  then $\varepsilon(j_1, \ldots, j_{s+1}) = s(j_1, \ldots, j_{s+1})$ also holds.}
  \begin{align*}
    \varepsilon(j_1, \ldots, j_{s+1})
    &= \varepsilon(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= s(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s} \mathrm{sgn}(j_q-j_p)
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s+1} \mathrm{sgn}(j_q - j_p) \\
    &= s(j_1, \ldots, j_{s+1}).
  \end{align*}

\item[(3)]
  Conclusion: Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement holds for every integer $k \geq 2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.15.}
\emph{If $\omega$ and $\lambda$ are $k$- and $m$-forms, respectively,
prove that}
\[
  \omega \wedge \lambda = (-1)^{km} \lambda \wedge \omega.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \omega = \sum_I b_I(\mathbf{x}) dx_I,
    \qquad
    \lambda = \sum_J c_J(\mathbf{x}) dx_J
  \]
  in the stardard presentations,
  where $I$ and $J$ range over all increasing $k$-indices
  and over all increasing $m$-indices taken from the set $\{1,\ldots,n\}$.

\item[(2)]
  \emph{Show that $dx_I \wedge dx_J = (-1)^{km} dx_J \wedge dx_I$.}
  \begin{align*}
    dx_I \wedge dx_J
    &= dx_{i_1} \wedge \cdots \wedge dx_{i_k}
      \wedge dx_J \\
    &= (-1)^m dx_{i_1} \wedge \cdots \wedge dx_{i_{k-1}}
      \wedge dx_J \wedge dx_{i_{k}} \\
    &= (-1)^{2m} dx_{i_1} \wedge \cdots \wedge dx_{i_{k-2}}
      \wedge dx_J \wedge dx_{i_{k-1}} \wedge dx_{i_{k}} \\
    &\cdots \\
    &= (-1)^{km} dx_J
      \wedge dx_{i_1} \wedge \cdots \wedge dx_{i_k} \\
    &= (-1)^{km} dx_J \wedge dx_I.
  \end{align*}

\item[(3)]
  \begin{align*}
    \omega \wedge \lambda
    &= \sum_{I,J} b_I(\mathbf{x}) c_J(\mathbf{x}) dx_I \wedge dx_J \\
    &= (-1)^{km} \sum_{J,I} c_J(\mathbf{x}) b_I(\mathbf{x}) dx_J \wedge dx_I \\
    &= (-1)^{km} \lambda \wedge \omega.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.16.}
\emph{If $k \geq 2$ and $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$
is an oriented affine $k$-simplex, prove that $\partial^2 \sigma = 0$,
directly from the definition of the boundary operator $\partial$.
Deduce from this that $\partial^2 \Psi = 0$ for every chain $\Psi$.
(Hint: For orientation, do it first for $k=2$, $k=3$.
In general, if $i < j$, let $\sigma_{ij}$ be the $(k-2)$-simplex obtained by
deleting $\mathbf{p}_i$ and $\mathbf{p}_j$ from $\sigma$.
Show that each $\sigma_{ij}$ occurs twice in $\partial^2\sigma$, with opposite sign.)} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Write the boundary of the oriented affine $k$-simplex
  $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$ as
  \[
    \partial \sigma
    = \sum_{i=0}^{k}(-1)^i
    [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]
  \]
  where where the oriented $(k-1)$-simplex
  $[\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]$
  is obtained by deleting $\sigma$'s $i$-th vertex (Boundaries 10.29).

\item[(2)]
  \begin{align*}
    \partial^2 \sigma
    =& \partial \left( \sum_{i}(-1)^{i}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \right) \\
    =& \sum_{i}(-1)^{i}
      \partial [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i}(-1)^{j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &+ \sum_{j>i} (-1)^{i}(-1)^{j-1}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &- \sum_{j>i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k].
  \end{align*}
  The latter two summations cancel since after switching $i$ and $j$ in the second sum.
  Therefore $\partial^2 \sigma = 0$.

\item[(3)]
  The boundary of a chain is the linear combination of boundaries of the simplices in the chain.
  Write $\Psi = \sum_{i=1}^{r} \sigma_i$. where $\sigma_i$ is an oriented affine simplex.
  Then
  \[
    \partial^2 \Psi
    = \partial \left(\partial \sum \sigma_i \right)
    = \partial \left( \sum \partial\sigma_i \right)
    = \sum \partial^2 \sigma_i
    = \sum 0
    = 0
  \]
  for any affine chain $\Psi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.17.}
\emph{Put $J^2 = \tau_1 + \tau_2$, where
\[
  \tau_1 = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2],
  \qquad
  \tau_2 = -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1].
\]
Explain why it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.
Show that $\partial J^2$ is the sum of $4$ oriented affine $1$-simplexes.
Find these.
What is $\partial(\tau_1 - \tau_2)$?} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that the unit square $I^2 \in \mathbb{R}^2$ is the union of
  $\tau_1(Q^2)$ and $\tau_2(Q_2)$, where
  \begin{align*}
    \tau_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2) \\
    &= \mathbf{0} + (\alpha_1+\alpha_2) \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 1 \\
        0 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  and
  \begin{align*}
    \tau_2(\mathbf{u})
    &= (-[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1])(\mathbf{u}) \\
    &= ([\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 (\mathbf{e}_1+\mathbf{e}_2) + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + (\alpha_1+\alpha_2) \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 0 \\
        1 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  where $\mathbf{u} = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \in \mathbb{R}^2$
  (as in Equation (78)).
  Both $\tau_1$ and $\tau_2$ have Jacobian $1 > 0$, or positively oriented
  (Affine simplexes 10.26).
  So it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.

\item[(2)]
  \begin{align*}
    \partial \tau_1
    &= [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_1], \\
    \partial \tau_2
    &= [\mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1] \\
    &= [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}]
      + [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2].
  \end{align*}

\item[(3)]
  By (2),
  \[
    \partial J^2
    = \partial \tau_1 + \partial \tau_2
    = [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}],
  \]
  which is the positively oriented boundary of $I^2$.

\item[(4)]
  By (2),
  \begin{align*}
    \partial(\tau_1 - \tau_2)
    =& \partial \tau_1 - \partial \tau_2 \\
    =& [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}] \\
      &+ [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}].
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.18.}
\emph{Consider the oriented affine $3$-simplex
\[
  \sigma_1
  = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3]
\]
in $\mathbb{R}^3$.
Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.
Thus $\sigma_1$ is positively oriented.} \\

\emph{Let $\sigma_2, \ldots, \sigma_6$ be five other oriented $3$-simplexes,
obtained as follows:
There are five permutations $(i_1, i_2, i_3)$ of $(1, 2, 3)$,
distinct from $(1, 2, 3)$.
Associate with each $(i_1, i_2, i_3)$ the simplex
\[
  s(i_1, i_2, i_3)
  [
    \mathbf{0},
    \mathbf{e}_{i_1},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
  ]
\]
where $s$ is the sign that occurs in the definition of the determinant.
(This is how $\tau_2$ was obtained from $\tau_1$ in Exercise 10.17.)
Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.} \\

\emph{Put $J^3 = \sigma_1+\cdots+\sigma_6$.
Then $J^3$ may be called the positively oriented unit cube in $\mathbb{R}^3$.
Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.
(These $12$ triangles cover the surface of the unit cube $I^3$.)} \\

\emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.} \\

\emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
and that their union covers $I^3$.
(Compared with Exercise 10.13; note that $3!=6$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.}
  Given any $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$,
  we have
  \begin{align*}
    \sigma_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3])
    (\mathbf{u}) \\
    &= \mathbf{0}
      + \alpha_1 \mathbf{e}_1
      + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2)
      + \alpha_3 (\mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3) \\
    &= \mathbf{0}
      + (\alpha_1+\alpha_2+\alpha_3) \mathbf{e}_1
      + (\alpha_2+\alpha_3) \mathbf{e}_2
      + \alpha_3 \mathbf{e}_3 \\
    &= \mathbf{0} +
      \underbrace{\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
      \end{bmatrix}}_{\text{say }A}
      \mathbf{u}.
  \end{align*}
  So
  \[
    \det(A)
    =
    \det
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    = 1.
  \]

\item[(2)]
  \emph{Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.}
  Define the permutation matrix $P_{(i_1,i_2,i_3)}$ corresponding to
  a permutation $(i_1,i_2,i_3)$ of $(1,2,3)$ by
  \[
    P_{(i_1,i_2,i_3)}
    =
    \begin{bmatrix}
      \mathbf{e}_{i_1} & \mathbf{e}_{i_2} & \mathbf{e}_{i_3}
    \end{bmatrix}.
  \]
  For example,
  \[
    P_{(2,3,1)}
    =
    \begin{bmatrix}
      \mathbf{e}_{2} & \mathbf{e}_{3} & \mathbf{e}_{1}
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}.
  \]
  Note that the sign $s(i_1,i_2,i_3)$ of the permutation $(i_1,i_2,i_3)$
  is exactly the same as the determinant of the permutation matrix $P_{(i_1,i_2,i_3)}$.
  Define a permutation $(j_1, j_2, 3)$ of $(1, 2, 3)$
  (for swapping the first and the second coordinates of $\mathbf{u}$)
  by
  \begin{equation*}
    (j_1, j_2, 3) =
      \begin{cases}
        (1, 2, 3) & \text{ if $s(i_1,i_2,i_3) = 1$}, \\
        (2, 1, 3) & \text{ if $s(i_1,i_2,i_3) = -1$}.
      \end{cases}
  \end{equation*}
  Write
  \[
    \sigma_{(i_1, i_2, i_3)}
    =
    s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ].
  \]
  (So that $\sigma_1 = \sigma_{(1,2,3)}$.)
  Hence,
  \begin{align*}
    &
    \sigma_{(i_1, i_2, i_3)}(\mathbf{u}) \\
    =& \mathbf{0}
      + \alpha_{j_1} \mathbf{e}_{i_1}
      + \alpha_{j_2} (\mathbf{e}_{i_1}+\mathbf{e}_{i_2})
      + \alpha_3 (\mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}) \\
    =& \mathbf{0}
      + (\alpha_{j_1}+\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_1}
      + (\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_2}
      + \alpha_3 \mathbf{e}_{i_3} \\
    =& \mathbf{0} + P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}
  \end{align*}
  where $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$.
  For example,
  \[
    P_{(2,3,1)} A P_{(1,2,3)}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 1 & 1 \\
      0 & 1 & 1
    \end{bmatrix}.
  \]
  So
  \begin{align*}
    \det(P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)})
    &= \det(P_{(i_1,i_2,i_3)}) \det(A) \det(P_{(j_1,j_2,3)}) \\
    &= s(i_1, i_2, i_3) \cdot 1 \cdot s(i_1, i_2, i_3) \\
    &= 1.
  \end{align*}

\item[(3)]
  \emph{Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.}
  Note that
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_1 < i_2}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_1}} -s(i_2, i_1, i_3)
      [
        \mathbf{0},
        \mathbf{e}_{i_2}+\mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =& \mathbf{0}
  \end{align*}
  and
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_2 < i_3}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_3 > i_2}} -s(i_1, i_3, i_2)
      [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =&
    \mathbf{0}.
  \end{align*}
  So
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} \partial \sigma_{(i_1, i_2, i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &+ s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &+ \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ].
  \end{align*}
  Thus,
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ]
  \end{align*}
  is the sum of $12$ oriented affine $2$-simplexes. (Note that $3! = 6$.)

\item[(4)]
  \emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
  if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.}
  \begin{enumerate}
  \item[(a)]
    By (1),
    $\mathbf{x}$ is in the range of $\sigma_1$ if and only if
    $\mathbf{x} = A\mathbf{u}$ for $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$, or
    \[
      \begin{bmatrix}
          x_1 \\
          x_2 \\
          x_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          1 & 1 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1
      \end{bmatrix}
      \begin{bmatrix}
          u_1 \\
          u_2 \\
          u_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          u_1+u_2+u_3 \\
          u_2+u_3 \\
          u_3
      \end{bmatrix}.
    \]

  \item[(b)]
    Since $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$,
    $u_1+u_2+u_3 \leq 1$ and $u_1,u_2,u_3 \geq 0$.
    Hence $0 \leq u_3 \leq u_2+u_3 \leq u_1+u_2+u_3 \leq 1$
    or $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.

  \item[(c)]
    Conversely, if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$,
    we define
    \[
      \mathbf{v}
      =
      \begin{bmatrix}
          v_1 \\
          v_2 \\
          v_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          x_1-x_2 \\
          x_2-x_3 \\
          x_3
      \end{bmatrix}.
    \]
    Clearly, $\mathbf{v} \in Q^3$.
  \end{enumerate}

\item[(5)]
  \emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
  and that their union covers $I^3$.}
  Similar to (4).
  By (2),
  $\mathbf{x} = P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  $P_{(i_1,i_2,i_3)^{-1}} \mathbf{x} = A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  \[
    \begin{bmatrix}
        x_{i_1} \\
        x_{i_2} \\
        x_{i_3}
    \end{bmatrix}
    =
    \begin{bmatrix}
        u_1+u_2+u_3 \\
        u_{j_2}+u_3 \\
        u_3
    \end{bmatrix}.
  \]
  In any case, we always have
  $0 \leq u_3 \leq u_{j_2}+u_3 \leq u_1+u_2+u_3 \leq 1$.
  Hence
  $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_{(i_1, i_2, i_3)}$
  if and only if
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1.
  \]
  The interior of $\sigma_{(i_1, i_2, i_3)}$ is
  \[
    \{ \mathbf{x} \in \mathbb{R}^3 : 0 < x_{i_3} < x_{i_2} < x_{i_1} < 1 \},
  \]
  and thus the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors.
  Also, any $\mathbf{x} \in I^3$ has the relation
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1
  \]
  for some permutation $(i_1,i_2,i_3)$ of $(1,2,3)$.
  Hence
  \[
    I^3
    = \bigcup_{(i_1,i_2,i_3)} \sigma_{(i_1,i_2,i_3)}(Q^3)
    = \bigcup_{i=1}^{6} \sigma_{i}(Q^3).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.19.}
\emph{Let $J^2$ and $J^3$ be as in Exercise 10.17 and Exercise 10.18.
Define
\begin{align*}
  B_{01}(u,v) = (0,u,v), &\qquad B_{11}(u,v) = (1,u,v), \\
  B_{02}(u,v) = (u,0,v), &\qquad B_{12}(u,v) = (u,1,v), \\
  B_{03}(u,v) = (u,v,0), &\qquad B_{13}(u,v) = (u,v,1).
\end{align*}
These are affine, and map $\mathbb{R}^2$ into $\mathbb{R}^3$.
Put $\beta_{ri} = B_{ri}(J^2)$, for $r=0,1$, $i=1,2,3$.
Each $\beta_{ri}$ is an affine-oriented $2$-chain. (See Section 10.30.)
Verify that
\[
  \partial J^3 = \sum_{i=1}^{3} (-1)^{i} (\beta_{0i}-\beta_{1i}),
\]
in agreement with Exercise 10.18.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  A direct calculation shows that
  \begin{align*}
    B_{01}(\tau_1) - B_{11}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_3]
      - [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_1) - B_{12}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3]
      - [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_1) - B_{13}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{01}(\tau_2) - B_{11}(\tau_2)
    =&
    -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_2) - B_{12}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_2) - B_{13}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3].
  \end{align*}

\item[(2)]
  To express the formula in (1) clearly, we define
  \[
    \omega_{(i_1,i_2,i_3)}
    =
    [
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ]
    -
    [
      \mathbf{0},
      \mathbf{e}_{i_2},
      \mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ],
  \]
  and thus
  \begin{align*}
    -(B_{01}(\tau_1) - B_{11}(\tau_1)) &= s(1,2,3) \omega_{(1,2,3)} \\
    B_{02}(\tau_1) - B_{12}(\tau_1) &= s(2,1,3) \omega_{(2,1,3)} \\
    -(B_{03}(\tau_1) - B_{13}(\tau_1)) &= s(3,1,2) \omega_{(3,1,2)} \\
    -(B_{01}(\tau_2) - B_{11}(\tau_2)) &= s(1,3,2) \omega_{(1,3,2)} \\
    B_{02}(\tau_2) - B_{12}(\tau_2) &= s(2,3,1) \omega_{(2,3,1)} \\
    -(B_{03}(\tau_2) - B_{13}(\tau_2)) &= s(3,2,1) \omega_{(3,2,1)}.
  \end{align*}

\item[(3)]
  Note that
  \begin{align*}
    \beta_{0i}-\beta_{1i}
    &= B_{0i}(J^2) - B_{1i}(J^2) \\
    &= B_{0i}(\tau_1+\tau_2) - B_{1i}(\tau_1+\tau_2) \\
    &= B_{0i}(\tau_1) + B_{0i}(\tau_2) - B_{1i}(\tau_1) - B_{1i}(\tau_2) \\
    &= (B_{0i}(\tau_1) - B_{1i}(\tau_1)) + (B_{0i}(\tau_2) - B_{1i}(\tau_2)).
  \end{align*}
  Thus,
  \begin{align*}
    &\sum_{i=1}^3 (-1)^{i} (\beta_{0i}-\beta_{1i}) \\
    =& \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_1) - B_{1i}(\tau_1))
      + \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_2) - B_{1i}(\tau_2)) \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1,i_2,i_3) \omega_{(i_1,i_2,i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \partial J^3.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.20.}
\emph{State conditions under which the formula
\[
  \int_{\Phi} fd\omega
  = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
\]
is valid, and show that it generalizes the formula for integration by parts.
(Hint: $d(f\omega) = (df) \wedge \omega + f d\omega$.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{If}
  \begin{enumerate}
  \item[(a)]
    \emph{$\Phi$ is a $k$-chain of class $\mathscr{C}''$ in an open set $V \subseteq \mathbb{R}^m$,}

  \item[(b)]
    \emph{$\omega$ is a $(k-1)$-form of class $\mathscr{C}'$ in $V$,}

  \item[(c)]
    \emph{$f$ is a $0$-form of class $\mathscr{C}'$ in $V$,}
  \end{enumerate}
  \emph{then}
  \[
    \int_{\Phi} fd\omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
  \]

\item[(2)]
  Theorem 10.20(a) implies that
  \[
    d(f\omega) = (df) \wedge \omega + f d\omega.
  \]

\item[(3)]
  The Stokes' theorem (Theorem 10.33) shows that
  \[
    \int_{\Phi} d(f\omega) = \int_{\partial\Phi} f\omega.
  \]
  Hence
  \[
    \int_{\Phi} fd\omega
    = \int_{\Phi} d(f\omega) - \int_{\Phi}(df) \wedge \omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega.
  \]

\item[(4)]
  Define $\Phi: Q^1 = [0,1] \to [a,b]$ by
  \[
    \Phi(\alpha) = a + \alpha (b - a).
  \]
  $\Phi$ is a $1$-simplex of class $\mathscr{C}''$ in an open set $V \supseteq [a,b]$.
  Also,
  \[
    \partial\Phi = [b] - [a].
  \]
  Let $\omega = g$ be a $0$-form of class $\mathscr{C}'(V)$.

\item[(5)]
  Note that
  \begin{align*}
    \int_{\Phi} fd\omega
    &= \int_{\Phi} fdg
    = \int_{0}^{1} f(\Phi(t)) g'(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f(u) g'(u) du, \\
    \int_{\partial\Phi} f\omega
    &= \int_{[b]} fg + \int_{-[a]} fg
    = f(b)g(b) + (-1) f(a)f(a), \\
    \int_{\Phi}(df) \wedge \omega
    &= \int_{\Phi}(df) g
    = \int_{0}^{1} f'(\Phi(t)) g(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f'(u) g(u) du.
  \end{align*}
  Hence
  \[
    \int_{a}^{b} f(u) g'(u) du = f(b)g(b) - f(a)f(a) - \int_{a}^{b} f'(u) g(u) du,
  \]
  which is the same as the integration by parts (Theorem 6.22).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.21.}
\emph{As in Example 10.36, consider the $1$-form
\[
  \eta = \frac{x dy - y dx}{x^2+y^2}
\]
in $\mathbb{R}^2-\{\mathbf{0}\}$.}
\begin{enumerate}
\item[(a)]
  \emph{Carry out the computation that leads to
  \[
    \int_{\gamma} \eta = 2\pi \neq 0,
  \]
  and prove that $d\eta = 0$.}

\item[(b)]
  \emph{Let $\gamma(t) = (r \cos t, r \sin t)$, for some $r > 0$,
  and let $\Gamma$ be a $\mathscr{C}''$-curve in $\mathbb{R}^2 - \{\mathbf{0}\}$,
  with parameter interval $[0,2\pi]$,
  with $\Gamma(0) = \Gamma(2\pi)$,
  such that the intervals $[\gamma(t),\Gamma(t)]$ do not contain $\mathbf{0}$
  for any $t \in [0,2\pi]$.
  Prove that
  \[
    \int_{\Gamma} \eta = 2\pi.
  \]
  (Hint: For $0 \leq t \leq 2\pi$, $0 \leq u \leq 1$, define
  \[
    \Phi(t,u) = (1-u)\Gamma(t) + u\gamma(t).
  \]
  Then $\Phi$ is a $2$-surface in $\mathbb{R}^2 - \{\mathbf{0}\}$
  whose parameter domain is the indicated rectangle.
  Because of cancellations (as in Example 10.32),
  \[
    \partial \Phi = \Gamma - \gamma.
  \]
  Use Stokes' theorem to deduce that
  \[
    \int_{\Gamma} \eta = \int_{\gamma} \eta
  \]
  because $d\eta = 0$.)}

\item[(c)]
  \emph{Take $\Gamma(t) = (a\cos t, b\sin t)$ where $a > 0$, $b > 0$ are fixed.
  Use part (b) to show that}
  \[
    \int_{0}^{2\pi} \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}dt = 2\pi.
  \]

\item[(d)]
  \emph{Show that
  \[
    \eta = d\left( \arctan\frac{y}{x} \right)
  \]
  in any convex open set in which $x \neq 0$, and that
  \[
    \eta = d\left( -\arctan\frac{x}{y} \right)
  \]
  in any convex open set in which $y \neq 0$.
  Explain why this justifies the notation $\eta = d\theta$,
  in spite of the fact that $\eta$ is not exact in $\mathbb{R}^2 - \{0\}$.}

\item[(e)]
  \emph{Show that (b) can be derived from (d).}

\item[(f)]
  \emph{If $\Gamma$ is any closed $\mathscr{C}'$-curve in $\mathbb{R}^2 - \{ \mathbf{0} \}$,
  prove that
  \[
    \frac{1}{2\pi} \int_{\Gamma} \eta = \mathrm{Ind}(\Gamma).
  \]
  (See Exercise 8.23 for the definition of the index of a curve.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    \int_{\gamma} \eta
    &= \int_{0}^{2\pi}
      \frac{(r\cos t) d(r\sin t) - (r\sin t) d(r\cos t)}{(r\cos t)^2 + (r\sin t)^2} \\
    &= \int_{0}^{2\pi}
      \frac{(r \cos t)(r \cos t) - (r\sin t) (-r\sin t)}{(r\cos t)^2 + (r\sin t)^2} dt \\
    &= \int_{0}^{2\pi} dt \\
    &= 2\pi.
  \end{align*}

\item[(2)]
  \begin{align*}
    d \eta
    =& d \left( \frac{x dy - y dx}{x^2+y^2} \right) \\
    =& d \left( \frac{x}{x^2+y^2} \right) \wedge dy
      - d \left( \frac{y}{x^2+y^2} \right) \wedge dx
      &(d^2 = 0) \\
    =& D_1\left(\frac{x}{x^2+y^2}\right) dx \wedge dy
      &(dy \wedge dy = 0) \\
      &- D_2\left(\frac{y}{x^2+y^2}\right) dy \wedge dx
      &(dx \wedge dx = 0) \\
    =& \left(\frac{1}{x^2+y^2} - \frac{2x^2}{(x^2+y^2)^2}\right) dx \wedge dy \\
      &+ \left(\frac{1}{x^2+y^2} - \frac{2y^2}{(x^2+y^2)^2}\right) dx \wedge dy \\
    =& 0
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
  $\eta$ is closed and locally exact, that is,
  $\eta = dt$ on $\mathbb{R}^2 - L$
  where $L$ is any line passing through $\mathbf{0}$.
  $\eta$ is not exact since $\int_{\gamma} \eta = 2\pi \neq 0$.
  (See Exercise 10.22(g).)

\item[(2)]
  \emph{(Poincar\'e's Lemma for $1$-form.)
  Let $\omega = \sum a_i dx_i$ be defined in an open set $U \subseteq \mathbb{R}^n$.
  Then $d\omega = 0$ if and only if for each $p \in U$ there is a neighborhood $V \subseteq U$
  of $p$ and a differentiable function $f: V \to \mathbb{R}^1$ with
  $df = \omega$ (i.e., $\omega$ is locally exact).} \\
\end{enumerate}



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  For $0 \leq t \leq 2\pi$, $0 \leq u \leq 1$, define
  \[
    \Phi(t,u) = (1-u)\Gamma(t) + u\gamma(t).
  \]
  Then $\Phi$ is a $2$-surface in $\mathbb{R}^2 - \{\mathbf{0}\}$
  whose parameter domain
  $D = \{(t,u) : 0 \leq t \leq 2\pi, 0 \leq u \leq 1 \}$ is the indicated rectangle.

\item[(2)]
  Similar to Example 10.32,
  \[
    \partial \Phi = \gamma_1 + \gamma_2 + \gamma_3 + \gamma_4
  \]
  where
  \begin{align*}
    \gamma_1(t) &= \Phi(t,0) = \Gamma(t), \\
    \gamma_2(u) &= \Phi(2\pi,u) = (1-u)\Gamma(2\pi) + u\gamma(2\pi), \\
    \gamma_3(t) &= \Phi(2\pi-t,1) = \gamma(2\pi-t), \\
    \gamma_4(u) &= \Phi(0,1-u) = u\Gamma(0) + (1-u)\gamma(0).
  \end{align*}
  Because of cancellations (as in Example 10.32), $\gamma(0) = \gamma(2\pi)$
  and $\Gamma(0) = \Gamma(2\pi)$,
  $\gamma_4 = -\gamma_2$ and $\gamma_3 = -\gamma$.
  Hence,
  \[
    \partial \Phi = \Gamma - \gamma.
  \]

\item[(3)]
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\Phi} d\eta
    = \int_{\partial\Phi} \eta
    = \int_{\Gamma - \gamma} \eta
    = \int_{\Gamma} \eta - \int_{\gamma} \eta.
  \]
  Hence,
  \[
    \int_{\Gamma} \eta = \int_{\gamma} \eta
  \]
  (since $d\eta = 0$ by (a)).
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  $\Gamma$ satisfies all conditions described in (b).
  So
  \[
    \int_{\Gamma} \eta = 2\pi.
  \]

\item[(2)]
  A direct calculation shows that
  \begin{align*}
    2\pi = \int_{\Gamma} \eta
    &= \int_{\Gamma} \frac{x dy - y dx}{x^2+y^2} \\
    &= \int_{0}^{2\pi}
      \frac{a \cos(t) d(b \sin(t)) - b \sin(t) d(a \cos(t))}{(a \cos(t))^2+(b \sin(t))^2} \\
    &= \int_{0}^{2\pi}
      \frac{ab (\cos^2 t + \sin^2 t)}{a^2\cos^2 t + b^2 \sin^2 t} \\
    &= \int_{0}^{2\pi}
      \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  In any convex open set in which $x \neq 0$,
  we have
  \begin{align*}
    d\left( \arctan\frac{y}{x} \right)
    &= \left( D_1\arctan\frac{y}{x} \right) dx
      + \left( D_2\arctan\frac{y}{x} \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(2)]
  In any convex open set in which $y \neq 0$,
  we have
  \begin{align*}
    d\left( -\arctan\frac{x}{y} \right)
    &= \left( D_1\left(-\arctan\frac{x}{y}\right) \right) dx
      + \left( D_2\left(-\arctan\frac{x}{y}\right) \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(3)]
  By (1)(2), $\eta$ is locally exact.
  Note that $\theta_1 = \arctan\frac{y}{x}$
  and $\theta_2 = -\arctan\frac{x}{y}$
  cannot be patched together to defined a global $0$-form $\theta$
  on $\mathbb{R}^2 - \{\mathbf{0}\}$.

\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
  Partition $[0,2\pi]$ into five subintervals
  \[
    I_i
    = \left[ \frac{(2i-3)\pi}{4}, \frac{(2i-1)\pi}{4} \right]
      \cap [0,2\pi].
  \]
  for $i = 1,2,3,4,5$.
  Hence
  \begin{align*}
    \int_{\gamma} \eta
    =& \: \sum_{i=1}^{5} \int_{\gamma(I_i)} \eta \\
    =& \: \sum_{i=1,3,5} \int_{\gamma(I_i)} d\left( \arctan\frac{y}{x} \right)
      + \sum_{i=2,4} \int_{\gamma(I_i)} d\left( -\arctan\frac{x}{y} \right).
  \end{align*}

\item[(2)]
  The Stokes' theorem (Theorem 10.33) implies that
  \begin{align*}
    \int_{\gamma(I_1)} d\left( \arctan\frac{y}{x} \right)
    &= \int_{\partial\gamma(I_1)} \arctan\frac{y}{x} \\
    &= \left[ \arctan\frac{r\cos t}{r\sin t} \right]_{t = 0}^{t = \frac{\pi}{4}} \\
    &= \left[ \arctan(\tan(t)) \right]_{t = 0}^{t = \frac{\pi}{4}} \\
    &= \frac{\pi}{4},
  \end{align*}
  and
  \begin{align*}
    \int_{\gamma(I_2)} d\left( -\arctan\frac{x}{y} \right)
    &= \int_{\partial\gamma(I_2)} -\arctan\frac{x}{y} \\
    &= \left[ \arctan\frac{r\sin t}{r\cos t} \right]_{t = \frac{\pi}{4}}^{t = \frac{3\pi}{4}} \\
    &= \left[ \arctan(\cot(t)) \right]_{t = \frac{\pi}{4}}^{t = \frac{3\pi}{4}} \\
    &= \frac{\pi}{2}.
  \end{align*}
  Similarly,
  \begin{align*}
    \int_{\gamma(I_3)} d\left( \arctan\frac{y}{x} \right)
    &= \frac{\pi}{2} \\
    \int_{\gamma(I_4)} d\left( -\arctan\frac{x}{y} \right)
    &= \frac{\pi}{2} \\
    \int_{\gamma(I_5)} d\left( \arctan\frac{y}{x} \right)
    &= \frac{\pi}{4}.
  \end{align*}

\item[(3)]
  Therefore,
  \[
    \int_{\gamma} \eta
    = \left( \frac{\pi}{4} + \frac{\pi}{2} + \frac{\pi}{4} \right)
      + \left( \frac{\pi}{2} + \frac{\pi}{2} \right)
    = 2\pi.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
  Regard $\Gamma(t)$ as a plane curve $(\Gamma_1(t), \Gamma_2(t))$ over $\mathbb{R}^2$
  or $\Gamma_1(t) + i \Gamma_2(t)$ over $\mathbb{C}^1$.
  Note that
  \begin{align*}
    \frac{\Gamma'(t)}{\Gamma(t)}
    &= \frac{\Gamma_1'(t) + i \Gamma_2'(t)}{\Gamma_1(t) + i \Gamma_2(t)} \\
    &= \frac{\Gamma_1'(t) \Gamma_1'(t) + \Gamma_2'(t) \Gamma_2'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}
      + i \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}.
  \end{align*}
  So
  \[
    \mathrm{Im}\left(\frac{\Gamma'(t)}{\Gamma(t)}\right)
    = \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
      {\Gamma_1(t)^2 + \Gamma_2(t)^2}.
  \]

\item[(2)]
  By Exercise 8.23,
  \[
    \mathrm{Ind}(\Gamma)
    = \frac{1}{2\pi i}\int_{0}^{2\pi} \frac{\Gamma'(t)}{\Gamma(t)} dt
  \]
  is always an integer.
  That is,
  \begin{align*}
    \mathrm{Ind}(\Gamma)
    &= \frac{1}{2\pi}\int_{0}^{2\pi} \mathrm{Im}\left(\frac{\Gamma'(t)}{\Gamma(t)}\right) dt \\
    &= \frac{1}{2\pi}\int_{0}^{2\pi}
      \frac{\Gamma_1(t) \Gamma_2'(t) - \Gamma_2(t) \Gamma_1'(t)}
        {\Gamma_1(t)^2 + \Gamma_2(t)^2} dt \\
    &= \frac{1}{2\pi}\int_{\Gamma} \frac{xdy - ydx}{x^2+y^2} \\
    &= \frac{1}{2\pi}\int_{\Gamma} \eta.
  \end{align*}
  (Note that $\mathrm{Ind}(\Gamma) = 1$ if $\Gamma$ is defined as in (c).
  Hence the integral in (c) is equal to $2\pi \mathrm{Ind}(\Gamma) = 2\pi$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.22.}
\emph{As in Example 10.37, define $\zeta$ in $\mathbb{R}^3-\{\mathbf{0}\}$ by
\[
  \zeta = \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3}
\]
where $r = (x^2+y^2+z^2)^{\frac{1}{2}}$,
let $D$ be the rectangle given by $0 \leq u \leq \pi$, $0 \leq v \leq 2\pi$,
and let $\Sigma$ be the $2$-surface in $\mathbb{R}^3$,
with parameter domain $D$, given by
\[
  x = \sin u \cos v,
  \qquad
  y = \sin u \sin v,
  \qquad
  z = \cos u.
\]}
\begin{enumerate}
\item[(a)]
  \emph{Prove that $d\zeta = 0$ in $\mathbb{R}^3 - \{ \mathbf{0} \}$.}

\item[(b)]
  \emph{Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subseteq D$.
  Prove that
  \[
    \int_{S} \zeta
    = \int_{E} \sin u \: du \: dv
    = A(S),
  \]
  where $A$ denotes area, as in Section 10.46.
  Note that this contains
  \[
    \int_{\Sigma} \zeta
    = \int_{D} \sin u \: du \: dv
    = 4\pi \neq 0
  \]
  as a special case.}

\item[(c)]
  \emph{Suppose $g, h_1, h_2, h_3$, are $\mathscr{C}''$-functions on $[0,1]$, $g > 0$.
  Let $(x,y,z) = \Phi(s,t)$ define a $2$-surface $\Phi$,
  with parameter domain $I^2$, by
  \[
    x = g(t)h_1(s),
    \qquad
    y = g(t)h_2(s),
    \qquad
    z = g(t)h_3(s).
  \]
  Prove that
  \[
    \int_{\Phi} \zeta = 0,
  \]
  directly from Equation (35) in Chapter 10.
  Note the shape of the range of $\Phi$:
  For fixed $s$, $\Phi(s,t)$ runs over an interval on a line through $\mathbf{0}$.
  The range of $\Phi$ thus lies in a ``cone'' with vertex at the origin.}

\item[(d)]
  \emph{Let $E$ be a closed rectangle in $D$, with edges parallel to those of $D$.
  Suppose $f \in \mathscr{C}''(D)$, $f > 0$.
  Let $\Omega$ be the $2$-surface with parameter domain $E$,
  defined by
  \[
    \Omega(u,v) = f(u,v)\Sigma(u,v).
  \]
  Define $S$ as in (b) and prove that
  \[
    \int_{\Omega} \zeta = \int_{S} \zeta = A(S).
  \]
  (Since $S$ is the ``radical projection'' of $\Omega$ into the unit sphere,
  this result makes it reasonable to call $\int_{\Omega} \zeta$ the ``solid angle''
  subtended by the range of $\Omega$ at the origin.)
  (Hint:
  Consider the $3$-surface $\Psi$ given by
  \[
    \Psi(t,u,v) = [1-t+tf(u,v)]\Sigma(u,v),
  \]
  where $(u,v) \in E$, $0 \leq t \leq 1$.
  For fixed $v$, the mapping $(t,u) \mapsto \Psi(t,u,v)$ is a $2$-surface $\Phi$
  to which (c) can be applied to show that $\int_{\Phi} \zeta = 0$.
  The same thing holds when $u$ is fixed.
  By (a) and Stokes' theorem,
  \[
    \int_{\partial \Psi} \zeta = \int_{\Psi} d\zeta = 0.)
  \]}

\item[(e)]
  \emph{Put $\lambda = -\frac{z}{r}\eta$, where
  \[
    \eta = \frac{xdy-ydx}{x^2+y^2},
  \]
  as in Exercise 10.21.
  Then $\lambda$ is a $1$-form in the open set $V \subseteq \mathbb{R}^3$ in which $x^2+y^2 > 0$.
  Show that $\zeta$ is exact in $V$ by showing that
  \[
    \zeta = d\lambda.
  \]}
\item[(f)]
  \emph{Derive (d) from (e), without using (c).
  (Hint: To begin with, assume $0 < u < \pi$ on $E$.
  By (e),
  \[
    \int_{\Omega} \zeta = \int_{\partial\Omega} \lambda
    \qquad
    \text{and}
    \qquad
    \int_{S} \zeta = \int_{\partial S} \lambda.
  \]
  Show that the two integrals of $\lambda$ are equal,
  by using part (d) of Exercise 10.21,
  and by noting that $\frac{z}{r}$ is the same at $\Sigma(u,v)$ as at $\Omega(u,v)$.)}

\item[(g)]
  \emph{Is $\zeta$ exact in the complement of every line through the origin?} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Note that $\zeta$ is well-defined on $\mathbb{R}^3 - \{ \mathbf{0} \}$.
  Hence,
  \begin{align*}
    d\zeta
    =& \: d\left( \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3} \right) \\
    =& \: d\left(\frac{x}{r^3}\right) \wedge dy \wedge dz
      + d\left(\frac{y}{r^3}\right) \wedge dz \wedge dx
      + d\left(\frac{z}{r^3}\right) \wedge dx \wedge dy \\
    =& \: D_1\left(\frac{x}{r^3}\right) dx \wedge dy \wedge dz
      + D_2\left(\frac{y}{r^3}\right) dy \wedge dz \wedge dx
      + D_3\left(\frac{z}{r^3}\right) dz \wedge dx \wedge dy \\
    =& \: \frac{r^3 - 3rx^2}{r^6} dx \wedge dy \wedge dz
      + \frac{r^3 - 3ry^2}{r^6} dy \wedge dz \wedge dx
      + \frac{r^3 - 3rz^2}{r^6} dz \wedge dx \wedge dy \\
    =& \: \left(\frac{r^3 - 3rx^2}{r^6}
      + \frac{r^3 - 3ry^2}{r^6}
      + \frac{r^3 - 3rz^2}{r^6}\right) dx \wedge dy \wedge dz \\
    =& \: 0 dx \wedge dy \wedge dz \\
    =& \: 0
  \end{align*}
  in $\mathbb{R}^3 - \{ \mathbf{0} \}$.

\item[(2)]
  Or write
  \[
    \mathbf{F}
    = \frac{x}{r^3} \mathbf{e}_1 + \frac{y}{r^3} \mathbf{e}_2 + \frac{z}{r^3}  \mathbf{e}_3
  \]
  as in Vector fields 10.42.
  So
  \[
    \omega_{\mathbf{F}} = \zeta
  \]
  and
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F}) dx \wedge dy \wedge dz
  \]
  as in the proof of the divergence theorem (Theorem 10.51).
  Note that the divergence of $\mathbf{F}$ is zero.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  By Area elements in $\mathbb{R}^3$ 10.46,
  \begin{align*}
    \mathbf{N}(u,v)
    &= \frac{\partial(y,z)}{\partial(u,v)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(u,v)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(u,v)}\mathbf{e}_3 \\
    &= (\sin^2 u \cos v) \mathbf{e}_1
      + (\sin^2 u \sin v) \mathbf{e}_2
      + (\sin u \cos u) \mathbf{e}_3.
  \end{align*}
  Here
  $\abs{\mathbf{N}(u,v)} = \sin u \geq 0$ (by noting that $u \in [0,\pi]$),
  and
  \[
    \mathbf{n}(u,v)
    = \frac{\mathbf{N}(u,v)}{ \abs{\mathbf{N}(u,v)} }
    = (\sin u \cos v, \sin u \sin v, \cos u).
  \]

\item[(2)]
  Note that $\zeta = x dy \wedge dz + y dz \wedge dx + z dx \wedge dy$ on $S \subseteq \Sigma$.
  Hence,
  by Integrals of $2$-forms in $\mathbb{R}^3$ 10.49,
  \begin{align*}
    \int_{S} \zeta
    =& \: \int_{S} x dy \wedge dz + y dz \wedge dx + z dx \wedge dy \\
    =& \: \int_{E} (\sin u \cos v, \sin u \sin v, \cos u) \cdot \mathbf{N}(u,v) \: du \: dv \\
    =& \: \int_{E} \mathbf{n}(u,v) \cdot \mathbf{n}(u,v) \abs{\mathbf{N}(u,v)} \: du \: dv \\
    =& \: \int_{E} \abs{\mathbf{N}(u,v)} \: du \: dv \\
    =& \: A(S).
  \end{align*}

\item[(3)]
  In particular,
  \begin{align*}
    \int_{\Sigma} \zeta
    =& \: \int_{D} \sin u \: du \: dv \\
    =& \: \int_{0}^{\pi} \int_{0}^{2\pi} \sin u \: du \: dv \\
    =& \: \left( \int_{0}^{\pi} \sin u \: du \right)
      \left( \int_{0}^{2\pi} dv \right) \\
    =& \: 2 \cdot 2 \pi \\
    =& \: 4 \pi.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  Similar to (b).
  \begin{align*}
    \mathbf{N}(s,t)
    =& \: \frac{\partial(y,z)}{\partial(s,t)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(s,t)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(s,t)}\mathbf{e}_3 \\
    =& \: g(t)g'(t)[ (h_1(s),h_2(s),h_3(s)) \times (h_1'(s),h_2'(s),h_3'(s)) ] \\
    =& \: g(t)g'(t)[ \mathbf{h}(s) \times \mathbf{h}'(s)],
  \end{align*}
  where $\mathbf{h}(s) = (h_1(s),h_2(s),h_3(s))$
  and $\mathbf{h}'(s) = (h_1'(s),h_2'(s),h_3'(s))$.
  (Here ``$\times$'' is the cross product in $\mathbb{R}^3$.)

\item[(2)]
  Assume $\zeta$ is well-defined, i.e., $\mathbf{h}(s) \neq \mathbf{0}$ for all $s \in [0,1]$.
  By Integrals of $2$-forms in $\mathbb{R}^3$ 10.49,
  \begin{align*}
    \int_{\Phi} \zeta
    =& \: \int_{\Phi} \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3} \\
    =& \: \int_{I^2} \frac{g(t)}{g(t)^3 |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot \mathbf{N}(s,t) \: ds \: dt \\
    =& \: \int_{I^2} \frac{g(t)}{g(t)^3 |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot g(t)g'(t)[ \mathbf{h}(s) \times \mathbf{h}'(s)] \: ds \: dt \\
    =& \: \int_{I^2} \frac{g'(t)}{g(t) |\mathbf{h}(s)|^3} \mathbf{h}(s)
      \cdot [ \mathbf{h}(s) \times \mathbf{h}'(s)] \: ds \: dt \\
    =& \: 0
  \end{align*}
  (since $\mathbf{h}(s) \cdot [ \mathbf{h}(s) \times \mathbf{h}'(s)] = 0$.)

\item[(3)]
  Note that $\Sigma$ in spherical coordinate system
  cannot be parameterized as
  $(x,y,z) = g(t)\mathbf{h}(s)$,
  and thus $\int_{S} \zeta$ could be nonzero as shown in (b).
\end{enumerate}
$\Box$ \\



\emph{Proof of (d) (Hint).}
\begin{enumerate}
\item[(1)]
  Consider the $3$-surface $\Psi$ given by
  \[
    \Psi(t,u,v) = [1-t+tf(u,v)]\Sigma(u,v),
  \]
  where $(u,v) \in E$, $0 \leq t \leq 1$.
  Write
  \[
    E = [a_1,b_1] \times [a_2,b_2] \subseteq D = [0,\pi] \times [0, 2\pi].
  \]
  Note that $\Psi(t,u,v) \subseteq \mathbb{R}^3 - \{ \mathbf{0} \}$.
  So the boundary of $\Psi$ is
  \begin{align*}
    \partial \Psi
    =& \: \Psi(0,u,v) - \Psi(1,u,v) \\
      &+ \Psi(t,a_1,v) - \Psi(t,b_1,v) \\
      &+ \Psi(t,u,a_2) - \Psi(t,u,b_2) \\
    =& \: S(u,v) - \Omega(u,v) \\
      &+ \Psi|_{u=a_1}(t,v) - \Psi|_{u=b_1}(t,v) \\
      &+ \Psi|_{v=a_2}(t,u) - \Psi|_{v=b_2}(t,u),
  \end{align*}
  where $\Psi|_{u=u_0}(t,v) = \Psi(t,u_0,v)$ and $\Psi|_{v=v_0}(t,u) = \Psi(t,u,v_0)$.

\item[(2)]
  \emph{Show that
  \[
    \int_{\Psi|_{v=v_0}} \zeta = 0
  \]
  for any fixed $v = v_0 \in [a_2, b_2]$.}
  Note that $\zeta$ is well-defined on $\Psi|_{v=v_0}$.
  Write $\Psi|_{v=v_0}(t,u) = (x,y,z) = (x(t,u),y(t,u),z(t,u))$.
  By definition of $\Psi$, we have
  \begin{align*}
    x &= g(t,u) \sin u \cos v_0 \\
    y &= g(t,u) \sin u \sin v_0 \\
    z &= g(t,u) \cos u,
  \end{align*}
  where $g(t,u) = 1-t+tf(u,v_0)$.
  Similar to (c),
  \begin{align*}
    \mathbf{N}(t,u)
    =& \: \frac{\partial(y,z)}{\partial(t,u)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(t,u)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(t,u)}\mathbf{e}_3 \\
    =& \: g(t,u) D_1 g(t,u) (-\sin v_0, \cos v_0, 0).
  \end{align*}
  Note that
  \[
    (x(t,u),y(t,u),z(t,u)) \cdot \mathbf{N}(t,u) = 0.
  \]
  So
  \begin{align*}
    \int_{\Psi|_{v=v_0}} \zeta
    =& \: \int_{\Psi|_{v=v_0}} r^{-3} (x dy \wedge dz + y dz \wedge dx + z dx \wedge dy) \\
    =& \: \int_{[0,1]\times[a_1,b_1]}
      r^{-3}(x(t,u),y(t,u),z(t,u)) \cdot \mathbf{N}(t,u) \: dt \: du \\
    =& \: \int_{[0,1]\times[a_1,b_1]} 0 \: dt \: du \\
    =& \: 0.
  \end{align*}

\item[(3)]
  \emph{Show that
  \[
    \int_{\Psi|_{u=u_0}} \zeta = 0
  \]
  for any fixed $u = u_0 \in [a_1, b_1]$.}
  Similar to (2).
  \begin{align*}
    \mathbf{N}(t,v)
    =& \: \frac{\partial(y,z)}{\partial(t,v)} \mathbf{e}_1
      + \frac{\partial(z,x)}{\partial(t,v)} \mathbf{e}_2
      + \frac{\partial(x,y)}{\partial(t,v)}\mathbf{e}_3 \\
    =& \: \sin u_0 g(t,v) D_1 g(t,v)(-\cos u_0 \cos v, -\cos u_0 \sin v, \sin u_0).
  \end{align*}
  where $g(t,v) = 1-t+tf(u_0,v)$.
  So $(x(t,v),y(t,v),z(t,v)) \cdot \mathbf{N}(t,v) = 0$ and thus
  $\int_{\Psi|_{u=u_0}} \zeta = 0$.

\item[(4)]
  So
  \begin{align*}
    0
    =& \: \int_{\Psi} d\zeta
      &(\text{$d\zeta = 0$ on $\mathbb{R}^3-\{\mathbf{0}\}$}) \\
    =& \: \int_{\partial \Psi} \zeta
      &(\text{Theorem 10.33}) \\
    =& \: \int_{S} \zeta - \int_{\Omega} \zeta \\
      &+ \underbrace{\int_{\Psi|_{u=a_1}} \zeta - \int_{\Psi|_{u=b_1}} \zeta}_{
        \text{all are zero by (2)}} \\
      &+ \underbrace{\int_{\Psi|_{v=a_2}} \zeta - \int_{\Psi|_{v=b_2}} \zeta}_{
        \text{all are zero by (3)}}
        &((1)) \\
    =& \: \int_{S} \zeta - \int_{\Omega} \zeta.
  \end{align*}
  Hence
  \[
    \int_{\Omega} \zeta = \underbrace{\int_{S} \zeta = A(S)}_{\text{by (b)}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    d \left( -\frac{z}{r} \right)
    = \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{r^2-z^2}{r^3} dz
    = \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{x^2+y^2}{r^3} dz
  \]
  since $r^2 = x^2+y^2+z^2$.

\item[(2)]
  \begin{align*}
    d\lambda
    &= d \left( -\frac{z}{r}\eta \right) \\
    &= \underbrace{d \left( -\frac{z}{r} \right)}_{\text{apply (1)}} \wedge \eta
      + (-1)^1 \left( -\frac{z}{r} \right) \wedge \underbrace{d\eta}_{= 0} \\
    &= \left( \frac{xz}{r^3} dx + \frac{yz}{r^3} dy - \frac{x^2+y^2}{r^3} dz \right)
      \wedge \left( \frac{-ydx + xdy}{x^2+y^2} \right) \\
    &= \left( \frac{x(x^2+y^2)}{r^3(x^2+y^2)} \right) dy \wedge dz
      + \left( \frac{y(x^2+y^2)}{r^3(x^2+y^2)} \right) dz \wedge dx
      + \left( \frac{x^2 z + y^2 z}{r^3(x^2+y^2)} \right) dx \wedge dy \\
    &= \left( \frac{x}{r^3} \right) dy \wedge dz
      + \left( \frac{y}{r^3} \right) dz \wedge dx
      + \left( \frac{z}{r^3} \right) dx \wedge dy \\
    &= \zeta.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
  To ensure that $\eta$ is well-defined on $E$,
  we might assume $x^2+y^2 = \sin^2 u \neq 0$ or $0 < u < \pi$ on $E$.
  It is fine since
  $\int_{\Omega} \zeta$ and $\int_{S} \zeta$ is well-defined on any closed rectangle in $D$
  and we can apply the argument in Exercise 6.7 to remove the additional restriction.

\item[(2)]
  By the Stokes' theorem (Theorem 10.33) and (e),
  \[
    \int_{\Omega} \zeta = \int_{\partial\Omega} \lambda
    \qquad
    \text{and}
    \qquad
    \int_{S} \zeta = \int_{\partial S} \lambda.
  \]
  So it suffices to show that
  \[
    \int_{\partial\Omega} \lambda = \int_{\partial S} \lambda.
  \]
  Note that $\lambda = -\frac{z}{r} \eta$,
  and thus it suffices to show that
  $\left.\frac{z}{r}\right|_{\partial\Omega} = \left.\frac{z}{r}\right|_{\partial\Sigma}$
  and $\eta|_{\partial\Omega} = \eta|_{\partial S}$.

\item[(3)]
  \emph{Show that
  $\left.\frac{z}{r}\right|_{\partial\Omega} = \left.\frac{z}{r}\right|_{\partial\Sigma}$.}
  For any $(x_{\Omega}, y_{\Omega}, z_{\Omega}) \in \partial\Omega$,
  \[
    (x_{\Omega}, y_{\Omega}, z_{\Omega}) = f(u,v)(x_{\Sigma}, y_{\Sigma}, z_{\Sigma})
  \]
  where $(x_{\Sigma}, y_{\Sigma}, z_{\Sigma}) \in \partial S$.
  So
  \begin{align*}
    \left.\frac{z}{r}\right|_{\partial\Omega}
    &= \frac{z_{\Omega}}{(x_{\Omega}^2+y_{\Omega}^2+z_{\Omega}^2)^{\frac{1}{2}}} \\
    &= \frac{f(u,v)z_{\Sigma}}{f(u,v)(x_{\Sigma}^2+y_{\Sigma}^2+z_{\Sigma}^2)^{\frac{1}{2}}} \\
    &= \frac{z_{\Sigma}}{(x_{\Sigma}^2+y_{\Sigma}^2+z_{\Sigma}^2)^{\frac{1}{2}}} \\
    &= \left.\frac{z}{r}\right|_{\partial S}.
  \end{align*}
  (Note that $f > 0$.)

\item[(4)]
  \emph{Show that $\eta|_{\partial\Omega} = \eta|_{\partial S}$.}
  Similar to (3).
  If $x_{\Omega} \neq 0$ (or $x_{\Sigma} \neq 0$),
  then by Exercise 10.21(d)
  \begin{align*}
    \eta|_{\partial\Omega}
    &= d\left(\arctan\frac{y_{\Omega}}{x_{\Omega}}\right) \\
    &= d\left(\arctan\frac{f(u,v)y_{\Sigma}}{f(u,v)x_{\Sigma}}\right) \\
    &= d\left(\arctan\frac{y_{\Sigma}}{x_{\Sigma}}\right) \\
    &= \eta|_{\partial S}.
  \end{align*}
  Similarly, $\eta|_{\partial\Omega} = \eta|_{\partial S}$ is also true if $y_{\Omega} \neq 0$.
  Note that $(x_{\Omega},y_{\Omega}) \neq (0,0)$ by assumption.
  Therefore the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (g).}
\begin{enumerate}
\item[(1)]
  Yes.
  Given any line $L$ passing through $\mathbf{0}$,
  say
  \[
    (r \sin u \cos v, r \sin u \sin v, r \cos u) \in L
    \qquad
    (r \in \mathbb{R}^1),
  \]
  for some $u \in [0,\pi]$ and $v \in [0,2\pi]$.
  We will show that $\zeta$ is exact in $U = \mathbb{R}^3 - L$.

\item[(2)]
  Linear algebra says that all rotation matrices $T \in SO(3)$ can be obtained from
  \begin{align*}
    R_x(u)
    &=
    \begin{bmatrix}
      1 &      0 &       0 \\
      0 & \cos u & -\sin u \\
      0 & \sin u &  \cos u
    \end{bmatrix} \\
    R_y(v)
    &=
    \begin{bmatrix}
      \cos v & 0 & -\sin v \\
           0 & 1 &       0 \\
      \sin v & 0 &  \cos v
    \end{bmatrix} \\
    R_z(w)
    &=
    \begin{bmatrix}
      \cos w & -\sin w & 0 \\
      \sin w &  \cos w & 0 \\
           0 &       0 & 1
    \end{bmatrix}
  \end{align*}
  using matrix multiplication,
  say $T = R_x(u)R_y(v)R_z(w)$.
  For example, the rotation
  \[
    T = R_y\left(u-\frac{\pi}{2}\right)R_z(-v)
  \]
  maps $L$ to the $z$-axis
  (by showing that $T(r \sin u \cos v, r \sin u \sin v, r \cos u) = (0,0,r)$).
  By Theorem 10.22
  it suffices to show that $\zeta$ is invariant under $R_x(u)$, $R_x(v)$ and $R_z(w)$.
  By the symmetricity of $\zeta$, it suffices to show that $\zeta$ is invariant under $T = R_x(u)$.

\item[(3)]
  \emph{Show that $\zeta$ is invariant under $T = R_x(u)$.}
  By
  \[
    T: (x,y,z) \mapsto (x, y \cos u - z \sin u, y \sin u + z \cos u),
  \]
  we have
  \begin{align*}
    r &\mapsto r \\
    dx &\mapsto dx \\
    dy &\mapsto \cos u dy - \sin u dz \\
    dz &\mapsto \sin u dy + \cos u dz.
  \end{align*}
  So
  \begin{align*}
    dy \wedge dz
      &\mapsto (\cos u dy - \sin u dz) \wedge (\sin u dy + \cos u dz) \\
      &= dy \wedge dz, \\
    dz \wedge dx
      &\mapsto (\sin u dy + \cos u dz) \wedge dx \\
      &= -\sin u dx \wedge dy + \cos u dz \wedge dx, \\
    dx \wedge dy
      &\mapsto dx \wedge (\sin u dy + \cos u dz) \\
      &= \cos u dx \wedge dy + \sin u dz \wedge dx.
  \end{align*}
  Thus
  \begin{align*}
    \zeta
    \mapsto& \:
      r^{-3} \{ x dy \wedge dz \\
      &\qquad + (y \cos u - z \sin u)(-\sin u dx \wedge dy + \cos u dz \wedge dx) \\
      &\qquad + (y \sin u + z \cos u)(\cos u dx \wedge dy + \sin u dz \wedge dx) \} \\
    =& \:
      r^{-3} \{ x dy \wedge dz \\
      &\qquad + [\cos u(y \cos u - z \sin u) + \sin u(y \sin u + z \cos u)] dz \wedge dx \\
      &\qquad + [-\sin u(y \cos u - z \sin u) + \cos u(y \sin u + z \cos u)] dx \wedge dy \} \\
    =& \:
      r^{-3} \{ x dy \wedge dz + y dz \wedge dx + z dx \wedge dy \} \\
    =& \: \zeta.
  \end{align*}

\item[(4)]
  Let $V = \mathbb{R}^3 - \text{$z$-axis}$.
  Since $\zeta_T = \zeta$ (by (3)) is well-defined in $V$,
  $\zeta_T = \zeta = d\lambda$ by (e).
  Here $\lambda$ is in $V$, not necessary in $U$ (if $L \neq$ $z$-axis).
  Luckily, we can use $T^{-1}$ to pullback $\lambda$ in $U$.
  Thus
  \[
    \zeta
    = (\zeta_T)_{T^{-1}}
    = (d\lambda)_{T^{-1}}
    = d(\lambda_{T^{-1}})
  \]
  by Theorems 10.22 and 10.23.
  That is, $\zeta$ is exact in $U = \mathbb{R}^3 - L$.
  (Or $\zeta$ is locally exact in $\mathbb{R}^3 - \{ \mathbf{0} \}$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.23.}
\emph{Fix $n$.
Define $r_k = (x_1^2+\cdots+x_k^2)^{\frac{1}{2}}$ for $1 \leq k \leq n$,
let $E_k$ be the set of all $\mathbf{x} \in \mathbb{R}^n$ at which $r_k > 0$,
and let $\omega_k$ be the $(k-1)$-form defined in $E_k$ by
\[
  \omega_k
  = (r_k)^{-k}
    \sum_{i=1}^{k} (-1)^{i-1} x_i
    dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k
\]
Note that $\omega_2 = \eta$, $\omega_3 = \zeta$ in the terminology of
Exercise 10.21 and Exercise 10.22.
Note also that}
\[
  E_1 \subseteq E_2 \subseteq \cdots \subseteq E_n = \mathbb{R}^n.
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that $d\omega_k = 0$ in $E_k$.}

\item[(b)]
  \emph{For $k=2,\ldots,n$, prove that $\omega_k$ is exact in $E_{k-1}$,
  by showing that
  \[
    \omega_k = d(f_k\omega_{k-1}) = df_k \wedge \omega_{k-1}
  \]
  where $f_k(\mathbf{x}) = (-1)^k g_k\left( \frac{x_k}{r_k} \right)$
  where
  \[
    g_k(t) = \int_{-1}^{t} (1-s^2)^{\frac{k-3}{2}} ds
    \qquad
    (-1 < t < 1).
  \]
  (Hint: $f_k$ satisfies the differential equations
  \[
    \mathbf{x} \cdot (\nabla f_k)(\mathbf{x}) = 0
  \]
  and
  \[
    (D_k f_k)(\mathbf{x}) = \frac{(-1)^k(r_{k-1})^{k-1}}{(r_k)^k}.)
  \]}

\item[(c)]
  \emph{Is $\omega_n$ exact in $E_n$?}

\item[(d)]
  \emph{Note that (b) is a generalization of part (e) of Exercise 10.22.
  Try to extend some of the other assertions of Exercise 10.21 and Exercise 10.22
  to $\omega_n$, for arbitrary $n$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    D_i r_k = \frac{1}{2r_k} \cdot (2x_i) = \frac{x_i}{r_k}.
  \]
\item[(2)]
  \begin{align*}
    d\omega_k
    &= \sum_{i=1}^{k} d\left(
      (-1)^{i-1} (r_k)^{-k} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k\right) \\
    &= \sum_{i=1}^{k} D_i \left( (-1)^{i-1} (r_k)^{-k} x_i \right)
      dx_i \wedge dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_k \\
    &= \sum_{i=1}^{k} (-1)^{i-1} \left(
      (r_k)^{-k} \cdot 1 + \underbrace{(-k)(r_k)^{-k-1} \frac{x_i}{r_k}}_{\text{chain rule}}
        \cdot x_i \right)
      \underbrace{(-1)^{i-1} dx_1 \wedge \cdots \wedge dx_k}_{\text{anticommutative relation}} \\
    &= (r_k)^{-k-2} \underbrace{\sum_{i=1}^{k} \left(
      (r_k)^2 - k x_i^2 \right)}_{= 0}
      dx_1 \wedge \cdots \wedge dx_k \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    D_i \left( \frac{x_k}{r_k} \right)
    = \frac{\delta_{ik}(r_k)^2 - x_i x_k}{(r_k)^3}
  \]
  where $\delta_{ik}$ is the Kronecker delta.
  So
  \begin{align*}
    (D_i f_k)(\mathbf{x})
    &= D_i \left( (-1)^k g_k\left(\frac{x_k}{r_k}\right) \right) \\
    &= D_i \left( (-1)^k \int_{-1}^{\frac{x_k}{r_k}}(1-s^2)^{\frac{k-3}{2}} ds \right) \\
    &= (-1)^k D_i\left( \frac{x_k}{r_k} \right)
      \left(1-\left(\frac{x_k}{r_k}\right)^2 \right)^{\frac{k-3}{2}} \\
    &= (-1)^k \frac{\delta_{ik}(r_k)^2 - x_i x_k}{(r_k)^3}
      \frac{(r_{k-1})^{k-3}}{(r_k)^{k-3}} \\
    &= (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}(\delta_{ik}(r_k)^2 - x_i x_k).
  \end{align*}
  In particular,
  \[
    (D_k f_k)(\mathbf{x})
    = (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}((r_k)^2 - (x_k)^2)
    = (-1)^k \frac{(r_{k-1})^{k-1}}{(r_k)^{k}}
  \]
  (since $(r_k)^2 - (x_k)^2 = (r_{k-1})^2$).

\item[(2)]
  Since
  \[
    \sum_{i} x_i (\delta_{ik}(r_k)^2 - x_i x_k)
    = (r_k)^2 \underbrace{\sum_{i} x_i\delta_{ik}}_{= x_k}
      - x_k \underbrace{\sum_{i} x_i^2}_{= (r_k)^2}
    = 0,
  \]
  we have
  \begin{align*}
    \mathbf{x} \cdot (\nabla f_k)(\mathbf{x})
    &= \sum_{i} x_i (D_i f_k)(\mathbf{x}) \\
    &= \sum_{i} x_i (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}}(\delta_{ik}(r_k)^2 - x_i x_k) \\
    &= (-1)^k \frac{(r_{k-1})^{k-3}}{(r_k)^{k}} \sum_{i} x_i (\delta_{ik}(r_k)^2 - x_i x_k) \\
    &= 0.
  \end{align*}

\item[(3)]
  On $E_{k-1} \subsetneq E_k$, we write
  \begin{align*}
    & \: d(f_k \omega_{k-1}) \\
    =& \: (df_k) \wedge \omega_{k-1} + (-1)^{0} f_k \wedge \underbrace{(d\omega_{k-1})}_{=0} \\
    =& \: (df_k) \wedge \omega_{k-1} \\
    =& \: \left\{
        \sum_{i=1}^{k} D_i f_k(\mathbf{x}) dx_i
      \right\}
      \wedge \\
      &\left\{
        \frac{1}{(r_{k-1})^{k-1}}
        \sum_{j=1}^{k-1} (-1)^{j-1} x_j dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1}
      \right\} \\
    =& \: \frac{1}{(r_{k-1})^{k-1}}
      \sum_{\substack{1 \leq i \leq k \\ 1 \leq j \leq k-1}}
      (-1)^{j-1} x_j D_i f_k(\mathbf{x})
      dx_i \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \: \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_j f_k(\mathbf{x})
      dx_j \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
      &+
      \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_k f_k(\mathbf{x})
      dx_k \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1}.
  \end{align*}

\item[(4)]
  By (2),
  \begin{align*}
    &\frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_j f_k(\mathbf{x})
      dx_j \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      x_j D_j f_k(\mathbf{x})
      dx_1 \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{1}{(r_{k-1})^{k-1}}
      (-D_k f_k(x) x_k)
      dx_1 \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{-D_k f_k(\mathbf{x})}{(r_{k-1})^{k-1}}
      x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}} \\
    =& \:
    (r_{k})^{-k}
      (-1)^{k-1} x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}}
      & ((1)).
  \end{align*}
  Also,
  \begin{align*}
    &\frac{1}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1}
      (-1)^{j-1} x_j D_k f_k(\mathbf{x})
      dx_k \wedge dx_1 \wedge \cdots
        \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k-1} \\
    =& \:
    \frac{(-1)^{k} D_k f_k(\mathbf{x})}{(r_{k-1})^{k-1}}
      \sum_{j=1}^{k-1} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \:
    (r_{k})^{-k}
      \sum_{j=1}^{k-1} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k}
      & ((1)).
  \end{align*}

\item[(5)]
  Hence,
  \begin{align*}
    & \: d(f_k \omega_{k-1}) \\
    =& \: (r_{k})^{-k}
      (-1)^{k-1} x_k dx_1 \wedge \cdots \wedge dx_{k-1} \wedge \widehat{dx_{k}} \\
      &+ \: (r_{k})^{-k}
        \sum_{j=1}^{k-1} (-1)^{j-1} x_j
          dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \: (r_{k})^{-k}
      \sum_{j=1}^{k} (-1)^{j-1} x_j
        dx_1 \wedge \cdots \wedge \widehat{dx_j} \wedge \cdots \wedge dx_{k} \\
    =& \: \omega_{k}.
  \end{align*}
\end{enumerate}
$\Box$ \\



% https://scholar.rose-hulman.edu/cgi/viewcontent.cgi?article=1064&context=rhumj



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  $\omega_n$ is not exact in $E_n$ (though it is locally exact).

\item[(2)]
  Let
  \begin{align*}
    \mathbb{S}^{n-1}
    &= \{ (x_1,\ldots,x_n) \in \mathbb{R}^n : x_1^2+\cdots+x_n^2 = 1 \} \\
    \mathbb{B}^{n}
    &= \{ (x_1,\ldots,x_n) \in \mathbb{R}^n : x_1^2+\cdots+x_n^2 \leq 1 \}.
  \end{align*}
  \emph{It suffices to show that}
  \[
    \int_{\mathbb{S}^{n-1}} \omega_n
    = \frac{n \pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)} \neq 0.
  \]
  Therefore, $\omega_n$ is not exact in $E_n$.

\item[(3)]
  Define
  \[
    \omega = \frac{1}{n} \sum_{i=1}^{n} (-1)^{i-1} x_i
    dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_{n}
  \]
  on $\mathbb{S}^{n-1}$.
  Note that
  \[
    \omega = \frac{1}{n} \omega_n
  \]
  on $\mathbb{S}^{n-1}$ (and that's why we pick $\mathbb{S}^{n-1}$).
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\mathbb{S}^{n-1}} \frac{1}{n} \omega_n
    = \int_{\partial \mathbb{B}^{n}} \omega
    = \int_{\mathbb{B}^{n}} d\omega
    = \int_{\mathbb{B}^{n}} dx_1 \wedge \cdots \wedge dx_n
    = \mathrm{vol}(\mathbb{B}^n),
  \]
  where $\mathrm{vol}(\mathbb{B}^n)$ is the volume of $\mathbb{B}^n$.
  Thus
  \emph{it suffices to show that}
  \[
    \mathrm{vol}(\mathbb{B}^n)
    = \frac{\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)} \neq 0.
  \]
  There are many proofs for this.
  We give a direct integration in spherical coordinates.

\item[(4)]
  Similar to Exercise 10.9.
  The spherical coordinate system has a radial coordinate $r$ and
  angular coordinates $\bm{\varphi} = (\varphi_1, \ldots, \varphi_{n-1})$,
  where the domain of each $\varphi_1, \ldots, \varphi_{n-2}$ is $[0,\pi]$
  and the domain of $\varphi_{n-1}$ is $[0,2\pi]$.
  That is,
  \begin{align*}
    x_1 &= \cos\varphi_1 \\
    x_2 &= \sin\varphi_1 \cos\varphi_2 \\
    x_3 &= \sin\varphi_1 \sin\varphi_2 \cos\varphi_3 \\
    & \cdots \\
    x_{n-1} &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1}.
  \end{align*}
  (It is different from Exercise 10.22.)
  The spherical volume element is
  \[
    r^{n-1} \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
    dr \: d\bm{\varphi}.
  \]
  Thus by Some consequences 8.21,
  \begin{align*}
    \mathrm{vol}(\mathbb{B}^n)
    &= \int_{\mathbb{B}^{n}} d\mathbf{x} \\
    &= \int_{0}^{1} \int_{0}^{\pi} \cdots \int_{0}^{2\pi}
      r^{n-1} \sin^{n-2} \varphi_1 \cdots \sin \varphi_{n-2}
      dr \: d\bm{\varphi} \\
    &= \left( \int_{0}^{1} r^{n-1} dr \right)
      \left(\int_{0}^{\pi} \sin^{n-2} \varphi_1 d\varphi_1 \right)
      \cdots
      \left(\int_{0}^{2\pi} d\varphi_{n-1} \right) \\
    &= \frac{1}{n}
      \cdot \frac{\Gamma(\frac{n-1}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n}{2})}
      \cdot \frac{\Gamma(\frac{n-2}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n-1}{2})}
      \cdots
      \frac{\Gamma(1)\Gamma(\frac{1}{2})}{\Gamma(\frac{3}{2})} \cdot 2\pi \\
    &= \frac{\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)}.
  \end{align*}
  (Use the similar argument in (d)(ii) to get the spherical volume element.)

\item[(5)]
  Note that we can apply the spherical coordinate system
  to $\int_{\mathbb{S}^{n-1}} \omega_n$ directly (without the Stokes' theorem).
  The area element is
  \[
    \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
    d\bm{\varphi}.
  \]
  A long calculation shows that
  \begin{align*}
    &\int_{\mathbb{S}^{n-1}} \omega_n \\
    =& \: \int_{0}^{\pi} \cdots \int_{0}^{2\pi}
      \sin^{n-2} \varphi_1 \sin^{n-3} \varphi_2 \cdots \sin \varphi_{n-2}
      d\bm{\varphi} \\
    =& \: \left(\int_{0}^{\pi} \sin^{n-2} \varphi_1 d\varphi_1 \right)
      \cdots
      \left(\int_{0}^{2\pi} d\varphi_{n-1} \right) \\
    =& \: \frac{\Gamma(\frac{n-1}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n}{2})}
      \cdot \frac{\Gamma(\frac{n-2}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{n-1}{2})}
      \cdots
      \frac{\Gamma(1)\Gamma(\frac{1}{2})}{\Gamma(\frac{3}{2})} \cdot 2\pi \\
    =& \: \frac{n\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}+1\right)}.
  \end{align*}
  (See (d)(ii) for more details.)
\end{enumerate}
$\Box$ \\




\emph{Outline of (d).}
\begin{enumerate}
\item[(i)]
  One generalization of Exercise 10.21(a) and 10.22(a).
  See Exercise 10.23(a).

\item[(ii)]
  One generalization of Exercise 10.22(b).
  \emph{Let $\Sigma = \mathbb{S}^{n-1}$ be the $(n-1)$-surface in $\mathbb{R}^n$,
  with parameter domain $D = [0,\pi]^{n-2} \times [0,2\pi]$,
  given by
  \begin{align*}
    x_1 &= \cos\varphi_1 \\
    x_2 &= \sin\varphi_1 \cos\varphi_2 \\
    x_3 &= \sin\varphi_1 \sin\varphi_2 \cos\varphi_3 \\
    & \cdots \\
    x_{n-1} &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1}.
  \end{align*}
  Let $S$ denote the restriction of $\Sigma$ to a parameter domain $E \subseteq D$.
  Prove that
  \begin{align*}
    \int_{S} \omega_n
    &= \int_{E} \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}
      d\bm{\varphi} \\
    &= A(S),
  \end{align*}
  where $A$ denotes surface area.}

\item[(iii)]
  One generalization of Exercise 10.22(c).
  \emph{Suppose $g \in \mathscr{C}''([0,1])$,
  $\mathbf{h} = (h_1, \ldots, h_{n}) \in \mathscr{C}''([0,1]^{n-2})$, and $g > 0$.
  Write
  $\mathbf{x} = (x_1,\ldots,x_n)$ and $\mathbf{s} = (s_1,\ldots,s_{n-2})$.
  Let
  \[
    \mathbf{x}
    = \Phi(\mathbf{s},t)
  \]
  define a $(n-1)$-surface $\Phi$, with parameter domain $[0,1]^{n-1}$, by
  \[
    \mathbf{x} = g(t)\mathbf{h}(\mathbf{s}).
  \]
  Prove that
  \[
    \int_{\Phi} \omega_n = 0.
  \]}

\item[(iv)]
  One generalization of Exercise 10.21(b) and 10.22(d).
  \emph{Let $E$ be a closed cell in $D$,
  with edges parallel to those of $D$.
  Suppose $f \in \mathscr{C}''(D)$, $f > 0$.
  Let $\Omega$ be the $(n-1)$-surface with parameter domain $E$,
  defined by
  \[
    \Omega(\bm{\varphi}) = f(\bm{\varphi}) \Sigma(\bm{\varphi}).
  \]
  Define $S$ as in (ii) and prove that
  \[
    \int_{\Omega} \omega_n = \int_{S} \omega_n = A(S).
  \]}

\item[(v)]
  One generalization of Exercise 10.21(d) and 10.22(e).
  See Exercise 10.23(b).

\item[(vi)]
  One generalization of Examples 10.36 and 10.37.
  See Exercise 10.23(c).

\item[(vii)]
  One generalization of Exercise 10.21(e) and 10.22(f).
  \emph{Derive (iv) from Exercise 10.23(b), without using (iii).}

\item[(viii)]
  One generalization of Exercise 10.21(f).
  $\pi_{n-1}(\mathbb{S}^{n-1}) = \mathbb{Z}$ (without proof).

\item[(ix)]
  One generalization of Exercise 10.22(g).
  \emph{Show that $\omega_n$ is exact in the complement of every line $L$ passing
  through the origin.} \\
\end{enumerate}



\emph{Proof of (d)(ii).}
\begin{enumerate}
\item[(1)]
  On $S \subseteq \mathbb{S}^{n-1}$, we have
    \begin{align*}
    \int_{S} \omega_n
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i(\bm{\varphi})
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i(\bm{\varphi})
      \det
      \begin{bmatrix}
        \frac{\partial x_1}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
        \vdots & \ddots & \vdots \\
        \widehat{\frac{\partial x_i}{\partial \varphi_1}}
          & \cdots
          & \widehat{\frac{\partial x_i}{\partial \varphi_{n-1}}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial x_n}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_n}{\partial \varphi_{n-1}}
      \end{bmatrix}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{S}
      \det
      \underbrace{\begin{bmatrix}
        x_1
          & \frac{\partial x_1}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_i
          & \frac{\partial x_i}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_i}{\partial \varphi_{n-1}} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_n & \frac{\partial x_n}{\partial \varphi_1}
          & \cdots
          & \frac{\partial x_n}{\partial \varphi_{n-1}}
      \end{bmatrix}}_{\text{say $A_n$}}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1}.
  \end{align*}
  Hence, it suffices to show that
  \[
    \det(A_n)
    = \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}.
  \]



\item[(2)]
  \emph{Show that $\det(A_n)= \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin\varphi_{n-2}$.}
  Induction on $n$.
  \begin{enumerate}
  \item[(a)]
    When $n = 3$, a straightforward computation shows that the determinant is
    \begin{align*}
      &\det(A_3) \\
      =& \: \det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2
          & \cos\varphi_1\sin\varphi_2
          & \sin\varphi_1\cos\varphi_2
      \end{bmatrix} \\
      =& \: \sin\varphi_1.
    \end{align*}

  \item[(b)]
    When $n = 4$,
    \begin{align*}
      &\det(A_4) \\
      =& \: \det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2
          & 0 \\
        \sin\varphi_1\sin\varphi_2\cos\varphi_3
          & \cos\varphi_1\sin\varphi_2\cos\varphi_3
          & \sin\varphi_1\cos\varphi_2\cos\varphi_3
          & -\sin\varphi_1\sin\varphi_2\sin\varphi_3 \\
        \sin\varphi_1\sin\varphi_2\sin\varphi_3
          & \cos\varphi_1\sin\varphi_2\sin\varphi_3
          & \sin\varphi_1\cos\varphi_2\sin\varphi_3
          & \sin\varphi_1\sin\varphi_2\cos\varphi_3
      \end{bmatrix}.
    \end{align*}
    Expand along the last column to get
    \begin{align*}
      &\det(A_4) \\
      =& \: (-1)^{3+4} (-\sin\varphi_1\sin\varphi_2\sin\varphi_3) \\
      &\det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2\sin\varphi_3
          & \cos\varphi_1\sin\varphi_2\sin\varphi_3
          & \sin\varphi_1\cos\varphi_2\sin\varphi_3
      \end{bmatrix} \\
      &+ (-1)^{4+4} (\sin\varphi_1\sin\varphi_2\cos\varphi_3) \\
      &\det
      \begin{bmatrix}
        \cos\varphi_1
          & -\sin\varphi_1
          & 0 \\
        \sin\varphi_1\cos\varphi_2
          & \cos\varphi_1\cos\varphi_2
          & -\sin\varphi_1\sin\varphi_2 \\
        \sin\varphi_1\sin\varphi_2\cos\varphi_3
          & \cos\varphi_1\sin\varphi_2\cos\varphi_3
          & \sin\varphi_1\cos\varphi_2\cos\varphi_3
      \end{bmatrix} \\
      =& \: (\sin\varphi_1\sin\varphi_2\sin^2\varphi_3) \det(A_3)
        + (\sin\varphi_1\sin\varphi_2\cos^2\varphi_3) \det(A_3) \\
      =& \: \sin\varphi_1\sin\varphi_2 \det(A_3) \\
      =& \: \sin^2 \varphi_1\sin\varphi_2.
    \end{align*}

  \item[(c)]
    Now for large $n$, as (b) we expand along the last column to get
    \begin{align*}
      &\det(A_n) \\
      =& \: (-1)^{(n-1)+n} (-\sin\varphi_1\cdots\sin\varphi_{n-2}\sin\varphi_{n-1})
        (\sin\varphi_{n-1}\det(A_{n-1})) \\
      &+ (-1)^{n+n} (\sin\varphi_1\cdots\sin\varphi_{n-2}\cos\varphi_{n-1})
        (\cos\varphi_{n-1}\det(A_{n-1})) \\
      =& \: (\sin\varphi_1\cdots\sin\varphi_{n-2}) \det(A_{n-1}) \\
      =& \: (\sin\varphi_1\cdots\sin\varphi_{n-2})
        (\sin^{n-3}\varphi_1 \sin^{n-4}\varphi_2 \cdots \sin\varphi_{n-3}) \\
      =& \: \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}.
    \end{align*}
  \end{enumerate}

\item[(3)]
  (Area elements in $\mathbb{R}^3$ 10.46.)
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in S$.
  Define the vector $\mathbf{N}(\bm{\varphi})$ by
  \[
    \mathbf{N}(\bm{\varphi})
    = \sum_{i=1}^{n}
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})} \mathbf{e}_i.
  \]
  So the area of $S$ is defined by
  \[
    A(S) = \int_{E} \abs{ \mathbf{N}(\bm{\varphi}) } d\bm{\varphi}.
  \]

\item[(4)]
  By the similar proof in (2),
  \begin{align*}
    & \: \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(\varphi_1,\ldots,\varphi_{n-1})} \\
    =& \:
    (-1)^{i-1}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      x_i
  \end{align*}
  if $i = 1, \ldots, n$.
  Since $\sum x_i^2 = 1$ on $S$,
  \[
    \abs{ \mathbf{N}(\bm{\varphi}) }
    = \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}.
  \]
  Thus,
  \begin{align*}
    A(S)
    &=
      \int_{E} \abs{ \mathbf{N}(\bm{\varphi}) } d\bm{\varphi} \\
    &=
    \int_{E}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2} d\bm{\varphi}.
  \end{align*}

\item[(5)]
  Note that we can apply (3) on (2) to get the same conclusion.
  \begin{align*}
    \int_{S} \omega_n
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
    \int_{S} \sum_{i=1}^{n} (-1)^{i-1} x_i
      \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
        {\partial(\varphi_1,\ldots,\varphi_{n-1})}
      d\varphi_1 \wedge \cdots \wedge d\varphi_{n-1} \\
    =& \:
    \int_{E} \sum_{i=1}^{n} (-1)^{i-1} x_i \\
      & \qquad (-1)^{i-1}
      \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      x_i d\bm{\varphi} \\
    =& \:
    \int_{E} \sin^{n-2}\varphi_1 \sin^{n-3}\varphi_2 \cdots \sin^2\varphi_{n-3} \sin\varphi_{n-2}
      d\bm{\varphi}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(iii).}
\begin{enumerate}
\item[(1)]
  Similar to Exercise 10.22(c).
  Assume that $\omega_n$ is well-defined, i.e., $\mathbf{h}(\mathbf{s}) \neq 0$
  for all $\mathbf{s} \in [0,1]^{n-2}$.
  \begin{align*}
    \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(s_1,\ldots,s_{n-2},t)}
    =& \:
    \det
    \begin{bmatrix}
      \frac{\partial x_1}{\partial s_1}
        & \cdots
        & \frac{\partial x_1}{\partial s_{n-2}}
        & \frac{\partial x_1}{\partial t} \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{\frac{\partial x_i}{\partial s_1}}
        & \cdots
        & \widehat{\frac{\partial x_i}{\partial s_{n-2}}}
        & \widehat{\frac{\partial x_i}{\partial t}} \\
      \vdots & \ddots & \vdots & \vdots \\
      \frac{\partial x_1}{\partial s_1}
        & \cdots
        & \frac{\partial x_n}{\partial s_{n-2}}
        & \frac{\partial x_n}{\partial t}
    \end{bmatrix} \\
    =& \:
    \det
    \begin{bmatrix}
      g \frac{\partial h_1}{\partial s_1}
        & \cdots
        & g \frac{\partial h_1}{\partial s_{n-2}}
        & g' h_1 \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{g \frac{\partial h_i}{\partial s_1}}
        & \cdots
        & \widehat{g \frac{\partial h_i}{\partial s_{n-2}}}
        & \widehat{g' h_i} \\
      \vdots & \ddots & \vdots & \vdots \\
      g \frac{\partial h_n}{\partial s_1}
        & \cdots
        & g \frac{\partial h_n}{\partial s_{n-2}}
        & g' h_n
    \end{bmatrix} \\
    =& \:
    g^{n-2} g' \det
    \underbrace{\begin{bmatrix}
      \frac{\partial h_1}{\partial s_1}
        & \cdots
        & \frac{\partial h_1}{\partial s_{n-2}}
        & h_1 \\
      \vdots & \ddots & \vdots & \vdots \\
      \widehat{\frac{\partial h_i}{\partial s_1}}
        & \cdots
        & \widehat{\frac{\partial h_i}{\partial s_{n-2}}}
        & \widehat{h_i} \\
      \vdots & \ddots & \vdots & \vdots \\
      \frac{\partial h_n}{\partial s_1}
        & \cdots
        & \frac{\partial h_n}{\partial s_{n-2}}
        & h_n
    \end{bmatrix}}_{\text{say $A$}}.
  \end{align*}
\item[(2)]
  So
  \begin{align*}
    \int_{\Phi} \omega_n
    =& \: \int_{[0,1]^{n-1}} \frac{1}{g(t)^{n}|\mathbf{h}(\mathbf{s})|^{n}}
      \sum_{i=1}^{n} (-1)^{i-1} g(t) h_i g(t)^{n-2} g'(t) \det(A) \: d\mathbf{s} \: dt \\
    =& \: \int_{[0,1]^{n-1}} \frac{g'(t)}{g(t)|\mathbf{h}(\mathbf{s})|^{n}}
      \sum_{i=1}^{n} (-1)^{i-1} h_i
      \det
      \begin{bmatrix}
        \frac{\partial h_1}{\partial s_1}
          & \cdots
          & \frac{\partial h_1}{\partial s_{n-2}}
          & h_1 \\
        \vdots & \ddots & \vdots & \vdots \\
        \widehat{\frac{\partial h_i}{\partial s_1}}
          & \cdots
          & \widehat{\frac{\partial h_i}{\partial s_{n-2}}}
          & \widehat{h_i} \\
        \vdots & \ddots & \vdots & \vdots \\
        \frac{\partial h_n}{\partial s_1}
          & \cdots
          & \frac{\partial h_n}{\partial s_{n-2}}
          & h_n
      \end{bmatrix}
      \: d\mathbf{s} \: dt \\
    =& \: \int_{[0,1]^{n-1}} \frac{g'(t)}{g(t)|\mathbf{h}(\mathbf{s})|^{n}}
      \det
      \underbrace{\begin{bmatrix}
        h_1
          & \frac{\partial h_1}{\partial s_1}
          & \cdots
          & \frac{\partial h_1}{\partial s_{n-2}}
          & h_1 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        h_n
          & \frac{\partial h_n}{\partial s_1}
          & \cdots
          & \frac{\partial h_n}{\partial s_{n-2}}
          & h_n
      \end{bmatrix}}_{\text{say $B$}}
      \: d\mathbf{s} \: dt.
  \end{align*}
  Since the first column is the same as the last column in $B$,
  $\det(B) = 0$ (Theorem 9.34(d)).
  Therefore, $\int_{\Phi} \omega_n = \int_{[0,1]^{n-1}} 0 \: d\mathbf{s} \: dt = 0$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(iv).}
\begin{enumerate}
\item[(1)]
  Consider the $n$-surface $\Psi$ given by
  \[
    \Psi(t,\bm{\varphi}) = [1-t+tf(\bm{\varphi})]\Sigma(\bm{\varphi}),
  \]
  where $\bm{\varphi} \in E \subseteq D$, $0 \leq t \leq 1$.

\item[(2)]
  Write
  \[
    E = [a_1,b_1] \times \cdots \times [a_{n-1},b_{n-1}] \subseteq D.
  \]
  Note that $\Psi(t,\bm{\varphi}) \subseteq \mathbb{R}^n - \{ \mathbf{0} \}$.
  So the boundary of $\Psi$ is
  \[
    \partial \Psi
    = \Psi(0,\bm{\varphi}) - \Psi(1,\bm{\varphi})
      + \sum_{i=1}^{n-1} ( \Psi|_{\varphi_i=a_i} - \Psi|_{\varphi_i=b_i}),
  \]
  where $\Psi|_{\varphi_i=\theta}:
  [a_1,b_1] \times \cdots \times \widehat{[a_i,b_i]} \times \cdots \times [a_{n-1},b_{n-1}]
  \to \Omega$ is a mapping defined by
  \begin{align*}
   \Psi|_{\varphi_i=\theta}(t,\varphi_1,\ldots,\widehat{\varphi_i},\ldots,\varphi_{n-1})
    &= \Psi(t,\varphi_1,\ldots,\varphi_{i-1},\theta,\varphi_{i+1},\ldots,\varphi_{n-1}) \\
    &= \Psi(t,\bm{\varphi} + (\theta-\varphi_i)\mathbf{e}_i).
  \end{align*}

\item[(3)]
  \emph{Show that
  \[
    \int_{\Psi|_{\varphi_1=\theta}} \omega_n = 0
  \]
  for any fixed $\varphi_1 = \theta \in [a_1, b_1]$.}
  Note that $\omega_n$ is well-defined on $\Psi|_{\varphi_1=\theta}$. \\
  Write
  \[
    \Psi|_{\varphi_1=\theta}(t,\widehat{\varphi_1},\varphi_2,\ldots,\varphi_{n-1})
    = \mathbf{x}(t,\widehat{\varphi_1},\varphi_2,\ldots,\varphi_{n-1}).
  \]
  By definition of $\Psi$, we have
  \begin{align*}
    x_1 &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \cos\theta \\
    x_2 &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cos\varphi_2 \\
    & \cdots \\
    x_{n-1} &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cdots \sin\varphi_{n-2} \cos\varphi_{n-1} \\
    x_n &= g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
      \sin\theta \cdots \sin\varphi_{n-2} \sin\varphi_{n-1},
  \end{align*}
  where
  $g(t,\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)
  = 1-t+tf(\bm{\varphi} + (\theta-\varphi_1)\mathbf{e}_1)$.

\item[(4)]
  Note that $r_n = g > 0$.
  Since
  \[
    \frac{\partial x_i}{\partial t}
    = \frac{\partial g}{\partial t} g^{-1} x_i,
  \]
  \begin{align*}
    \frac{\partial(x_1,\ldots,\widehat{x_i},\ldots,x_n)}
      {\partial(t,\widehat{\varphi_1}, \varphi_2,\ldots,\varphi_{n-1})}
    =& \: \det
    \begin{bmatrix}
      \frac{\partial x_1}{\partial t}
        & \frac{\partial x_1}{\partial \varphi_2}
        & \cdots
        & \frac{\partial x_1}{\partial \varphi_{n-1}} \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{\frac{\partial x_i}{\partial t}}
        & \widehat{\frac{\partial x_i}{\partial \varphi_2}}
        & \cdots
        & \widehat{\frac{\partial x_i}{\partial \varphi_{n-1}}} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{\partial x_n}{\partial t}
        & \frac{\partial x_n}{\partial \varphi_2}
        & \cdots
        & \frac{\partial x_n}{\partial \varphi_{n-1}}
    \end{bmatrix} \\
    =& \:
    \det
    \begin{bmatrix}
      \frac{\partial g}{\partial t} g^{-1} x_1
        & *
        & \cdots
        & * \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{\frac{\partial g}{\partial t} g^{-1} x_i}
        & \widehat{*}
        & \cdots
        & \widehat{*} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{\partial g}{\partial t} g^{-1} x_n
        & *
        & \cdots
        & *
    \end{bmatrix} \\
    =& \:
    \frac{\partial g}{\partial t} g^{-1}
    \det
    \underbrace{\begin{bmatrix}
      x_1
        & *
        & \cdots
        & * \\
      \vdots & \vdots & \ddots & \vdots \\
      \widehat{x_i}
        & \widehat{*}
        & \cdots
        & \widehat{*} \\
      \vdots & \vdots & \ddots & \vdots \\
      x_n
        & *
        & \cdots
        & *
    \end{bmatrix}}_{\text{say $A$}}.
  \end{align*}
  So
  \begin{align*}
    \int_{\Psi|_{\varphi_1=\theta}} \omega_n
    =& \: \int_{E} g^{-n}
      \sum_{i=1}^{n} (-1)^{i-1} x_i
      \frac{\partial g}{\partial t} g^{-1}
      \det(A)
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1} \\
    =& \: \int_{E} \frac{\partial g}{\partial t} g^{-n-1}
      \sum_{i=1}^{n} (-1)^{i-1} x_i
      \det
      \begin{bmatrix}
        x_1
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \ddots & \vdots \\
        \widehat{x_i}
          & \widehat{*}
          & \cdots
          & \widehat{*} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_n
          & *
          & \cdots
          & *
      \end{bmatrix}
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1} \\
    =& \: \int_{E} \frac{\partial g}{\partial t} g^{-n-1}
      \det
      \underbrace{\begin{bmatrix}
        x_1
          & x_1
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_i
          & x_i
          & *
          & \cdots
          & * \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        x_n
          & x_n
          & *
          & \cdots
          & *
      \end{bmatrix}}_{\text{say $B$}}
      \: dt \: d\varphi_2 \cdots d\varphi_{n-1}.
  \end{align*}
  Since the first column is the same as the second column in $B$,
  $\det(B) = 0$ (Theorem 9.34(d)).
  Therefore, $\int_{\Psi|_{\varphi_1=\theta}} \omega_n = 0$.

\item[(5)]
  $\int_{\Psi|_{\varphi_i=\theta}} \omega_n = 0$ is also true for all $i = 1,\ldots,n-1$
  by the same argument in (3)(4).
  Hence,
  \begin{align*}
    0
    =& \: \int_{\Psi} d\omega_n \\
    =& \: \int_{\partial \Psi} \omega_n \\
    =& \: \int_{S} \omega_n - \int_{\Omega} \omega_n
      + \sum_{i=1}^{n-1}
        \left( \int_{\Psi|_{\varphi_i=a_i}} \omega_n
        - \int_{\Psi|_{\varphi_i=b_i}} \omega_n \right) \\
    =& \: \int_{S} \omega_n - \int_{\Omega} \omega_n
  \end{align*}
  by (a) and the Stokes' theorem (Theorem 10.33),
  or
  \[
    \int_{\Omega} \omega_n = \underbrace{\int_{S} \omega_n = A(S)}_{\text{by (d)(ii)}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(vii).}
Similar to Exercise 10.22(f).
\begin{enumerate}
\item[(1)]
  To ensure that $\omega_n$ is well-defined on
  $E_1 \subseteq E_2 \subseteq \cdots \subseteq E_n = \mathbb{R}^n - \{ \mathbf{0} \}$,
  we might assume $0 < \varphi_1 < \pi$.
  It is fine since
  $\int_{\Omega} \omega_n$ and $\int_{S} \omega_n$ is well-defined on any closed rectangle in $D$
  and we can apply the argument in Exercise 6.7 to remove the additional restriction.

\item[(2)]
  By the Stokes' theorem (Theorem 10.33) and (b),
  \[
    \int_{\Omega} \omega_{n} = \int_{\partial\Omega} f_n \omega_{n-1}
    \qquad
    \text{and}
    \qquad
    \int_{S} \omega_{n} = \int_{\partial S} f_n \omega_{n-1}.
  \]
  So it suffices to show that
  \[
    \int_{\partial\Omega} f_n \omega_{n-1} = \int_{\partial S} f_n \omega_{n-1}.
  \]
  So it suffices to show that
  $\left.f_n\right|_{\partial\Omega} = \left.f_n\right|_{\partial S}$
  and $\omega_{n-1}|_{\partial\Omega} = \omega_{n-1}|_{\partial S}$.

\item[(3)]
  \emph{Show that
  $\left.f_n\right|_{\partial\Omega} = \left.f_n\right|_{\partial S}$.}
  For any $\mathbf{x}_{\Omega} \in \partial\Omega$,
  \[
    \mathbf{x}_{\Omega} = f(\bm{\varphi}) \mathbf{x}_{\Sigma}.
  \]
  So
  \begin{align*}
    f_n(\mathbf{x}_{\Omega})
    &= (-1)^n g_n\left(
      \frac{(x_n)_{\Omega}}
        {((x_1)_{\Omega}^2+\cdots+(x_n)_{\Omega}^2)^\frac{1}{2}}\right) \\
    &= (-1)^n g_n\left(
      \frac{f(\bm{\varphi})(x_n)_{\Sigma}}
        {f(\bm{\varphi})((x_1)_{\Sigma}^2+\cdots+(x_n)_{\Sigma}^2)^\frac{1}{2}}\right) \\
    &= (-1)^n g_n\left(
      \frac{(x_n)_{\Sigma}}
        {((x_1)_{\Sigma}^2+\cdots+(x_n)_{\Sigma}^2)^\frac{1}{2}}\right) \\
    &= f_n(\mathbf{x}_{\Sigma}).
  \end{align*}
  (Note that $f > 0$.)

\item[(4)]
  \emph{Show that $\omega_{n-1}|_{\partial\Omega} = \omega_{n-1}|_{\partial S}$.}
  Induction on $n$.
  When $n = 2$ or $n = 3$, it is proved in Exercise 10.22(f).
  Now for large $n-1$, (3) is also true for $n-1$.
  Hence,
  \[
    \left.\omega_{n-1}\right|_{\partial\Omega}
    = \left.d(f_{n-1}\omega_{n-2})\right|_{\partial\Omega}
    = \left.d(f_{n-1}\omega_{n-2})\right|_{\partial S}
    = \left.\omega_{n-1}\right|_{\partial S}.
  \]
  By induction, the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d)(ix).}
Similar to Exercise 10.22(g).
\begin{enumerate}
\item[(1)]
  Given any line $L$ passing through $\mathbf{0}$,
  say
  \[
    (r \cos\varphi_1, \cdots, \sin\varphi_1 \cdots \sin\varphi_{n-2} \sin\varphi_{n-1})
    \in L
    \subseteq \mathbb{R}^n
  \]
  where $r \in \mathbb{R}^1$
  for some $\bm{\varphi} \in [0,\pi]^{n-2} \times [0,2\pi]$.
  We will show that $\omega_n$ is exact in $U = \mathbb{R}^n - L$.

\item[(2)]
  Linear algebra says that all rotation matrices $T \in SO(n)$ can be obtained from
  \[
    R_i(u) = \begin{bmatrix}
      1 &               &   &      &   &               &   \\
        &        \ddots &   &      &   & \text{\huge0} &   \\
        &               & 1 &      &   &               &   \\
        &               &   & R(u) &   &               &   \\
        &               &   &      & 1 &               &   \\
        & \text{\huge0} &   &      &   &        \ddots &   \\
        &               &   &      &   &               & 1
    \end{bmatrix}
  \]
  using matrix multiplication.
  Here
  \[
    R(u) =
    \begin{bmatrix}
      \cos u & -\sin u \\
      \sin u &  \cos u
    \end{bmatrix}
  \]
  is a $2$-by-$2$ rotation matrix at the $i$th row and $i$th column.
  For example, the rotation
  \[
    T
    = R_1(-\varphi_1) R_2(-\varphi_2) \cdots
      R_{n-2}(-\varphi_{n-2}) R_{n-1}(-\varphi_{n-1})
  \]
  maps $L$ to the $x_n$-axis.
  Similar to Exercise 10.22(g), it suffices to show that $\omega_n$ is invariant under $T = R_1(u)$.

\item[(3)]
  \emph{Show that $\omega_n$ is invariant under $T = R_1(u)$.}
  By
  \[
    T: \mathbf{x} \mapsto (x_1 \cos u - x_2 \sin u, x_1 \sin u + x_2 \cos u, x_3, \ldots, x_n),
  \]
  we have
  \begin{align*}
    r_n &\mapsto r_n \\
    dx_1 &\mapsto \cos u dx_1 - \sin u dx_2 \\
    dx_2 &\mapsto \sin u dx_1 + \cos u dx_2 \\
    dx_3 &\mapsto dx_3 \\
    &\cdots \\
    dx_n &\mapsto dx_n.
  \end{align*}
  So $dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n$ maps to
  \begin{equation*}
    \begin{cases}
      \cos u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
      + \sin u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n
      & \text{if $i = 1$} \\
      -\sin u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
        + \cos u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n
      & \text{if $i = 2$} \\
      dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n
      & \text{otherwise}.
    \end{cases}
  \end{equation*}
  Thus
  \begin{align*}
    \omega_n
    \mapsto& \:
      (r_n)^{-n} (x_1 \cos u - x_2 \sin u) \\
        &\qquad \left(\cos u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
          + \sin u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n\right) \\
      & + (r_n)^{-n}(x_1 \sin u + x_2 \cos u) \\
        &\qquad \left(-\sin u \: \widehat{dx_1} \wedge \cdots \wedge dx_n
          + \cos u \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n\right) \\
      & + (r_n)^{-n}\sum_{i=3}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
      (r_n)^{-n} x_1 \widehat{dx_1} \wedge \cdots \wedge dx_n \\
      & - (r_n)^{-n} x_2 \: dx_1 \wedge \widehat{dx_2} \wedge \cdots \wedge dx_n \\
      & + (r_n)^{-n} \sum_{i=3}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \:
      (r_n)^{-n} \sum_{i=1}^{n}(-1)^{i-1} x_i
        dx_1 \wedge \cdots \wedge \widehat{dx_i} \wedge \cdots \wedge dx_n \\
    =& \: \omega_n.
  \end{align*}

\item[(4)]
  Similar to Exercise 10.22(g),
  $\omega_n$ is exact in $\mathbb{R}^n - L$.
  (Or $\omega_n$ is locally exact in $\mathbb{R}^n - \{ \mathbf{0} \}$.)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.24.}
\emph{Let $\omega = \sum a_i(\mathbf{x}) dx_i$ be a $1$-form of class $\mathscr{C}''$
in a convex open set $E \subseteq \mathbb{R}^n$.
Assume $d\omega = 0$ and prove that $\omega$ is exact in $E$,
by completing the following outline:} \\

\emph{Fix $\mathbf{p} \in E$.
Define
\[
  f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
  \qquad
  (\mathbf{x} \in E).
\]
Apply Stokes' theorem to affine-oriented $2$-simplexs $[\mathbf{p},\mathbf{x},\mathbf{y}]$ in $E$.
Deduce that
\[
  f(\mathbf{y}) - f(\mathbf{x})
  = \sum_{i=1}^{n}(y_i - x_i) \int_{0}^{1} a_i((1-t)\mathbf{x} + t\mathbf{y}) dt
\]
for $\mathbf{x} \in E$, $\mathbf{y} \in E$.
Hence $(D_i f)(\mathbf{x}) = a_i(\mathbf{x})$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]

\item[(2)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  The affine-oriented $2$-simplexs $\Psi = [\mathbf{p},\mathbf{x},\mathbf{y}]$ is in $E$
  by the convexity of $E$.
  (If $E$ is open but not convex,
  we can show that $\omega = df$ \textbf{\emph{locally}} as the note in Exercise 10.21(a).
  That is why we say that $\omega$ is locally exact.
  The proof is exactly the same.)

\item[(3)]
  Note that
  \[
    \partial \Psi
    = \partial [\mathbf{p},\mathbf{x},\mathbf{y}]
    = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}].
  \]
  The Stokes' theorem (Theorem 10.33) implies that
  \begin{align*}
    \int_{\Psi} d\omega
    = \int_{\partial \Psi} \omega
    &\Longleftrightarrow
    \int_{\Psi} 0
    = \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &\Longleftrightarrow
    0 = \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}) \\
    &\Longleftrightarrow
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega.
  \end{align*}

\item[(4)]
  Define $\gamma: [0,1] \to E$ by
  \begin{align*}
    \gamma(t)
    &= \mathbf{x} + t(\mathbf{y}-\mathbf{x}) \\
    &= \sum_{i=1}^{n} x_i + t(y_i - x_i)
  \end{align*}
  (where $\mathbf{x} = (x_1, \ldots, x_n)$ and $\mathbf{y} = (y_1, \ldots, y_n)$).
  Hence $[0,1]$ is the parameter domain of $[\mathbf{x},\mathbf{y}]$ with respect to $\gamma$.
  So
  \begin{align*}
    \int_{[\mathbf{x},\mathbf{y}]} \omega
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\gamma(t))
      \frac{\partial (x_i + t(y_i - x_i))}{\partial t} dt \\
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x}))(y_i - x_i) dt \\
    &= \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \end{align*}
  Thus,
  \[
    f(\mathbf{y}) - f(\mathbf{x})
    = \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \]

\item[(5)]
  Note that
  \begin{align*}
    f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})
    &= \sum_{i=1}^{n} ((x_i + h\delta_{ij}) - x_i)
      \int_{0}^{1} a_i(\mathbf{x} + t((\mathbf{x} + h \mathbf{e}_j)-\mathbf{x})) dt \\
    &= \sum_{i=1}^{n} h\delta_{ij}
      \int_{0}^{1} a_i(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= h \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt.
  \end{align*}
  (Here $\delta_{ij}$ is the Kronecker delta.)
  So
  \begin{align*}
    (D_j f)(\mathbf{x})
    &= \lim_{h \to 0}
      \frac{f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})}{h} \\
    &= \lim_{h \to 0}
      \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= \int_{0}^{1} a_j(\mathbf{x}) dt
      &(a_j \in \mathscr{C}'') \\
    &= a_j(\mathbf{x}).
  \end{align*}
  Thus,
  \[
    df
    = \sum_{j=1}^{n} (D_j f)(\mathbf{x}) dx_j
    = \sum_{j=1}^{n} a_j(\mathbf{x}) dx_j
    = \omega,
  \]
  or $\omega$ is exact in $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.25.}
\emph{Assume $\omega$ is a $1$-form in an open set $E \subseteq \mathbb{R}^n$
such that
\[
  \int_{\gamma} \omega = 0
\]
for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.
Prove that $\omega$ is exact in $E$,
by imitating part of the argument sketched in Exercise 10.24.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Assume that $E$ is a \textbf{connected} open subset of $\mathbb{R}^n$.
  Show that $\omega$ is exact in $E$
  if $\int_{\gamma} \omega = 0$
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.}

\item[(2)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]
  It is well-defined since $E$ is connected and
  $\int_{\gamma} \omega = 0$ for every closed curve $\gamma$ in $E$.

\item[(3)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  Let
  \[
    \gamma = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}]
  \]
  be a closed curve in $E$.
  Hence,
  \begin{align*}
    0
    &= \int_{\gamma} \omega
      &(\text{Assumption}) \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}).
  \end{align*}
  So
  \begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega
  \end{align*}

\item[(4)]
  Similar to (4)(5) in the proof of Exercise 10.24, we have $df = \omega$.
  So the statement in (1) is proved.
  In general, we can define each $f_{\alpha}$
  on each connected component $E_{\alpha}$ (which is open) of $E$
  such that $d f_{\alpha} = \omega$ on $E_{\alpha}$.
  Take
  \[
    f|_{E_{\alpha}} = f_{\alpha}
  \]
  on $E$.
  Hence, $df = \omega$ on the whole $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.26.}
\emph{Assume $\omega$ is a $1$-form in $\mathbb{R}^3 - \{\mathbf{0}\}$,
of class $\mathscr{C}'$ and $d\omega = 0$.
Prove that $\omega$ is exact in $\mathbb{R}^3 - \{\mathbf{0}\}$.
(Hint: Every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
Apply Stokes' theorem and Exercise 10.25.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Let $E = \mathbb{R}^3 - \{\mathbf{0}\}$.
  By Exercise 10.25,
  it suffices to show that
  \[
    \int_{\gamma} \omega = 0
  \]
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.

\item[(2)]
  Intuitively, every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
  is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
  So there is some $2$-surface $\Psi$ such that $\partial \Psi = \gamma$.
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\gamma} \omega
    = \int_{\partial \Psi} \omega
    = \int_{\Psi} d\omega
    = \int_{\Psi} 0
    = 0.
  \]
\end{enumerate}
$\Box$ \\\\



% Seifert surface



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.27.}
\emph{Let $E$ be an open $3$-cell in $\mathbb{R}^3$,
with edges parallel to the coordinate axes.
Suppose $(a,b,c) \in E$, $f_i \in \mathscr{C}'(E)$ for $i=1,2,3$,
\[
  \omega
  = f_1 dy \wedge dz + f_2 dz \wedge dx + f_3 dx \wedge dy,
\]
and assume that $d\omega = 0$ in $E$.
Define
\[
  \lambda = g_1 dx + g_2 dy
\]
where
\begin{align*}
  g_1(x,y,z) &= \int_{c}^{z} f_2(x,y,s) ds - \int_{b}^{y} f_3(x,t,c)dt \\
  g_2(x,y,z) &= - \int_{c}^{z} f_1(x,y,s) ds,
\end{align*}
for $(x,y,z) \in E$.
Prove that $d\lambda = \omega$ in $E$.
Evaluate these integrals when $\omega = \zeta$ and thus find the form $\lambda$
that occurs in part (e) of Exercise 10.22.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{F} = f_1 \mathbf{e}_1 + f_2 \mathbf{e}_2 + f_3 \mathbf{e}_3$
  as in Vector fields 10.42.
  Then
  \[
    d\omega = (\nabla \cdot F) dx \wedge dy \wedge dz.
  \]
  As $d\omega = 0$ by assumption, $\nabla \cdot F = D_1 f_1 + D_2 f_2 + D_3 f_3 = 0$.

\item[(2)]
  As
  \begin{align*}
    d\lambda
    =& \: d(g_1 dx + g_2 dy) \\
    =& \: (D_1g_1 dx + D_2g_1 dy + D_3g_1 dz) \wedge dx \\
      &+ (D_1g_2 dx + D_2g_2 dy + D_3g_2 dz) \wedge dy \\
    =& \: (-D_3 g_2) dy \wedge dz
      + (D_3 g_1) dz \wedge dx + (D_1 g_2 - D_2 g_1) dx \wedge dy,
  \end{align*}
  it suffices to show that
  \begin{align*}
    f_1 &= -D_3 g_2, \\
    f_2 &= D_3 g_1, \\
    f_3 &= D_1 g_2 - D_2 g_1
  \end{align*}
  on $E$.

\item[(3)]
  Theorem 6.20 implies that
  \[
    -D_3 g_2 = D_3 \int_{c}^{z} f_1(x,y,s) ds = f_1(x,y,z)
  \]
  and
  \[
    D_3 g_1
    = D_3 \int_{c}^{z} f_2(x,y,s) ds - D_3 \int_{b}^{y} f_3(x,t,c)dt \\
    = f_2(x,y,z).
  \]
  Also,
  \begin{align*}
    & \: D_1 g_2 - D_2 g_1 \\
    =& \: D_1 \left( -\int_{c}^{z} f_1(x,y,s) ds \right) \\
      &- D_2 \left( \int_{c}^{z} f_2(x,y,s) ds - \int_{b}^{y} f_3(x,t,c)dt \right) \\
    =& - \int_{c}^{z} D_1 f_1(x,y,s) ds
      &(f_1 \in \mathscr{C}') \\
      &- \int_{c}^{z} D_2 f_2(x,y,s) ds + f_3(x,y,c)
        &(\text{$f_2 \in \mathscr{C}'$, Theorem 6.20}) \\
    =& \int_{c}^{z} D_3 f_3(x,y,s) ds + f_3(x,y,c)
      &((1)) \\
    =& f_3(x,y,z)
      &(\text{Theorem 6.21}).
  \end{align*}
  Therefore, $d\lambda = \omega$ in $E$.

\item[(4)]
  When $\omega = \zeta = r^{-3} (x dy \wedge dz + y dz \wedge dx + z dx \wedge dy)$,
  we get
  \begin{align*}
    f_1(x,y,z) &= x (x^2+y^2+z^2)^{-\frac{3}{2}}, \\
    f_2(x,y,z) &= y (x^2+y^2+z^2)^{-\frac{3}{2}}, \\
    f_3(x,y,z) &= z (x^2+y^2+z^2)^{-\frac{3}{2}}.
  \end{align*}
  So,
  \begin{align*}
    \int_{c}^{z} f_2(x,y,s) ds
    &= \left[ ys (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z}, \\
    \int_{b}^{y} f_3(x,t,c) dt
    &= \left[ ct (x^2+c^2)^{-1}(x^2+t^2+c^2)^{-\frac{1}{2}} \right]_{t=b}^{t=y}, \\
    \int_{c}^{z} f_1(x,y,s) ds
    &= \left[ xs (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z}.
  \end{align*}
  Hence,
  \begin{align*}
    \lambda
    =& \: g_1 dx + g_2 dy \\
    =& \: \left[ ys (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z} dx \\
      &- \left[ ct (x^2+c^2)^{-1}(x^2+t^2+c^2)^{-\frac{1}{2}} \right]_{t=b}^{t=y} dx \\
      &+ \left[ xs (x^2+y^2)^{-1}(x^2+y^2+s^2)^{-\frac{1}{2}} \right]_{s=c}^{s=z} dy \\
    =& - \left[ zr^{-1} - c(x^2+y^2+c^2)^{-\frac{1}{2}} \right] \eta
      &(\text{Definition of $\eta$}) \\
      &- c(x^2+c^2)^{-1} \left[ y(x^2+y^2+c^2)^{-\frac{1}{2}}
        - b(x^2+b^2+c^2)^{-\frac{1}{2}} \right] dx.
  \end{align*}
  As we pick $(a,b,c) = (a,0,0) \in \mathbb{R}^3 - \{ \mathbf{0} \}$ (or $a \neq 0$),
  we have $\lambda = -zr^{-1} \eta$ such that $d\lambda = \omega = \zeta$,
  which is the same as part (e) in Exercise 10.22.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.28.}
\emph{Fix $b > a > 0$, define
\[
  \Phi(r,\theta) = (r\cos\theta, r\sin\theta)
\]
for $a \leq r \leq b$, $0 \leq \theta \leq 2\pi$.
(The range of $\Phi$ is an annulus in $\mathbb{R}^2$.)
Put $\omega = x^3 dy$,
and compute both
\[
  \int_{\Phi} d\omega
  \qquad
  \text{and}
  \qquad
  \int_{\partial\Phi} \omega
\]
to verify that they are equal.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    \frac{\partial(x,y)}{\partial(r,\theta)}
    =
    \det
    \begin{bmatrix}
      \cos\theta & -r\sin\theta \\
      \sin\theta & r\cos\theta
    \end{bmatrix}
    = r.
  \]
  So
  \begin{align*}
    \int_{\Phi} d\omega
    &= \int_{\Phi} 3x^2 dx \wedge dy
      &(dy \wedge dy = 0) \\
    &= \int_{[a,b]\times[0,2\pi]} 3(r\cos\theta)^2
      \frac{\partial(x,y)}{\partial(r,\theta)} dr d\theta \\
    &= \int_{a}^{b} \int_{0}^{2\pi} 3 r^3 (\cos\theta)^2 dr d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(2)]
  Similar to Exercise 10.21(b),
  write
  \[
    \partial \Phi = \Gamma - \gamma,
  \]
  where $\Gamma(t) = (b\cos t,b\sin t)$ on $[0,2\pi]$
  and $\gamma(t) = (a\cos t,a\sin t)$ on $[0,2\pi]$.
  Hence
  \begin{align*}
    \int_{\partial\Phi} \omega
    &= \int_{\Gamma} \omega - \int_{\gamma} \omega \\
    &= \int_{\Gamma} x^3 dy - \int_{\gamma} x^3 dy \\
    &= \int_{[0,2\pi]} (b\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta
      - \int_{[0,2\pi]} (a\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta \\
    &= \int_{0}^{2\pi} b^4 (\cos\theta)^4 d\theta
      - \int_{0}^{2\pi} a^4 (\cos\theta)^4 d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(3)]
  \[
    \int_{\Phi} d\omega
    = \int_{\partial\Phi} \omega
    = \frac{3\pi}{4}(b^4-a^4).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.29.}
\emph{Prove the existence of a function $\alpha$
with the properties needed in the proof of Theorem 10.38,
and prove that the resulting function $F$ is of class $\mathscr{C}'$.
(Both assertions become trivial if $E$ is an open cell or an open ball,
since $\alpha$ can then be taken to be a constant.
Refer to Theorem 9.42.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]

\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.30.}
\emph{If $\mathbf{N}$ is the vector given by
\[
  \mathbf{N}
  = (\alpha_2 \beta_3 - \alpha_3 \beta_2) \mathbf{e}_1
    + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \mathbf{e}_2
    + (\alpha_1 \beta_2 - \alpha_2 \beta_1) \mathbf{e}_3
\]
(Equation (135)), prove that
\[
  \det\begin{bmatrix}
    \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
    \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
    \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
  \end{bmatrix}
  = \abs{\mathbf{N}}^2
\]
Also, verify
\[
  \mathbf{N} \cdot (T\mathbf{e}_1) = \mathbf{N} \cdot (T\mathbf{e}_2)
\]
(Equation (137)).} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Laplace's expansion along the third column,
    \begin{align*}
    &\det\begin{bmatrix}
      \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
      \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
      \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
    \end{bmatrix} \\
    =& (-1)^{1+3} (\alpha_2 \beta_3 - \alpha_3 \beta_2)
      \det\begin{bmatrix}
        \alpha_2 & \beta_2 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{2+3} (\alpha_3 \beta_1 - \alpha_1 \beta_3)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{3+3} (\alpha_1 \beta_2 - \alpha_2 \beta_1)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_2 & \beta_2
      \end{bmatrix} \\
    =& (\alpha_2 \beta_3 - \alpha_3 \beta_2)^2
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3)^2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)^2 \\
    =& \abs{\mathbf{N}}^2.
  \end{align*}

\item[(2)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_1)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\alpha_1, \alpha_2, \alpha_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \alpha_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \alpha_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \alpha_3 \\
    &= (\alpha_3 \alpha_2 - \alpha_2 \alpha_3) \beta_1
      + (\alpha_1 \alpha_3 - \alpha_3 \alpha_1) \beta_2
      + (\alpha_2 \alpha_1 - \alpha_1 \alpha_2)\beta_3 \\
    &= 0.
  \end{align*}

\item[(3)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_2)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\beta_1, \beta_2, \beta_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \beta_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \beta_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \beta_3 \\
    &= (\beta_2 \beta_3 - \beta_3 \beta_2) \alpha_1
      + (\beta_3 \beta_1 - \beta_1 \beta_3) \alpha_2
      + (\beta_1 \beta_2 - \beta_2 \beta_1) \alpha_3 \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.31.}
\emph{Let $E \subseteq \mathbb{R}^3$ be open,
suppose $g \in \mathscr{C}''(E)$, $h \in \mathscr{C}''(E)$,
and consider the vector field}
\[
    \mathbf{F} = g \nabla h
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  where $\nabla^2 h = \nabla \cdot (\nabla h) = \sum\frac{\partial^2 h}{\partial x_i^2}$
  is the so-called ``Laplacian'' of $h$.}

\item[(b)]
  \emph{If $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$
  (as in Theorem 10.51), prove that
  \[
    \int_{\Omega} [ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  where (as is customary) we have written $\frac{\partial h}{\partial n}$
  in place of $(\nabla h) \cdot \mathbf{n}$.
  (Thus $\frac{\partial h}{\partial n}$ is the directional derivative of $h$
  in the direction of the outward normal to $\partial \Omega$,
  the so-called \textbf{normal derivative} of $h$.)
  Interchange $g$ and $h$,
  substract the resulting formula from the first one, to obtain
  \[
    \int_{\Omega} ( g\nabla^2 h - h \nabla^2 g) dV
    = \int_{\partial \Omega}
      \left( g \frac{\partial h}{\partial n} - h \frac{\partial g}{\partial n} \right) dA.
  \]
  These two formulas are usually called \textbf{Green's identities}.}

\item[(c)]
  \emph{Assume that $h$ is \textbf{harmonic} in $E$;
  this means that $\nabla^2 h = 0$.
  Take $g = 1$ and conclude that
  \[
    \int_{\partial \Omega} \frac{\partial h}{\partial n} dA = 0.
  \]
  Take $g = h$, and conclude that $h = 0$ in $\Omega$ if $h = 0$ on $\partial \Omega$.}

\item[(d)]
  \emph{Show that Green's identities are also valid in $\mathbb{R}^2$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since
  \[
    \mathbf{F}
    = g \nabla h
    = g \left(\sum (D_i h)\mathbf{e}_i \right)
    = \sum g(D_i h)\mathbf{e}_i,
  \]
  we have
  \begin{align*}
    \nabla \cdot \mathbf{F}
    &= \nabla \cdot \left( \sum g(D_i h)\mathbf{e}_i \right) \\
    &= \sum D_i(g(D_i h)) \\
    &= \sum \{ (D_i g)(D_i h) + g D_i(D_i h) \} \\
    &= \sum (D_i g)(D_i h) + g \sum D_i(D_i h).
  \end{align*}

\item[(2)]
  Also,
  \begin{align*}
    g \nabla^2 h + (\nabla g) \cdot (\nabla h)
    &= g \nabla \cdot (\nabla h) + (\nabla g) \cdot (\nabla h) \\
    &= g \nabla \cdot \left(\sum (D_i h)\mathbf{e}_i \right)
      + \left(\sum (D_i g)\mathbf{e}_i \right) \cdot \left(\sum (D_i h)\mathbf{e}_i \right) \\
    &= g \sum D_i(D_i h) + \sum (D_i g)(D_i h).
  \end{align*}

\item[(3)]
  By (1)(2), the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  The divergence theorem (Theorem 10.51) implies that
  \begin{align*}
    &\int_{\Omega} (\nabla \cdot \mathbf{F}) dV
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) dA \\
    \Longrightarrow&
    \int_{\Omega} [g \nabla^2 h + (\nabla g) \cdot (\nabla h)] dV
    = \int_{\partial\Omega} g \underbrace{\nabla h \cdot \mathbf{n}}_{=\frac{\partial h}{\partial n}} dA.
  \end{align*}

\item[(2)]
  Green's identities are a set of three identities in vector calculus
  relating the bulk with the boundary of a region on which differential operators act.
  \emph{(Green's third identity.)
  Assume that $h$ is harmonic in $E$.
  If $G(\mathbf{x},\mathbf{x}_0)$ is the Green's function,
  then}
  \[
    h(\mathbf{x}_0)
    = \int_{\partial \Omega}
      \left[ h(\mathbf{x}) \frac{\partial G(\mathbf{x},\mathbf{x}_0)}{\partial n}
      - G(\mathbf{x},\mathbf{x}_0) \frac{\partial h(\mathbf{x})}{\partial n} \right] dA.
  \]
  For example, in $\mathbb{R}^3$
  \[
    G(\mathbf{x},\mathbf{x}_0) = -\frac{1}{4\pi\norm{\mathbf{x} - \mathbf{x}_0}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
Assume $\nabla^2 h = 0$.
\begin{enumerate}
\item[(1)]
  Take $g = 1$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get the conclusion.
  (Here $\nabla g = \mathbf{0}$ as $g = 1$.)

\item[(2)]
  Assume $h = 0$ on $\partial\Omega$.
  Take $g = h$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get
  \[
    \int_{\Omega} |\nabla h|^2 dV
    = \int_{\partial \Omega} h \frac{\partial h}{\partial n} dA
    = 0
  \]
  (since $h = 0$ on $\partial\Omega$).
  Since $h \in \mathscr{C}'(\Omega)$, Exercise 6.2 implies that
  $|\nabla h|^2 = 0$ on $\Omega$.
  So $D_1 h = D_2 h = D_3 h = 0$ on $\Omega$.
  Since $h \in \mathscr{C}'(\Omega)$, Theorem 9.21 implies that
  $h = 0$ on $\Omega$, or
  $h$ is locally constant in $\Omega$ (Exercise 9.9).
  Note that $h = 0$ globally on $\partial \Omega$,
  and thus $h = 0$ globally on $\Omega$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \emph{(The divergence theorem in $\mathbb{R}^2$.)
  If $\mathbf{F} = F_1 \mathbf{e}_1 + F_2 \mathbf{e}_2$ is a vector field of class
  $\mathscr{C}'$ in an open set $E \subseteq \mathbb{R}^2$,
  and if $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial\Omega$
  then}
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]
  Define a $1$-form by
  \[
    \omega_{\mathbf{F}} = F_1 dy - F_2 dx.
  \]
  So
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F})dx \wedge dy
    = (\nabla \cdot \mathbf{F})dA.
  \]
  Hence the Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\Omega} d\omega_{\mathbf{F}}
    = \int_{\partial\Omega} \omega_{\mathbf{F}}
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]

\item[(2)]
  Note that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  is also true in $\mathbb{R}^2$.
  Similar to (b), two Green's identities are also true in $\mathbb{R}^2$.
  (In $\mathbb{R}^1$, the Green's first identity is
  the integration by parts (Theorem 6.22).)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% http://ms.mcmaster.ca/gabardo/moebius.pdf



\textbf{Exercise 10.32 (M\"obius band).}
\emph{Fix $\delta$, $0 < \delta < 1$.
Let $D$ be the set of all $(\theta,t) \in \mathbb{R}^2$ such that $0 \leq \theta \leq \pi$,
$-\delta \leq t \leq \delta$.
Let $\Phi$ be the $2$-surface in $\mathbb{R}^3$, with parameter domain $D$, given by
\begin{align*}
  x &= (1-t\sin\theta) \cos(2\theta) \\
  y &= (1-t\sin\theta) \sin(2\theta) \\
  z &= t \cos\theta
\end{align*}
where $(x,y,z) = \Phi(\theta,t)$.
Note that $\Phi(\pi,t) = \Phi(0,-t)$, and that $\Phi$ is one-to-one on the rest of $D$.} \\

\emph{The range $M = \Phi(D)$ of $\Phi$ is known as a \textbf{M\"obius band}.
It is the simplest example of a nonorientable surface.} \\

\emph{Prove the various assertions made in the following description:
Put
$\mathbf{p}_1 = (0,-\delta)$,
$\mathbf{p}_2 = (\pi,-\delta)$,
$\mathbf{p}_3 = (\pi,\delta)$,
$\mathbf{p}_4 = (0,\delta)$,
$\mathbf{p}_5 = \mathbf{p}_1$.
Put $\gamma_i = [\mathbf{p}_i,\mathbf{p}_{i+1}]$, $i=1,\ldots,4$,
and put $\Gamma_i = \Phi \circ \gamma_i$.
Then
\[
  \partial \Phi = \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4.
\]
Put $\mathbf{a} = (1,0,-\delta)$, $\mathbf{b} = (1,0,\delta)$.
Then
\[
  \Phi(\mathbf{p}_1) = \Phi(\mathbf{p}_3) = \mathbf{a},
  \qquad
  \Phi(\mathbf{p}_2) = \Phi(\mathbf{p}_4) = \mathbf{b},
\]
and $\partial \Phi$ can be described as follows.}
\begin{enumerate}
\item[(1)]
  \emph{$\Gamma_1$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $+1$ around the origin.
  (See Exercise 8.23.)}

\item[(2)]
  \emph{$\Gamma_2 = [\mathbf{b}, \mathbf{a}]$.}

\item[(3)]
  \emph{$\Gamma_3$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $-1$ around the origin.}

\item[(4)]
  \emph{$\Gamma_4 = [\mathbf{b}, \mathbf{a}]$.}
\end{enumerate}
\emph{Thus $\partial \Phi = \Gamma_1 + \Gamma_3 + 2 \Gamma_2.$} \\

\emph{If we go from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_1$
and continue along the ``edge'' of $M$ until we return to $\mathbf{a}$,
the curve traced out is
\[
  \Gamma = \Gamma_1 - \Gamma_3,
\]
which may also be represented on the parameter interval $[0,2\pi]$ by the equations
\begin{align*}
  x &= (1+\delta\sin\theta) \cos(2\theta) \\
  y &= (1+\delta\sin\theta) \sin(2\theta) \\
  z &= -\delta\cos\theta.
\end{align*}
It should be emphasized that $\Gamma \neq \partial \Phi$:
Let $\eta = \frac{xdy-ydx}{x^2+y^2}$
be the $1$-form discussed in Exercise 10.21 and Exercise 10.22.
Since $d\eta = 0$, Stokes' theorem shows that
\[
  \int_{\partial \Phi} \eta = 0.
\]
But although $\Gamma$ is the ``geometric'' boundary of $M$, we have
\[
  \int_{\Gamma} \eta = 4 \pi.
\]
In order to avoid this possible source of confusion,
Stokes' formula (Theorem 10.50) is frequently stated only for orientable surfaces $\Phi$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\partial \Phi = \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4$.}
  \begin{align*}
    \partial \Phi
    &= \Phi \circ (\partial D) \\
    &= \Phi \circ (\gamma_1 + \gamma_2 + \gamma_3 + \gamma_4) \\
    &= \Phi \circ \gamma_1 + \Phi \circ \gamma_2 + \Phi \circ \gamma_3 + \Phi \circ \gamma_4 \\
    &= \Gamma_1 + \Gamma_2 + \Gamma_3 + \Gamma_4.
  \end{align*}

\item[(2)]
  It is trivial that
  $\Phi(\mathbf{p}_1) = \Phi(\mathbf{p}_3) = \mathbf{a} = (1,0,-\delta)$
  and
  $\Phi(\mathbf{p}_2) = \Phi(\mathbf{p}_4) = \mathbf{b} = (1,0,\delta)$
  by the definition of $\Phi$.

\item[(3)]
  \emph{Show that $\Gamma_1$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $+1$ around the origin.}
  By definition,
  $\Gamma_1 = \Phi \circ \gamma_1 = \Phi([\mathbf{p}_1, \mathbf{p}_2])$.
  That is,
  $\Gamma_1$ spirals up from $\Phi(\mathbf{p}_1) = \mathbf{a}$
  to $\Phi(\mathbf{p}_2) = \mathbf{b}$.
  Besides, the projection $P_{\Gamma_1}$ of $\Gamma_1$ into the $(x,y)$-plane ($z = 0$)
  can be parameterized as
  \begin{align*}
    x &= \left(1+\delta\sin\frac{t}{2}\right) \cos t \\
    y &= \left(1+\delta\sin\frac{t}{2}\right) \sin t
  \end{align*}
  for $0 \leq t \leq 2\pi$.
  Note that $P_{\Gamma_1}$ satisfies the condition in Exercise 10.21(b).
  Hence $\int_{P_{\Gamma_1}} \eta = 2\pi$.
  (Here $\eta$ is well-defined.)
  Apply Exercise 10.21(f) to get
  \[
    \mathrm{Ind}(P_{\Gamma_1})
    = \frac{1}{2\pi} \int_{P_{\Gamma_1}} \eta
    = \frac{1}{2\pi} \cdot 2\pi
    = 1.
  \]

\item[(4)]
  \emph{Show that $\Gamma_2 = [\mathbf{b}, \mathbf{a}]$.}
  By definition,
  $\Gamma_2 = \Phi \circ \gamma_2 = \Phi([\mathbf{p}_2, \mathbf{p}_3])$
  is $[\mathbf{b}, \mathbf{a}]$ exactly.

\item[(5)]
  \emph{Show that $\Gamma_3$ spirals up from $\mathbf{a}$ to $\mathbf{b}$;
  its projection into the $(x,y)$-plane has winding number $-1$ around the origin.}
  Similar to (3),
  $\Gamma_3$ spirals up from $\Phi(\mathbf{p}_3) = \mathbf{a}$
  to $\Phi(\mathbf{p}_4) = \mathbf{b}$.
  Now we consider $-\Gamma_3$ instead of $\Gamma_3$.
  The projection $P_{-\Gamma_3}$ of $-\Gamma_3$ into the $(x,y)$-plane ($z = 0$)
  can be parameterized as
  \begin{align*}
    x &= \left(1-\delta\sin\frac{t}{2}\right) \cos t \\
    y &= \left(1-\delta\sin\frac{t}{2}\right) \sin t
  \end{align*}
  for $0 \leq t \leq 2\pi$.
  Similar to (3), $\mathrm{Ind}(P_{-\Gamma_3}) = 1$. Therefore,
  \[
    \mathrm{Ind}(P_{\Gamma_3})
    = -\mathrm{Ind}(-P_{\Gamma_3})
    = -\mathrm{Ind}(P_{-\Gamma_3})
    = -1.
  \]

\item[(6)]
  \emph{Show that $\Gamma_4 = [\mathbf{b}, \mathbf{a}]$.}
  Similar to (4).

\item[(7)]
  \emph{Show that $\Gamma = \Gamma_1 - \Gamma_3$
  is the trace of from $\mathbf{a}$ to $\mathbf{b}$ along $\Gamma_1$
  and continue along the ``edge'' of $M$ until we return to $\mathbf{a}$.}
  By definition, $\Gamma$ can be parameterized as
  \begin{align*}
    x &= (1+\delta\sin t) \cos(2t) \\
    y &= (1+\delta\sin t) \sin(2t) \\
    z &= -\delta \cos t
  \end{align*}
  for $t \in [0,2\pi]$.
  Thus, $\Gamma$ is $\Gamma_1$ if $t \in [0,\pi]$
  and $\Gamma$ is $-\Gamma_3$ if $t \in [\pi,2\pi]$ by (3)(5).
  So $\Gamma = \Gamma_1 - \Gamma_3$.

\item[(8)]
  \emph{Show that $\int_{\partial \Phi} \eta = 0$.}
  Note that $\eta$ is well-defined since $M$ does not intersect the $z$-axis.
  So the Stokes' theorem (Theorem 10.33) and $d\eta = 0$ on $M$ implies that
  \[
    \int_{\partial \Phi} \eta
    = \int_{\Phi} d\eta
    = 0.
  \]

\item[(9)]
  \emph{Show that $\int_{\Gamma} \eta = 4 \pi$.}
  \begin{align*}
    \int_{\Gamma} \eta
    &= \int_{\Gamma} \frac{xdy-ydx}{x^2+y^2} \\
    &= \int_{0}^{2\pi} \frac{x(t)y'(t) - y(t)x'(t)}{x(t)^2+y(t)^2} dt
      &((7)) \\
    &= \int_{0}^{2\pi} 2 \: dt \\
    &= 4 \pi.
  \end{align*}
  (So the winding number of $\Gamma$ around of $\mathbf{0}$ is $2$.)

\item[(10)]
  By (8)(9), $\Gamma \neq \partial \Phi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}