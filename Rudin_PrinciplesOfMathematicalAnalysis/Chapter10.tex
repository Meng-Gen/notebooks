\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt

\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\begin{document}

\textbf{\Large Chapter 10: Integration of Differential Forms} \\\\



\emph{Author: Meng-Gen Tsai} \\
\emph{Email: plover@gmail.com} \\\\



% http://pages.cs.wisc.edu/~wentaowu/other-docs/POMA_Solution_Sheet.pdf
% https://linearalgebras.com/baby-rudin-chapter-10.html
% https://www.researchgate.net/publication/248817777_Partitions_of_Unity_for_Countable_Covers
% https://www.math.lsu.edu/~lawson/Chapter4.pdf
% https://www2.math.upenn.edu/~ryrogers/HW7-solutions.pdf



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.1.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.2.}
\emph{For $i=1,2,3,\ldots$, let $\varphi_i \in \mathscr{C}(\mathbb{R}^1)$ have support
in $(2^{-i},2^{1-i})$, such that $\int \varphi_i = 1$.
Put
\[
  f(x,y) = \sum_{i=1}^{\infty}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
\]
Then $f$ has compact support in $\mathbb{R}^2$,
$f$ is continuous except at $(0,0)$,
and
\[
  \int dy \int f(x,y) dx = 0
  \qquad
  \text{ but }
  \qquad
  \int dx \int f(x,y) dy = 1.
\]
Observe that $f$ is unbounded in every neighborhood of $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.3.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.4.}
\emph{For $(x,y) \in \mathbb{R}^2$, define
\[
  \mathbf{F}(x,y) = (e^x \cos y - 1, e^x \sin y)
\]
Prove that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$, where
\begin{align*}
  \mathbf{G}_1(x,y) &= (e^x \cos y - 1, y) \\
  \mathbf{G}_2(u,v) &= (u, (1+u) \tan v)
\end{align*}
are primitive in some neighborhood of $(0,0)$.
Compute the Jacobians of $\mathbf{G}_1$, $\mathbf{G}_2$, $\mathbf{F}$ at $(0,0)$.
Define
\[
  \mathbf{H}_2(x,y) = (x, e^x \sin y)
\]
and find
\[
  \mathbf{H}_1(u,v) = (h(u,v),v)
\]
so that $\mathbf{F} = \mathbf{H}_1 \circ \mathbf{H}_2$
is in some neighborhood of $(0,0)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Definition 10.5,
  \begin{align*}
    \mathbf{G}_1(x,y) &= (e^x \cos y - 1) \mathbf{e}_1 + y \mathbf{e}_2, \\
    \mathbf{G}_2(u,v) &= u \mathbf{e}_1 + ((1+u) \tan v) \mathbf{e}_2
  \end{align*}
  are primitive in some neighborhood of $(0,0)$.

\item[(2)]
  \emph{Show that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$.}
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{G}_2 \circ \mathbf{G}_1)(x,y)
    &= \mathbf{G}_2(\mathbf{G}_1(x,y)) \\
    &= \mathbf{G}_2(e^x \cos y - 1, y) \\
    &= (e^x \cos y - 1, (1+(e^x \cos y - 1)) \tan y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\item[(3)]
  Since
  \begin{align*}
    J_{\mathbf{G}_1}(x,y)
    &=
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      0 & 1
    \end{bmatrix} \\
    J_{\mathbf{G}_2}(x,y)
    &=
    \begin{bmatrix}
      1 & 0 \\
      \tan y & (1+x)\sec^2 y
    \end{bmatrix} \\
    J_{\mathbf{F}}(x,y)
    &=
    \begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y & e^x \cos y
    \end{bmatrix},
  \end{align*}
  \begin{align*}
    J_{\mathbf{G}_1}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix} \\
     J_{\mathbf{G}_2}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix} \\
     J_{\mathbf{F}}(0,0)
    &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}.
  \end{align*}

\item[(4)]
  Define $h(u,v) = \sqrt{e^{2u} - v^{2}} - 1$ on
  \[
    B\left((0,0);\frac{1}{64}\right) \subseteq \mathbb{R}^2.
  \]
  $h(u,v)$ is well-defined since $e^{2u}-v^2 > 0$
  for all $(u,v) \in B\left((0,0);\frac{1}{64}\right)$.

\item[(5)]
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{H}_1 \circ \mathbf{H}_2)(x,y)
    &= \mathbf{H}_1(\mathbf{H}_2(x,y)) \\
    &= \mathbf{H}_1(x, e^x \sin y) \\
    &= (\sqrt{e^{2x} - (e^x \sin y)^2} - 1, e^x \sin y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.5.}
\emph{Formulate and prove an analogue of Theorem 10.8,
in which $K$ is a compact subset of an arbitrary metric space.
(Replace the functions $\varphi_i$ that occur in the proof of Theorem 10.8
by functions of the type constructed in Exercise 4.22.)} \\

\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  \emph{(Partitions of unity.)
  Suppose $K$ is a compact subset of a metric space $X$,
  and $\{V_{\alpha}\}$ is an open cover of $K$.
  Then there exist functions $\psi_1, \ldots, \psi_s \in \mathscr{C}(X)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_i \leq 1$ for $1 \leq i \leq s$.}

  \item[(b)]
    \emph{each $\psi_i$ has its support in some $V_{\alpha}$, and}

  \item[(c)]
    \emph{$\psi_1(x) + \cdots + \psi_s(x) = 1$ for every $x \in K$.}
  \end{enumerate}

\item[(2)]
  It is trivial that some $V_{\alpha} = X$
  by taking $s = 1$ and $\psi_1(x) = 1 \in \mathscr{C}(X)$.
  Now we assume that all $V_{\alpha} \subsetneq X$.

\item[(3)]
  Associate with each $x \in K$ an index $\alpha(x)$ so that $x \in V_{\alpha(x)}$.
  Then there are open balls $B(x)$ and $W(x)$, centered at $x$,
  with
  \[
    x
    \in B(x)
    \subseteq \overline{B(x)}
    \subseteq W(x)
    \subseteq \overline{W(x)}
    \subseteq V_{\alpha(x)}
  \]
  (Since $V_{\alpha(x)}$ is open, there exists $r > 0$
  such that $B(x;r) \subseteq V_{\alpha(x)}$.
  Take $B(x) = B\left(x;\frac{r}{89}\right)$
  and $W(x) = B\left(x;\frac{r}{64}\right)$.)

\item[(4)]
  Since $K$ is compact, there are finitely many points
  $x_1, \ldots, x_s \in K$ such that
  \[
    K \subseteq B(x_1) \cup \cdots \cup B(x_s).
  \]
  Note that
  \begin{enumerate}
  \item[(a)]
    $\overline{B(x_i)}$ is a nonempty closed set since $x_i \in B(x_i) \subseteq \overline{B(x_i)}$.

  \item[(b)]
    $X - W(x_i) \supseteq X - V_{\alpha(x_i)}$ is a nonempty closed set by the assumption in (2).

  \item[(c)]
    $\overline{B(x_i)} \cap (X - W(x_i)) \subseteq W(x_i) \cap (X - W(x_i)) = \varnothing$.

  \end{enumerate}
  By Exercise 4.22, there is a function
  \[
    \varphi_i(x)
    = \frac{\rho_{\overline{B(x_i)}}(x)}{\rho_{\overline{B(x_i)}}(x) + \rho_{X-W(x_i)}(x)}
    \in \mathscr{C}(X)
  \]
  such that $\varphi_i(x) = 1$ on $\overline{B(x_i)}$,
  $\varphi_i(x) = 0$ outside $W(x_i)$, and $0 \leq \varphi_i(x) \leq 1$ on $X$
  for $1 \leq i \leq s$.

\item[(5)]
  Define $\psi_{1} = \varphi_{1} \in \mathscr{C}(X)$ and
  \[
    \psi_{i+1} = (1-\varphi_{1}) \cdots (1-\varphi_{i})\varphi_{i+1} \in \mathscr{C}(X)
  \]
  for $1 \leq i \leq s-1$.
  Properties (a) and (b) in (1) are clear.
  Also,
  \[
    \psi_1(x) + \cdots + \psi_s(x) = 1 - (1-\varphi_1(x)) \cdots (1-\varphi_s(x))
  \]
  by the construction of $\psi_i$.
  If $x \in K$, then $x \in B(x_i)$ for some $i$, hence $\varphi_i(x)=1$,
  and the product $(1-\varphi_1(x)) \cdots (1-\varphi_s(x)) = 0$.
  This proves property (c) in (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.6.}
\emph{Strengthen the conclusion of Theorem 10.8 by showing that
the functions $\psi_i$ can be made differentiable, and even infinitely differentiable.
(Use Exercise 8.1 in the construction of the auxiliary functions $\psi_i$.)} \\



\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  It is trivial that some $V_{\alpha} = \mathbb{R}^n$
  by taking $s = 1$ and $\psi_1(\mathbf{x}) = 1 \in \mathscr{C}^{\infty}(\mathbb{R}^n)$.
  Now we assume that all $V_{\alpha} \subsetneq \mathbb{R}^n$.

\item[(2)]
  Associate with each $\mathbf{x} \in K$ an index $\alpha(x)$ so that $\mathbf{x} \in V_{\alpha(x)}$.
  Then there are open $n$-cells $B(\mathbf{x})$ and $W(\mathbf{x})$ (Definition 10.1),
  centered at $\mathbf{x}$,
  with
  \[
    \mathbf{x}
    \in B(\mathbf{x})
    \subseteq \overline{B(\mathbf{x})}
    \subseteq W(\mathbf{x})
    \subseteq \overline{W(\mathbf{x})}
    \subseteq V_{\alpha(\mathbf{x})}
  \]
  (Since $V_{\alpha(\mathbf{x})}$ is open, there exists $r > 0$
  such that $B(\mathbf{x};r) \subseteq V_{\alpha(\mathbf{x})}$.
  Take
  \[
    B(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{89\sqrt{n}}\right),
    \qquad
    W(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{64\sqrt{n}}\right)
  \]
  where $I(\mathbf{p};r)$ is the open $n$-cell centered at $\mathbf{p} = (p_1,\ldots,p_n)$
  defined by
  \[
    I(\mathbf{p};r)
    = (p_1-r,p_1+r) \times \cdots \times (p_n-r,p_n+r) \subseteq \mathbb{R}^n.)
  \]

\item[(3)]
  Define
  \begin{equation*}
    f(y) =
    \begin{cases}
      e^{-\frac{1}{y^2}} & (y > 0), \\
      0 & (y \leq 0).
    \end{cases}
  \end{equation*}
  $f(y) \in \mathscr{C}^{\infty}(\mathbb{R}^1)$
  by applying the similar argument in Exercise 8.1.

\item[(4)]
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in K$
  and construct $B(\mathbf{x})$ and $W(\mathbf{x})$ as in (2).
  Define
  \[
    g_{x_j}(y_j)
    = \frac{f(y_j)}{f(y_j)+f\left(\frac{r}{64\sqrt{n}}-\frac{r}{89\sqrt{n}}-y_j\right)}
  \]
  for $1 \leq j \leq n$.
  $g_{x_j}$ is well-defined and $g_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    g_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq 0, \\
      \text{strictly increasing}
        & \text{if } 0 \leq y_j \leq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } y_j \geq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Next, define
  \[
    h_{x_j}(y_j)
    = g_{x_j}\left(y_j-x_j+\frac{r}{64\sqrt{n}}\right)
      g_{x_j}\left(x_j+\frac{r}{64\sqrt{n}}-y_j\right)
  \]
  for $1 \leq j \leq n$.
  $h_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    h_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq x_j - \frac{r}{64\sqrt{n}}, \\
      \text{strictly increasing}
        & \text{if } x_j - \frac{r}{64\sqrt{n}} \leq y_j \leq x_j - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } x_j - \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{89\sqrt{n}}, \\
      \text{strictly decreasing}
        & \text{if } x_j + \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{64\sqrt{n}}, \\
      0
        & \text{if } y_j \geq x_j + \frac{r}{64\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Finally we define $\mathbf{h}_{\mathbf{x}}: \mathbb{R}^n \to \mathbb{R}^1$ by
  \[
    \mathbf{h}_{\mathbf{x}}(\mathbf{y})
    = \prod_{j=1}^{n} h_{x_j}(y_j)
  \]
  where $\mathbf{y} = (y_1,\ldots,y_n) \in \mathbb{R}^n$.
  Hence, $\mathbf{h}_{\mathbf{x}} \in \mathscr{C}^{\infty}(\mathbb{R}^n)$ (Theorem 9.21).
  Also, $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 1$ on $\overline{B(\mathbf{x})}$,
  $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 0$ outside $W(\mathbf{x})$,
  and $0 \leq \mathbf{h}_{\mathbf{x}}(\mathbf{y}) \leq 1$.

\item[(5)]
  Since $K$ is compact, there are finitely many points
  $\mathbf{x}_1, \ldots, \mathbf{x}_s \in K$ such that
  \[
    K \subseteq B(\mathbf{x}_1) \cup \cdots \cup B(\mathbf{x}_s).
  \]
  Take
  \[
    \varphi_i(\mathbf{x})
    = \mathbf{h}_{\mathbf{x}_i}(\mathbf{x})
    \in \mathscr{C}^{\infty}(\mathbb{R}^n)
  \]
  for $1 \leq i \leq s$.

\item[(6)]
  The rest are the same as the proof of Theorem 10.8 or Exercise 10.5.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.7.}
\begin{enumerate}
\item[(a)]
  \emph{Show that the simplex $Q^k$ is the smallest convex subset of $\mathbb{R}^k$
  such that contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}

\item[(b)]
  \emph{Show that affine mappings take convex sets to convex sets.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $Q^k$ contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}
  Recall
  \[
    Q^k = \{ (x_1,\ldots,x_k) \in \mathbb{R}^k :
      x_1 + \cdots + x_k \leq 1 \text{ and }
      x_1, \ldots, x_k \geq 0 \}
  \]
  (Example 10.14).
  Hence $\mathbf{0} = (0,\ldots,0) \in Q^k$ and
  \[
    \mathbf{e}_i = (0,\ldots,\underbrace{1}_{\text{$i$th coordinate}},\ldots,0) \in Q^k.
  \]

\item[(2)]
  \emph{Show that $Q^k$ is a convex subset of $\mathbb{R}^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$,
  $\mathbf{y} = (y_1,\ldots,y_k) \in Q^k$ and $0 < \lambda < 1$.
  Hence
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda x_1 + (1-\lambda)y_1, \ldots, \lambda x_k + (1-\lambda)y_k) \in Q^k
  \]
  since each $\lambda x_i + (1-\lambda)y_i \geq 0$
  and
  \[
    \sum_{i=1}^{k} (\lambda x_i + (1-\lambda)y_i)
    = \lambda \sum_{i=1}^{k} x_i + (1-\lambda) \sum_{i=1}^{k} y_i
    \leq \lambda + (1-\lambda)
    = 1.
  \]

\item[(3)]
  \emph{Given any convex set $E \subseteq \mathbb{R}^k$ containing
  $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.
  Show that $E \supseteq Q^k$.}
  \begin{enumerate}
  \item[(a)]
    Induction on $k$.
    Base case: $k = 1$. Given any $\mathbf{x} = (x_1) \in Q^1$.
    We have $0 \leq x_1 \leq 1$ by the definition of $Q^1$.
    So that $\mathbf{x} = x_1 \mathbf{e}_1 + (1-x_1) \mathbf{0} \in E$
    since $\mathbf{0}, \mathbf{e}_1 \in E$ and $E$ is convex.

  \item[(b)]
    Inductive step: suppose the statement holds for $k = n$.
    Given any $\mathbf{x} = (x_1,\ldots,x_n,x_{n+1}) \in Q^{n+1}$.
    If $x_{n+1} = 1$, then $x_1 = \cdots = x_n = 0$ by the definition of $Q^{n+1}$.
    So $\mathbf{x} = \mathbf{e}_{n+1} \in E$ by the assumption of $E$.
    If $0 \leq x_{n+1} < 1$, then $x_1 + \cdots + x_n \leq 1 - x_{n+1}$ or
    \[
      \frac{x_1}{1-x_{n+1}} + \cdots + \frac{x_{n}}{1-x_{n+1}} \leq 1.
    \]
    So the point
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}} \right)
      \in Q^n,
    \]
    or
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}}, 0 \right),
      \text{ say } \widehat{\mathbf{x}}, \in E
    \]
    by the induction hypothesis.
    Note that $\mathbf{e}_{n+1} \in E$.
    Hence
    \[
      \mathbf{x}
      = x_{n+1} \mathbf{e}_{n+1} + (1-x_{n+1})\widehat{\mathbf{x}}
      \in E
    \]
    by the convexity of $E$.

  \item[(c)]
    Conclusion: Since both the base case and the inductive step have been proved as true,
    by mathematical induction the statement holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{f}$ be an affine mapping that carries a vector space $X$ into a vector space $Y$
  such that
  \[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{0}) + A\mathbf{x}
  \]
  for some $A \in L(X,Y)$.

\item[(2)]
  Given any convex subset $C$ of $X$.
  To show that $\mathbf{f}(C)$ is convex, it suffices to show that
  \[
    \lambda \mathbf{y}_1 + (1-\lambda) \mathbf{y}_2 \in \mathbf{f}(C)
  \]
  for any $\mathbf{y}_1, \mathbf{y}_2 \in \mathbf{f}(C)$ and $0 < \lambda < 1$.
  Write $\mathbf{y}_1 = \mathbf{f}(\mathbf{x}_1)$,
  $\mathbf{y}_2 = \mathbf{f}(\mathbf{x}_2)$ for some $\mathbf{x}_1, \mathbf{x}_2 \in C$.
  Note that $\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2 \in C$ by the convexity of $C$.
  Hence
  \begin{align*}
    &\mathbf{f}(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + A(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + \lambda A\mathbf{x}_1 + (1-\lambda) A\mathbf{x}_2
      &(A \in L(X,Y)) \\
    =& \lambda(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_1)
      + (1-\lambda)(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_2) \\
    =& \lambda \mathbf{f}(\mathbf{x}_1) + (1-\lambda)\mathbf{f}(\mathbf{x}_2) \\
    =& \lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 \in \mathbf{f}(C).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.8.}
\emph{Let $H$ be the parallelogram in $\mathbb{R}^2$ whose vertices are
$(1,1)$, $(3,2)$, $(4,5)$, $(2,4)$.
Find the affine map $T$ which sends
$(0,0)$ to $(1,1)$, $(1,0)$ to $(3,2)$, $(1,1)$ to $(4,5)$, $(0,1)$ to $(2,4)$.
Show that $J_{T} = 5$.
Use $T$ to convert the integral
\[
  \alpha = \int_{H} e^{x-y} dxdy
\]
to an integral over $I^2$ and thus compute $\alpha$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Affine simplexes 10.26,
  \[
    T(\mathbf{x}) = T(\mathbf{0}) + A\mathbf{x},
  \]
  where $A \in L(\mathbb{R}^2, \mathbb{R}^2)$, say
  $A = \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}$.
  Note that $T:
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} \mapsto
  \begin{bmatrix}
    1 \\
    1
  \end{bmatrix}$.
  Thus
  \[
    T:
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix} \mapsto
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
    +
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix}
    =
    \begin{bmatrix}
      1+ax+by \\
      1+cx+dy
    \end{bmatrix}.
  \]

\item[(2)]
  By $T: (1,0) \mapsto (3,2)$ and $T: (0,1) \mapsto (2,4)$,
  we can solve $A$ as
  \[
    A = \begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}.
  \]
  It is easy to verify such
  \[
    T:
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    \mapsto
    \underbrace{\begin{bmatrix}
      1 \\
      1
    \end{bmatrix}}_{T(\mathbf{0})}
    +
    \underbrace{\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    =
    \begin{bmatrix}
      1+2x+y \\
      1+x+3y
    \end{bmatrix}
  \]
  satisfying our requirement.

\item[(3)]
  \[
    J_T
    =
    \det\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}
    = 5.
  \]

\item[(4)]
  \begin{align*}
    \int_{H} e^{x-y} dxdy
    &= \int_{[0,1]^2} e^{(1+2u+v)-(1+u+3v)} \abs{J_T} du dv \\
    &= 5 \int_{[0,1]^2} e^{u-2v} du dv \\
    &= 5 \left\{ \int_{0}^{1} e^u du \right\}\left\{ \int_{0}^{1} e^{-2v} dv \right\}
      &(\text{Theorem 10.2}) \\
    &= \frac{5}{2}(e-1)(1-e^{-2}).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.9.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.10.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.11.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.12.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.13.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.14 (Levi-Civita symbol).}
\emph{Prove $\varepsilon(j_1, \ldots, j_k) = s(j_1, \ldots, j_k)$,
where}
\[
  s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p).
\] \\

It is usually to define the Levi-Civita symbol by
\begin{equation*}
\varepsilon(j_1, \ldots, j_k) =
  \begin{cases}
    1
      & \text{ if $(j_1,\cdots,j_k)$ is an even permutation of $J$}, \\
    -1
      & \text{ if $(j_1,\cdots,j_k)$ is an odd permutation of $J$}, \\
    0
      & \text{otherwise}
  \end{cases}
\end{equation*}
(Basic $k$-forms 10.14).
Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
So $\varepsilon(j_1, \ldots, j_k)$ is equivalent to an explicit expression
$s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p)$. \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Induction on $k$.
  Base case: \emph{Show that $\varepsilon(j_1,j_2) = s(j_1,j_2)$.}
  Since
  \begin{equation*}
  \varepsilon(j_1,j_2) =
    \begin{cases}
      1
        & \text{ if $j_1 < j_2$} \\
      -1
        & \text{ if $j_1 > j_2$},
    \end{cases}
  \end{equation*}
  $\varepsilon(j_1,j_2) = \mathrm{sgn}(j_2-j_1) = s(j_1,j_2)$.

\item[(2)]
  Inductive step: \emph{Show that for any $s \geq 2$,
  if $\varepsilon(j_1, \ldots, j_{s}) = s(j_1, \ldots, j_{s})$ holds,
  then $\varepsilon(j_1, \ldots, j_{s+1}) = s(j_1, \ldots, j_{s+1})$ also holds.}
  \begin{align*}
    \varepsilon(j_1, \ldots, j_{s+1})
    &= \varepsilon(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= s(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s} \mathrm{sgn}(j_q-j_p)
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s+1} \mathrm{sgn}(j_q - j_p) \\
    &= s(j_1, \ldots, j_{s+1}).
  \end{align*}

\item[(3)]
  Conclusion: Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement holds for every integer $k \geq 2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.15.}
\emph{If $\omega$ and $\lambda$ are $k$- and $m$-forms, respectively,
prove that}
\[
  \omega \wedge \lambda = (-1)^{km} \lambda \wedge \omega.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \omega = \sum_I b_I(\mathbf{x}) dx_I,
    \qquad
    \lambda = \sum_J c_J(\mathbf{x}) dx_J
  \]
  in the stardard presentations,
  where $I$ and $J$ range over all increasing $k$-indices
  and over all increasing $m$-indices taken from the set $\{1,\ldots,n\}$.

\item[(2)]
  \emph{Show that $dx_I \wedge dx_J = (-1)^{km} dx_J \wedge dx_I$.}
  \begin{align*}
    dx_I \wedge dx_J
    &= dx_{i_1} \wedge \cdots \wedge dx_{i_k}
      \wedge dx_J \\
    &= (-1)^m dx_{i_1} \wedge \cdots \wedge dx_{i_{k-1}}
      \wedge dx_J \wedge dx_{i_{k}} \\
    &= (-1)^{2m} dx_{i_1} \wedge \cdots \wedge dx_{i_{k-2}}
      \wedge dx_J \wedge dx_{i_{k-1}} \wedge dx_{i_{k}} \\
    &\cdots \\
    &= (-1)^{km} dx_J
      \wedge dx_{i_1} \wedge \cdots \wedge dx_{i_k} \\
    &= (-1)^{km} dx_J \wedge dx_I.
  \end{align*}

\item[(3)]
  \begin{align*}
    \omega \wedge \lambda
    &= \sum_{I,J} b_I(\mathbf{x}) c_J(\mathbf{x}) dx_I \wedge dx_J \\
    &= (-1)^{km} \sum_{J,I} c_J(\mathbf{x}) b_I(\mathbf{x}) dx_J \wedge dx_I \\
    &= (-1)^{km} \lambda \wedge \omega.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.16.}
\emph{If $k \geq 2$ and $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$
is an oriented affine $k$-simplex, prove that $\partial^2 \sigma = 0$,
directly from the definition of the boundary operator $\partial$.
Deduce from this that $\partial^2 \Psi = 0$ for every chain $\Psi$.
(Hint: For orientation, do it first for $k=2$, $k=3$.
In general, if $i < j$, let $\sigma_{ij}$ be the $(k-2)$-simplex obtained by
deleting $\mathbf{p}_i$ and $\mathbf{p}_j$ from $\sigma$.
Show that each $\sigma_{ij}$ occurs twice in $\partial^2\sigma$, with opposite sign.)} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Write the boundary of the oriented affine $k$-simplex
  $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$ as
  \[
    \partial \sigma
    = \sum_{i=0}^{k}(-1)^i
    [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]
  \]
  where where the oriented $(k-1)$-simplex
  $[\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]$
  is obtained by deleting $\sigma$'s $i$-th vertex (Boundaries 10.29).

\item[(2)]
  \begin{align*}
    \partial^2 \sigma
    =& \partial \left( \sum_{i}(-1)^{i}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \right) \\
    =& \sum_{i}(-1)^{i}
      \partial [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i}(-1)^{j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &+ \sum_{j>i} (-1)^{i}(-1)^{j-1}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &- \sum_{j>i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k].
  \end{align*}
  The latter two summations cancel since after switching $i$ and $j$ in the second sum.
  Therefore $\partial^2 \sigma = 0$.

\item[(3)]
  The boundary of a chain is the linear combination of boundaries of the simplices in the chain.
  Write $\Psi = \sum_{i=1}^{r} \sigma_i$. where $\sigma_i$ is an oriented affine simplex.
  Then
  \[
    \partial^2 \Psi
    = \partial \left(\partial \sum \sigma_i \right)
    = \partial \left( \sum \partial\sigma_i \right)
    = \sum \partial^2 \sigma_i
    = \sum 0
    = 0
  \]
  for any affine chain $\Psi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.17.}
\emph{Put $J^2 = \tau_1 + \tau_2$, where
\[
  \tau_1 = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2],
  \qquad
  \tau_2 = -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1].
\]
Explain why it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.
Show that $\partial J^2$ is the sum of $4$ oriented affine $1$-simplexes.
Find these.
What is $\partial(\tau_1 - \tau_2)$?} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that the unit square $I^2 \in \mathbb{R}^2$ is the union of
  $\tau_1(Q^2)$ and $\tau_2(Q_2)$, where
  \begin{align*}
    \tau_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2) \\
    &= \mathbf{0} + (\alpha_1+\alpha_2) \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 1 \\
        0 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  and
  \begin{align*}
    \tau_2(\mathbf{u})
    &= (-[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1])(\mathbf{u}) \\
    &= ([\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 (\mathbf{e}_1+\mathbf{e}_2) + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + (\alpha_1+\alpha_2) \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 0 \\
        1 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  where $\mathbf{u} = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \in \mathbb{R}^2$
  (as in Equation (78)).
  Both $\tau_1$ and $\tau_2$ have Jacobian $1 > 0$, or positively oriented
  (Affine simplexes 10.26).
  So it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.

\item[(2)]
  \begin{align*}
    \partial \tau_1
    &= [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_1], \\
    \partial \tau_2
    &= [\mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1] \\
    &= [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}]
      + [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2].
  \end{align*}

\item[(3)]
  By (2),
  \[
    \partial J^2
    = \partial \tau_1 + \partial \tau_2
    = [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}],
  \]
  which is the positively oriented boundary of $I^2$.

\item[(4)]
  By (2),
  \begin{align*}
    \partial(\tau_1 - \tau_2)
    =& \partial \tau_1 - \partial \tau_2 \\
    =& [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}] \\
      &+ [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}].
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.18.}
\emph{Consider the oriented affine $3$-simplex
\[
  \sigma_1
  = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3]
\]
in $\mathbb{R}^3$.
Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.
Thus $\sigma_1$ is positively oriented.} \\

\emph{Let $\sigma_2, \ldots, \sigma_6$ be five other oriented $3$-simplexes,
obtained as follows:
There are five permutations $(i_1, i_2, i_3)$ of $(1, 2, 3)$,
distinct from $(1, 2, 3)$.
Associate with each $(i_1, i_2, i_3)$ the simplex
\[
  s(i_1, i_2, i_3)
  [
    \mathbf{0},
    \mathbf{e}_{i_1},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
  ]
\]
where $s$ is the sign that occurs in the definition of the determinant.
(This is how $\tau_2$ was obtained from $\tau_1$ in Exercise 10.17.)
Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.} \\

\emph{Put $J^3 = \sigma_1+\cdots+\sigma_6$.
Then $J^3$ may be called the positively oriented unit cube in $\mathbb{R}^3$.
Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.
(These $12$ triangles cover the surface of the unit cube $I^3$.)} \\

\emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.} \\

\emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
and that their union covers $I^3$.
(Compared with Exercise 10.13; note that $3!=6$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.}
  Given any $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$,
  we have
  \begin{align*}
    \sigma_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3])
    (\mathbf{u}) \\
    &= \mathbf{0}
      + \alpha_1 \mathbf{e}_1
      + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2)
      + \alpha_3 (\mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3) \\
    &= \mathbf{0}
      + (\alpha_1+\alpha_2+\alpha_3) \mathbf{e}_1
      + (\alpha_2+\alpha_3) \mathbf{e}_2
      + \alpha_3 \mathbf{e}_3 \\
    &= \mathbf{0} +
      \underbrace{\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
      \end{bmatrix}}_{\text{say }A}
      \mathbf{u}.
  \end{align*}
  So
  \[
    \det(A)
    =
    \det
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    = 1.
  \]

\item[(2)]
  \emph{Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.}
  Define the permutation matrix $P_{(i_1,i_2,i_3)}$ corresponding to
  a permutation $(i_1,i_2,i_3)$ of $(1,2,3)$ by
  \[
    P_{(i_1,i_2,i_3)}
    =
    \begin{bmatrix}
      \mathbf{e}_{i_1} & \mathbf{e}_{i_2} & \mathbf{e}_{i_3}
    \end{bmatrix}.
  \]
  For example,
  \[
    P_{(2,3,1)}
    =
    \begin{bmatrix}
      \mathbf{e}_{2} & \mathbf{e}_{3} & \mathbf{e}_{1}
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}.
  \]
  Note that the sign $s(i_1,i_2,i_3)$ of the permutation $(i_1,i_2,i_3)$
  is exactly the same as the determinant of the permutation matrix $P_{(i_1,i_2,i_3)}$.
  Define a permutation $(j_1, j_2, 3)$ of $(1, 2, 3)$
  (for swapping the first and the second coordinates of $\mathbf{u}$)
  by
  \begin{equation*}
    (j_1, j_2, 3) =
      \begin{cases}
        (1, 2, 3) & \text{ if $s(i_1,i_2,i_3) = 1$}, \\
        (2, 1, 3) & \text{ if $s(i_1,i_2,i_3) = -1$}.
      \end{cases}
  \end{equation*}
  Write
  \[
    \sigma_{(i_1, i_2, i_3)}
    =
    s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ].
  \]
  (So that $\sigma_1 = \sigma_{(1,2,3)}$.)
  Hence,
  \begin{align*}
    &
    \sigma_{(i_1, i_2, i_3)}(\mathbf{u}) \\
    =& \mathbf{0}
      + \alpha_{j_1} \mathbf{e}_{i_1}
      + \alpha_{j_2} (\mathbf{e}_{i_1}+\mathbf{e}_{i_2})
      + \alpha_3 (\mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}) \\
    =& \mathbf{0}
      + (\alpha_{j_1}+\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_1}
      + (\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_2}
      + \alpha_3 \mathbf{e}_{i_3} \\
    =& \mathbf{0} + P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}
  \end{align*}
  where $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$.
  For example,
  \[
    P_{(2,3,1)} A P_{(1,2,3)}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 1 & 1 \\
      0 & 1 & 1
    \end{bmatrix}.
  \]
  So
  \begin{align*}
    \det(P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)})
    &= \det(P_{(i_1,i_2,i_3)}) \det(A) \det(P_{(j_1,j_2,3)}) \\
    &= s(i_1, i_2, i_3) \cdot 1 \cdot s(i_1, i_2, i_3) \\
    &= 1.
  \end{align*}

\item[(3)]
  \emph{Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.}
  Note that
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_1 < i_2}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_1}} -s(i_2, i_1, i_3)
      [
        \mathbf{0},
        \mathbf{e}_{i_2}+\mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =& \mathbf{0}
  \end{align*}
  and
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_2 < i_3}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_3 > i_2}} -s(i_1, i_3, i_2)
      [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =&
    \mathbf{0}.
  \end{align*}
  So
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} \partial \sigma_{(i_1, i_2, i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &+ s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &+ \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ].
  \end{align*}
  Thus,
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ]
  \end{align*}
  is the sum of $12$ oriented affine $2$-simplexes. (Note that $3! = 6$.)

\item[(4)]
  \emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
  if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.}
  \begin{enumerate}
  \item[(a)]
    By (1),
    $\mathbf{x}$ is in the range of $\sigma_1$ if and only if
    $\mathbf{x} = A\mathbf{u}$ for $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$, or
    \[
      \begin{bmatrix}
          x_1 \\
          x_2 \\
          x_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          1 & 1 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1
      \end{bmatrix}
      \begin{bmatrix}
          u_1 \\
          u_2 \\
          u_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          u_1+u_2+u_3 \\
          u_2+u_3 \\
          u_3
      \end{bmatrix}.
    \]

  \item[(b)]
    Since $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$,
    $u_1+u_2+u_3 \leq 1$ and $u_1,u_2,u_3 \geq 0$.
    Hence $0 \leq u_3 \leq u_2+u_3 \leq u_1+u_2+u_3 \leq 1$
    or $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.

  \item[(c)]
    Conversely, if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$,
    we define
    \[
      \mathbf{v}
      =
      \begin{bmatrix}
          v_1 \\
          v_2 \\
          v_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          x_1-x_2 \\
          x_2-x_3 \\
          x_3
      \end{bmatrix}.
    \]
    Clearly, $\mathbf{v} \in Q^3$.
  \end{enumerate}

\item[(5)]
  \emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
  and that their union covers $I^3$.}
  Similar to (4).
  By (2),
  $\mathbf{x} = P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  $P_{(i_1,i_2,i_3)^{-1}} \mathbf{x} = A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  \[
    \begin{bmatrix}
        x_{i_1} \\
        x_{i_2} \\
        x_{i_3}
    \end{bmatrix}
    =
    \begin{bmatrix}
        u_1+u_2+u_3 \\
        u_{j_2}+u_3 \\
        u_3
    \end{bmatrix}.
  \]
  In any case, we always have
  $0 \leq u_3 \leq u_{j_2}+u_3 \leq u_1+u_2+u_3 \leq 1$.
  Hence
  $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_{(i_1, i_2, i_3)}$
  if and only if
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1.
  \]
  The interior of $\sigma_{(i_1, i_2, i_3)}$ is
  \[
    \{ \mathbf{x} \in \mathbb{R}^3 : 0 < x_{i_3} < x_{i_2} < x_{i_1} < 1 \},
  \]
  and thus the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors.
  Also, any $\mathbf{x} \in I^3$ has the relation
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1
  \]
  for some permutation $(i_1,i_2,i_3)$ of $(1,2,3)$.
  Hence
  \[
    I^3
    = \bigcup_{(i_1,i_2,i_3)} \sigma_{(i_1,i_2,i_3)}(Q^3)
    = \bigcup_{i=1}^{6} \sigma_{i}(Q^3).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.19.}
\emph{Let $J^2$ and $J^3$ be as in Exercise 10.17 and Exercise 10.18.
Define
\begin{align*}
  B_{01}(u,v) = (0,u,v), &\qquad B_{11}(u,v) = (1,u,v), \\
  B_{02}(u,v) = (u,0,v), &\qquad B_{12}(u,v) = (u,1,v), \\
  B_{03}(u,v) = (u,v,0), &\qquad B_{13}(u,v) = (u,v,1).
\end{align*}
These are affine, and map $\mathbb{R}^2$ into $\mathbb{R}^3$.
Put $\beta_{ri} = B_{ri}(J^2)$, for $r=0,1$, $i=1,2,3$.
Each $\beta_{ri}$ is an affine-oriented $2$-chain. (See Section 10.30.)
Verify that
\[
  \partial J^3 = \sum_{i=1}^{3} (-1)^{i} (\beta_{0i}-\beta_{1i}),
\]
in agreement with Exercise 10.18.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  A direct calculation shows that
  \begin{align*}
    B_{01}(\tau_1) - B_{11}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_3]
      - [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_1) - B_{12}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3]
      - [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_1) - B_{13}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{01}(\tau_2) - B_{11}(\tau_2)
    =&
    -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_2) - B_{12}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_2) - B_{13}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3].
  \end{align*}

\item[(2)]
  To express the formula in (1) clearly, we define
  \[
    \omega_{(i_1,i_2,i_3)}
    =
    [
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ]
    -
    [
      \mathbf{0},
      \mathbf{e}_{i_2},
      \mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ],
  \]
  and thus
  \begin{align*}
    -(B_{01}(\tau_1) - B_{11}(\tau_1)) &= s(1,2,3) \omega_{(1,2,3)} \\
    B_{02}(\tau_1) - B_{12}(\tau_1) &= s(2,1,3) \omega_{(2,1,3)} \\
    -(B_{03}(\tau_1) - B_{13}(\tau_1)) &= s(3,1,2) \omega_{(3,1,2)} \\
    -(B_{01}(\tau_2) - B_{11}(\tau_2)) &= s(1,3,2) \omega_{(1,3,2)} \\
    B_{02}(\tau_2) - B_{12}(\tau_2) &= s(2,3,1) \omega_{(2,3,1)} \\
    -(B_{03}(\tau_2) - B_{13}(\tau_2)) &= s(3,2,1) \omega_{(3,2,1)}.
  \end{align*}

\item[(3)]
  Note that
  \begin{align*}
    \beta_{0i}-\beta_{1i}
    &= B_{0i}(J^2) - B_{1i}(J^2) \\
    &= B_{0i}(\tau_1+\tau_2) - B_{1i}(\tau_1+\tau_2) \\
    &= B_{0i}(\tau_1) + B_{0i}(\tau_2) - B_{1i}(\tau_1) - B_{1i}(\tau_2) \\
    &= (B_{0i}(\tau_1) - B_{1i}(\tau_1)) + (B_{0i}(\tau_2) - B_{1i}(\tau_2)).
  \end{align*}
  Thus,
  \begin{align*}
    &\sum_{i=1}^3 (-1)^{i} (\beta_{0i}-\beta_{1i}) \\
    =& \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_1) - B_{1i}(\tau_1))
      + \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_2) - B_{1i}(\tau_2)) \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1,i_2,i_3) \omega_{(i_1,i_2,i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \partial J^3.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.20.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.21.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.22.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.23.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.24.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.25.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.26.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.27.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.28.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.29.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.30.}
\emph{If $\mathbf{N}$ is the vector given by
\[
  \mathbf{N}
  = (\alpha_2 \beta_3 - \alpha_3 \beta_2) \mathbf{e}_1
    + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \mathbf{e}_2
    + (\alpha_1 \beta_2 - \alpha_2 \beta_1) \mathbf{e}_3
\]
(Equation (135)), prove that
\[
  \det\begin{bmatrix}
    \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
    \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
    \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
  \end{bmatrix}
  = \abs{\mathbf{N}}^2
\]
Also, verify
\[
  \mathbf{N} \cdot (T\mathbf{e}_1) = \mathbf{N} \cdot (T\mathbf{e}_2)
\]
(Equation (137)).} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Laplace's expansion along the third column,
    \begin{align*}
    &\det\begin{bmatrix}
      \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
      \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
      \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
    \end{bmatrix} \\
    =& (-1)^{1+3} (\alpha_2 \beta_3 - \alpha_3 \beta_2)
      \det\begin{bmatrix}
        \alpha_2 & \beta_2 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{2+3} (\alpha_3 \beta_1 - \alpha_1 \beta_3)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{3+3} (\alpha_1 \beta_2 - \alpha_2 \beta_1)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_2 & \beta_2
      \end{bmatrix} \\
    =& (\alpha_2 \beta_3 - \alpha_3 \beta_2)^2
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3)^2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)^2 \\
    =& \abs{\mathbf{N}}^2.
  \end{align*}

\item[(2)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_1)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\alpha_1, \alpha_2, \alpha_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \alpha_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \alpha_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \alpha_3 \\
    &= (\alpha_3 \alpha_2 - \alpha_2 \alpha_3) \beta_1
      + (\alpha_1 \alpha_3 - \alpha_3 \alpha_1) \beta_2
      + (\alpha_2 \alpha_1 - \alpha_1 \alpha_2)\beta_3 \\
    &= 0.
  \end{align*}

\item[(3)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_2)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\beta_1, \beta_2, \beta_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \beta_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \beta_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \beta_3 \\
    &= (\beta_2 \beta_3 - \beta_3 \beta_2) \alpha_1
      + (\beta_3 \beta_1 - \beta_1 \beta_3) \alpha_2
      + (\beta_1 \beta_2 - \beta_2 \beta_1) \alpha_3 \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.31.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.32.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}