\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage{mathrsfs}
\usepackage{physics}
\parindent=0pt

\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\begin{document}

\textbf{\Large Chapter 10: Integration of Differential Forms} \\\\



\emph{Author: Meng-Gen Tsai} \\
\emph{Email: plover@gmail.com} \\\\



% http://pages.cs.wisc.edu/~wentaowu/other-docs/POMA_Solution_Sheet.pdf
% https://linearalgebras.com/baby-rudin-chapter-10.html
% https://www.researchgate.net/publication/248817777_Partitions_of_Unity_for_Countable_Covers
% https://www.math.lsu.edu/~lawson/Chapter4.pdf
% https://www2.math.upenn.edu/~ryrogers/HW7-solutions.pdf
% https://www.math.ucla.edu/~tao/preprints/forms.pdf



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.1.}
\emph{Let $H$ be a compact convex set in $\mathbb{R}^k$, with nonempty interior.
Let $f \in \mathscr{C}(H)$, put $f(\mathbf{x}) = 0$ in the complement of $H$,
and define $\int_{H} f$ as in Definition 10.3.
Prove that $\int_{H} f$ is independent of the order in which the $k$ integrations are carried out.
(Hint: Approximate $f$ by functions that are continuous on $\mathbb{R}^k$
and whose supports are in $H$,
as was done in Example 10.4.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.2.}
\emph{For $i=1,2,3,\ldots$, let $\varphi_i \in \mathscr{C}(\mathbb{R}^1)$ have support
in $(2^{-i},2^{1-i})$, such that $\int \varphi_i = 1$.
Put
\[
  f(x,y) = \sum_{i=1}^{\infty}[ \varphi_i(x)-\varphi_{i+1}(x) ] \varphi_i(y)
\]
Then $f$ has compact support in $\mathbb{R}^2$,
$f$ is continuous except at $(0,0)$,
and
\[
  \int dy \int f(x,y) dx = 0
  \qquad
  \text{ but }
  \qquad
  \int dx \int f(x,y) dy = 1.
\]
Observe that $f$ is unbounded in every neighborhood of $(0,0)$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.3.}
\begin{enumerate}
\item[(a)]
  \emph{If $\mathbf{F}$ is as in Theorem 10.7,
  put $\mathbf{A} = \mathbf{F}'(\mathbf{0})$,
  $\mathbf{F}_1(\mathbf{x}) = \mathbf{A}^{-1} \mathbf{F}(\mathbf{x})$.
  Then $\mathbf{F}_1(\mathbf{0}) = \mathbf{I}$.
  Show that
  \[
    \mathbf{F}_1(\mathbf{x})
    = \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x})
  \]
  in some neighborhood of $\mathbf{0}$,
  for certain primitive mappings $\mathbf{G}_{1}, \ldots, \mathbf{G}_{n}$.
  This gives another version of Theorem 10.7:}
  \[
    \mathbf{F}(\mathbf{x})
    =
    \mathbf{F}'(\mathbf{0})
    \mathbf{G}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1(\mathbf{x}).
  \]

\item[(b)]
  \emph{Prove that the mapping $(x,y) \mapsto (y,x)$ of $\mathbb{R}^2$ onto $\mathbb{R}^2$
  is not the composition of any two primitive mappings,
  in any neighborhood of the origin.
  (This shows that the flips $B_i$ cannot be omitted from the statement of Theorem 10.7.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Suppose $\mathbf{F}$ is a $\mathscr{C}'$-mapping of an open set $E \subseteq \mathbb{R}^n$
  into $\mathbb{R}^n$, $\mathbf{0} \in E$, $\mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'(\mathbf{0})$ is invertible.

\item[(2)]
  Similar to the proof of Theorem 10.7.
  Put $\mathbf{F}_1 = \mathbf{F}$.

\item[(3)]
  As $m = 1$, there is an open neighborhood $V_1 \subseteq E$ of $\mathbf{0}$
  such that
  $\mathbf{F}_1(\mathbf{0}) = (\mathbf{F}'(\mathbf{0}))^{-1} \mathbf{F}(\mathbf{0}) = \mathbf{0}$,
  $\mathbf{F}'_1(\mathbf{0}) = \mathbf{I}$ is invertible, and
  \[
    \mathbf{F}_1(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_1$.
  Hence
  \[
    \mathbf{F}'_1(\mathbf{0})\mathbf{e}_1
    = \sum_{i=1}^{n} (D_1 \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_1 \alpha_1)(\mathbf{0}) = 1 \neq 0$, and we might pick $B_1 = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_1(\mathbf{x})
    = \mathbf{x} + [\alpha_1(\mathbf{x}) - x_1] \mathbf{e}_1
    \qquad
    (\mathbf{x} \in V_1).
  \]
  Then $\mathbf{G}_1 \in \mathscr{C}'(V_1)$, $\mathbf{G}_1$ is primitive,
  and $\mathbf{G}'_1(\mathbf{0}) = \mathbf{I}$ is invertible.

\item[(4)]
  Now we make the induction hypothesis for $1 \leq m \leq n-1$.

\item[(5)]
  Since $\mathbf{G}'_{m}(\mathbf{0}) = \mathbf{I}$ is invertible,
  the inverse function theorem shows that there is an open set $U_{m}$,
  with $\mathbf{0} \in U_{m} \subseteq V_{m}$, such that
  $\mathbf{G}_m$ is an injective mapping of $U_{m}$ onto a neighborhood $V_{m+1}$ of $\mathbf{0}$,
  in which $\mathbf{G}_m^{-1} \in \mathscr{C}'(V_{m+1})$.
  Define $\mathbf{F}_{m+1}$ by
  \[
    \mathbf{F}_{m+1}(\mathbf{y}) = \mathbf{F}_{m} \circ \mathbf{G}_m^{-1}(\mathbf{y})
    \qquad
    (\mathbf{y} \in V_{m+1}).
  \]
  Then $\mathbf{F}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{F}_m(\mathbf{0}) = \mathbf{0}$,
  and $\mathbf{F}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible by the chain rule
  and the inverse function theorem.
  So
  \[
    \mathbf{F}_{m+1}(\mathbf{x})
    = P_m \mathbf{x}
      + \sum_{i=m+1}^{n} \alpha_i(\mathbf{x}) \mathbf{e}_i,
  \]
  where $\alpha_1, \ldots, \alpha_n$ are real $\mathscr{C}'$-functions in $V_{m+1}$.
  Hence
  \[
    \mathbf{F}'_{m+1}(\mathbf{0})\mathbf{e}_{m+1}
    = \sum_{i=m+1}^{n} (D_{m+1} \alpha_i)(\mathbf{0}) \mathbf{e}_i.
  \]
  Note that $(D_{m+1} \alpha_{m+1})(\mathbf{0}) = 1 \neq 0$,
  and we might pick $B_{m+1} = \mathbf{I}$.
  Thus we can define
  \[
    \mathbf{G}_{m+1}(\mathbf{x})
    = \mathbf{x} + [\alpha_{m+1}(\mathbf{x}) - x_{m+1}] \mathbf{e}_{m+1}
    \qquad
    (\mathbf{x} \in V_{m+1}).
  \]
  Then $\mathbf{G}_{m+1} \in \mathscr{C}'(V_{m+1})$, $\mathbf{G}_{m+1}$ is primitive,
  and $\mathbf{G}'_{m+1}(\mathbf{0}) = \mathbf{I}$ is invertible.
  Our induction hypothesis holds therefore with $m+1$ in place of $m$.

\item[(6)]
  Note that
  \[
    \mathbf{F}_{m}(\mathbf{x}) = \mathbf{F}_{m+1}(\mathbf{G}_{m}(\mathbf{x}))
    \qquad
    (\mathbf{x} \in U_{m}).
  \]
  If we apply this with $m = 1, \ldots, n-1$, we successively obtain
  \[
    \mathbf{F}_1
    = \mathbf{F}_{n} \circ \mathbf{G}_{n-1} \circ \cdots \circ \mathbf{G}_1
  \]
  in some open neighborhood of $\mathbf{0}$.
  Note that $\mathbf{F}_{n}$ is primitive since
  \[
    \mathbf{F}_{n}(\mathbf{x})
    = P_{n-1} \mathbf{x} + \alpha_n(\mathbf{x}) \mathbf{e}_n.
  \]
  This completes the proof.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  For $(x,y) \in \mathbb{R}^2$, define
  \[
    \mathbf{F}(x,y) = (y,x).
  \]

\item[(2)]
  (Reductio ad absurdum)
  If $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$ for some primitive mappings
  $\mathbf{G}_{i}$ ($i = 1,2$) in some neighborhood $V_{i}$ of the origin,
  $\mathbf{G}_{i}(\mathbf{0}) = \mathbf{0}$ and $\mathbf{G}_{i}'$ is invertible,
  then we may assume that
  \[
    \mathbf{G}_1(x,y) = (x, g_1(x,y))
    \qquad
    \text{and}
    \qquad
    \mathbf{G}_2(x,y) = (g_2(x,y),y).
  \]
  Here the case $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  is similar to the above case.
  Besides, $\mathbf{G}_1(x,y) = (x,g_1(x,y))$ and $\mathbf{G}_2(x,y) = (x,g_2(x,y))$
  implies that
  \[
    \mathbf{G}_2 \circ \mathbf{G}_1(x,y) = (x,g_2(x,g_1(x,y)))
    \neq (y,x) = \mathbf{F}(x,y).
  \]
  Same reason for
  $\mathbf{G}_1(x,y) = (g_1(x,y),y)$ and $\mathbf{G}_2(x,y) = (g_2(x,y),y)$.

\item[(3)]
  Note that
  \[
    \mathbf{F}'(\mathbf{0})
    =
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
  \]
  Since
  \[
    \mathbf{F}'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{G}_1(\mathbf{0})) \mathbf{G}_1'(\mathbf{0})
    = \mathbf{G}_2'(\mathbf{0}) \mathbf{G}_1'(\mathbf{0}),
  \]
  we have
  \begin{align*}
    \begin{bmatrix}
      0 & 1 \\
      1 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      D_1 g_2(0,0) & D_2 g_2(0,0) \\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      * & * \\
      D_1 g_1(0,0) & D_2 g_1(0,0)
    \end{bmatrix}.
  \end{align*}
  So $D_1 g_1(0,0) = 1$ and $D_2 g_1(0,0) = 0$, and thus
  $\mathbf{G}_1'(\mathbf{0})
  =
  \begin{bmatrix}
    1 & 0 \\
    1 & 0
  \end{bmatrix}$
  is not invertible, which is absurd.


\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.4.}
\emph{For $(x,y) \in \mathbb{R}^2$, define
\[
  \mathbf{F}(x,y) = (e^x \cos y - 1, e^x \sin y)
\]
Prove that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$, where
\begin{align*}
  \mathbf{G}_1(x,y) &= (e^x \cos y - 1, y) \\
  \mathbf{G}_2(u,v) &= (u, (1+u) \tan v)
\end{align*}
are primitive in some neighborhood of $(0,0)$.
Compute the Jacobians of $\mathbf{G}_1$, $\mathbf{G}_2$, $\mathbf{F}$ at $(0,0)$.
Define
\[
  \mathbf{H}_2(x,y) = (x, e^x \sin y)
\]
and find
\[
  \mathbf{H}_1(u,v) = (h(u,v),v)
\]
so that $\mathbf{F} = \mathbf{H}_1 \circ \mathbf{H}_2$
is in some neighborhood of $(0,0)$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Definition 10.5,
  \begin{align*}
    \mathbf{G}_1(x,y) &= (e^x \cos y - 1) \mathbf{e}_1 + y \mathbf{e}_2, \\
    \mathbf{G}_2(u,v) &= u \mathbf{e}_1 + ((1+u) \tan v) \mathbf{e}_2
  \end{align*}
  are primitive in some neighborhood of $(0,0)$.

\item[(2)]
  \emph{Show that $\mathbf{F} = \mathbf{G}_2 \circ \mathbf{G}_1$.}
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{G}_2 \circ \mathbf{G}_1)(x,y)
    &= \mathbf{G}_2(\mathbf{G}_1(x,y)) \\
    &= \mathbf{G}_2(e^x \cos y - 1, y) \\
    &= (e^x \cos y - 1, (1+(e^x \cos y - 1)) \tan y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\item[(3)]
  Since
  \begin{align*}
    J_{\mathbf{G}_1}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      0 & 1
    \end{bmatrix}
    = e^x \cos y \\
    J_{\mathbf{G}_2}(x,y)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      \tan y & (1+x)\sec^2 y
    \end{bmatrix}
    = (1+x)\sec^2 y \\
    J_{\mathbf{F}}(x,y)
    &=
    \det\begin{bmatrix}
      e^x \cos y & -e^x \sin y \\
      e^x \sin y & e^x \cos y
    \end{bmatrix}
    = e^{2x},
  \end{align*}
  \begin{align*}
    J_{\mathbf{G}_1}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{G}_2}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1 \\
    J_{\mathbf{F}}(0,0)
    &=
    \det\begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
    = 1.
  \end{align*}

\item[(4)]
  Define $h(u,v) = \sqrt{e^{2u} - v^{2}} - 1$ on
  \[
    B\left((0,0);\frac{1}{64}\right) \subseteq \mathbb{R}^2.
  \]
  $h(u,v)$ is well-defined since $e^{2u}-v^2 > 0$
  for all $(u,v) \in B\left((0,0);\frac{1}{64}\right)$.

\item[(5)]
  Given any $(x,y) \in \mathbb{R}^2$, we have
  \begin{align*}
    (\mathbf{H}_1 \circ \mathbf{H}_2)(x,y)
    &= \mathbf{H}_1(\mathbf{H}_2(x,y)) \\
    &= \mathbf{H}_1(x, e^x \sin y) \\
    &= (\sqrt{e^{2x} - (e^x \sin y)^2} - 1, e^x \sin y) \\
    &= (e^x \cos y - 1, e^x \sin y) \\
    &= \mathbf{F}(x,y).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.5.}
\emph{Formulate and prove an analogue of Theorem 10.8,
in which $K$ is a compact subset of an arbitrary metric space.
(Replace the functions $\varphi_i$ that occur in the proof of Theorem 10.8
by functions of the type constructed in Exercise 4.22.)} \\

\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  \emph{(Partitions of unity.)
  Suppose $K$ is a compact subset of a metric space $X$,
  and $\{V_{\alpha}\}$ is an open cover of $K$.
  Then there exist functions $\psi_1, \ldots, \psi_s \in \mathscr{C}(X)$ such that}
  \begin{enumerate}
  \item[(a)]
    \emph{$0 \leq \psi_i \leq 1$ for $1 \leq i \leq s$.}

  \item[(b)]
    \emph{each $\psi_i$ has its support in some $V_{\alpha}$, and}

  \item[(c)]
    \emph{$\psi_1(x) + \cdots + \psi_s(x) = 1$ for every $x \in K$.}
  \end{enumerate}

\item[(2)]
  It is trivial that some $V_{\alpha} = X$
  by taking $s = 1$ and $\psi_1(x) = 1 \in \mathscr{C}(X)$.
  Now we assume that all $V_{\alpha} \subsetneq X$.

\item[(3)]
  Associate with each $x \in K$ an index $\alpha(x)$ so that $x \in V_{\alpha(x)}$.
  Then there are open balls $B(x)$ and $W(x)$, centered at $x$,
  with
  \[
    x
    \in B(x)
    \subseteq \overline{B(x)}
    \subseteq W(x)
    \subseteq \overline{W(x)}
    \subseteq V_{\alpha(x)}
  \]
  (Since $V_{\alpha(x)}$ is open, there exists $r > 0$
  such that $B(x;r) \subseteq V_{\alpha(x)}$.
  Take $B(x) = B\left(x;\frac{r}{89}\right)$
  and $W(x) = B\left(x;\frac{r}{64}\right)$.)

\item[(4)]
  Since $K$ is compact, there are finitely many points
  $x_1, \ldots, x_s \in K$ such that
  \[
    K \subseteq B(x_1) \cup \cdots \cup B(x_s).
  \]
  Note that
  \begin{enumerate}
  \item[(a)]
    $\overline{B(x_i)}$ is a nonempty closed set since $x_i \in B(x_i) \subseteq \overline{B(x_i)}$.

  \item[(b)]
    $X - W(x_i) \supseteq X - V_{\alpha(x_i)}$ is a nonempty closed set by the assumption in (2).

  \item[(c)]
    $\overline{B(x_i)} \cap (X - W(x_i)) \subseteq W(x_i) \cap (X - W(x_i)) = \varnothing$.

  \end{enumerate}
  By Exercise 4.22, there is a function
  \[
    \varphi_i(x)
    = \frac{\rho_{\overline{B(x_i)}}(x)}{\rho_{\overline{B(x_i)}}(x) + \rho_{X-W(x_i)}(x)}
    \in \mathscr{C}(X)
  \]
  such that $\varphi_i(x) = 1$ on $\overline{B(x_i)}$,
  $\varphi_i(x) = 0$ outside $W(x_i)$, and $0 \leq \varphi_i(x) \leq 1$ on $X$
  for $1 \leq i \leq s$.

\item[(5)]
  Define $\psi_{1} = \varphi_{1} \in \mathscr{C}(X)$ and
  \[
    \psi_{i+1} = (1-\varphi_{1}) \cdots (1-\varphi_{i})\varphi_{i+1} \in \mathscr{C}(X)
  \]
  for $1 \leq i \leq s-1$.
  Properties (a) and (b) in (1) are clear.
  Also,
  \[
    \psi_1(x) + \cdots + \psi_s(x) = 1 - (1-\varphi_1(x)) \cdots (1-\varphi_s(x))
  \]
  by the construction of $\psi_i$.
  If $x \in K$, then $x \in B(x_i)$ for some $i$, hence $\varphi_i(x)=1$,
  and the product $(1-\varphi_1(x)) \cdots (1-\varphi_s(x)) = 0$.
  This proves property (c) in (1).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.6.}
\emph{Strengthen the conclusion of Theorem 10.8 by showing that
the functions $\psi_i$ can be made differentiable, and even infinitely differentiable.
(Use Exercise 8.1 in the construction of the auxiliary functions $\psi_i$.)} \\



\emph{Proof (Theorem 10.8).}
\begin{enumerate}
\item[(1)]
  It is trivial that some $V_{\alpha} = \mathbb{R}^n$
  by taking $s = 1$ and $\psi_1(\mathbf{x}) = 1 \in \mathscr{C}^{\infty}(\mathbb{R}^n)$.
  Now we assume that all $V_{\alpha} \subsetneq \mathbb{R}^n$.

\item[(2)]
  Associate with each $\mathbf{x} \in K$ an index $\alpha(x)$ so that $\mathbf{x} \in V_{\alpha(x)}$.
  Then there are open $n$-cells $B(\mathbf{x})$ and $W(\mathbf{x})$ (Definition 10.1),
  centered at $\mathbf{x}$,
  with
  \[
    \mathbf{x}
    \in B(\mathbf{x})
    \subseteq \overline{B(\mathbf{x})}
    \subseteq W(\mathbf{x})
    \subseteq \overline{W(\mathbf{x})}
    \subseteq V_{\alpha(\mathbf{x})}
  \]
  (Since $V_{\alpha(\mathbf{x})}$ is open, there exists $r > 0$
  such that $B(\mathbf{x};r) \subseteq V_{\alpha(\mathbf{x})}$.
  Take
  \[
    B(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{89\sqrt{n}}\right),
    \qquad
    W(\mathbf{x}) = I\left(\mathbf{x};\frac{r}{64\sqrt{n}}\right)
  \]
  where $I(\mathbf{p};r)$ is the open $n$-cell centered at $\mathbf{p} = (p_1,\ldots,p_n)$
  defined by
  \[
    I(\mathbf{p};r)
    = (p_1-r,p_1+r) \times \cdots \times (p_n-r,p_n+r) \subseteq \mathbb{R}^n.)
  \]

\item[(3)]
  Define
  \begin{equation*}
    f(y) =
    \begin{cases}
      e^{-\frac{1}{y^2}} & (y > 0), \\
      0 & (y \leq 0).
    \end{cases}
  \end{equation*}
  $f(y) \in \mathscr{C}^{\infty}(\mathbb{R}^1)$
  by applying the similar argument in Exercise 8.1.

\item[(4)]
  Given any $\mathbf{x} = (x_1,\ldots,x_n) \in K$
  and construct $B(\mathbf{x})$ and $W(\mathbf{x})$ as in (2).
  Define
  \[
    g_{x_j}(y_j)
    = \frac{f(y_j)}{f(y_j)+f\left(\frac{r}{64\sqrt{n}}-\frac{r}{89\sqrt{n}}-y_j\right)}
  \]
  for $1 \leq j \leq n$.
  $g_{x_j}$ is well-defined and $g_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    g_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq 0, \\
      \text{strictly increasing}
        & \text{if } 0 \leq y_j \leq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } y_j \geq \frac{r}{64\sqrt{n}} - \frac{r}{89\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Next, define
  \[
    h_{x_j}(y_j)
    = g_{x_j}\left(y_j-x_j+\frac{r}{64\sqrt{n}}\right)
      g_{x_j}\left(x_j+\frac{r}{64\sqrt{n}}-y_j\right)
  \]
  for $1 \leq j \leq n$.
  $h_{x_j} \in \mathscr{C}^{\infty}(\mathbb{R}^1)$.
  So
  \begin{equation*}
    h_{x_j}(y_j) =
    \begin{cases}
      0
        & \text{if } y_j \leq x_j - \frac{r}{64\sqrt{n}}, \\
      \text{strictly increasing}
        & \text{if } x_j - \frac{r}{64\sqrt{n}} \leq y_j \leq x_j - \frac{r}{89\sqrt{n}}, \\
      1
        & \text{if } x_j - \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{89\sqrt{n}}, \\
      \text{strictly decreasing}
        & \text{if } x_j + \frac{r}{89\sqrt{n}} \leq y_j \leq x_j + \frac{r}{64\sqrt{n}}, \\
      0
        & \text{if } y_j \geq x_j + \frac{r}{64\sqrt{n}}.
    \end{cases}
  \end{equation*}
  Finally we define $\mathbf{h}_{\mathbf{x}}: \mathbb{R}^n \to \mathbb{R}^1$ by
  \[
    \mathbf{h}_{\mathbf{x}}(\mathbf{y})
    = \prod_{j=1}^{n} h_{x_j}(y_j)
  \]
  where $\mathbf{y} = (y_1,\ldots,y_n) \in \mathbb{R}^n$.
  Hence, $\mathbf{h}_{\mathbf{x}} \in \mathscr{C}^{\infty}(\mathbb{R}^n)$ (Theorem 9.21).
  Also, $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 1$ on $\overline{B(\mathbf{x})}$,
  $\mathbf{h}_{\mathbf{x}}(\mathbf{y}) = 0$ outside $W(\mathbf{x})$,
  and $0 \leq \mathbf{h}_{\mathbf{x}}(\mathbf{y}) \leq 1$.

\item[(5)]
  Since $K$ is compact, there are finitely many points
  $\mathbf{x}_1, \ldots, \mathbf{x}_s \in K$ such that
  \[
    K \subseteq B(\mathbf{x}_1) \cup \cdots \cup B(\mathbf{x}_s).
  \]
  Take
  \[
    \varphi_i(\mathbf{x})
    = \mathbf{h}_{\mathbf{x}_i}(\mathbf{x})
    \in \mathscr{C}^{\infty}(\mathbb{R}^n)
  \]
  for $1 \leq i \leq s$.

\item[(6)]
  The rest are the same as the proof of Theorem 10.8 or Exercise 10.5.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.7.}
\begin{enumerate}
\item[(a)]
  \emph{Show that the simplex $Q^k$ is the smallest convex subset of $\mathbb{R}^k$
  such that contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}

\item[(b)]
  \emph{Show that affine mappings take convex sets to convex sets.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \emph{Show that $Q^k$ contains $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.}
  Recall
  \[
    Q^k = \{ (x_1,\ldots,x_k) \in \mathbb{R}^k :
      x_1 + \cdots + x_k \leq 1 \text{ and }
      x_1, \ldots, x_k \geq 0 \}
  \]
  (Example 10.14).
  Hence $\mathbf{0} = (0,\ldots,0) \in Q^k$ and
  \[
    \mathbf{e}_i = (0,\ldots,\underbrace{1}_{\text{$i$th coordinate}},\ldots,0) \in Q^k.
  \]

\item[(2)]
  \emph{Show that $Q^k$ is a convex subset of $\mathbb{R}^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$,
  $\mathbf{y} = (y_1,\ldots,y_k) \in Q^k$ and $0 < \lambda < 1$.
  Hence
  \[
    \lambda \mathbf{x} + (1-\lambda) \mathbf{y}
    = (\lambda x_1 + (1-\lambda)y_1, \ldots, \lambda x_k + (1-\lambda)y_k) \in Q^k
  \]
  since each $\lambda x_i + (1-\lambda)y_i \geq 0$
  and
  \[
    \sum_{i=1}^{k} (\lambda x_i + (1-\lambda)y_i)
    = \lambda \sum_{i=1}^{k} x_i + (1-\lambda) \sum_{i=1}^{k} y_i
    \leq \lambda + (1-\lambda)
    = 1.
  \]

\item[(3)]
  \emph{Given any convex set $E \subseteq \mathbb{R}^k$ containing
  $\mathbf{0}, \mathbf{e}_1, \ldots, \mathbf{e}_k$.
  Show that $E \supseteq Q^k$.}
  \begin{enumerate}
  \item[(a)]
    Induction on $k$.
    Base case: $k = 1$. Given any $\mathbf{x} = (x_1) \in Q^1$.
    We have $0 \leq x_1 \leq 1$ by the definition of $Q^1$.
    So that $\mathbf{x} = x_1 \mathbf{e}_1 + (1-x_1) \mathbf{0} \in E$
    since $\mathbf{0}, \mathbf{e}_1 \in E$ and $E$ is convex.

  \item[(b)]
    Inductive step: suppose the statement holds for $k = n$.
    Given any $\mathbf{x} = (x_1,\ldots,x_n,x_{n+1}) \in Q^{n+1}$.
    If $x_{n+1} = 1$, then $x_1 = \cdots = x_n = 0$ by the definition of $Q^{n+1}$.
    So $\mathbf{x} = \mathbf{e}_{n+1} \in E$ by the assumption of $E$.
    If $0 \leq x_{n+1} < 1$, then $x_1 + \cdots + x_n \leq 1 - x_{n+1}$ or
    \[
      \frac{x_1}{1-x_{n+1}} + \cdots + \frac{x_{n}}{1-x_{n+1}} \leq 1.
    \]
    So the point
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}} \right)
      \in Q^n,
    \]
    or
    \[
      \left( \frac{x_1}{1-x_{n+1}}, \ldots, \frac{x_{n}}{1-x_{n+1}}, 0 \right),
      \text{ say } \widehat{\mathbf{x}}, \in E
    \]
    by the induction hypothesis.
    Note that $\mathbf{e}_{n+1} \in E$.
    Hence
    \[
      \mathbf{x}
      = x_{n+1} \mathbf{e}_{n+1} + (1-x_{n+1})\widehat{\mathbf{x}}
      \in E
    \]
    by the convexity of $E$.

  \item[(c)]
    Conclusion: Since both the base case and the inductive step have been proved as true,
    by mathematical induction the statement holds.
  \end{enumerate}
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  Let $\mathbf{f}$ be an affine mapping that carries a vector space $X$ into a vector space $Y$
  such that
  \[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{0}) + A\mathbf{x}
  \]
  for some $A \in L(X,Y)$.

\item[(2)]
  Given any convex subset $C$ of $X$.
  To show that $\mathbf{f}(C)$ is convex, it suffices to show that
  \[
    \lambda \mathbf{y}_1 + (1-\lambda) \mathbf{y}_2 \in \mathbf{f}(C)
  \]
  for any $\mathbf{y}_1, \mathbf{y}_2 \in \mathbf{f}(C)$ and $0 < \lambda < 1$.
  Write $\mathbf{y}_1 = \mathbf{f}(\mathbf{x}_1)$,
  $\mathbf{y}_2 = \mathbf{f}(\mathbf{x}_2)$ for some $\mathbf{x}_1, \mathbf{x}_2 \in C$.
  Note that $\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2 \in C$ by the convexity of $C$.
  Hence
  \begin{align*}
    &\mathbf{f}(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + A(\lambda\mathbf{x}_1 + (1-\lambda) \mathbf{x}_2) \\
    =& \mathbf{f}(\mathbf{0}) + \lambda A\mathbf{x}_1 + (1-\lambda) A\mathbf{x}_2
      &(A \in L(X,Y)) \\
    =& \lambda(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_1)
      + (1-\lambda)(\mathbf{f}(\mathbf{0}) + A\mathbf{x}_2) \\
    =& \lambda \mathbf{f}(\mathbf{x}_1) + (1-\lambda)\mathbf{f}(\mathbf{x}_2) \\
    =& \lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 \in \mathbf{f}(C).
  \end{align*}

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.8.}
\emph{Let $H$ be the parallelogram in $\mathbb{R}^2$ whose vertices are
$(1,1)$, $(3,2)$, $(4,5)$, $(2,4)$.
Find the affine map $T$ which sends
$(0,0)$ to $(1,1)$, $(1,0)$ to $(3,2)$, $(1,1)$ to $(4,5)$, $(0,1)$ to $(2,4)$.
Show that $J_{T} = 5$.
Use $T$ to convert the integral
\[
  \alpha = \int_{H} e^{x-y} dx \: dy
\]
to an integral over $I^2$ and thus compute $\alpha$.} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Affine simplexes 10.26,
  \[
    T(\mathbf{x}) = T(\mathbf{0}) + A\mathbf{x},
  \]
  where $A \in L(\mathbb{R}^2, \mathbb{R}^2)$, say
  $A = \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}$.
  Note that $T:
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} \mapsto
  \begin{bmatrix}
    1 \\
    1
  \end{bmatrix}$.
  Thus
  \[
    T:
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix} \mapsto
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
    +
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y
    \end{bmatrix}
    =
    \begin{bmatrix}
      1+ax+by \\
      1+cx+dy
    \end{bmatrix}.
  \]

\item[(2)]
  By $T: (1,0) \mapsto (3,2)$ and $T: (0,1) \mapsto (2,4)$,
  we can solve $A$ as
  \[
    A = \begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}.
  \]
  It is easy to verify such
  \[
    T:
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    \mapsto
    \underbrace{\begin{bmatrix}
      1 \\
      1
    \end{bmatrix}}_{T(\mathbf{0})}
    +
    \underbrace{\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}}_{A}
    \underbrace{\begin{bmatrix}
      x \\
      y
    \end{bmatrix}}_{\mathbf{x}}
    =
    \begin{bmatrix}
      1+2x+y \\
      1+x+3y
    \end{bmatrix}
  \]
  satisfying our requirement.

\item[(3)]
  \[
    J_T
    =
    \det\begin{bmatrix}
      2 & 1 \\
      1 & 3
    \end{bmatrix}
    = 5.
  \]

\item[(4)]
  By Example 10.4 and Theorem 10.9, we have
  \begin{align*}
    \int_{H} e^{x-y} dx \: dy
    &= \int_{I^2} e^{(1+2u+v)-(1+u+3v)} \abs{J_T} du \: dv \\
    &= 5 \int_{I^2} e^{u-2v} du \: dv \\
    &= 5 \left\{ \int_{0}^{1} e^u du \right\}\left\{ \int_{0}^{1} e^{-2v} dv \right\}
      &(\text{Theorem 10.2}) \\
    &= \frac{5}{2}(e-1)(1-e^{-2}).
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.9.}
\emph{Define $(x,y) = T(r,\theta)$ one the rectangle
\[
  0 \leq r \leq a,
  \qquad
  0 \leq \theta \leq 2\pi
\]
by the equations
\[
  x = r \cos\theta,
  \qquad
  y = r \sin\theta.
\]
Show that $T$ maps this rectangle onto the closed disc $D$ with center at $(0,0)$ and radius $a$,
that $T$ is one-to-one in the interior of the rectangle,
and that $J_T(r,\theta) = r$.
If $f \in \mathscr{C}(D)$, prove the formula for integration in polar coordinates:
\[
  \int_{D} f(x,y)dx \: dy
  = \int_{0}^{a} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta.
\]
(Hint: Let $D_0$ be the interior of $D$,
minus the interval from $(0,0)$ to $(0,a)$.
As it stands, Theorem 10.9 applies to continuous functions $f$
whose support lies in $D_0$.
To remove this restriction,
proceed as in Example 10.4.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.10.}
\emph{Let $a \to \infty$ in Exercise 10.9 and prove that
\[
  \int_{\mathbb{R}^2} f(x,y) dx \: dy
  = \int_{0}^{\infty} \int_{0}^{2\pi} f(T(r,\theta))r dr \: d\theta,
\]
for continuous functions $f$ that decrease sufficiently rapidly
as $|x|+|y| \to \infty$.
(Find a more precise formulation.)
Apply this to
\[
  f(x,y) = \exp(-x^2-y^2)
\]
to derive formula}
\[
  \int_{-\infty}^{\infty} e^{-s^2} ds = \sqrt{\pi}.
\]



\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.11.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.12.}
\emph{Let $I^k$ be the set of all $\mathbf{u} = (u_1,\ldots,u_k) \in \mathbb{R}^k$
with $0 \leq u_i \leq 1$ for all $i$;
let $Q^k$ be the set of all $\mathbf{x} = (x_1,\ldots,x_k) \in \mathbb{R}^k$
with $x_i \geq 0$, $\sum x_i \leq 1$.
($I^k$ is the unit cube; $Q^k$ is the standard simplex in $\mathbb{R}^k$.)
Define $\mathbf{x} = T(\mathbf{u})$ by
\begin{align*}
  x_1 &= u_1 \\
  x_2 &= (1-u_1)u_2 \\
  &\cdots \\
  x_k &= (1-u_1) \cdots (1-u_{k-1})u_k.
\end{align*}
Show that
\[
  \sum_{i=1}^{k} x_i = 1 - \prod_{i=1}^{k} (1-u_i).
\]
Show that $T$ maps $I^k$ onto $Q^k$, that $T$ is $1$-$1$ in the interior of $I^k$,
and that its inverse $S$ is defined in the interior of $Q^k$ by $u_1 = x_1$ and
\[
  u_i = \frac{x_i}{1-x_1-\cdots-x_{i-1}}
\]
for $i = 2, \ldots, k$.
Show that
\[
  J_T(\mathbf{u}) = (1-u_1)^{k-1}(1-u_2)^{k-2} \cdots (1-u_{k-1}),
\]
and}
\[
  J_S(\mathbf{x}) = [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
\] \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that
  \[
    \sum_{i=1}^{m} x_i = 1 - \prod_{i=1}^{m} (1-u_i)
  \]
  for all $1 \leq m \leq k$.}
  Induction on $m$.
  Base case: $x_1 = 1 - (1-u_1)$.
  Inductive step: Suppose the case $m = h$ is true.
  Consider the the case $m = h+1$:
  \begin{align*}
    \sum_{i=1}^{h+1} x_i
    &= \left( \sum_{i=1}^{h} x_i \right) + x_{h+1} \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + x_{h+1}
      &(\text{Induction hypothesis}) \\
    &= 1 - \prod_{i=1}^{h} (1-u_i) + u_{h+1}\prod_{i=1}^{h} (1-u_i)
      &(\text{Definition of $x_{h+1}$}) \\
    &= 1 - (1-u_{h+1})\prod_{i=1}^{h} (1-u_i) \\
    &= 1 - \prod_{i=1}^{h+1} (1-u_i).
  \end{align*}
  Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement is established.

\item[(2)]
  \emph{Show that $T$ maps $I^k$ onto $Q^k$.}
  Given any $\mathbf{x} = (x_1,\ldots,x_k) \in Q^k$.
  It is equivalent to solve $\mathbf{u} = (u_1, \ldots, u_k)$ from
  \begin{align*}
    x_1 &= u_1 \\
    x_2 &= (1-u_1)u_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k
  \end{align*}
  in terms of $\mathbf{x} = (x_1, \ldots, x_k)$.
  It is clear that $u_1 = x_1$
  and
  \begin{equation*}
    u_i =
      \begin{cases}
        x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
          & \text{ if $x_1+\cdots+x_{i-1} \neq 1$}, \\
        0
          & \text{ if $x_1+\cdots+x_{i-1} = 1$}.
      \end{cases}
  \end{equation*}
  for $i = 2, \ldots, k$.
  (If $x_1+\cdots+x_{i-1} \neq 1$, by (1) we have
  \[
    \prod_{j=1}^{i-1}(1-u_j)
    = 1 - \sum_{j=1}^{i-1} x_i
    \neq 0
  \]
  and thus
  \[
    u_i
    = x_i \left\{ \prod_{j=1}^{i-1}(1-u_j) \right\}^{-1}
    = x_i(1 - x_1 - \cdots - x_{i-1})^{-1}.
  \]
  If $x_1+\cdots+x_{i-1} = 1$,
  then $x_{i} = \cdots = x_{k} = 0$.
  We may take $u_{i} = 0$ to set the expression $x_{i} = (1-u_1)\cdots(1-u_{i-1})u_i$ to zero.)
  Note that the solution $\mathbf{u} \in I^k$ is well-defined by construction,
  or $T(I^k) = Q^k$.

\item[(3)]
  \emph{Show that $T$ is $1$-$1$ in the interior of $I^k$.}
  Suppose $T(\mathbf{u}) = T(\mathbf{v}) = \mathbf{x}$ with
  $\mathbf{u}, \mathbf{v} \in \mathrm{int}(I^k)$.
  Then we consider the following equation:
  \begin{align*}
    x_1 &= u_1 = v_1 \\
    x_2 &= (1-u_1)u_2 = (1-v_1)v_2 \\
    &\cdots \\
    x_k &= (1-u_1) \cdots (1-u_{k-1})u_k = (1-v_1) \cdots (1-v_{k-1})v_k.
  \end{align*}
  By (1),
  \[
    \mathbf{x} \in \mathrm{int}(Q^k)
    = \left\{ (x_1,\ldots,x_k) \in \mathbb{R}^k : x_i > 0, \sum x_i < 1 \right\}.
  \]
  Hence,
  \begin{align*}
    u_1 = v_1 &= x_1 \\
    u_2 = v_1 &= x_2(1-x_1)^{-1} \\
    &\cdots \\
    u_k = v_k &= x_k(1-x_1-\cdots-x_{k-1})^{-1}.
  \end{align*}
  Here all $(1-x_1)^{-1}, \ldots, (1-x_1-\cdots-x_{i})^{-1}$
  are well-defined since $\mathbf{x} \in \mathrm{int}(Q^k)$.
  Therefore, $T$ is injective on $\mathrm{int}(I^k)$.

\item[(4)]
  By (2)(3), $T$ maps $\mathrm{int}(I^k)$ onto $\mathrm{int}(Q^k)$.
  That is, given any $\mathbf{x} = (x_1,\ldots,x_k) \in \mathrm{int}(Q^k)$,
  we can pick
  \begin{align*}
    u_1 &= x_1 \\
    u_i &= x_i(1 - x_1 - \cdots - x_{i-1})^{-1}
    \qquad
    (i = 2, \ldots, k)
  \end{align*}
  such that $\mathbf{u} \in \mathrm{int}(I^k)$ and $T(\mathbf{u}) = \mathbf{x}$ .

\item[(5)]
  Note that
  $T(\mathbf{u}) = (u_1, (1-u_1)u_2, \ldots, (1-u_1)\cdots(1-u_{k-1})u_{k})$
  on $\mathrm{int}(I^k)$.
  So
  \[
    T'(\mathbf{u})
    =
    \begin{bmatrix}
      1      & 0       & 0                      & \cdots & 0 \\
      *      & (1-u_1) & 0                      & \cdots & 0 \\
      *      & *       & \prod_{i=1}^{2}(1-u_i) & \cdots & 0 \\
      \vdots &  \vdots & \vdots                 & \ddots & \vdots \\
      *      & *       & *                      & \cdots & \prod_{i=1}^{k-1}(1-u_i)
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_T(\mathbf{u})
    &= \det T'(\mathbf{u}) \\
    &= 1 \cdot (1-u_1) \cdot \prod_{i=1}^{2}(1-u_i) \cdots \prod_{i=1}^{k-1}(1-u_i) \\
    &= \prod_{i=1}^{k-1}(1-u_i)^{k-i}.
  \end{align*}

\item[(6)]
  Similar to (5).
  $S(\mathbf{x}) = (x_1, x_2(1-x_1)^{-1}, \ldots, x_k(1-x_1-\cdots-x_{k-1})^{-1})$
  on $\mathrm{int}(Q^k)$.
  So
  \[
    S'(\mathbf{x})
    =
    \begin{bmatrix}
      1      & 0            & 0                & \cdots & 0 \\
      *      & (1-x_1)^{-1} & 0                & \cdots & 0 \\
      *      & *            & (1-x_1-x_2)^{-1} & \cdots & 0 \\
      \vdots &  \vdots      & \vdots           & \ddots & \vdots \\
      *      & *            & *                & \cdots & (1-x_1-\cdots-x_{k-1})^{-1}
    \end{bmatrix}
  \]
  is a lower triangular matrix.
  Hence,
  \begin{align*}
    J_S(\mathbf{x})
    &= \det S'(\mathbf{x}) \\
    &= 1 \cdot (1-x_1)^{-1} \cdot (1-x_1-x_2)^{-1} \cdots (1-x_1-\cdots-x_{k-1})^{-1} \\
    &= [(1-x_1)(1-x_1-x_2) \cdots (1-x_1-\cdots-x_{k-1})]^{-1}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.13.}
\emph{Let $r_1, \ldots, r_k$ be nonnegative integers, and prove that
\[
  \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
  =
  \frac{r_1! \cdots r_k!}{(k+r_1+\cdots+r_k)!}
\]
(Hint: Use Exercise 10.12, Theorems 10.9 and 8.20.)
Note that the special case $r_1 = \cdots = r_k = 0$
shows that the volume of $Q^k$ is $\frac{1}{k!}$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Define $T: I^k$ onto $Q^k$ as in Exercise 10.12,
  and $f: Q^k \to \mathbb{R}^1$ by
  \[
    f(\mathbf{x})
    = f(x_1, \ldots, x_k)
    = x_1^{r_1} \cdots x_k^{r_k}
    = \prod_{i=1}^{k} x_i^{r_i}.
  \]

\item[(2)]
  By Exercise 10.12, Example 10.4 and Theorems 10.9, we have
  \begin{align*}
    \int_{Q^k} x_1^{r_1} \cdots x_k^{r_k} d\mathbf{x}
    &=
    \int_{Q^k} f(\mathbf{x}) d\mathbf{x} \\
    &=
    \int_{I^k} f(T(\mathbf{u})) |J_T(\mathbf{u})| d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} \left( u_i \prod_{j=1}^{i-1}(1-u_j) \right)^{r_i}
      \prod_{i=1}^{k}(1-u_i)^{k-i} d\mathbf{u} \\
    &=
    \int_{I^k}
      \prod_{i=1}^{k} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} d\mathbf{u} \\
    &=
    \prod_{i=1}^{k}
      \int_{0}^{1} u_i^{r_i} (1-u_i)^{k-i+\sum_{j=i+1}^{k}r_j} du_i
      & (\text{Theorem 10.2}) \\
    &=
    \prod_{i=1}^{k}
      \frac{r_i! \left(k-i+\sum_{j=i+1}^{k}r_j\right)!}
        {\left(k-i+1+\sum_{j=i}^{k}r_j\right)!}
      & (\text{Theorem 8.20}) \\
    &=
    \frac{r_1! \cdots r_k!}{(k + r_1 + \cdots + r_k)!}.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.14 (Levi-Civita symbol).}
\emph{Prove $\varepsilon(j_1, \ldots, j_k) = s(j_1, \ldots, j_k)$,
where}
\[
  s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p).
\] \\

It is usually to define the Levi-Civita symbol by
\begin{equation*}
\varepsilon(j_1, \ldots, j_k) =
  \begin{cases}
    1
      & \text{ if $(j_1,\cdots,j_k)$ is an even permutation of $J$}, \\
    -1
      & \text{ if $(j_1,\cdots,j_k)$ is an odd permutation of $J$}, \\
    0
      & \text{otherwise}
  \end{cases}
\end{equation*}
(Basic $k$-forms 10.14).
Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
So $\varepsilon(j_1, \ldots, j_k)$ is equivalent to an explicit expression
$s(j_1, \ldots, j_k) = \prod_{p < q} \mathrm{sgn}(j_q - j_p)$. \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Induction on $k$.
  Base case: \emph{Show that $\varepsilon(j_1,j_2) = s(j_1,j_2)$.}
  Since
  \begin{equation*}
  \varepsilon(j_1,j_2) =
    \begin{cases}
      1
        & \text{ if $j_1 < j_2$} \\
      -1
        & \text{ if $j_1 > j_2$},
    \end{cases}
  \end{equation*}
  $\varepsilon(j_1,j_2) = \mathrm{sgn}(j_2-j_1) = s(j_1,j_2)$.

\item[(2)]
  Inductive step: \emph{Show that for any $s \geq 2$,
  if $\varepsilon(j_1, \ldots, j_{s}) = s(j_1, \ldots, j_{s})$ holds,
  then $\varepsilon(j_1, \ldots, j_{s+1}) = s(j_1, \ldots, j_{s+1})$ also holds.}
  \begin{align*}
    \varepsilon(j_1, \ldots, j_{s+1})
    &= \varepsilon(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= s(j_1, \ldots, j_{s})
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s} \mathrm{sgn}(j_q-j_p)
      \prod_{\substack{1 \leq p \leq s \\ q=s+1}} \mathrm{sgn}(j_q-j_p) \\
    &= \prod_{1 \leq p < q \leq s+1} \mathrm{sgn}(j_q - j_p) \\
    &= s(j_1, \ldots, j_{s+1}).
  \end{align*}

\item[(3)]
  Conclusion: Since both the base case and the inductive step have been proved as true,
  by mathematical induction the statement holds for every integer $k \geq 2$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.15.}
\emph{If $\omega$ and $\lambda$ are $k$- and $m$-forms, respectively,
prove that}
\[
  \omega \wedge \lambda = (-1)^{km} \lambda \wedge \omega.
\]

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Write
  \[
    \omega = \sum_I b_I(\mathbf{x}) dx_I,
    \qquad
    \lambda = \sum_J c_J(\mathbf{x}) dx_J
  \]
  in the stardard presentations,
  where $I$ and $J$ range over all increasing $k$-indices
  and over all increasing $m$-indices taken from the set $\{1,\ldots,n\}$.

\item[(2)]
  \emph{Show that $dx_I \wedge dx_J = (-1)^{km} dx_J \wedge dx_I$.}
  \begin{align*}
    dx_I \wedge dx_J
    &= dx_{i_1} \wedge \cdots \wedge dx_{i_k}
      \wedge dx_J \\
    &= (-1)^m dx_{i_1} \wedge \cdots \wedge dx_{i_{k-1}}
      \wedge dx_J \wedge dx_{i_{k}} \\
    &= (-1)^{2m} dx_{i_1} \wedge \cdots \wedge dx_{i_{k-2}}
      \wedge dx_J \wedge dx_{i_{k-1}} \wedge dx_{i_{k}} \\
    &\cdots \\
    &= (-1)^{km} dx_J
      \wedge dx_{i_1} \wedge \cdots \wedge dx_{i_k} \\
    &= (-1)^{km} dx_J \wedge dx_I.
  \end{align*}

\item[(3)]
  \begin{align*}
    \omega \wedge \lambda
    &= \sum_{I,J} b_I(\mathbf{x}) c_J(\mathbf{x}) dx_I \wedge dx_J \\
    &= (-1)^{km} \sum_{J,I} c_J(\mathbf{x}) b_I(\mathbf{x}) dx_J \wedge dx_I \\
    &= (-1)^{km} \lambda \wedge \omega.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.16.}
\emph{If $k \geq 2$ and $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$
is an oriented affine $k$-simplex, prove that $\partial^2 \sigma = 0$,
directly from the definition of the boundary operator $\partial$.
Deduce from this that $\partial^2 \Psi = 0$ for every chain $\Psi$.
(Hint: For orientation, do it first for $k=2$, $k=3$.
In general, if $i < j$, let $\sigma_{ij}$ be the $(k-2)$-simplex obtained by
deleting $\mathbf{p}_i$ and $\mathbf{p}_j$ from $\sigma$.
Show that each $\sigma_{ij}$ occurs twice in $\partial^2\sigma$, with opposite sign.)} \\

\emph{Proof (Brute-force).}
\begin{enumerate}
\item[(1)]
  Write the boundary of the oriented affine $k$-simplex
  $\sigma = [\mathbf{p}_0,\mathbf{p}_1,\ldots,\mathbf{p}_k]$ as
  \[
    \partial \sigma
    = \sum_{i=0}^{k}(-1)^i
    [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]
  \]
  where where the oriented $(k-1)$-simplex
  $[\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k]$
  is obtained by deleting $\sigma$'s $i$-th vertex (Boundaries 10.29).

\item[(2)]
  \begin{align*}
    \partial^2 \sigma
    =& \partial \left( \sum_{i}(-1)^{i}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \right) \\
    =& \sum_{i}(-1)^{i}
      \partial [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i}(-1)^{j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &+ \sum_{j>i} (-1)^{i}(-1)^{j-1}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k] \\
    =& \sum_{j<i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_j},\ldots,\widehat{\mathbf{p}_i},\ldots,\mathbf{p}_k] \\
      &- \sum_{j>i} (-1)^{i+j}
      [\mathbf{p}_0,\ldots,\widehat{\mathbf{p}_i},\ldots,\widehat{\mathbf{p}_j},\ldots,\mathbf{p}_k].
  \end{align*}
  The latter two summations cancel since after switching $i$ and $j$ in the second sum.
  Therefore $\partial^2 \sigma = 0$.

\item[(3)]
  The boundary of a chain is the linear combination of boundaries of the simplices in the chain.
  Write $\Psi = \sum_{i=1}^{r} \sigma_i$. where $\sigma_i$ is an oriented affine simplex.
  Then
  \[
    \partial^2 \Psi
    = \partial \left(\partial \sum \sigma_i \right)
    = \partial \left( \sum \partial\sigma_i \right)
    = \sum \partial^2 \sigma_i
    = \sum 0
    = 0
  \]
  for any affine chain $\Psi$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.17.}
\emph{Put $J^2 = \tau_1 + \tau_2$, where
\[
  \tau_1 = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2],
  \qquad
  \tau_2 = -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1].
\]
Explain why it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.
Show that $\partial J^2$ is the sum of $4$ oriented affine $1$-simplexes.
Find these.
What is $\partial(\tau_1 - \tau_2)$?} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that the unit square $I^2 \in \mathbb{R}^2$ is the union of
  $\tau_1(Q^2)$ and $\tau_2(Q_2)$, where
  \begin{align*}
    \tau_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2) \\
    &= \mathbf{0} + (\alpha_1+\alpha_2) \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 1 \\
        0 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  and
  \begin{align*}
    \tau_2(\mathbf{u})
    &= (-[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_1])(\mathbf{u}) \\
    &= ([\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2])(\mathbf{u}) \\
    &= \mathbf{0} + \alpha_1 (\mathbf{e}_1+\mathbf{e}_2) + \alpha_2 \mathbf{e}_2 \\
    &= \mathbf{0} + \alpha_1 \mathbf{e}_1 + (\alpha_1+\alpha_2) \mathbf{e}_2 \\
    &= \mathbf{0} +
      \begin{bmatrix}
        1 & 0 \\
        1 & 1
      \end{bmatrix}
      \mathbf{u}
  \end{align*}
  where $\mathbf{u} = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 \in \mathbb{R}^2$
  (as in Equation (78)).
  Both $\tau_1$ and $\tau_2$ have Jacobian $1 > 0$, or positively oriented
  (Affine simplexes 10.26).
  So it is reasonable to call $J^2$ the positively oriented unit square in $\mathbb{R}^2$.

\item[(2)]
  \begin{align*}
    \partial \tau_1
    &= [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_1], \\
    \partial \tau_2
    &= [\mathbf{e}_2+\mathbf{e}_1, \mathbf{e}_2]
      - [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{0}, \mathbf{e}_2+\mathbf{e}_1] \\
    &= [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}]
      + [\mathbf{0}, \mathbf{e}_1+\mathbf{e}_2].
  \end{align*}

\item[(3)]
  By (2),
  \[
    \partial J^2
    = \partial \tau_1 + \partial \tau_2
    = [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{0}],
  \]
  which is the positively oriented boundary of $I^2$.

\item[(4)]
  By (2),
  \begin{align*}
    \partial(\tau_1 - \tau_2)
    =& \partial \tau_1 - \partial \tau_2 \\
    =& [\mathbf{0}, \mathbf{e}_1]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}] \\
      &+ [\mathbf{0}, \mathbf{e}_2]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_1+\mathbf{e}_2, \mathbf{0}].
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.18.}
\emph{Consider the oriented affine $3$-simplex
\[
  \sigma_1
  = [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3]
\]
in $\mathbb{R}^3$.
Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.
Thus $\sigma_1$ is positively oriented.} \\

\emph{Let $\sigma_2, \ldots, \sigma_6$ be five other oriented $3$-simplexes,
obtained as follows:
There are five permutations $(i_1, i_2, i_3)$ of $(1, 2, 3)$,
distinct from $(1, 2, 3)$.
Associate with each $(i_1, i_2, i_3)$ the simplex
\[
  s(i_1, i_2, i_3)
  [
    \mathbf{0},
    \mathbf{e}_{i_1},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
    \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
  ]
\]
where $s$ is the sign that occurs in the definition of the determinant.
(This is how $\tau_2$ was obtained from $\tau_1$ in Exercise 10.17.)
Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.} \\

\emph{Put $J^3 = \sigma_1+\cdots+\sigma_6$.
Then $J^3$ may be called the positively oriented unit cube in $\mathbb{R}^3$.
Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.
(These $12$ triangles cover the surface of the unit cube $I^3$.)} \\

\emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.} \\

\emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
and that their union covers $I^3$.
(Compared with Exercise 10.13; note that $3!=6$.)} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Show that $\sigma_1$ (regarded as a linear transformation) has determinant $1$.}
  Given any $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$,
  we have
  \begin{align*}
    \sigma_1(\mathbf{u})
    &= ([\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3])
    (\mathbf{u}) \\
    &= \mathbf{0}
      + \alpha_1 \mathbf{e}_1
      + \alpha_2 (\mathbf{e}_1+\mathbf{e}_2)
      + \alpha_3 (\mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3) \\
    &= \mathbf{0}
      + (\alpha_1+\alpha_2+\alpha_3) \mathbf{e}_1
      + (\alpha_2+\alpha_3) \mathbf{e}_2
      + \alpha_3 \mathbf{e}_3 \\
    &= \mathbf{0} +
      \underbrace{\begin{bmatrix}
        1 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 1
      \end{bmatrix}}_{\text{say }A}
      \mathbf{u}.
  \end{align*}
  So
  \[
    \det(A)
    =
    \det
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    = 1.
  \]

\item[(2)]
  \emph{Show that $\sigma_2, \ldots, \sigma_6$ are positively oriented.}
  Define the permutation matrix $P_{(i_1,i_2,i_3)}$ corresponding to
  a permutation $(i_1,i_2,i_3)$ of $(1,2,3)$ by
  \[
    P_{(i_1,i_2,i_3)}
    =
    \begin{bmatrix}
      \mathbf{e}_{i_1} & \mathbf{e}_{i_2} & \mathbf{e}_{i_3}
    \end{bmatrix}.
  \]
  For example,
  \[
    P_{(2,3,1)}
    =
    \begin{bmatrix}
      \mathbf{e}_{2} & \mathbf{e}_{3} & \mathbf{e}_{1}
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}.
  \]
  Note that the sign $s(i_1,i_2,i_3)$ of the permutation $(i_1,i_2,i_3)$
  is exactly the same as the determinant of the permutation matrix $P_{(i_1,i_2,i_3)}$.
  Define a permutation $(j_1, j_2, 3)$ of $(1, 2, 3)$
  (for swapping the first and the second coordinates of $\mathbf{u}$)
  by
  \begin{equation*}
    (j_1, j_2, 3) =
      \begin{cases}
        (1, 2, 3) & \text{ if $s(i_1,i_2,i_3) = 1$}, \\
        (2, 1, 3) & \text{ if $s(i_1,i_2,i_3) = -1$}.
      \end{cases}
  \end{equation*}
  Write
  \[
    \sigma_{(i_1, i_2, i_3)}
    =
    s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ].
  \]
  (So that $\sigma_1 = \sigma_{(1,2,3)}$.)
  Hence,
  \begin{align*}
    &
    \sigma_{(i_1, i_2, i_3)}(\mathbf{u}) \\
    =& \mathbf{0}
      + \alpha_{j_1} \mathbf{e}_{i_1}
      + \alpha_{j_2} (\mathbf{e}_{i_1}+\mathbf{e}_{i_2})
      + \alpha_3 (\mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}) \\
    =& \mathbf{0}
      + (\alpha_{j_1}+\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_1}
      + (\alpha_{j_2}+\alpha_3) \mathbf{e}_{i_2}
      + \alpha_3 \mathbf{e}_{i_3} \\
    =& \mathbf{0} + P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}
  \end{align*}
  where $\mathbf{u}
  = \alpha_1 \mathbf{e}_1 + \alpha_2 \mathbf{e}_2 + \alpha_3 \mathbf{e}_3 \in \mathbb{R}^3$.
  For example,
  \[
    P_{(2,3,1)} A P_{(1,2,3)}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 0 & 0 \\
      0 & 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 1 & 1 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 0 & 1 \\
      1 & 1 & 1 \\
      0 & 1 & 1
    \end{bmatrix}.
  \]
  So
  \begin{align*}
    \det(P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)})
    &= \det(P_{(i_1,i_2,i_3)}) \det(A) \det(P_{(j_1,j_2,3)}) \\
    &= s(i_1, i_2, i_3) \cdot 1 \cdot s(i_1, i_2, i_3) \\
    &= 1.
  \end{align*}

\item[(3)]
  \emph{Show that $\partial J^3$ is the sum of $12$ oriented affine $2$-simplexes.}
  Note that
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_1 < i_2}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_1 > i_2}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_1}} -s(i_2, i_1, i_3)
      [
        \mathbf{0},
        \mathbf{e}_{i_2}+\mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =& \mathbf{0}
  \end{align*}
  and
  \begin{align*}
    \sum_{(i_1,i_2,i_3)} \sigma_{(i_1, i_2, i_3)}
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} \sigma_{(i_1, i_2, i_3)}
      + \sum_{\substack{(i_1,i_2,i_3) \\ i_2 < i_3}} \sigma_{(i_1, i_2, i_3)} \\
    =&
    \sum_{\substack{(i_1,i_2,i_3) \\ i_2 > i_3}} s(i_1, i_2, i_3)
    [
      \mathbf{0},
      \mathbf{e}_{i_1},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ] \\
    &+ \sum_{\substack{(i_1,i_2,i_3) \\ i_3 > i_2}} -s(i_1, i_3, i_2)
      [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
    =&
    \mathbf{0}.
  \end{align*}
  So
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} \partial \sigma_{(i_1, i_2, i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &+ s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}+\mathbf{e}_{i_3}
      ] \\
      &- s(i_1, i_2, i_3)[
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &+ \underbrace{\sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ]}_{= \mathbf{0}} \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ].
  \end{align*}
  Thus,
  \begin{align*}
    \partial J^3
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ]
  \end{align*}
  is the sum of $12$ oriented affine $2$-simplexes. (Note that $3! = 6$.)

\item[(4)]
  \emph{Show that $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_1$
  if and only if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.}
  \begin{enumerate}
  \item[(a)]
    By (1),
    $\mathbf{x}$ is in the range of $\sigma_1$ if and only if
    $\mathbf{x} = A\mathbf{u}$ for $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$, or
    \[
      \begin{bmatrix}
          x_1 \\
          x_2 \\
          x_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          1 & 1 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1
      \end{bmatrix}
      \begin{bmatrix}
          u_1 \\
          u_2 \\
          u_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          u_1+u_2+u_3 \\
          u_2+u_3 \\
          u_3
      \end{bmatrix}.
    \]

  \item[(b)]
    Since $\mathbf{u} = (u_1,u_2,u_3) \in Q^3$,
    $u_1+u_2+u_3 \leq 1$ and $u_1,u_2,u_3 \geq 0$.
    Hence $0 \leq u_3 \leq u_2+u_3 \leq u_1+u_2+u_3 \leq 1$
    or $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$.

  \item[(c)]
    Conversely, if $0 \leq x_3 \leq x_2 \leq x_1 \leq 1$,
    we define
    \[
      \mathbf{v}
      =
      \begin{bmatrix}
          v_1 \\
          v_2 \\
          v_3
      \end{bmatrix}
      =
      \begin{bmatrix}
          x_1-x_2 \\
          x_2-x_3 \\
          x_3
      \end{bmatrix}.
    \]
    Clearly, $\mathbf{v} \in Q^3$.
  \end{enumerate}

\item[(5)]
  \emph{Show that the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors,
  and that their union covers $I^3$.}
  Similar to (4).
  By (2),
  $\mathbf{x} = P_{(i_1,i_2,i_3)} A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  $P_{(i_1,i_2,i_3)^{-1}} \mathbf{x} = A P_{(j_1,j_2,3)}\mathbf{u}$,
  or
  \[
    \begin{bmatrix}
        x_{i_1} \\
        x_{i_2} \\
        x_{i_3}
    \end{bmatrix}
    =
    \begin{bmatrix}
        u_1+u_2+u_3 \\
        u_{j_2}+u_3 \\
        u_3
    \end{bmatrix}.
  \]
  In any case, we always have
  $0 \leq u_3 \leq u_{j_2}+u_3 \leq u_1+u_2+u_3 \leq 1$.
  Hence
  $\mathbf{x} = (x_1, x_2, x_3)$ is in the range of $\sigma_{(i_1, i_2, i_3)}$
  if and only if
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1.
  \]
  The interior of $\sigma_{(i_1, i_2, i_3)}$ is
  \[
    \{ \mathbf{x} \in \mathbb{R}^3 : 0 < x_{i_3} < x_{i_2} < x_{i_1} < 1 \},
  \]
  and thus the range of $\sigma_1, \ldots, \sigma_6$ have disjoint interiors.
  Also, any $\mathbf{x} \in I^3$ has the relation
  \[
    0 \leq x_{i_3} \leq x_{i_2} \leq x_{i_1} \leq 1
  \]
  for some permutation $(i_1,i_2,i_3)$ of $(1,2,3)$.
  Hence
  \[
    I^3
    = \bigcup_{(i_1,i_2,i_3)} \sigma_{(i_1,i_2,i_3)}(Q^3)
    = \bigcup_{i=1}^{6} \sigma_{i}(Q^3).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.19.}
\emph{Let $J^2$ and $J^3$ be as in Exercise 10.17 and Exercise 10.18.
Define
\begin{align*}
  B_{01}(u,v) = (0,u,v), &\qquad B_{11}(u,v) = (1,u,v), \\
  B_{02}(u,v) = (u,0,v), &\qquad B_{12}(u,v) = (u,1,v), \\
  B_{03}(u,v) = (u,v,0), &\qquad B_{13}(u,v) = (u,v,1).
\end{align*}
These are affine, and map $\mathbb{R}^2$ into $\mathbb{R}^3$.
Put $\beta_{ri} = B_{ri}(J^2)$, for $r=0,1$, $i=1,2,3$.
Each $\beta_{ri}$ is an affine-oriented $2$-chain. (See Section 10.30.)
Verify that
\[
  \partial J^3 = \sum_{i=1}^{3} (-1)^{i} (\beta_{0i}-\beta_{1i}),
\]
in agreement with Exercise 10.18.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  A direct calculation shows that
  \begin{align*}
    B_{01}(\tau_1) - B_{11}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_2, \mathbf{e}_2+\mathbf{e}_3]
      - [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_1) - B_{12}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3]
      - [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_1) - B_{13}(\tau_1)
    =&
    [\mathbf{0}, \mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_2]
      - [\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{01}(\tau_2) - B_{11}(\tau_2)
    =&
    -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3]
      + [\mathbf{e}_1, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{02}(\tau_2) - B_{12}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_3]
      + [\mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3] \\
    B_{03}(\tau_2) - B_{13}(\tau_2)
    =& -[\mathbf{0}, \mathbf{e}_2, \mathbf{e}_1+\mathbf{e}_2]
      + [\mathbf{e}_3, \mathbf{e}_2+\mathbf{e}_3, \mathbf{e}_1+\mathbf{e}_2+\mathbf{e}_3].
  \end{align*}

\item[(2)]
  To express the formula in (1) clearly, we define
  \[
    \omega_{(i_1,i_2,i_3)}
    =
    [
      \mathbf{e}_{i_1},
      \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
      \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
    ]
    -
    [
      \mathbf{0},
      \mathbf{e}_{i_2},
      \mathbf{e}_{i_2}+\mathbf{e}_{i_3}
    ],
  \]
  and thus
  \begin{align*}
    -(B_{01}(\tau_1) - B_{11}(\tau_1)) &= s(1,2,3) \omega_{(1,2,3)} \\
    B_{02}(\tau_1) - B_{12}(\tau_1) &= s(2,1,3) \omega_{(2,1,3)} \\
    -(B_{03}(\tau_1) - B_{13}(\tau_1)) &= s(3,1,2) \omega_{(3,1,2)} \\
    -(B_{01}(\tau_2) - B_{11}(\tau_2)) &= s(1,3,2) \omega_{(1,3,2)} \\
    B_{02}(\tau_2) - B_{12}(\tau_2) &= s(2,3,1) \omega_{(2,3,1)} \\
    -(B_{03}(\tau_2) - B_{13}(\tau_2)) &= s(3,2,1) \omega_{(3,2,1)}.
  \end{align*}

\item[(3)]
  Note that
  \begin{align*}
    \beta_{0i}-\beta_{1i}
    &= B_{0i}(J^2) - B_{1i}(J^2) \\
    &= B_{0i}(\tau_1+\tau_2) - B_{1i}(\tau_1+\tau_2) \\
    &= B_{0i}(\tau_1) + B_{0i}(\tau_2) - B_{1i}(\tau_1) - B_{1i}(\tau_2) \\
    &= (B_{0i}(\tau_1) - B_{1i}(\tau_1)) + (B_{0i}(\tau_2) - B_{1i}(\tau_2)).
  \end{align*}
  Thus,
  \begin{align*}
    &\sum_{i=1}^3 (-1)^{i} (\beta_{0i}-\beta_{1i}) \\
    =& \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_1) - B_{1i}(\tau_1))
      + \sum_{i=1}^3 (-1)^{i} (B_{0i}(\tau_2) - B_{1i}(\tau_2)) \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1,i_2,i_3) \omega_{(i_1,i_2,i_3)} \\
    =& \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3)
      [
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2},
        \mathbf{e}_{1}+\mathbf{e}_{2}+\mathbf{e}_{3}
      ] \\
      &- \sum_{(i_1,i_2,i_3)} s(i_1, i_2, i_3) [
        \mathbf{0},
        \mathbf{e}_{i_1},
        \mathbf{e}_{i_1}+\mathbf{e}_{i_2}
      ] \\
    =& \partial J^3.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.20.}
\emph{State conditions under which the formula
\[
  \int_{\Phi} fd\omega
  = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
\]
is valid, and show that it generalizes the formula for integration by parts.
(Hint: $d(f\omega) = (df) \wedge \omega + f d\omega$.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{If}
  \begin{enumerate}
  \item[(a)]
    \emph{$\Phi$ is a $k$-chain of class $\mathscr{C}''$ in an open set $V \subseteq \mathbb{R}^m$,}

  \item[(b)]
    \emph{$\omega$ is a $(k-1)$-form of class $\mathscr{C}'$ in $V$,}

  \item[(c)]
    \emph{$f$ is a $0$-form of class $\mathscr{C}'$ in $V$,}
  \end{enumerate}
  \emph{then}
  \[
    \int_{\Phi} fd\omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega
  \]

\item[(2)]
  Theorem 10.20(a) implies that
  \[
    d(f\omega) = (df) \wedge \omega + f d\omega.
  \]

\item[(3)]
  The Stokes' theorem (Theorem 10.33) shows that
  \[
    \int_{\Phi} d(f\omega) = \int_{\partial\Phi} f\omega.
  \]
  Hence
  \[
    \int_{\Phi} fd\omega
    = \int_{\Phi} d(f\omega) - \int_{\Phi}(df) \wedge \omega
    = \int_{\partial\Phi} f\omega - \int_{\Phi}(df) \wedge \omega.
  \]

\item[(4)]
  Define $\Phi: Q^1 = [0,1] \to [a,b]$ by
  \[
    \Phi(\alpha) = a + \alpha (b - a).
  \]
  $\Phi$ is a $1$-simplex of class $\mathscr{C}''$ in an open set $V \supseteq [a,b]$.
  Also,
  \[
    \partial\Phi = [b] - [a].
  \]
  Let $\omega = g$ be a $0$-form of class $\mathscr{C}'(V)$.

\item[(5)]
  Note that
  \begin{align*}
    \int_{\Phi} fd\omega
    &= \int_{\Phi} fdg
    = \int_{0}^{1} f(\Phi(t)) g'(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f(u) g'(u) du, \\
    \int_{\partial\Phi} f\omega
    &= \int_{[b]} fg + \int_{-[a]} fg
    = f(b)g(b) + (-1) f(a)f(a), \\
    \int_{\Phi}(df) \wedge \omega
    &= \int_{\Phi}(df) g
    = \int_{0}^{1} f'(\Phi(t)) g(\Phi(t)) \Phi'(t) dt
    = \int_{a}^{b} f'(u) g(u) du.
  \end{align*}
  Hence
  \[
    \int_{a}^{b} f(u) g'(u) du = f(b)g(b) - f(a)f(a) - \int_{a}^{b} f'(u) g(u) du,
  \]
  which is the same as the integration by parts (Theorem 6.22).
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.21.}
\emph{As in Example 10.36, consider the $1$-form
\[
  \eta = \frac{x dy - y dx}{x^2+y^2}
\]
in $\mathbb{R}^2-\{\mathbf{0}\}$.}
\begin{enumerate}
\item[(a)]
  \emph{Carry out the computation that leads to
  \[
    \int_{\gamma} \eta = 2\pi \neq 0,
  \]
  and prove that $d\eta = 0$.}

\item[(b)]

\item[(c)]
  \emph{Take $\Gamma(t) = (a\cos t, b\sin t)$ where $a > 0$, $b > 0$ are fixed.
  Use part (b) to show that}
  \[
    \int_{0}^{2\pi} \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}dt = 2\pi.
  \]

\item[(d)]
  \emph{Show that
  \[
    \eta = d\left( \arctan\frac{y}{x} \right)
  \]
  in any convex open set in which $x \neq 0$, and that
  \[
    \eta = d\left( -\arctan\frac{x}{y} \right)
  \]
  in any convex open set in which $y \neq 0$.
  Explain why this justifies the notation $\eta = d\theta$,
  in spite of the fact that $\eta$ is not exact in $\mathbb{R}^2 - \{0\}$.}

\item[(e)]
  \emph{Show that (b) can be derived from (d).}

\item[(f)]
  \emph{If $\Gamma$ is any closed $\mathscr{C}'$-curve in $\mathbb{R}^2 - \{ \mathbf{0} \}$,
  prove that
  \[
    \frac{1}{2\pi} \int_{\Gamma} \eta = \mathrm{Ind}(\Gamma).
  \]
  (See Exercise 8.23 for the definition of the index of a curve.)} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  \begin{align*}
    \int_{\gamma} \eta
    &= \int_{0}^{2\pi}
      \frac{(r\cos t) d(r\sin t) - (r\sin t) d(r\cos t)}{(r\cos t)^2 + (r\sin t)^2} \\
    &= \int_{0}^{2\pi}
      \frac{(r \cos t)(r \cos t) - (r\sin t) (-r\sin t)}{(r\cos t)^2 + (r\sin t)^2} dt \\
    &= \int_{0}^{2\pi} dt \\
    &= 2\pi.
  \end{align*}

\item[(2)]
  \begin{align*}
    d \eta
    =& d \left( \frac{x dy - y dx}{x^2+y^2} \right) \\
    =& d \left( \frac{x}{x^2+y^2} \right) \wedge dy
      + \frac{x}{x^2+y^2} \wedge {d^2y} \\
      &- d \left( \frac{y}{x^2+y^2} \right) \wedge dx
      - \frac{y}{x^2+y^2} \wedge {d^2x} \\
    =& d \left( \frac{x}{x^2+y^2} \right) \wedge dy
      - d \left( \frac{y}{x^2+y^2} \right) \wedge dx
      &(d^2 = 0) \\
    =& \left\{ D_1\left(\frac{x}{x^2+y^2}\right) dx
      + D_2\left(\frac{y}{x^2+y^2}\right) dy \right\} \wedge dy \\
      &- \left\{ D_1\left(\frac{x}{x^2+y^2}\right) dx
      + D_2\left(\frac{y}{x^2+y^2}\right) dy \right\} \wedge dx \\
    =& D_1\left(\frac{x}{x^2+y^2}\right) dx \wedge dy
      &(dy \wedge dy = 0) \\
      &- D_2\left(\frac{y}{x^2+y^2}\right) dy \wedge dx
      &(dx \wedge dx = 0) \\
    =& 0
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Note.}
\begin{enumerate}
\item[(1)]
  $\eta$ is closed and locally exact, that is,
  $\eta = dt$ on $\mathbb{R}^2 - L$
  where $L$ is a half-line issuing from $\mathbf{0}$.
  $\eta$ is not exact since $\int_{\gamma} \eta = 2\pi \neq 0$.

\item[(2)]
  \emph{(Poincar\'e's Lemma for $1$-form.)
  Let $\omega = \sum a_i dx_i$ be defined in an open set $U \subseteq \mathbb{R}^n$.
  Then $d\omega = 0$ if and only if for each $p \in U$ there is a neighborhood $V \subseteq U$
  of $p$ and a differentiable function $f: V \to \mathbb{R}^1$ with
  $df = \omega$ (i.e., $\omega$ is locally exact).} \\
\end{enumerate}



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
  $\Gamma$ satisfies all conditions described in (b).
  So
  \[
    \int_{\Gamma} \eta = 2\pi.
  \]

\item[(2)]
  A direct calculation shows that
  \begin{align*}
    2\pi = \int_{\Gamma} \eta
    &= \int_{\Gamma} \frac{x dy - y dx}{x^2+y^2} \\
    &= \int_{0}^{2\pi}
      \frac{a \cos(t) d(b \sin(t)) - b \sin(t) d(a \cos(t))}{(a \cos(t))^2+(b \sin(t))^2} \\
    &= \int_{0}^{2\pi}
      \frac{ab (\cos^2 t + \sin^2 t)}{a^2\cos^2 t + b^2 \sin^2 t} \\
    &= \int_{0}^{2\pi}
      \frac{ab}{a^2\cos^2 t + b^2 \sin^2 t}.
  \end{align*}
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  In any convex open set in which $x \neq 0$,
  we have
  \begin{align*}
    d\left( \arctan\frac{y}{x} \right)
    &= \left( D_1\arctan\frac{y}{x} \right) dx
      + \left( D_2\arctan\frac{y}{x} \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(2)]
  In any convex open set in which $y \neq 0$,
  we have
  \begin{align*}
    d\left( -\arctan\frac{x}{y} \right)
    &= \left( D_1\left(-\arctan\frac{x}{y}\right) \right) dx
      + \left( D_2\left(-\arctan\frac{x}{y}\right) \right) dy \\
    &= -\frac{y}{x^2+y^2} dx + \frac{x}{x^2+y^2} dy \\
    &= \eta.
  \end{align*}

\item[(3)]
  By (1)(2), $\eta$ is locally exact.
  Note that $\theta_1 = \arctan\frac{y}{x}$
  and $\theta_2 = -\arctan\frac{x}{y}$
  cannot be patched together to defined a global $0$-form $\theta$
  on $\mathbb{R}^2 - \{\mathbf{0}\}$.

\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.22.}
\emph{As in Example 10.37, define $\zeta$ in $\mathbb{R}^3-\{\mathbf{0}\}$ by
\[
  \zeta = \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3}
\]
where $r = (x^2+y^2+z^2)^{\frac{1}{2}}$,
let $D$ be the rectangle given by $0 \leq u \leq \pi$, $0 \leq v \leq 2\pi$,
and let $\Sigma$ be the $2$-surface in $\mathbb{R}^3$,
with parameter domain $D$, given by
\[
  x = \sin u \cos v,
  \qquad
  y = \sin u \sin v,
  \qquad
  z = \cos u.
\]}

\begin{enumerate}
\item[(a)]
  \emph{Prove that $d\zeta = 0$ in $\mathbb{R}^3 - \{ \mathbf{0} \}$.}

\item[(b)]

\item[(c)]

\item[(d)]

\item[(e)]

\item[(f)]

\item[(g)]
  \emph{Is $\zeta$ exact in the complement of every line through the origin?} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Note that $\zeta$ is well-defined on $\mathbb{R}^3 - \{ \mathbf{0} \}$.
  Hence,
  \begin{align*}
    d\zeta
    =& \: d\left( \frac{x dy \wedge dz + y dz \wedge dx + z dx \wedge dy}{r^3} \right) \\
    =& \: d\left(\frac{x}{r^3}\right) \wedge dy \wedge dz
      + d\left(\frac{y}{r^3}\right) \wedge dz \wedge dx
      + d\left(\frac{z}{r^3}\right) \wedge dx \wedge dy \\
    =& \: D_1\left(\frac{x}{r^3}\right) dx \wedge dy \wedge dz
      + D_2\left(\frac{y}{r^3}\right) dy \wedge dz \wedge dx
      + D_3\left(\frac{z}{r^3}\right) dz \wedge dx \wedge dy \\
    =& \: \frac{r^3 - 3rx^2}{r^6} dx \wedge dy \wedge dz
      + \frac{r^3 - 3ry^2}{r^6} dy \wedge dz \wedge dx
      + \frac{r^3 - 3rz^2}{r^6} dz \wedge dx \wedge dy \\
    =& \: \left(\frac{r^3 - 3rx^2}{r^6}
      + \frac{r^3 - 3ry^2}{r^6}
      + \frac{r^3 - 3rz^2}{r^6}\right) dx \wedge dy \wedge dz \\
    =& \: 0 dx \wedge dy \wedge dz \\
    =& \: 0
  \end{align*}
  in $\mathbb{R}^3 - \{ \mathbf{0} \}$.

\item[(2)]
  Or write
  \[
    \mathbf{F}
    = \frac{x}{r^3} \mathbf{e}_1 + \frac{y}{r^3} \mathbf{e}_2 + \frac{z}{r^3}  \mathbf{e}_3
  \]
  as in Vector fields 10.42.
  So
  \[
    \omega_{\mathbf{F}} = \zeta
  \]
  and
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F}) dx \wedge dy \wedge dz
  \]
  as in the proof of the divergence theorem (Theorem 10.51).
  Note that the divergence of $\mathbf{F}$ is zero.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (e).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (f).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (g).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.23.}
\emph{...} \\



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.24.}
\emph{Let $\omega = \sum a_i(\mathbf{x}) dx_i$ be a $1$-form of class $\mathscr{C}''$
in a convex open set $E \subseteq \mathbb{R}^n$.
Assume $d\omega = 0$ and prove that $\omega$ is exact in $E$,
by completing the following outline:} \\

\emph{Fix $\mathbf{p} \in E$.
Define
\[
  f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
  \qquad
  (\mathbf{x} \in E).
\]
Apply Stokes' theorem to affine-oriented $2$-simplexs $[\mathbf{p},\mathbf{x},\mathbf{y}]$ in $E$.
Deduce that
\[
  f(\mathbf{y}) - f(\mathbf{x})
  = \sum_{i=1}^{n}(y_i - x_i) \int_{0}^{1} a_i((1-t)\mathbf{x} + t\mathbf{y}) dt
\]
for $\mathbf{x} \in E$, $\mathbf{y} \in E$.
Hence $(D_i f)(\mathbf{x}) = a_i(\mathbf{x})$.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]

\item[(2)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  The affine-oriented $2$-simplexs $\Psi = [\mathbf{p},\mathbf{x},\mathbf{y}]$ is in $E$
  by the convexity of $E$.
  (If $E$ is open but not convex,
  we can show that $\omega = df$ \textbf{\emph{locally}} as the note in Exercise 10.21(a).
  That is why we say that $\omega$ is locally exact.
  The proof is exactly the same.)

\item[(3)]
  Note that
  \[
    \partial \Psi
    = \partial [\mathbf{p},\mathbf{x},\mathbf{y}]
    = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}].
  \]
  The Stokes' theorem (Theorem 10.33) implies that
  \begin{align*}
    \int_{\Psi} d\omega
    = \int_{\partial \Psi} \omega
    &\Longleftrightarrow
    \int_{\Psi} 0
    = \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &\Longleftrightarrow
    0 = \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}) \\
    &\Longleftrightarrow
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega.
  \end{align*}

\item[(4)]
  Define $\gamma: [0,1] \to E$ by
  \begin{align*}
    \gamma(t)
    &= \mathbf{x} + t(\mathbf{y}-\mathbf{x}) \\
    &= \sum_{i=1}^{n} x_i + t(y_i - x_i)
  \end{align*}
  (where $\mathbf{x} = (x_1, \ldots, x_n)$ and $\mathbf{y} = (y_1, \ldots, y_n)$).
  Hence $[0,1]$ is the parameter domain of $[\mathbf{x},\mathbf{y}]$ with respect to $\gamma$.
  So
  \begin{align*}
    \int_{[\mathbf{x},\mathbf{y}]} \omega
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\gamma(t))
      \frac{\partial (x_i + t(y_i - x_i))}{\partial t} dt \\
    &= \int_{0}^{1} \sum_{i=1}^{n} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x}))(y_i - x_i) dt \\
    &= \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \end{align*}
  Thus,
  \[
    f(\mathbf{y}) - f(\mathbf{x})
    = \sum_{i=1}^{n} (y_i - x_i) \int_{0}^{1} a_i(\mathbf{x} + t(\mathbf{y}-\mathbf{x})) dt.
  \]

\item[(5)]
  Note that
  \begin{align*}
    f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})
    &= \sum_{i=1}^{n} ((x_i + h\delta_{ij}) - x_i)
      \int_{0}^{1} a_i(\mathbf{x} + t((\mathbf{x} + h \mathbf{e}_j)-\mathbf{x})) dt \\
    &= \sum_{i=1}^{n} h\delta_{ij}
      \int_{0}^{1} a_i(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= h \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt.
  \end{align*}
  (Here $\delta_{ij}$ is the Kronecker delta.)
  So
  \begin{align*}
    (D_j f)(\mathbf{x})
    &= \lim_{h \to 0}
      \frac{f(\mathbf{x} + h \mathbf{e}_j) - f(\mathbf{x})}{h} \\
    &= \lim_{h \to 0}
      \int_{0}^{1} a_j(\mathbf{x} + th \mathbf{e}_j) dt \\
    &= \int_{0}^{1} a_j(\mathbf{x}) dt
      &(a_j \in \mathscr{C}'') \\
    &= a_j(\mathbf{x}).
  \end{align*}
  Thus,
  \[
    df
    = \sum_{j=1}^{n} (D_j f)(\mathbf{x}) dx_j
    = \sum_{j=1}^{n} a_j(\mathbf{x}) dx_j
    = \omega,
  \]
  or $\omega$ is exact in $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.25.}
\emph{Assume $\omega$ is a $1$-form in an open set $E \subseteq \mathbb{R}^n$
such that
\[
  \int_{\gamma} \omega = 0
\]
for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.
Prove that $\omega$ is exact in $E$,
by imitating part of the argument sketched in Exercise 10.24.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  \emph{Assume that $E$ is a \textbf{connected} open subset of $\mathbb{R}^n$.
  Show that $\omega$ is exact in $E$
  if $\int_{\gamma} \omega = 0$
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.}

\item[(2)]
  Fix $\mathbf{p} \in E$.
  Define
  \[
    f(\mathbf{x}) = \int_{[\mathbf{p},\mathbf{x}]} \omega
    \qquad
    (\mathbf{x} \in E).
  \]
  It is well-defined since $E$ is connected and
  $\int_{\gamma} \omega = 0$ for every closed curve $\gamma$ in $E$.

\item[(3)]
  Given any $\mathbf{x} \in E$, $\mathbf{y} \in E$, and $\mathbf{x} \neq \mathbf{y}$.
  Let
  \[
    \gamma = [\mathbf{x},\mathbf{y}] - [\mathbf{p},\mathbf{y}] + [\mathbf{p},\mathbf{x}]
  \]
  be a closed curve in $E$.
  Hence,
  \begin{align*}
    0
    &= \int_{\gamma} \omega
      &(\text{Assumption}) \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega
      - \int_{[\mathbf{p},\mathbf{y}]} \omega
      + \int_{[\mathbf{p},\mathbf{x}]} \omega \\
    &= \int_{[\mathbf{x},\mathbf{y}]} \omega - f(\mathbf{y}) + f(\mathbf{x}).
  \end{align*}
  So
  \begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = \int_{[\mathbf{x},\mathbf{y}]} \omega
  \end{align*}

\item[(4)]
  Similar to (4)(5) in the proof of Exercise 10.24, we have $df = \omega$.
  So the statement in (1) is proved.
  In general, we can define each $f_{\alpha}$
  on each connected component $E_{\alpha}$ (which is open) of $E$
  such that $d f_{\alpha} = \omega$ on $E_{\alpha}$.
  Take
  \[
    f|_{E_{\alpha}} = f_{\alpha}
  \]
  on $E$.
  Hence, $df = \omega$ on the whole $E$.
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.26.}
\emph{Assume $\omega$ is a $1$-form in $\mathbb{R}^3 - \{\mathbf{0}\}$,
of class $\mathscr{C}'$ and $d\omega = 0$.
Prove that $\omega$ is exact in $\mathbb{R}^3 - \{\mathbf{0}\}$.
(Hint: Every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
Apply Stokes' theorem and Exercise 10.25.)} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Let $E = \mathbb{R}^3 - \{\mathbf{0}\}$.
  By Exercise 10.25,
  it suffices to show that
  \[
    \int_{\gamma} \omega = 0
  \]
  for every closed curve $\gamma$ in $E$, of class $\mathscr{C}'$.

\item[(2)]
  Intuitively, every closed continuously differentiable curve in $\mathbb{R}^3 - \{\mathbf{0}\}$
  is the boundary of a $2$-surface in $\mathbb{R}^3 - \{\mathbf{0}\}$.
  So there is some $2$-surface $\Psi$ such that $\partial \Psi = \gamma$.
  The Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\gamma} \omega
    = \int_{\partial \Psi} \omega
    = \int_{\Psi} d\omega
    = \int_{\Psi} 0
    = 0.
  \]
\end{enumerate}
$\Box$ \\\\



% Seifert surface



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.27.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.28.}
\emph{Fix $b > a > 0$, define
\[
  \Phi(r,\theta) = (r\cos\theta, r\sin\theta)
\]
for $a \leq r \leq b$, $0 \leq \theta \leq 2\pi$.
(The range of $\Phi$ is an annulus in $\mathbb{R}^2$.)
Put $\omega = x^3 dy$,
and compute both
\[
  \int_{\Phi} d\omega
  \qquad
  \text{and}
  \qquad
  \int_{\partial\Phi} \omega
\]
to verify that they are equal.} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  Note that
  \[
    \frac{\partial(x,y)}{\partial(r,\theta)}
    =
    \det
    \begin{bmatrix}
      \cos\theta & -r\sin\theta \\
      \sin\theta & r\cos\theta
    \end{bmatrix}
    = r.
  \]
  So
  \begin{align*}
    \int_{\Phi} d\omega
    &= \int_{\Phi} 3x^2 dx \wedge dy
      &(dy \wedge dy = 0) \\
    &= \int_{[a,b]\times[0,2\pi]} 3(r\cos\theta)^2
      \frac{\partial(x,y)}{\partial(r,\theta)} dr d\theta \\
    &= \int_{a}^{b} \int_{0}^{2\pi} 3 r^3 (\cos\theta)^2 dr d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(2)]
  Similar to Exercise 10.21(b),
  write
  \[
    \partial \Phi = \Gamma - \gamma,
  \]
  where $\Gamma(t) = (b\cos t,b\sin t)$ on $[0,2\pi]$
  and $\gamma(t) = (a\cos t,a\sin t)$ on $[0,2\pi]$.
  Hence
  \begin{align*}
    \int_{\partial\Phi} \omega
    &= \int_{\Gamma} \omega - \int_{\gamma} \omega \\
    &= \int_{\Gamma} x^3 dy - \int_{\gamma} x^3 dy \\
    &= \int_{[0,2\pi]} (b\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta
      - \int_{[0,2\pi]} (a\cos\theta)^3 \frac{\partial y}{\partial \theta}d\theta \\
    &= \int_{0}^{2\pi} b^4 (\cos\theta)^4 d\theta
      - \int_{0}^{2\pi} a^4 (\cos\theta)^4 d\theta \\
    &= \frac{3\pi}{4}(b^4-a^4).
  \end{align*}

\item[(3)]
  \[
    \int_{\Phi} d\omega
    = \int_{\partial\Phi} \omega
    = \frac{3\pi}{4}(b^4-a^4).
  \]
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.29.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.30.}
\emph{If $\mathbf{N}$ is the vector given by
\[
  \mathbf{N}
  = (\alpha_2 \beta_3 - \alpha_3 \beta_2) \mathbf{e}_1
    + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \mathbf{e}_2
    + (\alpha_1 \beta_2 - \alpha_2 \beta_1) \mathbf{e}_3
\]
(Equation (135)), prove that
\[
  \det\begin{bmatrix}
    \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
    \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
    \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
  \end{bmatrix}
  = \abs{\mathbf{N}}^2
\]
Also, verify
\[
  \mathbf{N} \cdot (T\mathbf{e}_1) = \mathbf{N} \cdot (T\mathbf{e}_2)
\]
(Equation (137)).} \\



\emph{Proof.}
\begin{enumerate}
\item[(1)]
  By Laplace's expansion along the third column,
    \begin{align*}
    &\det\begin{bmatrix}
      \alpha_1 & \beta_1 & \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
      \alpha_2 & \beta_2 & \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
      \alpha_3 & \beta_3 & \alpha_1 \beta_2 - \alpha_2 \beta_1
    \end{bmatrix} \\
    =& (-1)^{1+3} (\alpha_2 \beta_3 - \alpha_3 \beta_2)
      \det\begin{bmatrix}
        \alpha_2 & \beta_2 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{2+3} (\alpha_3 \beta_1 - \alpha_1 \beta_3)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_3 & \beta_3
      \end{bmatrix} \\
      &+ (-1)^{3+3} (\alpha_1 \beta_2 - \alpha_2 \beta_1)
      \det\begin{bmatrix}
        \alpha_1 & \beta_1 \\
        \alpha_2 & \beta_2
      \end{bmatrix} \\
    =& (\alpha_2 \beta_3 - \alpha_3 \beta_2)^2
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3)^2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)^2 \\
    =& \abs{\mathbf{N}}^2.
  \end{align*}

\item[(2)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_1)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\alpha_1, \alpha_2, \alpha_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \alpha_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \alpha_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \alpha_3 \\
    &= (\alpha_3 \alpha_2 - \alpha_2 \alpha_3) \beta_1
      + (\alpha_1 \alpha_3 - \alpha_3 \alpha_1) \beta_2
      + (\alpha_2 \alpha_1 - \alpha_1 \alpha_2)\beta_3 \\
    &= 0.
  \end{align*}

\item[(3)]
  \begin{align*}
    \mathbf{N} \cdot (T\mathbf{e}_2)
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2,
      \alpha_3 \beta_1 - \alpha_1 \beta_3,
      \alpha_1 \beta_2 - \alpha_2 \beta_1) \cdot
      (\beta_1, \beta_2, \beta_3) \\
    &= (\alpha_2 \beta_3 - \alpha_3 \beta_2) \beta_1
      + (\alpha_3 \beta_1 - \alpha_1 \beta_3) \beta_2
      + (\alpha_1 \beta_2 - \alpha_2 \beta_1)) \beta_3 \\
    &= (\beta_2 \beta_3 - \beta_3 \beta_2) \alpha_1
      + (\beta_3 \beta_1 - \beta_1 \beta_3) \alpha_2
      + (\beta_1 \beta_2 - \beta_2 \beta_1) \alpha_3 \\
    &= 0.
  \end{align*}
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.31.}
\emph{Let $E \subseteq \mathbb{R}^3$ be open,
suppose $g \in \mathscr{C}''(E)$, $h \in \mathscr{C}''(E)$,
and consider the vector field}
\[
    \mathbf{F} = g \nabla h
\]
\begin{enumerate}
\item[(a)]
  \emph{Prove that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  where $\nabla^2 h = \nabla \cdot (\nabla h) = \sum\frac{\partial^2 h}{\partial x_i^2}$
  is the so-called ``Laplacian'' of $h$.}

\item[(b)]
  \emph{If $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial \Omega$
  (as in Theorem 10.51), prove that
  \[
    \int_{\Omega} [ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  where (as is customary) we have written $\frac{\partial h}{\partial n}$
  in place of $(\nabla h) \cdot \mathbf{n}$.
  (Thus $\frac{\partial h}{\partial n}$ is the directional derivative of $h$
  in the direction of the outward normal to $\partial \Omega$,
  the so-called \textbf{normal derivative} of $h$.)
  Interchange $g$ and $h$,
  substract the resulting formula from the first one, to obtain
  \[
    \int_{\Omega} ( g\nabla^2 h - h \nabla^2 g) dV
    = \int_{\partial \Omega}
      \left( g \frac{\partial h}{\partial n} - h \frac{\partial g}{\partial n} \right) dA.
  \]
  These two formulas are usually called \textbf{Green's identities}.}

\item[(c)]
  \emph{Assume that $h$ is \textbf{harmonic} in $E$;
  this means that $\nabla^2 h = 0$.
  Take $g = 1$ and conclude that
  \[
    \int_{\partial \Omega} \frac{\partial h}{\partial n} dA = 0.
  \]
  Take $g = h$, and conclude that $h = 0$ in $\Omega$ if $h = 0$ on $\partial \Omega$.}

\item[(d)]
  \emph{Show that Green's identities are also valid in $\mathbb{R}^2$.} \\
\end{enumerate}



\emph{Proof of (a).}
\begin{enumerate}
\item[(1)]
  Since
  \[
    \mathbf{F}
    = g \nabla h
    = g \left(\sum (D_i h)\mathbf{e}_i \right)
    = \sum g(D_i h)\mathbf{e}_i,
  \]
  we have
  \begin{align*}
    \nabla \cdot \mathbf{F}
    &= \nabla \cdot \left( \sum g(D_i h)\mathbf{e}_i \right) \\
    &= \sum D_i(g(D_i h)) \\
    &= \sum \{ (D_i g)(D_i h) + g D_i(D_i h) \} \\
    &= \sum (D_i g)(D_i h) + g \sum D_i(D_i h).
  \end{align*}

\item[(2)]
  Also,
  \begin{align*}
    g \nabla^2 h + (\nabla g) \cdot (\nabla h)
    &= g \nabla \cdot (\nabla h) + (\nabla g) \cdot (\nabla h) \\
    &= g \nabla \cdot \left(\sum (D_i h)\mathbf{e}_i \right)
      + \left(\sum (D_i g)\mathbf{e}_i \right) \cdot \left(\sum (D_i h)\mathbf{e}_i \right) \\
    &= g \sum D_i(D_i h) + \sum (D_i g)(D_i h).
  \end{align*}

\item[(3)]
  By (1)(2), the result is established.
\end{enumerate}
$\Box$ \\



\emph{Proof of (b).}
\begin{enumerate}
\item[(1)]
  The divergence theorem (Theorem 10.51) implies that
  \begin{align*}
    &\int_{\Omega} (\nabla \cdot \mathbf{F}) dV
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) dA \\
    \Longrightarrow&
    \int_{\Omega} [g \nabla^2 h + (\nabla g) \cdot (\nabla h)] dV
    = \int_{\partial\Omega} g \underbrace{\nabla h \cdot \mathbf{n}}_{=\frac{\partial h}{\partial n}} dA.
  \end{align*}

\item[(2)]
  Green's identities are a set of three identities in vector calculus
  relating the bulk with the boundary of a region on which differential operators act.
  \emph{(Green's third identity.)
  Assume that $h$ is harmonic in $E$.
  If $G(\mathbf{x},\mathbf{x}_0)$ is the Green's function,
  then}
  \[
    h(\mathbf{x}_0)
    = \int_{\partial \Omega}
      \left[ h(\mathbf{x}) \frac{\partial G(\mathbf{x},\mathbf{x}_0)}{\partial n}
      - G(\mathbf{x},\mathbf{x}_0) \frac{\partial h(\mathbf{x})}{\partial n} \right] dA.
  \]
  For example, in $\mathbb{R}^3$
  \[
    G(\mathbf{x},\mathbf{x}_0) = -\frac{1}{4\pi\norm{\mathbf{x} - \mathbf{x}_0}}.
  \]
\end{enumerate}
$\Box$ \\



\emph{Proof of (c).}
Assume $\nabla^2 h = 0$.
\begin{enumerate}
\item[(1)]
  Take $g = 1$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get the conclusion.
  (Here $\nabla g = \mathbf{0}$ as $g = 1$.)

\item[(2)]
  Assume $h = 0$ on $\partial\Omega$.
  Take $g = h$ in
  \[
    \int_{\Omega}[ g\nabla^2 h + (\nabla g)\cdot(\nabla h) ] dV
    = \int_{\partial \Omega} g \frac{\partial h}{\partial n} dA
  \]
  to get
  \[
    \int_{\Omega} |\nabla h|^2 dV
    = \int_{\partial \Omega} h \frac{\partial h}{\partial n} dA
    = 0
  \]
  (since $h = 0$ on $\partial\Omega$).
  Since $h \in \mathscr{C}'(\Omega)$, Exercise 6.2 implies that
  $|\nabla h|^2 = 0$ on $\Omega$.
  So $D_1 h = D_2 h = D_3 h = 0$ on $\Omega$.
  Since $h \in \mathscr{C}'(\Omega)$, Theorem 9.21 implies that
  $h = 0$ on $\Omega$, or
  $h$ is locally constant in $\Omega$ (Exercise 9.9).
  Note that $h = 0$ globally on $\partial \Omega$,
  and thus $h = 0$ globally on $\Omega$.
\end{enumerate}
$\Box$ \\



\emph{Proof of (d).}
\begin{enumerate}
\item[(1)]
  \emph{(The divergence theorem in $\mathbb{R}^2$.)
  If $\mathbf{F} = F_1 \mathbf{e}_1 + F_2 \mathbf{e}_2$ is a vector field of class
  $\mathscr{C}'$ in an open set $E \subseteq \mathbb{R}^2$,
  and if $\Omega$ is a closed subset of $E$ with positively oriented boundary $\partial\Omega$
  then}
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]
  Define a $1$-form by
  \[
    \omega_{\mathbf{F}} = F_1 dy - F_2 dx.
  \]
  So
  \[
    d\omega_{\mathbf{F}}
    = (\nabla \cdot \mathbf{F})dx \wedge dy
    = (\nabla \cdot \mathbf{F})dA.
  \]
  Hence the Stokes' theorem (Theorem 10.33) implies that
  \[
    \int_{\Omega} (\nabla \cdot \mathbf{F}) dA
    = \int_{\Omega} d\omega_{\mathbf{F}}
    = \int_{\partial\Omega} \omega_{\mathbf{F}}
    = \int_{\partial\Omega} (\mathbf{F} \cdot \mathbf{n}) ds.
  \]

\item[(2)]
  Note that
  \[
    \nabla \cdot \mathbf{F} = g \nabla^2 h + (\nabla g) \cdot (\nabla h)
  \]
  is also true in $\mathbb{R}^2$.
  Similar to (b), two Green's identities are also true in $\mathbb{R}^2$.
  (In $\mathbb{R}^1$, the Green's first identity is
  the integration by parts (Theorem 6.22).)
\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Exercise 10.32.}
\emph{...} \\

\emph{Proof.}
\begin{enumerate}
\item[(1)]
\item[(2)]

\end{enumerate}
$\Box$ \\\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}